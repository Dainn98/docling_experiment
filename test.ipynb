{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457ba390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35dbb46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-12-15 07:36:53,977 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 07:36:53,996 - INFO - Going to convert document batch...\n",
      "2025-12-15 07:36:53,997 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 07:36:54,008 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-15 07:36:54,010 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-12-15 07:36:54,026 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-15 07:36:54,029 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-12-15 07:36:54,030 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 07:36:54,030 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 07:36:54,215 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 07:36:54,235 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:36:54,242 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:36:54,255 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:36:54,257 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:36:54,723 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:36:54,724 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:36:54,726 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:36:54,727 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:36:54,808 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:36:54,809 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:36:54,831 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:36:54,832 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 07:36:55,067 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 07:36:55,078 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-15 07:36:55,080 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2025-12-15 07:36:55,085 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:36:55,691 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-15 07:36:55,692 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2025-12-15 07:36:55,946 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:36:56,319 - INFO - Processing document 2408.09869v5.pdf\n",
      "2025-12-15 07:37:46,103 - INFO - Finished converting document 2408.09869v5.pdf in 52.23 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!-- image -->\n",
      "\n",
      "## Docling Technical Report\n",
      "\n",
      "## Version 1.0\n",
      "\n",
      "Christoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\n",
      "\n",
      "AI4K Group, IBM Research R¨ uschlikon, Switzerland\n",
      "\n",
      "## Abstract\n",
      "\n",
      "This technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Converting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\n",
      "\n",
      "With Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\n",
      "\n",
      "Here is what Docling delivers today:\n",
      "\n",
      "- Converts PDF documents to JSON or Markdown format, stable and lightning fast\n",
      "- Understands detailed page layout, reading order, locates figures and recovers table structures\n",
      "- Extracts metadata from the document, such as title, authors, references and language\n",
      "- Optionally applies OCR, e.g. for scanned PDFs\n",
      "- Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\n",
      "- Can leverage different accelerators (GPU, MPS, etc).\n",
      "\n",
      "## 2 Getting Started\n",
      "\n",
      "To use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.\n",
      "\n",
      "Docling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.\n",
      "\n",
      "from docling.document\\_converter import DocumentConverter\n",
      "\n",
      "```\n",
      "source = \"https://arxiv.org/pdf/2206.01062\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \"## DocLayNet: A Large Human -Annotated Dataset for Document -Layout Analysis [...]\"\n",
      "```\n",
      "\n",
      "Optionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.\n",
      "\n",
      "## 3 Processing pipeline\n",
      "\n",
      "Docling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.\n",
      "\n",
      "## 3.1 PDF backends\n",
      "\n",
      "Two basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling's PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive\n",
      "\n",
      "1 see huggingface.co/ds4sd/docling-models/\n",
      "\n",
      "Figure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "licensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14].\n",
      "\n",
      "We therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings.\n",
      "\n",
      "## 3.2 AI models\n",
      "\n",
      "As part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.\n",
      "\n",
      "## Layout Analysis Model\n",
      "\n",
      "Our layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\n",
      "\n",
      "The Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.\n",
      "\n",
      "## Table Structure Recognition\n",
      "\n",
      "The TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\n",
      "\n",
      "The Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.\n",
      "\n",
      "## OCR\n",
      "\n",
      "Docling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page).\n",
      "\n",
      "We are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements.\n",
      "\n",
      "## 3.3 Assembly\n",
      "\n",
      "In the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request.\n",
      "\n",
      "## 3.4 Extensibility\n",
      "\n",
      "Docling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.\n",
      "\n",
      "Implementations of model classes must satisfy the python Callable interface. The \\_\\_call\\_\\_ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.\n",
      "\n",
      "## 4 Performance\n",
      "\n",
      "In this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable ) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1.\n",
      "\n",
      "If you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery.\n",
      "\n",
      "Establishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and\n",
      "\n",
      "torch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report.\n",
      "\n",
      "Table 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.\n",
      "\n",
      "| CPU                     | Thread budget   | native backend   | native backend   | native backend   | pypdfium backend   | pypdfium backend   | pypdfium backend   |\n",
      "|-------------------------|-----------------|------------------|------------------|------------------|--------------------|--------------------|--------------------|\n",
      "|                         |                 | TTS              | Pages/s          | Mem              | TTS                | Pages/s            | Mem                |\n",
      "| Apple M3 Max (16 cores) | 4 16            | 177 s 167 s      | 1.27 1.34        | 6.20 GB          | 103 s 92 s         | 2.18 2.45          | 2.56 GB            |\n",
      "| Intel(R) Xeon E5-2690   | 4 16            | 375 s 244 s      | 0.60 0.92        | 6.16 GB          | 239 s 143 s        | 0.94 1.57          | 2.42 GB            |\n",
      "\n",
      "## 5 Applications\n",
      "\n",
      "Thanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.\n",
      "\n",
      "## 6 Future work and contributions\n",
      "\n",
      "Docling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\n",
      "\n",
      "We encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.\n",
      "\n",
      "## References\n",
      "\n",
      "- [1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\n",
      "- [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster\n",
      "\n",
      "machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24) . ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf .\n",
      "\n",
      "- [3] C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing (CLOUD) , pages 363-373. IEEE, 2022.\n",
      "- [4] J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf .\n",
      "- [5] O. R. developers. Onnx runtime. https://onnxruntime.ai/ , 2024. Version: 1.18.1.\n",
      "- [6] IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit .\n",
      "- [7] A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF .\n",
      "- [8] J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama\\_index .\n",
      "- [9] M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San Jos´ e, CA, USA, August 21-26, 2023, Proceedings, Part II , pages 37-50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8\\_3 .\n",
      "- [10] L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. Statements: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christiaen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024) , pages 193-214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15 .\n",
      "- [11] L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of chemical structures in patent documents. Nature Communications , 15(1):6532, August 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/ s41467-024-50779-y .\n",
      "- [12] A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4614-4623, 2022.\n",
      "- [13] B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large humanannotated dataset for document-layout segmentation. pages 3743-3751, 2022.\n",
      "- [14] pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/ py-pdf/pypdf .\n",
      "- [15] P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/ pypdfium2-team/pypdfium2 .\n",
      "- [16] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on real-time object detection, 2023.\n",
      "\n",
      "## Appendix\n",
      "\n",
      "In this section, we illustrate a few examples of Docling's output in Markdown and JSON.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1. Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation (not shown).\n",
      "\n",
      "KDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\n",
      "\n",
      "Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.\n",
      "\n",
      "|                                                                                                        | human                                                                   | MRCNN R50 R101                                                                                                          | FRCNN R101                                                  | YOLO v5x6                                                   |\n",
      "|--------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------|\n",
      "| Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All | 84-89 83-91 83-85 87-88 93-94 85-89 69-71 83-84 77-81 84-86 60-72 82-83 | 68.4 71.5 70.9 71.8 60.1 63.4 81.2 80.8 61.6 59.3 71.9 70.0 71.7 72.7 67.6 69.3 82.2 82.9 84.6 85.8 76.7 80.4 72.4 73.5 | 70.1 73.7 63.5 81.0 58.9 72.0 72.0 68.4 82.2 85.4 79.9 73.4 | 77.7 77.2 66.2 86.2 61.1 67.9 77.1 74.6 86.3 88.1 82.7 76.8 |\n",
      "\n",
      "to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\n",
      "\n",
      "## 5 EXPERIMENTS\n",
      "\n",
      "The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\n",
      "\n",
      "Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "paper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\n",
      "\n",
      "In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].\n",
      "\n",
      "## Baselines for Object Detection\n",
      "\n",
      "In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\n",
      "\n",
      "Table 2: Prediction perlormance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 beckbone were rained based on the network architecturesfrom the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utized was YOLOv5x6 [13]. All models were initialised using pre-trained weightsfrom the COCO 2017 dataset.\n",
      "\n",
      "|                | human   |   MRCNN |   MRCNNFRCNN |      |   YOLO |\n",
      "|----------------|---------|---------|--------------|------|--------|\n",
      "| Caption        | 84-89   |    68.4 |         71.5 | 70.1 |   77.7 |\n",
      "| Footnote       | 83-91   |    70.9 |         71.8 | 73.7 |   77.2 |\n",
      "| Formula        | 83-85   |    60.1 |         63.4 | 63.5 |   66.2 |\n",
      "| List-item      | 89-48   |    81.2 |         80.8 | 81   |   86.2 |\n",
      "| Page-ooer      | 93-94   |    61.6 |         59.3 | 58.9 |   61.1 |\n",
      "| Page-header    | 85-89   |    71.9 |         70   | 72   |   67.9 |\n",
      "| Picture        | 69-71   |    71.7 |         72.7 | 72   |   77.1 |\n",
      "| Section-header | 83-84   |    67.6 |         69.3 | 68.4 |   74.6 |\n",
      "| Table          | 77-81   |    82.2 |         82.9 | 82.2 |   86.3 |\n",
      "| x1             | 84-86   |    84.6 |         85.8 | 85.4 |   88.1 |\n",
      "| Title          | 60-72   |    76.7 |         80.4 | 79.9 |   82.7 |\n",
      "| AlIl           | 82-83   |    72.4 |         73.5 | 73.4 |   76.8 |\n",
      "\n",
      "to avoid this at any cost in order to have clear, unbiased baseline numbers for human dcument-layout annotation. hird, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS -pxa fjeund je o sjoo-xe posopu oug punoe xoq-upunoq wnwu o o xoq umep-jesnne syuus fgeogewone joo uogeqouue based segments, which excludes only Table and Picture For the later, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cellsis that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\n",
      "\n",
      "## 5EXPERIMENTS\n",
      "\n",
      "The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthemore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\n",
      "\n",
      "Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNN network with ResNet50 backbone trained on increasing fractions of the DocLayNet dataset. The leaming curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with simlar data ill not yield significantly better predictions.\n",
      "\n",
      "paper and leave the detaled evaluation of more reoent methods mentioned in Section 2 for future work.\n",
      "\n",
      "In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in 0 S'0 wo ofue pu sdeμano o  (dvw) uospeud oene ueu Susn suogoped e po Agenb u aenje m om oeqnd 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API[16]\n",
      "\n",
      "## BaselinesforObjectDetection\n",
      "\n",
      "In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pbxels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overallbetween 6 and 0 eug uogeopu poo6 e senj6 sL sobed popeoue-edjμ uo suogegoue ueunq esujed oug wo. pnduoo dvw oug ueg emo %01 DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixelupoau auou ou puey saugo au uo suogopeud sogeq ueqo o djou lou sep soxoq-ugpunoq wou panuap uogeuoubes oteu poseq Yolov5x model does very welland even out-performs humans on selected labels such as Text, Table and Picture  Thisis not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\n",
      "\n",
      "Figure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing first under the title. Elements recognized as page headers or footers are suppressed in Markdown to deliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph in '5. Experiments' wrapping over the column end is broken up in two and interrupted by the table.\n",
      "\n",
      "KDD '22, August 14-18, 2022, Washington, DC, USA\n",
      "\n",
      "Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\n",
      "\n",
      "Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as %\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "of row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric we distributed the annotation workload and performed continuous\n",
      "\n",
      "only. For phases three and four, a group of 40 dedicated annotators quality controls. Phase one and two required a small team of experts\n",
      "\n",
      "were assembled and supervised.\n",
      "\n",
      "while coverage ensures that all meaningful items on a page can to a document category, such as\n",
      "\n",
      "be annotated. We refrained from class labels that are very specific\n",
      "\n",
      "Abstract in the\n",
      "\n",
      "Scientific Articles semantics of the text. Labels such as\n",
      "\n",
      "category. We also avoided class labels that are tightly linked to the\n",
      "\n",
      "Author\n",
      "\n",
      "Affiliation\n",
      "\n",
      "teria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources in DocBank, are often only distinguishable by discriminating on 3 https://arxiv.org/ Figure 4: Table 1 from the DocLayNet paper in the original PDF (A), as rendered Markdown (B) and in JSON representation (C). Spanning table cells, such as the multi-column header 'triple interannotator mAP@0.5-0.95 (%)', is repeated for each column in the Markdown representation (B), which guarantees that every data point can be traced back to row and column headings only by its grid coordinates in the table. In the JSON representation, the span information is reflected in the fields of each table cell (C).\n",
      "\n",
      "and\n",
      "\n",
      ", as seen\n",
      "\n",
      "Phase 1: Data selection and preparation.\n",
      "\n",
      "Our inclusion cri-\n"
     ]
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "# import cv2\n",
    "\n",
    "source = \"https://arxiv.org/pdf/2408.09869\"  # file path or URL\n",
    "converter = DocumentConverter()\n",
    "doc = converter.convert(source).document\n",
    "print(doc.export_to_markdown())  # output: \"### Docling Technical Report[...]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5bcd776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing system_resources.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile system_resources.py\n",
    "import psutil\n",
    "import torch\n",
    "\n",
    "def check_system_resources():\n",
    "    cpu_count = psutil.cpu_count(logical=False)\n",
    "    cpu_percent = psutil.cpu_percent(interval=0.2)\n",
    "\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    gpu_mem_free = None\n",
    "\n",
    "    if gpu_available:\n",
    "        free, total = torch.cuda.mem_get_info()\n",
    "        gpu_mem_free = free // (1024 ** 2)  # MB\n",
    "\n",
    "    return {\n",
    "        \"cpu_count\": cpu_count,\n",
    "        \"cpu_percent\": cpu_percent,\n",
    "        \"gpu_available\": gpu_available,\n",
    "        \"gpu_mem_free_mb\": gpu_mem_free\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e5d14fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing worker.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile worker.py\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "def convert_to_markdown(source: str, use_gpu: bool) -> str | None:\n",
    "    try:\n",
    "        converter = DocumentConverter()\n",
    "        doc = converter.convert(source).document\n",
    "        return doc.export_to_markdown()\n",
    "    except Exception as e:\n",
    "        return f\"[ERROR] {source}: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55507223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile parallel_exec.py\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from system_resources import check_system_resources\n",
    "from worker import convert_to_markdown\n",
    "\n",
    "BASE_OUTPUT_DIR = \"output/docling\"\n",
    "\n",
    "def create_storage():\n",
    "    uid = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(BASE_OUTPUT_DIR, uid)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    return run_dir, uid\n",
    "\n",
    "def init_storage(run_dir: str, sources: list[str]):\n",
    "    output_path = os.path.join(run_dir, \"results.json\")\n",
    "    data = {\n",
    "        \"meta\": {\n",
    "            \"total\": len(sources),\n",
    "            \"start_time\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"documents\": []\n",
    "    }\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    return output_path\n",
    "\n",
    "def append_result(output_json: str, record: dict):\n",
    "    with open(output_json, \"r+\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        data[\"documents\"].append(record)\n",
    "        f.seek(0)\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        f.truncate()\n",
    "\n",
    "def convert_list(sources: list[str], batch_size: int = 4):\n",
    "\n",
    "    run_dir, uid = create_storage()\n",
    "    output_json = init_storage(run_dir, sources)\n",
    "\n",
    "    print(f\"Output folder: {run_dir}\")\n",
    "\n",
    "    resources = check_system_resources()\n",
    "    cpu_count = resources[\"cpu_count\"]\n",
    "    gpu_available = resources[\"gpu_available\"]\n",
    "\n",
    "    max_workers = min(cpu_count, batch_size)\n",
    "\n",
    "    print(f\"Starting conversion with {max_workers} workers\")\n",
    "    print(f\"GPU available: {gpu_available}\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(convert_to_markdown, src, gpu_available): src\n",
    "            for src in sources\n",
    "        }\n",
    "\n",
    "        for idx, future in enumerate(as_completed(futures), 1):\n",
    "            src = futures[future]\n",
    "            try:\n",
    "                content = future.result()\n",
    "                record = {\n",
    "                    \"source\": src,\n",
    "                    \"status\": \"success\",\n",
    "                    \"content\": content\n",
    "                }\n",
    "                results.append(result)\n",
    "                print(f\"[{idx}/{len(sources)}] Done: {src}\")\n",
    "            except Exception as e:\n",
    "                record = {\n",
    "                    \"source\": src,\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "                print(f\"Error processing {src}: {e}\")\n",
    "            \n",
    "            append_result(output_json, record)\n",
    "\n",
    "            # resource check mỗi 3 file\n",
    "            if idx % 3 == 0:\n",
    "                res = check_system_resources()\n",
    "                if res[\"cpu_percent\"] > 90:\n",
    "                    print(\"⚠ CPU HIGH USAGE detected\")\n",
    "\n",
    "                if res[\"gpu_available\"] and res[\"gpu_mem_free_mb\"] < 2048:\n",
    "                    print(\"⚠ GPU memory low → future jobs still on CPU\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6af077e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "import psutil\n",
    "import concurrent.futures\n",
    "import torch\n",
    "import time\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "def check_system_resources():\n",
    "    \"\"\"Kiểm tra tài nguyên hệ thống, đặc biệt là CPU và GPU.\"\"\"\n",
    "    # Kiểm tra CPU\n",
    "    cpu_count = psutil.cpu_count(logical=False)  # số lõi CPU thực\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)  # Tỉ lệ sử dụng CPU trong 1 giây\n",
    "    \n",
    "    # Kiểm tra GPU (nếu có)\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    gpu_count = torch.cuda.device_count() if gpu_available else 0\n",
    "    gpu_name = torch.cuda.get_device_name(0) if gpu_available else None\n",
    "    gpu_memory = torch.cuda.memory_allocated(0) if gpu_available else 0\n",
    "    \n",
    "    print(f\"CPU: {cpu_count} cores, {cpu_percent}% used\")\n",
    "    if gpu_available:\n",
    "        print(f\"GPU: {gpu_name}, {gpu_count} devices, {gpu_memory // (1024 * 1024)} MB memory used\")\n",
    "    else:\n",
    "        print(\"No GPU available\")\n",
    "    \n",
    "    return cpu_count, cpu_percent, gpu_available, gpu_memory\n",
    "\n",
    "def convert_to_markdown(source:str,gpu_available: bool):\n",
    "    \"\"\"Chuyển đổi từng tài liệu thành markdown\"\"\"\n",
    "    try:\n",
    "        converter = DocumentConverter()\n",
    "        if gpu_available:\n",
    "            print(f\"Processing {source} on GPU...\")\n",
    "        else:\n",
    "            print(f\"Processing {source} on CPU...\")\n",
    "        doc = converter.convert(source).document\n",
    "        return doc.export_to_markdown()\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {source}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def convertList(sources:list[str]):\n",
    "    \"\"\"Hàm chính để xử lý danh sách các tài liệu song song\"\"\"\n",
    "    cpu_count, cpu_percent, gpu_available, gpu_memory = check_system_resources() # Kiểm tra tài nguyên hệ thống\n",
    "    max_workers = cpu_count # Sử dụng tối đa số worker bằng số lõi CPU có sẵn (may be change into gpu_count, however i use cpu at that time)\n",
    "    \n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for i, result in enumerate(executor.map(lambda source: convert_to_markdown(source, gpu_available), sources)):\n",
    "            results.append(result)\n",
    "            \n",
    "            \n",
    "            print(f\"Completed processing document {i + 1}\") # Kiểm tra lại tài nguyên sau mỗi lần xử lý\n",
    "            cpu_count, cpu_percent, gpu_available, gpu_memory = check_system_resources()\n",
    "\n",
    "            \n",
    "            if cpu_percent > 90: # Điều chỉnh số worker nếu CPU hoặc GPU memory bị quá tải\n",
    "                print(\"CPU usage is high, reducing number of workers.\")\n",
    "                max_workers = max(1, cpu_count // 2)  # Giảm worker\n",
    "                executor._max_workers = max_workers  # Cập nhật worker\n",
    "            elif gpu_available and gpu_memory < 2 * 1024 * 1024 * 1024:  # Kiểm tra nếu bộ nhớ GPU dưới 2GB\n",
    "                print(\"GPU memory is low, switching to CPU.\")\n",
    "                gpu_available = False  # Chuyển qua sử dụng CPU\n",
    "\n",
    "            time.sleep(1) # Để tránh kiểm tra quá nhanh, thêm một khoảng dừng nhỏ\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94cfd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ví dụ sử dụng\n",
    "sources = [\n",
    "    \"https://arxiv.org/pdf/2408.09869\",  # Paper 1\n",
    "    \"https://arxiv.org/pdf/2311.04155\",  # Paper 2\n",
    "    \"https://arxiv.org/pdf/1706.03762\",  # Paper 3\n",
    "    \"https://arxiv.org/pdf/2302.09664\",  # Paper 4\n",
    "    \"https://arxiv.org/pdf/2003.12771\",  # Paper 5\n",
    "    \"https://arxiv.org/pdf/1910.02707\",  # Paper 6\n",
    "    \"https://arxiv.org/pdf/1705.04510\",  # Paper 7\n",
    "    \"https://arxiv.org/pdf/1506.01497\",  # Paper 8\n",
    "    \"https://arxiv.org/pdf/1802.05365\",  # Paper 9\n",
    "    \"https://arxiv.org/pdf/2004.09602\",  # Paper 10\n",
    "    \"https://arxiv.org/pdf/1602.02235\",  # Paper 11\n",
    "    \"https://arxiv.org/pdf/1801.06538\",  # Paper 12\n",
    "    \"https://arxiv.org/pdf/1611.09060\",  # Paper 13\n",
    "    \"https://arxiv.org/pdf/1912.05483\",  # Paper 14\n",
    "    \"https://arxiv.org/pdf/1709.08624\",  # Paper 15\n",
    "    \"https://arxiv.org/pdf/1810.04805\",  # Paper 16\n",
    "    \"https://arxiv.org/pdf/1502.03167\",  # Paper 17\n",
    "    \"https://arxiv.org/pdf/1807.02547\",  # Paper 18\n",
    "    \"https://arxiv.org/pdf/2103.00634\",   # Paper 19\n",
    "    \"https://arxiv.org/pdf/2103.00634\"\n",
    "]\n",
    "\n",
    "\n",
    "# CPU times: user 294 ms, sys: 198 ms, total: 492 ms\n",
    "# Wall time: 13min 30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226f3b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting conversion with 2 workers\n",
      "GPU available: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 07:47:41,242 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 07:47:41,260 - INFO - Going to convert document batch...\n",
      "2025-12-15 07:47:41,262 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 07:47:41,269 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 07:47:41,278 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-15 07:47:41,283 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-12-15 07:47:41,288 - INFO - Going to convert document batch...\n",
      "2025-12-15 07:47:41,290 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 07:47:41,300 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-15 07:47:41,306 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-12-15 07:47:41,307 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 07:47:41,309 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 07:47:41,309 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-15 07:47:41,313 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-12-15 07:47:41,329 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-15 07:47:41,333 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-12-15 07:47:41,335 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 07:47:41,337 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 07:47:41,717 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:47:41,726 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:41,739 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:41,743 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:41,748 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:41,752 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:41,757 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:41,758 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:41,766 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:41,768 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:42,023 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:42,024 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:42,028 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:42,029 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:42,062 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:42,063 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:42,065 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:42,066 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:42,116 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:42,117 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:42,142 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:42,144 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:42,166 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:42,168 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:42,192 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:47:42,194 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 07:47:42,406 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 07:47:42,417 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-15 07:47:42,422 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2025-12-15 07:47:42,430 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:47:42,450 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 07:47:42,461 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-15 07:47:42,465 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2025-12-15 07:47:42,473 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:47:42,997 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-15 07:47:42,999 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2025-12-15 07:47:43,053 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-15 07:47:43,055 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2025-12-15 07:47:43,261 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:47:43,304 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:47:44,509 - INFO - Processing document 2311.04155v3.pdf\n",
      "2025-12-15 07:47:44,673 - INFO - Processing document 2408.09869v5.pdf\n",
      "2025-12-15 07:49:03,247 - INFO - Finished converting document 2408.09869v5.pdf in 82.12 sec.\n",
      "2025-12-15 07:49:03,439 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 07:49:03,446 - INFO - Going to convert document batch...\n",
      "2025-12-15 07:49:03,447 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 07:49:03,449 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 07:49:03,450 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 07:49:03,452 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 07:49:03,479 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:49:03,480 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:49:03,494 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:49:03,498 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [1/20] Done: https://arxiv.org/pdf/2408.09869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 07:49:03,951 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:49:03,952 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:49:03,956 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:49:03,957 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:49:04,067 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:49:04,069 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:49:04,094 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:49:04,097 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 07:49:04,419 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 07:49:04,422 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:49:05,182 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:49:05,786 - INFO - Processing document 1706.03762v7.pdf\n",
      "2025-12-15 07:50:29,631 - INFO - Finished converting document 1706.03762v7.pdf in 86.29 sec.\n",
      "2025-12-15 07:50:29,804 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 07:50:29,811 - INFO - Going to convert document batch...\n",
      "2025-12-15 07:50:29,813 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 07:50:29,814 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 07:50:29,816 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 07:50:29,819 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:29,852 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:29,855 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:29,870 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:29,872 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [2/20] Done: https://arxiv.org/pdf/1706.03762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 07:50:30,339 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:30,342 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:30,344 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:30,346 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:30,456 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:30,461 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:30,489 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:30,495 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 07:50:31,018 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 07:50:31,021 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:50:31,785 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:50:32,271 - INFO - Processing document 2302.09664v3.pdf\n",
      "2025-12-15 07:50:45,714 - INFO - Finished converting document 2311.04155v3.pdf in 184.58 sec.\n",
      "2025-12-15 07:50:46,068 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 07:50:46,073 - INFO - Going to convert document batch...\n",
      "2025-12-15 07:50:46,076 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 07:50:46,078 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 07:50:46,080 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 07:50:46,082 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:46,123 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:46,126 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [3/20] Done: https://arxiv.org/pdf/2311.04155\n",
      "⚠ CPU HIGH USAGE detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 07:50:46,163 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:46,172 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:46,857 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:46,860 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:46,866 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:46,868 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:47,012 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:47,015 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:47,043 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:50:47,048 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 07:50:47,835 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 07:50:47,837 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:50:48,708 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:50:49,955 - INFO - Processing document 2003.12771v2.pdf\n",
      "2025-12-15 07:51:08,250 - INFO - Finished converting document 2003.12771v2.pdf in 22.29 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [4/20] Done: https://arxiv.org/pdf/2003.12771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 07:51:20,461 - INFO - Finished converting document 2302.09664v3.pdf in 50.76 sec.\n",
      "2025-12-15 07:51:20,582 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 07:51:20,584 - INFO - Going to convert document batch...\n",
      "2025-12-15 07:51:20,585 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 07:51:20,586 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 07:51:20,587 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 07:51:20,588 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:20,613 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:20,614 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:20,627 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:20,628 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [5/20] Done: https://arxiv.org/pdf/2302.09664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 07:51:20,817 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:20,819 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:20,821 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:20,822 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:20,900 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:20,901 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:20,924 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:20,925 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 07:51:21,138 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 07:51:21,139 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:51:21,826 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:51:21,872 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 07:51:21,899 - INFO - Going to convert document batch...\n",
      "2025-12-15 07:51:21,902 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 07:51:21,903 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 07:51:21,904 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 07:51:21,905 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:21,931 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:21,932 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:21,945 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:21,946 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:22,226 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:22,228 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:22,231 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:22,233 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:22,352 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:22,355 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:22,381 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:51:22,383 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 07:51:22,980 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 07:51:22,983 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:51:22,990 - INFO - Processing document 1705.04510v1.pdf\n",
      "2025-12-15 07:51:23,746 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:51:24,685 - INFO - Processing document 1910.02707v1.pdf\n",
      "2025-12-15 07:52:28,002 - INFO - Finished converting document 1705.04510v1.pdf in 67.49 sec.\n",
      "2025-12-15 07:52:28,291 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [6/20] Done: https://arxiv.org/pdf/1705.04510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 07:52:28,313 - INFO - Going to convert document batch...\n",
      "2025-12-15 07:52:28,320 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 07:52:28,326 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 07:52:28,328 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 07:52:28,330 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:28,365 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:28,368 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:28,385 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:28,389 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ CPU HIGH USAGE detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 07:52:28,786 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:28,791 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:28,794 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:28,798 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:28,973 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:28,976 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:29,004 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:29,009 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 07:52:29,641 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 07:52:29,643 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:52:30,572 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:52:31,225 - INFO - Processing document 1506.01497v3.pdf\n",
      "2025-12-15 07:52:51,373 - INFO - Finished converting document 1910.02707v1.pdf in 103.06 sec.\n",
      "2025-12-15 07:52:51,590 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 07:52:51,594 - INFO - Going to convert document batch...\n",
      "2025-12-15 07:52:51,601 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 07:52:51,610 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 07:52:51,615 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 07:52:51,619 - INFO - Accelerator device: 'cpu'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [7/20] Done: https://arxiv.org/pdf/1910.02707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 07:52:51,668 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:51,673 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:51,699 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:51,701 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:52,315 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:52,320 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:52,324 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:52,327 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:52,466 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:52,475 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:52,524 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:52:52,531 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 07:52:53,012 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 07:52:53,018 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:52:53,937 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:52:55,051 - INFO - Processing document 1802.05365v2.pdf\n",
      "2025-12-15 07:54:01,456 - INFO - Finished converting document 1802.05365v2.pdf in 70.00 sec.\n",
      "2025-12-15 07:54:01,609 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 07:54:01,612 - INFO - Going to convert document batch...\n",
      "2025-12-15 07:54:01,613 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 07:54:01,615 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 07:54:01,618 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 07:54:01,619 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:01,646 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:01,647 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:01,660 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:01,662 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [8/20] Done: https://arxiv.org/pdf/1802.05365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 07:54:02,016 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:02,023 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:02,025 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:02,026 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:02,156 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:02,158 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:02,186 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:02,189 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 07:54:02,508 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 07:54:02,510 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:54:03,381 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:54:03,884 - INFO - Processing document 2004.09602v1.pdf\n",
      "2025-12-15 07:54:22,599 - INFO - Finished converting document 1506.01497v3.pdf in 114.48 sec.\n",
      "2025-12-15 07:54:22,794 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 07:54:22,798 - INFO - Going to convert document batch...\n",
      "2025-12-15 07:54:22,801 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 07:54:22,803 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 07:54:22,806 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 07:54:22,808 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:22,839 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:22,842 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:22,859 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:22,863 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [9/20] Done: https://arxiv.org/pdf/1506.01497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 07:54:23,258 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:23,263 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:23,271 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:23,274 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:23,459 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:23,463 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:23,495 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:54:23,499 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 07:54:24,225 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 07:54:24,231 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:54:25,148 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:54:26,925 - INFO - Processing document 1602.02235v1.pdf\n",
      "2025-12-15 07:55:04,824 - INFO - Finished converting document 1602.02235v1.pdf in 42.12 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [10/20] Done: https://arxiv.org/pdf/1602.02235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 07:55:12,345 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 07:55:12,357 - INFO - Going to convert document batch...\n",
      "2025-12-15 07:55:12,360 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 07:55:12,362 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 07:55:12,363 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 07:55:12,364 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 07:55:12,394 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:55:12,397 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:55:12,411 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:55:12,418 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:55:12,674 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:55:12,676 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:55:12,680 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:55:12,681 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:55:12,783 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:55:12,786 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:55:12,810 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:55:12,814 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 07:55:13,110 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 07:55:13,112 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:55:14,274 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:55:15,271 - INFO - Processing document 1801.06538v1.pdf\n",
      "2025-12-15 07:56:15,034 - INFO - Finished converting document 2004.09602v1.pdf in 133.51 sec.\n",
      "2025-12-15 07:56:15,316 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 07:56:15,325 - INFO - Going to convert document batch...\n",
      "2025-12-15 07:56:15,328 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 07:56:15,331 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 07:56:15,333 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 07:56:15,334 - INFO - Accelerator device: 'cpu'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [11/20] Done: https://arxiv.org/pdf/2004.09602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 07:56:15,389 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:56:15,391 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:56:15,409 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:56:15,416 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:56:16,033 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:56:16,036 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:56:16,044 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:56:16,046 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:56:16,190 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:56:16,193 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:56:16,223 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:56:16,226 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 07:56:17,058 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 07:56:17,061 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:56:17,969 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:56:19,331 - INFO - Processing document 1611.09060v3.pdf\n",
      "2025-12-15 07:57:41,082 - INFO - Finished converting document 1611.09060v3.pdf in 85.91 sec.\n",
      "2025-12-15 07:57:41,296 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 07:57:41,301 - INFO - Going to convert document batch...\n",
      "2025-12-15 07:57:41,303 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 07:57:41,304 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 07:57:41,306 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 07:57:41,307 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 07:57:41,332 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:57:41,334 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:57:41,348 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:57:41,350 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [12/20] Done: https://arxiv.org/pdf/1611.09060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 07:57:41,822 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:57:41,824 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:57:41,827 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:57:41,829 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:57:41,950 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:57:41,952 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:57:41,978 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:57:41,980 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 07:57:42,293 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 07:57:42,296 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:57:43,077 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:57:44,544 - INFO - Processing document 1912.05483v1.pdf\n",
      "2025-12-15 07:58:02,382 - INFO - Finished converting document 1912.05483v1.pdf in 21.19 sec.\n",
      "2025-12-15 07:58:02,562 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 07:58:02,567 - INFO - Going to convert document batch...\n",
      "2025-12-15 07:58:02,569 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 07:58:02,571 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 07:58:02,573 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 07:58:02,576 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:02,607 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:02,609 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:02,627 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:02,632 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [13/20] Done: https://arxiv.org/pdf/1912.05483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 07:58:03,161 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:03,163 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:03,166 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:03,167 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:03,285 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:03,287 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:03,312 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:03,314 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 07:58:03,726 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 07:58:03,728 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:58:04,436 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:58:05,178 - INFO - Processing document 1709.08624v2.pdf\n",
      "2025-12-15 07:58:28,178 - INFO - Finished converting document 1801.06538v1.pdf in 203.29 sec.\n",
      "2025-12-15 07:58:28,472 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 07:58:28,477 - INFO - Going to convert document batch...\n",
      "2025-12-15 07:58:28,479 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 07:58:28,481 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 07:58:28,484 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 07:58:28,489 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:28,522 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:28,524 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:28,540 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:28,542 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [14/20] Done: https://arxiv.org/pdf/1801.06538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 07:58:29,112 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:29,117 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:29,119 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:29,122 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:29,261 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:29,264 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:29,296 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:58:29,300 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 07:58:29,875 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 07:58:29,876 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:58:30,599 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:58:32,349 - INFO - Processing document 1810.04805v2.pdf\n",
      "2025-12-15 07:59:26,298 - INFO - Finished converting document 1709.08624v2.pdf in 83.85 sec.\n",
      "2025-12-15 07:59:26,458 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 07:59:26,462 - INFO - Going to convert document batch...\n",
      "2025-12-15 07:59:26,463 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 07:59:26,465 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 07:59:26,466 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 07:59:26,469 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:26,500 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:26,507 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:26,522 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:26,525 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [15/20] Done: https://arxiv.org/pdf/1709.08624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 07:59:26,798 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:26,800 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:26,802 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:26,803 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:26,908 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:26,910 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:26,936 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:26,937 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 07:59:27,251 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 07:59:27,253 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:59:28,022 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:59:28,523 - INFO - Processing document 1502.03167v3.pdf\n",
      "2025-12-15 07:59:54,989 - INFO - Finished converting document 1810.04805v2.pdf in 86.62 sec.\n",
      "2025-12-15 07:59:55,178 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 07:59:55,186 - INFO - Going to convert document batch...\n",
      "2025-12-15 07:59:55,189 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 07:59:55,192 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 07:59:55,195 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 07:59:55,199 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:55,228 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:55,229 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:55,243 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:55,247 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [16/20] Done: https://arxiv.org/pdf/1810.04805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 07:59:55,714 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:55,717 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:55,720 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:55,722 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:55,824 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:55,827 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:55,854 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 07:59:55,857 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 07:59:56,120 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 07:59:56,122 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:59:56,885 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 07:59:58,125 - INFO - Processing document 1807.02547v2.pdf\n",
      "2025-12-15 08:00:15,740 - INFO - Finished converting document 1502.03167v3.pdf in 49.36 sec.\n",
      "2025-12-15 08:00:15,874 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 08:00:15,879 - INFO - Going to convert document batch...\n",
      "2025-12-15 08:00:15,881 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 08:00:15,884 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 08:00:15,885 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 08:00:15,886 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:15,918 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:15,928 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:15,951 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:15,955 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [17/20] Done: https://arxiv.org/pdf/1502.03167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 08:00:16,389 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:16,392 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:16,395 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:16,402 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:16,548 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:16,551 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:16,582 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:16,588 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 08:00:17,016 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 08:00:17,021 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:00:17,892 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:00:18,541 - INFO - Processing document 2103.00634v4.pdf\n",
      "2025-12-15 08:00:46,843 - INFO - Finished converting document 2103.00634v4.pdf in 31.06 sec.\n",
      "2025-12-15 08:00:47,004 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 08:00:47,009 - INFO - Going to convert document batch...\n",
      "2025-12-15 08:00:47,010 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 08:00:47,013 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 08:00:47,015 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 08:00:47,019 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:47,054 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:47,056 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:47,069 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:47,071 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [18/20] Done: https://arxiv.org/pdf/2103.00634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 08:00:47,434 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:47,436 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:47,440 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:47,441 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:47,560 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:47,562 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:47,586 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:00:47,593 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 08:00:48,221 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 08:00:48,222 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:00:48,992 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:00:49,554 - INFO - Processing document 2103.00634v4.pdf\n",
      "2025-12-15 08:01:02,711 - INFO - Finished converting document 1807.02547v2.pdf in 67.62 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [19/20] Done: https://arxiv.org/pdf/1807.02547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 08:01:11,328 - INFO - Finished converting document 2103.00634v4.pdf in 24.43 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [20/20] Done: https://arxiv.org/pdf/2103.00634\n",
      "Document 1:\n",
      "<!-- image -->\n",
      "\n",
      "## Docling Technical Report\n",
      "\n",
      "## Version 1.0\n",
      "\n",
      "Christoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\n",
      "\n",
      "AI4K Group, IBM Research R¨ uschlikon, Switzerland\n",
      "\n",
      "## Abstract\n",
      "\n",
      "This technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Converting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\n",
      "\n",
      "With Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\n",
      "\n",
      "Here is what Docling delivers today:\n",
      "\n",
      "- Converts PDF documents to JSON or Markdown format, stable and lightning fast\n",
      "- Understands detailed page layout, reading order, locates figures and recovers table structures\n",
      "- Extracts metadata from the document, such as title, authors, references and language\n",
      "- Optionally applies OCR, e.g. for scanned PDFs\n",
      "- Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\n",
      "- Can leverage different accelerators (GPU, MPS, etc).\n",
      "\n",
      "## 2 Getting Started\n",
      "\n",
      "To use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.\n",
      "\n",
      "Docling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.\n",
      "\n",
      "from docling.document\\_converter import DocumentConverter\n",
      "\n",
      "```\n",
      "source = \"https://arxiv.org/pdf/2206.01062\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \"## DocLayNet: A Large Human -Annotated Dataset for Document -Layout Analysis [...]\"\n",
      "```\n",
      "\n",
      "Optionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.\n",
      "\n",
      "## 3 Processing pipeline\n",
      "\n",
      "Docling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.\n",
      "\n",
      "## 3.1 PDF backends\n",
      "\n",
      "Two basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling's PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive\n",
      "\n",
      "1 see huggingface.co/ds4sd/docling-models/\n",
      "\n",
      "Figure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "licensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14].\n",
      "\n",
      "We therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings.\n",
      "\n",
      "## 3.2 AI models\n",
      "\n",
      "As part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.\n",
      "\n",
      "## Layout Analysis Model\n",
      "\n",
      "Our layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\n",
      "\n",
      "The Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.\n",
      "\n",
      "## Table Structure Recognition\n",
      "\n",
      "The TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\n",
      "\n",
      "The Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.\n",
      "\n",
      "## OCR\n",
      "\n",
      "Docling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page).\n",
      "\n",
      "We are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements.\n",
      "\n",
      "## 3.3 Assembly\n",
      "\n",
      "In the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request.\n",
      "\n",
      "## 3.4 Extensibility\n",
      "\n",
      "Docling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.\n",
      "\n",
      "Implementations of model classes must satisfy the python Callable interface. The \\_\\_call\\_\\_ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.\n",
      "\n",
      "## 4 Performance\n",
      "\n",
      "In this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable ) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1.\n",
      "\n",
      "If you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery.\n",
      "\n",
      "Establishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and\n",
      "\n",
      "torch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report.\n",
      "\n",
      "Table 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.\n",
      "\n",
      "| CPU                     | Thread budget   | native backend   | native backend   | native backend   | pypdfium backend   | pypdfium backend   | pypdfium backend   |\n",
      "|-------------------------|-----------------|------------------|------------------|------------------|--------------------|--------------------|--------------------|\n",
      "|                         |                 | TTS              | Pages/s          | Mem              | TTS                | Pages/s            | Mem                |\n",
      "| Apple M3 Max (16 cores) | 4 16            | 177 s 167 s      | 1.27 1.34        | 6.20 GB          | 103 s 92 s         | 2.18 2.45          | 2.56 GB            |\n",
      "| Intel(R) Xeon E5-2690   | 4 16            | 375 s 244 s      | 0.60 0.92        | 6.16 GB          | 239 s 143 s        | 0.94 1.57          | 2.42 GB            |\n",
      "\n",
      "## 5 Applications\n",
      "\n",
      "Thanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.\n",
      "\n",
      "## 6 Future work and contributions\n",
      "\n",
      "Docling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\n",
      "\n",
      "We encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.\n",
      "\n",
      "## References\n",
      "\n",
      "- [1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\n",
      "- [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster\n",
      "\n",
      "machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24) . ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf .\n",
      "\n",
      "- [3] C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing (CLOUD) , pages 363-373. IEEE, 2022.\n",
      "- [4] J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf .\n",
      "- [5] O. R. developers. Onnx runtime. https://onnxruntime.ai/ , 2024. Version: 1.18.1.\n",
      "- [6] IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit .\n",
      "- [7] A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF .\n",
      "- [8] J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama\\_index .\n",
      "- [9] M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San Jos´ e, CA, USA, August 21-26, 2023, Proceedings, Part II , pages 37-50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8\\_3 .\n",
      "- [10] L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. Statements: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christiaen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024) , pages 193-214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15 .\n",
      "- [11] L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of chemical structures in patent documents. Nature Communications , 15(1):6532, August 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/ s41467-024-50779-y .\n",
      "- [12] A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4614-4623, 2022.\n",
      "- [13] B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large humanannotated dataset for document-layout segmentation. pages 3743-3751, 2022.\n",
      "- [14] pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/ py-pdf/pypdf .\n",
      "- [15] P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/ pypdfium2-team/pypdfium2 .\n",
      "- [16] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on real-time object detection, 2023.\n",
      "\n",
      "## Appendix\n",
      "\n",
      "In this section, we illustrate a few examples of Docling's output in Markdown and JSON.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1. Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation (not shown).\n",
      "\n",
      "KDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\n",
      "\n",
      "Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.\n",
      "\n",
      "|                                                                                                        | human                                                                   | MRCNN R50 R101                                                                                                          | FRCNN R101                                                  | YOLO v5x6                                                   |\n",
      "|--------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------|\n",
      "| Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All | 84-89 83-91 83-85 87-88 93-94 85-89 69-71 83-84 77-81 84-86 60-72 82-83 | 68.4 71.5 70.9 71.8 60.1 63.4 81.2 80.8 61.6 59.3 71.9 70.0 71.7 72.7 67.6 69.3 82.2 82.9 84.6 85.8 76.7 80.4 72.4 73.5 | 70.1 73.7 63.5 81.0 58.9 72.0 72.0 68.4 82.2 85.4 79.9 73.4 | 77.7 77.2 66.2 86.2 61.1 67.9 77.1 74.6 86.3 88.1 82.7 76.8 |\n",
      "\n",
      "to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\n",
      "\n",
      "## 5 EXPERIMENTS\n",
      "\n",
      "The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\n",
      "\n",
      "Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "paper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\n",
      "\n",
      "In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].\n",
      "\n",
      "## Baselines for Object Detection\n",
      "\n",
      "In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\n",
      "\n",
      "Table 2: Prediction perlormance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 beckbone were rained based on the network architecturesfrom the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utized was YOLOv5x6 [13]. All models were initialised using pre-trained weightsfrom the COCO 2017 dataset.\n",
      "\n",
      "|                | human   |   MRCNN |   MRCNNFRCNN |      |   YOLO |\n",
      "|----------------|---------|---------|--------------|------|--------|\n",
      "| Caption        | 84-89   |    68.4 |         71.5 | 70.1 |   77.7 |\n",
      "| Footnote       | 83-91   |    70.9 |         71.8 | 73.7 |   77.2 |\n",
      "| Formula        | 83-85   |    60.1 |         63.4 | 63.5 |   66.2 |\n",
      "| List-item      | 89-48   |    81.2 |         80.8 | 81   |   86.2 |\n",
      "| Page-ooer      | 93-94   |    61.6 |         59.3 | 58.9 |   61.1 |\n",
      "| Page-header    | 85-89   |    71.9 |         70   | 72   |   67.9 |\n",
      "| Picture        | 69-71   |    71.7 |         72.7 | 72   |   77.1 |\n",
      "| Section-header | 83-84   |    67.6 |         69.3 | 68.4 |   74.6 |\n",
      "| Table          | 77-81   |    82.2 |         82.9 | 82.2 |   86.3 |\n",
      "| x1             | 84-86   |    84.6 |         85.8 | 85.4 |   88.1 |\n",
      "| Title          | 60-72   |    76.7 |         80.4 | 79.9 |   82.7 |\n",
      "| AlIl           | 82-83   |    72.4 |         73.5 | 73.4 |   76.8 |\n",
      "\n",
      "to avoid this at any cost in order to have clear, unbiased baseline numbers for human dcument-layout annotation. hird, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS -pxa fjeund je o sjoo-xe posopu oug punoe xoq-upunoq wnwu o o xoq umep-jesnne syuus fgeogewone joo uogeqouue based segments, which excludes only Table and Picture For the later, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cellsis that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\n",
      "\n",
      "## 5EXPERIMENTS\n",
      "\n",
      "The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthemore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\n",
      "\n",
      "Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNN network with ResNet50 backbone trained on increasing fractions of the DocLayNet dataset. The leaming curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with simlar data ill not yield significantly better predictions.\n",
      "\n",
      "paper and leave the detaled evaluation of more reoent methods mentioned in Section 2 for future work.\n",
      "\n",
      "In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in 0 S'0 wo ofue pu sdeμano o  (dvw) uospeud oene ueu Susn suogoped e po Agenb u aenje m om oeqnd 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API[16]\n",
      "\n",
      "## BaselinesforObjectDetection\n",
      "\n",
      "In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pbxels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overallbetween 6 and 0 eug uogeopu poo6 e senj6 sL sobed popeoue-edjμ uo suogegoue ueunq esujed oug wo. pnduoo dvw oug ueg emo %01 DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixelupoau auou ou puey saugo au uo suogopeud sogeq ueqo o djou lou sep soxoq-ugpunoq wou panuap uogeuoubes oteu poseq Yolov5x model does very welland even out-performs humans on selected labels such as Text, Table and Picture  Thisis not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\n",
      "\n",
      "Figure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing first under the title. Elements recognized as page headers or footers are suppressed in Markdown to deliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph in '5. Experiments' wrapping over the column end is broken up in two and interrupted by the table.\n",
      "\n",
      "KDD '22, August 14-18, 2022, Washington, DC, USA\n",
      "\n",
      "Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\n",
      "\n",
      "Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as %\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "of row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric we distributed the annotation workload and performed continuous\n",
      "\n",
      "only. For phases three and four, a group of 40 dedicated annotators quality controls. Phase one and two required a small team of experts\n",
      "\n",
      "were assembled and supervised.\n",
      "\n",
      "while coverage ensures that all meaningful items on a page can to a document category, such as\n",
      "\n",
      "be annotated. We refrained from class labels that are very specific\n",
      "\n",
      "Abstract in the\n",
      "\n",
      "Scientific Articles semantics of the text. Labels such as\n",
      "\n",
      "category. We also avoided class labels that are tightly linked to the\n",
      "\n",
      "Author\n",
      "\n",
      "Affiliation\n",
      "\n",
      "teria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources in DocBank, are often only distinguishable by discriminating on 3 https://arxiv.org/ Figure 4: Table 1 from the DocLayNet paper in the original PDF (A), as rendered Markdown (B) and in JSON representation (C). Spanning table cells, such as the multi-column header 'triple interannotator mAP@0.5-0.95 (%)', is repeated for each column in the Markdown representation (B), which guarantees that every data point can be traced back to row and column headings only by its grid coordinates in the table. In the JSON representation, the span information is reflected in the fields of each table cell (C).\n",
      "\n",
      "and\n",
      "\n",
      ", as seen\n",
      "\n",
      "Phase 1: Data selection and preparation.\n",
      "\n",
      "Our inclusion cri-\n",
      "Document 2:\n",
      "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\n",
      "\n",
      "## Attention Is All You Need\n",
      "\n",
      "Ashish Vaswani ∗ Google Brain avaswani@google.com\n",
      "\n",
      "Noam Shazeer ∗ Google Brain noam@google.com\n",
      "\n",
      "Llion Jones ∗ Google Research llion@google.com\n",
      "\n",
      "Niki Parmar ∗ Google Research nikip@google.com\n",
      "\n",
      "Aidan N. Gomez ∗ † University of Toronto aidan@cs.toronto.edu\n",
      "\n",
      "Jakob Uszkoreit ∗ Google Research usz@google.com\n",
      "\n",
      "Łukasz Kaiser ∗ Google Brain lukaszkaiser@google.com\n",
      "\n",
      "Illia Polosukhin ∗ ‡\n",
      "\n",
      "illia.polosukhin@gmail.com\n",
      "\n",
      "## Abstract\n",
      "\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
      "\n",
      "∗ Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n",
      "\n",
      "† Work performed while at Google Brain.\n",
      "\n",
      "‡ Work performed while at Google Research.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].\n",
      "\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h t , as a function of the previous hidden state h t -1 and the input for position t . This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\n",
      "\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\n",
      "\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "\n",
      "## 2 Background\n",
      "\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n",
      "\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\n",
      "\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "\n",
      "## 3 Model Architecture\n",
      "\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations ( x 1 , ..., x n ) to a sequence of continuous representations z = ( z 1 , ..., z n ) . Given z , the decoder then generates an output sequence ( y 1 , ..., y m ) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.\n",
      "\n",
      "Figure 1: The Transformer - model architecture.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\n",
      "\n",
      "## 3.1 Encoder and Decoder Stacks\n",
      "\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm( x +Sublayer( x )) , where Sublayer( x ) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d model = 512 .\n",
      "\n",
      "Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i .\n",
      "\n",
      "## 3.2 Attention\n",
      "\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "\n",
      "## Scaled Dot-Product Attention\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
      "\n",
      "## 3.2.1 Scaled Dot-Product Attention\n",
      "\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension d k , and values of dimension d v . We compute the dot products of the query with all keys, divide each by √ d k , and apply a softmax function to obtain the weights on the values.\n",
      "\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q . The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1 √ d k . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
      "\n",
      "While for small values of d k the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d k [3]. We suspect that for large values of d k , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4 . To counteract this effect, we scale the dot products by 1 √ d k .\n",
      "\n",
      "## 3.2.2 Multi-Head Attention\n",
      "\n",
      "Instead of performing a single attention function with d model-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k , d k and d v dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d v -dimensional\n",
      "\n",
      "4 To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1 . Then their dot product, q · k = ∑ d k i =1 q i k i , has mean 0 and variance d k .\n",
      "\n",
      "output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\n",
      "\n",
      "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Where the projections are parameter matrices W Q i ∈ R d model × d k , W i K ∈ R d model × d k , W V i ∈ R d model × d v and W O ∈ R hd v × d model .\n",
      "\n",
      "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use d k = d v = d model /h = 64 . Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
      "\n",
      "## 3.2.3 Applications of Attention in our Model\n",
      "\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "\n",
      "- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\n",
      "- The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
      "- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to -∞ ) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\n",
      "\n",
      "## 3.3 Position-wise Feed-Forward Networks\n",
      "\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is d model = 512 , and the inner-layer has dimensionality d ff = 2048 .\n",
      "\n",
      "## 3.4 Embeddings and Softmax\n",
      "\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √ d model.\n",
      "\n",
      "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\n",
      "\n",
      "| Layer Type                  | Complexity per Layer   | Sequential Operations   | Maximum Path Length   |\n",
      "|-----------------------------|------------------------|-------------------------|-----------------------|\n",
      "| Self-Attention              | O ( n 2 · d )          | O (1)                   | O (1)                 |\n",
      "| Recurrent                   | O ( n · d 2 )          | O ( n )                 | O ( n )               |\n",
      "| Convolutional               | O ( k · n · d 2 )      | O (1)                   | O ( log k ( n ))      |\n",
      "| Self-Attention (restricted) | O ( r · n · d )        | O (1)                   | O ( n/r )             |\n",
      "\n",
      "## 3.5 Positional Encoding\n",
      "\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d model as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\n",
      "\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2 π to 10000 · 2 π . We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k , PE pos + k can be represented as a linear function of PE pos .\n",
      "\n",
      "We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
      "\n",
      "## 4 Why Self-Attention\n",
      "\n",
      "In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations ( x 1 , ..., x n ) to another sequence of equal length ( z 1 , ..., z n ) , with x i , z i ∈ R d , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\n",
      "\n",
      "One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n",
      "\n",
      "The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n",
      "\n",
      "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O ( n ) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
      "\n",
      "length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O ( n/r ) . We plan to investigate this approach further in future work.\n",
      "\n",
      "A single convolutional layer with kernel width k &lt; n does not connect all pairs of input and output positions. Doing so requires a stack of O ( n/k ) convolutional layers in the case of contiguous kernels, or O ( log k ( n )) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k . Separable convolutions [6], however, decrease the complexity considerably, to O ( k · n · d + n · d 2 ) . Even with k = n , however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\n",
      "\n",
      "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\n",
      "\n",
      "## 5 Training\n",
      "\n",
      "This section describes the training regime for our models.\n",
      "\n",
      "## 5.1 Training Data and Batching\n",
      "\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n",
      "\n",
      "## 5.2 Hardware and Schedule\n",
      "\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\n",
      "\n",
      "## 5.3 Optimizer\n",
      "\n",
      "We used the Adam optimizer [20] with β 1 = 0 . 9 , β 2 = 0 . 98 and ϵ = 10 -9 . We varied the learning rate over the course of training, according to the formula:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "This corresponds to increasing the learning rate linearly for the first warmup \\_ steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup \\_ steps = 4000 .\n",
      "\n",
      "## 5.4 Regularization\n",
      "\n",
      "We employ three types of regularization during training:\n",
      "\n",
      "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      "\n",
      "| Model                           | BLEU   | BLEU   | Training Cost (FLOPs)   | Training Cost (FLOPs)   |\n",
      "|---------------------------------|--------|--------|-------------------------|-------------------------|\n",
      "|                                 | EN-DE  | EN-FR  | EN-DE                   | EN-FR                   |\n",
      "| ByteNet [18]                    | 23.75  |        |                         |                         |\n",
      "| Deep-Att + PosUnk [39]          |        | 39.2   |                         | 1 . 0 · 10 20           |\n",
      "| GNMT + RL [38]                  | 24.6   | 39.92  | 2 . 3 · 10 19           | 1 . 4 · 10 20           |\n",
      "| ConvS2S [9]                     | 25.16  | 40.46  | 9 . 6 · 10 18           | 1 . 5 · 10 20           |\n",
      "| MoE [32]                        | 26.03  | 40.56  | 2 . 0 · 10 19           | 1 . 2 · 10 20           |\n",
      "| Deep-Att + PosUnk Ensemble [39] |        | 40.4   |                         | 8 . 0 · 10 20           |\n",
      "| GNMT + RL Ensemble [38]         | 26.30  | 41.16  | 1 . 8 · 10 20           | 1 . 1 · 10 21           |\n",
      "| ConvS2S Ensemble [9]            | 26.36  | 41.29  | 7 . 7 · 10 19           | 1 . 2 · 10 21           |\n",
      "| Transformer (base model)        | 27.3   | 38.1   | 3 . 3 · 10 18           | 3 . 3 · 10 18           |\n",
      "| Transformer (big)               | 28.4   | 41.8   | 2 . 3 · 10 19           | 2 . 3 · 10 19           |\n",
      "\n",
      "Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P drop = 0 . 1 .\n",
      "\n",
      "Label Smoothing During training, we employed label smoothing of value ϵ ls = 0 . 1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "\n",
      "## 6 Results\n",
      "\n",
      "## 6.1 Machine Translation\n",
      "\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2 . 0 BLEU, establishing a new state-of-the-art BLEU score of 28 . 4 . The configuration of this model is listed in the bottom line of Table 3. Training took 3 . 5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n",
      "\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41 . 0 , outperforming all of the previously published single models, at less than 1 / 4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate P drop = 0 . 1 , instead of 0 . 3 .\n",
      "\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0 . 6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50 , but terminate early when possible [38].\n",
      "\n",
      "Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5 .\n",
      "\n",
      "## 6.2 Model Variations\n",
      "\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\n",
      "\n",
      "5 We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
      "\n",
      "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n",
      "\n",
      "|      | N                                         | d model                                   | d ff                                      | h                                         | d k                                       | d v                                       | P drop                                    | ϵ ls                                      | train steps   |   PPL (dev) |   BLEU (dev) | params × 10 6   |\n",
      "|------|-------------------------------------------|-------------------------------------------|-------------------------------------------|-------------------------------------------|-------------------------------------------|-------------------------------------------|-------------------------------------------|-------------------------------------------|---------------|-------------|--------------|-----------------|\n",
      "| base | 6                                         | 512                                       | 2048                                      | 8                                         | 64                                        | 64                                        | 0.1                                       | 0.1                                       | 100K          |        4.92 |         25.8 | 65              |\n",
      "|      |                                           |                                           |                                           | 1                                         | 512                                       | 512                                       |                                           |                                           |               |        5.29 |         24.9 |                 |\n",
      "|      |                                           |                                           |                                           | 4                                         | 128                                       | 128                                       |                                           |                                           |               |        5    |         25.5 |                 |\n",
      "| (A)  |                                           |                                           |                                           | 16                                        | 32                                        | 32                                        |                                           |                                           |               |        4.91 |         25.8 |                 |\n",
      "|      |                                           |                                           |                                           | 32                                        | 16                                        | 16                                        |                                           |                                           |               |        5.01 |         25.4 |                 |\n",
      "|      |                                           |                                           |                                           |                                           | 16                                        |                                           |                                           |                                           |               |        5.16 |         25.1 | 58              |\n",
      "| (B)  |                                           |                                           |                                           |                                           | 32                                        |                                           |                                           |                                           |               |        5.01 |         25.4 | 60              |\n",
      "|      | 2                                         |                                           |                                           |                                           |                                           |                                           |                                           |                                           |               |        6.11 |         23.7 | 36              |\n",
      "|      | 4                                         |                                           |                                           |                                           |                                           |                                           |                                           |                                           |               |        5.19 |         25.3 | 50              |\n",
      "|      | 8                                         |                                           |                                           |                                           |                                           |                                           |                                           |                                           |               |        4.88 |         25.5 | 80              |\n",
      "| (C)  |                                           | 256                                       |                                           |                                           | 32                                        | 32                                        |                                           |                                           |               |        5.75 |         24.5 | 28              |\n",
      "|      |                                           | 1024                                      |                                           |                                           | 128                                       | 128                                       |                                           |                                           |               |        4.66 |         26   | 168             |\n",
      "|      |                                           |                                           | 1024                                      |                                           |                                           |                                           |                                           |                                           |               |        5.12 |         25.4 | 53              |\n",
      "|      |                                           |                                           | 4096                                      |                                           |                                           |                                           |                                           |                                           |               |        4.75 |         26.2 | 90              |\n",
      "|      |                                           |                                           |                                           |                                           |                                           |                                           | 0.0                                       |                                           |               |        5.77 |         24.6 |                 |\n",
      "|      |                                           |                                           |                                           |                                           |                                           |                                           | 0.2                                       |                                           |               |        4.95 |         25.5 |                 |\n",
      "| (D)  |                                           |                                           |                                           |                                           |                                           |                                           |                                           | 0.0                                       |               |        4.67 |         25.3 |                 |\n",
      "|      |                                           |                                           |                                           |                                           |                                           |                                           |                                           | 0.2                                       |               |        5.47 |         25.7 |                 |\n",
      "| (E)  | positional embedding instead of sinusoids | positional embedding instead of sinusoids | positional embedding instead of sinusoids | positional embedding instead of sinusoids | positional embedding instead of sinusoids | positional embedding instead of sinusoids | positional embedding instead of sinusoids | positional embedding instead of sinusoids |               |        4.92 |         25.7 |                 |\n",
      "| big  | 6                                         | 1024                                      | 4096                                      | 16                                        |                                           |                                           | 0.3                                       |                                           | 300K          |        4.33 |         26.4 | 213             |\n",
      "\n",
      "development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\n",
      "\n",
      "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "\n",
      "In Table 3 rows (B), we observe that reducing the attention key size d k hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\n",
      "\n",
      "## 6.3 English Constituency Parsing\n",
      "\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
      "\n",
      "We trained a 4-layer transformer with d model = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n",
      "\n",
      "We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\n",
      "\n",
      "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\n",
      "\n",
      "| Parser                             | Training                 |   WSJ 23 F1 |\n",
      "|------------------------------------|--------------------------|-------------|\n",
      "| Vinyals &Kaiser el al. (2014) [37] | WSJ only, discriminative |        88.3 |\n",
      "| Petrov et al. (2006) [29]          | WSJ only, discriminative |        90.4 |\n",
      "| Zhu et al. (2013) [40]             | WSJ only, discriminative |        90.4 |\n",
      "| Dyer et al. (2016) [8]             | WSJ only, discriminative |        91.7 |\n",
      "| Transformer (4 layers)             | WSJ only, discriminative |        91.3 |\n",
      "| Zhu et al. (2013) [40]             | semi-supervised          |        91.3 |\n",
      "| Huang &Harper (2009) [14]          | semi-supervised          |        91.3 |\n",
      "| McClosky et al. (2006) [26]        | semi-supervised          |        92.1 |\n",
      "| Vinyals &Kaiser el al. (2014) [37] | semi-supervised          |        92.1 |\n",
      "| Transformer (4 layers)             | semi-supervised          |        92.7 |\n",
      "| Luong et al. (2015) [23]           | multi-task               |        93   |\n",
      "| Dyer et al. (2016) [8]             | generative               |        93.3 |\n",
      "\n",
      "increased the maximum output length to input length + 300 . We used a beam size of 21 and α = 0 . 3 for both WSJ only and the semi-supervised setting.\n",
      "\n",
      "Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\n",
      "\n",
      "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.\n",
      "\n",
      "## 7 Conclusion\n",
      "\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\n",
      "\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "\n",
      "The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor .\n",
      "\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\n",
      "\n",
      "## References\n",
      "\n",
      "- [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016.\n",
      "- [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR , abs/1409.0473, 2014.\n",
      "- [3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR , abs/1703.03906, 2017.\n",
      "- [4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733 , 2016.\n",
      "\n",
      "- [5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR , abs/1406.1078, 2014.\n",
      "- [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357 , 2016.\n",
      "- [7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\n",
      "- [8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL , 2016.\n",
      "- [9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\n",
      "- [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 , 2013.\n",
      "- [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770-778, 2016.\n",
      "- [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\n",
      "- [13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation , 9(8):1735-1780, 1997.\n",
      "- [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing , pages 832-841. ACL, August 2009.\n",
      "- [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\n",
      "- [16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS) , 2016.\n",
      "- [17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR) , 2016.\n",
      "- [18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 , 2017.\n",
      "- [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations , 2017.\n",
      "- [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\n",
      "- [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722 , 2017.\n",
      "- [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130 , 2017.\n",
      "- [23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\n",
      "- [24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attentionbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\n",
      "\n",
      "- [25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics , 19(2):313-330, 1993.\n",
      "- [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference , pages 152-159. ACL, June 2006.\n",
      "- [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing , 2016.\n",
      "- [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304 , 2017.\n",
      "- [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL , pages 433-440. ACL, July 2006.\n",
      "- [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859 , 2016.\n",
      "- [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909 , 2015.\n",
      "- [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 , 2017.\n",
      "- [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research , 15(1):1929-1958, 2014.\n",
      "- [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2440-2448. Curran Associates, Inc., 2015.\n",
      "- [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems , pages 3104-3112, 2014.\n",
      "- [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\n",
      "- [37] Vinyals &amp; Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems , 2015.\n",
      "- [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144 , 2016.\n",
      "- [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\n",
      "- [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers) , pages 434-443. ACL, August 2013.\n",
      "\n",
      "## Attention Visualizations Input-Input Layer5\n",
      "\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult'. Attentions here shown only for the word 'making'. Different colors represent different heads. Best viewed in color.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Input-Input Layer5\n",
      "\n",
      "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word 'its' for attention heads 5 and 6. Note that the attentions are very sharp for this word.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Input-Input Layer5\n",
      "\n",
      "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "\n",
      "<!-- image -->\n",
      "Document 3:\n",
      "## Black-Box Prompt Optimization: Aligning Large Language Models without Model Training\n",
      "\n",
      "Jiale Cheng 1 , 2 * , Xiao Liu 3 , 2 * , Kehan Zheng 1 , Pei Ke 1 , Hongning Wang 1 , Yuxiao Dong 3 , Jie Tang 3 , Minlie Huang 1 †\n",
      "\n",
      "1 The Conversational Artificial Intelligence (CoAI) Group, Tsinghua University 2 Zhipu AI\n",
      "\n",
      "3\n",
      "\n",
      "The Knowledge Engineering Group (KEG), Tsinghua University chengjl23@mails.tsinghua.edu.cn, shawliu9@gmail.com, aihuang@tsinghua.edu.cn\n",
      "\n",
      "## Abstract\n",
      "\n",
      "Large language models (LLMs) have shown impressive success in various applications. However, these models are often not well aligned with human intents, which calls for additional treatments on them; that is, the alignment problem. To make LLMs better follow user instructions, existing alignment methods primarily focus on further training them. However, the extra training of LLMs is usually expensive in terms of GPU computing; even worse, some LLMs are not accessible for userdemanded training, such as GPTs. In this work, we take a different perspectiveBlack-Box Prompt Optimization (BPO)-to perform alignments. The idea is to optimize user prompts to suit LLMs' input understanding, so as to best realize users' intents without updating LLMs' parameters. BPO leverages human preferences to optimize prompts, thus making it superior to LLM (e.g., ChatGPT) as a prompt engineer. Moreover, BPO is model-agnostic, and the empirical results demonstrate that the BPOaligned ChatGPT yields a 22% increase in the win rate against its original version and 10% for GPT-4. Notably, the BPO-aligned LLMs can outperform the same models aligned by PPO and DPO, and it also brings additional performance gains when combining BPO with PPO or DPO. Code and datasets are released at https://github.com/thu-coai/BPO .\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Recently, the field of Natural Language Processing has made remarkable progress, largely thanks to the advent of Large Language Models (LLMs) (Brown et al., 2020b; Chowdhery et al., 2022; Zhang et al., 2022; Zeng et al., 2022; Touvron et al., 2023). After elaborate alignment (Gabriel, 2020; Ji et al., 2023), these models have demonstrated a strong ability of\n",
      "\n",
      "* JC and XL made equal contributions.\n",
      "\n",
      "† Corresponding author.\n",
      "\n",
      "2 Work done when JC interned at Zhipu AI.\n",
      "\n",
      "Figure 1: (Upper) Two directions of LLM alignment: Black-Box Prompt Optimization (BPO) and Learning from Feedback (PPO, DPO). BPO offers a conceptually new perspective to bridge the gap between humans and LLMs. (Lower) On Vicuna Eval's pairwise evaluation, we show that BPO further aligns gpt-3.5-turbo and claude-2 without training. It also outperforms both PPO &amp; DPO and presents orthogonal improvements.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "instruction-following and human preference understanding, yielding products like ChatGPT (OpenAI, 2022) that have attracted widespread attention.\n",
      "\n",
      "However, aligning LLMs to human preferences is not trivial. The major challenge lies in narrowing the gap between human intents (conveyed by prompts ) and LLMs' understanding of them. Significant effort has been focused on steering LLMs to approach human preference, including reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022), reinforcement learning from AI feedback (RLAIF) (Bai et al., 2022b; Lee et al., 2023), or Direct Preference Optimization (DPO) (Rafailov et al., 2023). Nevertheless, these methods suffer from various deficiencies:\n",
      "\n",
      "- Efficiency: As LLMs grow larger, it becomes far more expensive and difficult to train these\n",
      "- models, especially when using notoriously unstable RL algorithms for the purpose.\n",
      "- Accessibility: As most best-performing LLMs, such as GPT-4 (OpenAI, 2023) and Claude2 (Anthropic, 2023a), are close-sourced and only can be accessed by API, these trainingbased methods are not applicable for users outside the organization to enhance alignment.\n",
      "- Interpretability: The modeling and exact consequent improvements of human preference are uninterpretable when using these approaches.\n",
      "\n",
      "Distinct from the aforementioned alignment methods, we propose to steer human prompts to accommodate LLMs' understanding . While the idea is closely related to ' prompt engineering ', its automated prototypes would trace back to AutoPrompt (Shin et al., 2020) and prompt tuning (i.e., P-Tuning) (Liu et al., 2021; Lester et al., 2021), where prompts are optimized to improve task performance without training the LMs. Our new alignment method, Black-Box Prompt Optimization (BPO) , presents an efficient and interpretable paradigm that aligns LLMs without modifying these models. The central idea behind BPO is to create an automatic prompt optimizer that rewrites human prompts, which are usually less organized or ambiguous, to prompts that better deliver human intent. Consequently, these prompts could be more LLM-preferred and yield better human-preferred responses.\n",
      "\n",
      "In BPO, the prompt preference optimizer is learned from preference comparisons. We curate a subset of publicly available SFT datasets with either human or AI preferences. Each instance of our training data contains a prompt along with a pair of favorable and unfavorable responses. We then employ LLMs to delineate and criticize the paired responses, and subsequently ask the LLMs to refine the input prompt to explicitly incorporate the features that shift the responses from unfavorable to favorable. In this way, we construct 14K pairs of the original instruction and its optimized version to train a sequence-to-sequence model that optimizes user instructions.\n",
      "\n",
      "Our extensive experiments demonstrate that without LLM training, BPO can improve the alignment of both API-based and open-sourced LLMs remarkably: increasing win rates by 8.8% to 22.0% on gpt-3.5-turbo , gpt-4 , claude-2 , llama-2-chat , vicuna etc. Moreover, we show that BPO not only outperforms RLHF via\n",
      "\n",
      "PPO (Schulman et al., 2017) and DPO (Rafailov et al., 2023) but also further improves LLMs' alignment after these RLHF's training. We also show that BPO can align LLMs in supervised fine-tuning by optimizing response quality in the experiment of Alpaca. In addition, we have demonstrated the superiority of BPO over the direct use of LLM as a prompt engineer, highlighting the importance of incorporating human feedback.\n",
      "\n",
      "Our contributions can be summarized as follows:\n",
      "\n",
      "- We propose a novel prompt optimization method BPO, which enhances LLMs' alignment to human preferences without training these models, demonstrating improvements over a wide variety of LLMs, including APIbased and open-sourced ones.\n",
      "- We empirically justify that BPO is a novel and competitive alignment approach, in addition to existing RLHF and preference learning methods, outperforming PPO and DPO on extensive experiments. Moreover, we show that it is orthogonal to RLHF's alignment, which adds additional gain on top of conventional alignment pipelines.\n",
      "- We systematically analyze how BPO refines the original prompts from the perspectives of prompt explanation, clarification, enrichment, and safety enhancement. We demonstrate its better interpretability than existing preference learning algorithms when aligning LLMs.\n",
      "\n",
      "## 2 Related Work\n",
      "\n",
      "LLMs pre-trained on massive corpus can generate fluent text but are not well aligned to follow users' instructions. Therefore, aligning LLMs with human intents has become an important research problem. Existing efforts in alignment mostly follow the paradigm proposed by Ouyang et al. (2022), consisting of two main stages: SFT and RLHF.\n",
      "\n",
      "Supervised Fine-tuning (SFT). SFT alignment endows LLMs with preliminary instruction-following abilities. Nonetheless, it heavily relies on abundant high-quality fine-tuning data. Since the high cost of human-written data, self-instruct data augmentation (Wang et al., 2022) based on a small human-created seed set has become a predominant approach in academia (Taori et al., 2023; BELLEGroup, 2023). However, SFT alignment still suffers from hallucinations, inferior scalability, and poor understanding of human preference.\n",
      "\n",
      "Reinforcement Learning from Human Feedback\n",
      "\n",
      "Figure 2: BPO consists of three main steps: collecting feedback data (we adopt open-sourced feedback data), constructing prompt optimization pairs based on the feedback data, and building a prompt optimization model using these pairs. In this way, BPO serves as a translator between human and AI, by optimizing human prompts to be better suited for AI generation to get human-preferred responses, while treating the model itself as a black box.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "(RLHF). RLHF alignment is proposed to further align LLMs with scalable feedback. The standard framework (Stiennon et al., 2020; Ouyang et al., 2022) consists of reward modeling and policy training. Due to the significant cost of manual effort (Ouyang et al., 2022; Ji et al., 2024b), several studies have explored incorporating AI feedback and shown impressive results (Bai et al., 2022b; Lee et al., 2023). Moreover, considering the cumbersome procedures and unstable RL training, some works have sought other methods beyond RLHF to learn from preference feedback. Rafailov et al. (2023) introduces feedback into the design of the loss function. Furthermore, some studies also explore self-improvement (Yuan et al., 2024; Xu et al., 2024) and alignment of agents (Lai et al., 2024).\n",
      "\n",
      "Prompt Engineering and Prompt Tuning. Since the pre-trained language models are proposed, leveraging prompt tuning to accomplish NLP tasks has gradually become a new paradigm (Brown et al., 2020a; Liu et al., 2021). There are two main types of prompt tuning: hard and soft. Hard prompt tuning, or prompt engineering, often requires extensive manual effort. Therefore, many works explore how to automate this process, which can be traced back to AutoPrompt (Shin et al., 2020). Recently, with the advent of LLMs, utilizing language models for automated prompt engineering has demonstrated remarkable performance (Zhou et al., 2022; Yang et al., 2023; Pryzant et al., 2023; Pan et al.,\n",
      "\n",
      "2023; Li et al., 2024). However, existing methods primarily focus on specific tasks rather than alignment and require searching for each task. In addition, these methods necessitate optimization for an individual model, rendering them not universally applicable across all models, which further limits their usability. Soft prompt tuning (Liu et al., 2021; Lester et al., 2021; Li and Liang, 2021) further improves effectiveness by enabling optimization in the embedding space rather than limited token vocabulary, but it requires tuning of the model parameters, which is not as flexible as hard prompting.\n",
      "\n",
      "Prompt tuning and model training have been two parallel ways to improve pre-trained model performance. Current alignment strategies primarily focus on adjusting models to follow user intents and instructions, and few works have explored plugand-play alignment tools (Ji et al., 2024a). Under the context of LLMs, models have become huge and difficult to train or even obtain (e.g. API-based models). Therefore, we argue that prompt optimization desires its attention, and LLM alignment can also be achieved by optimizing the input prompt without modifying the LLMs.\n",
      "\n",
      "## 3 Black-Box Prompt Optimization\n",
      "\n",
      "The overall process of BPO is shown in Figure 2. BPO is to enhance the alignment between model output and human preference by optimizing the input prompt. To this end, we first collect several\n",
      "\n",
      "Table 1: Preference data statistics. We sampled prompts from open-sourced prompt datasets and filter them to form the preference training dataset.\n",
      "\n",
      "| Dataset       | Sampled   | Sampled      | Generating &Filtering   | Generating &Filtering   |\n",
      "|---------------|-----------|--------------|-------------------------|-------------------------|\n",
      "|               | Number    | Distinct-4 ↑ | Number                  | Distinct-4 ↑            |\n",
      "| OASST1        | 3000      | 0.953        | 2940                    | 0.963                   |\n",
      "| HH-RLHF       | 2000      | 0.957        | 1961                    | 0.957                   |\n",
      "| Chatbot Arena | 5000      | 0.804        | 4494                    | 0.899                   |\n",
      "| Alpaca-GPT4   | 5000      | 0.938        | 5000                    | 0.938                   |\n",
      "| Overall       | 15000     | 0.860        | 14395                   | 0.913                   |\n",
      "\n",
      "instruction-tuning datasets with human preference annotations, carefully curate and filter low-quality data. Subsequently, we employ an LLM to capture the difference between responses favored and disfavored by human, based on which we leverage the LLM to refine the input. We then get a pair of original instruction and its improved version, using which we further train a sequence-to-sequence model to automatically optimize user inputs.\n",
      "\n",
      "## 3.1 Task Definition\n",
      "\n",
      "As discussed above, our task is to optimize user input to help LLMs generate better responses. Formally, we denote user input as X user . Our goal is to build a function F that maps X user to its optimized version, denoted as X opt . In order to get this, we introduce annotated human preferences, as the preferred response indicates good model output, while the other one suggests inferior output. By capturing the differences between these preference data, we can incorporate the attributes human favor into user instructions to make them more aligned with what LLMs can do, thus bringing LLMs' outputs better into alignment with human preferences. Inspired by recent work utilizing LLMs as evaluators (Wang et al., 2023; Zheng et al., 2023), we believe that LLMs possess the capacity to understand different features within various responses. Consequently, we leverage LLMs to get X opt . Specifically, each sample is represented as ( X user , Y good , Y bad ) , where Y good stands for the favorable response and Y bad is for the unfavorable one. Thus, the prompt optimization process with LLM can be expressed as X opt = LLM ( X user , Y good , Y bad ) . Finally, we build the F function by training a smaller sequenceto-sequence model over the pairs of ( X user , X opt ) .\n",
      "\n",
      "## 3.2 Training Data Construction\n",
      "\n",
      "To construct the optimized prompts, we begin by collecting datasets with human preferences. In to- tal, we employ four instruction-tuning datasets with human preference annotations, as shown in Table 1. The detailed description of these datasets can be found in Appendix A. After collecting and reformatting these datasets, we carefully eliminate low-quality instances with manually crafted rules (e.g. too short instructions tend to be low quality) and use self-bleu to perform a strict diversity filtering. Finally, we get 14k diverse samples in the format of ( X user , Y good , Y bad ) . In this work, we mainly focus on single-turn response generation and leave the multi-turn setting for our future work.\n",
      "\n",
      "Subsequently, we leverage ChatGPT (OpenAI, 2022) to refine these instructions. After meticulous prompt engineering efforts, we employ two types of prompts for different data formats as illustrated in Appendix B. Then, we conduct quality filtering by rule-based methods to drop wrong optimizations (e.g., wrong format). Following the whole procedure, our dataset comprises about 14k pairs of instruction before and after optimization, with the final distribution shown in Table 1. The overall distinct score (Li et al., 2016) demonstrates the high diversity of our dataset.\n",
      "\n",
      "## 3.3 Model Training\n",
      "\n",
      "Based on the constructed dataset, we learn a small sequence-to-sequence model to automatically optimize user instruction. Formally, we generate X opt conditioned on the given input X user , where the loss function is specified as,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where N is the length of X opt and x t represents the t -th token in X opt . In this work, we choose to use llama2-7b-chat as the backbone model, as we believe a stronger model can learn the implicit preference mapping between X user and X opt better. Meanwhile, the number of parameters in a 7B model is small among LLMs, which can be more efficient for training and inference. And we leave the model scaling explorations to future work.\n",
      "\n",
      "## 3.4 Comparison with Existing Methods\n",
      "\n",
      "As shown in Table 2, BPO exhibits several preferred advantages compared to existing alignment methods. While the ultimate goal is to align LLMs' outputs with human preferences, RLHF (Ouyang\n",
      "\n",
      "Table 2: Comparison to RLHF (PPO), DPO, OPRO. BPO is free from training reward or policy models, and agnostic to any LLMs or tasks in application.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "| Method                      | Reward -free   | Policy -free   | LLM -agnostic   | Task -agnostic   |\n",
      "|-----------------------------|----------------|----------------|-----------------|------------------|\n",
      "| PPO (Ouyang et al., 2022)   |                |                |                 | ✓                |\n",
      "| DPO (Rafailov et al., 2023) | ✓              |                |                 | ✓                |\n",
      "| OPRO (Yang et al., 2023)    | ✓              | ✓              |                 |                  |\n",
      "| BPO (ours)                  | ✓              | ✓              | ✓               | ✓                |\n",
      "\n",
      "et al., 2022) and DPO (Rafailov et al., 2023) modify the LLMs' parameters to fit human preferences. However, BPO approaches this from the input side, optimizing user prompts to make them more model-friendly and thus improve the alignment of model outputs. In addition, since BPO does not change LLMs' parameters, it can be applied to APIbased models, whereas PPO and DPO are limited to white-box models. Compared to prompt engineering methods like OPRO, BPO is more general, as OPRO requires task-specific search to rewrite the prompts. Moreover, OPRO does not do samplelevel optimization: it uses the same learned prompt for all samples in each task, which can cause low stability. Furthermore, PPO, DPO, and OPRO only optimize specific LLMs, but BPO, once learned, is model-agnostic. As stated in section Section 3.1, we aim to learn a universal mapping from user prompts to optimized prompts following human preferences, which is achieved by incorporating multiple LLMs models' generations in the training data. The incorporation of human preferences allows BPO to outperform prompt optimization using LLM (e.g., ChatGPT) directly.\n",
      "\n",
      "## 4 Experiments\n",
      "\n",
      "To comprehensively showcase the capabilities of BPO, we have conducted extensive experiments encompassing diverse aspects, including alignment on black-box models, comparisons with existing feedback learning techniques (DPO &amp; PPO), SFT data quality enhancement capability, iterative improvement capability, comparisons with prompt engineering method (Appendix H), and ablation study on feedback. Implementation details can be found in Appendix C.\n",
      "\n",
      "## 4.1 Evaluation of Alignment\n",
      "\n",
      "As it remains a significant challenge to comprehensively evaluate a language model's alignment quality, in this work, we adopt the widely-used setting of employing strong LLMs to evaluate the model's performance on instruction-following datasets.\n",
      "\n",
      "Test Datasets In order to evaluate the quality of alignment more accurately, we selected multiple instruction datasets for assessment.\n",
      "\n",
      "- Dolly Eval is a subset of 200 instances randomly sampled from the dolly (Conover et al., 2023) dataset, which is human-generated and contains eight categories of tasks.\n",
      "- Vicuna Eval (Chiang et al., 2023) contains 80 diverse questions in 8 categories.\n",
      "- Self-Instruct Eval is the human evaluation dataset created by Wang et al. (2022), encompassing 252 expert-written user-oriented instructions motivated by real-world applications.\n",
      "- BPO-test Eval is a split of our dataset, containing 200 samples from the four datasets we used when constructing the training set.\n",
      "\n",
      "Evaluation Methods As existing studies (Wang et al., 2023; Zheng et al., 2023) demonstrated, strong LLMs can be good evaluators. Following Li et al. (2023), we use both GPT-4 (OpenAI, 2023) and Claude (Anthropic, 2023b) for evaluation and, we employ a pairwise scoring setup to intuitively show the alignment capability differences. The prompt for GPT-4 scoring is from MT-bench (Zheng et al., 2023), and the prompt for Claude scoring is from Alpaca Eval (Li et al., 2023), which can be found in Appendix D. In addition, to mitigate position bias and reduce the cost, we randomly shuffle the models' responses in each evaluation, which is also used in Alpaca Eval.\n",
      "\n",
      "## 4.2 Black-Box Alignment Results\n",
      "\n",
      "Detailed experiment results can be found in Table 3 and Table 4. Our method achieves a higher win rate on all datasets across all models with our optimized prompts vs. original prompts. Notably, on gpt-3.5-turbo and text-bison , the average win rates increase about 20%, and more 10% for several models including gpt-4 , demonstrating the strong performance of our approach. Moreover, consistent gains are achieved across models of varying capabilities, from smaller open-sourced models like llama2-7b-chat and vicuna-7b to powerful large-scale models like gpt-4 and claude-2 , highlighting BPO's robust generalization for various models. Additionally, across these four test sets, the most significant gain occurs on VicunaEval, where under the GPT-4's evaluation, many\n",
      "\n",
      "Table 3: Win rates between BPO-aligned and original LLM APIs, evaluated by gpt-4 (Cf. Table 8 for claude-v1.3 's evaluation). Without training these LLMs, BPO can significantly improve block-box LLM APIs' alignment. ('ori.' denotes 'original', and 'WR' denotes 'win rates').\n",
      "\n",
      "|                    | Method   | Method   | Vicuna Eval   | Vicuna Eval   | Self-instruct Eval   | Self-instruct Eval   | Self-instruct Eval   | Dolly Eval   | Dolly Eval   | Dolly Eval   | BPO-test Eval   | BPO-test Eval   | BPO-test Eval   |       |\n",
      "|--------------------|----------|----------|---------------|---------------|----------------------|----------------------|----------------------|--------------|--------------|--------------|-----------------|-----------------|-----------------|-------|\n",
      "| Base LLM           | A B      |          | A win         | tie B win     | A win                | tie                  | B win                | A win        | tie          | B win        | A win           | tie             | B win           | ∆ WR  |\n",
      "| gpt-3.5-turbo      | BPO      | ori.     | 60.0 8.7      | 31.3          | 50.4                 | 12.3                 | 37.3                 | 55.0         | 16.0         | 29.0         | 51.0            | 18.0            | 31.0            | +22.0 |\n",
      "| gpt-4              | BPO ori. | 41.3     | 23.7          | 35.0          | 39.7                 | 22.6                 | 37.7                 | 51.0         | 26.0         | 23.0         | 39.0            | 26.0            | 35.0            | +10.1 |\n",
      "| claude-instant-1.2 | BPO ori. | 66.3     | 5.0           | 28.7          | 50.0                 | 9.1                  | 40.9                 | 45.0         | 14.5         | 40.5         | 45.0            | 10.5            | 44.5            | +12.9 |\n",
      "| claude-2           | BPO ori. | 57.5     | 5.0           | 37.5          | 48.8                 | 12.7                 | 38.5                 | 44.5         | 13.0         | 42.5         | 45.0            | 13.0            | 42.0            | +8.8  |\n",
      "| text-bison         | BPO ori. | 65.0     | 10.0          | 25.0          | 47.0                 | 21.9                 | 31.1                 | 42.0         | 30.5         | 27.5         | 50.5            | 10.5            | 39.0            | +20.5 |\n",
      "\n",
      "Table 4: Win rates between BPO-aligned and original llama-2-chat and vicuna-v1.3 LLMs, evaluated by gpt-4 (Cf. Table 9 for claude-v1.3 's evaluation). Training-free BPO improves alignment substantially, even making llama-2-13b-chat outperform llama-2-70b-chat . ('WR' denotes 'win rates').\n",
      "\n",
      "| Base LLM      | Method    | Method   | Method   | Vicuna Eval   | Vicuna Eval   | Vicuna Eval   | Self-instruct Eval   | Self-instruct Eval   | Self-instruct Eval   | Dolly Eval   | Dolly Eval   | Dolly Eval   | BPO-test Eval   | BPO-test Eval   | BPO-test Eval   |       |\n",
      "|---------------|-----------|----------|----------|---------------|---------------|---------------|----------------------|----------------------|----------------------|--------------|--------------|--------------|-----------------|-----------------|-----------------|-------|\n",
      "| Base LLM      | A         | B        | A win    | tie           | B win         | A win         | tie                  | B win                | A                    | win          | tie B        | win          | A win           | tie             | B win           | ∆ WR  |\n",
      "| llama-2 -chat | 7B + BPO  | 7B       | 60.0     | 2.5           | 37.5          | 53.6          | 9.9                  | 36.5                 | 52.0                 | 9.5          | 38.5         |              | 53.0            | 10.5            | 36.5            | +17.4 |\n",
      "| llama-2 -chat | 13B + BPO | 13B      | 61.3     | 2.5           | 36.2          | 51.2          | 11.9                 | 36.9                 | 50.5                 | 13.5         | 36.0         |              | 53.0            | 12.5            | 34.5            | +18.1 |\n",
      "| llama-2 -chat | 7B + BPO  | 70B      | 48.8     | 3.7           | 47.5          | 40.1          | 5.1                  | 54.8                 | 49.0                 | 2.0          | 49.0         |              | 40.0            | 5.0             | 55.0            | -7.1  |\n",
      "| llama-2 -chat | 13B + BPO | 70B      | 61.3     | 0.0           | 38.7          | 48.4          | 4.8                  | 46.8                 | 54.0                 | 6.5          | 39.5         |              | 51.0            | 7.0             | 42.0            | +11.9 |\n",
      "| llama-2 -chat | 70B + BPO | 70B      | 59.3     | 5.5           | 35.2          | 46.0          | 13.1                 | 40.9                 | 51.0                 | 18.0         | 31.0         |              | 53.5            | 11.0            | 35.5            | +16.8 |\n",
      "| vicuna        | 7B + BPO  | 7B       | 65.0     | 8.7           | 26.3          | 42.0          | 21.1                 | 36.9                 | 47.0                 | 22.0         | 31.0         |              | 46.0            | 22.0            | 32.0            | +18.5 |\n",
      "| -v1.3         | 13B + BPO | 13B      | 52.5     | 3.7           | 43.8          | 46.4          | 13.9                 | 39.7                 | 52.0                 | 8.0          | 40.0         |              | 59.5            | 6.0             | 34.5            | +13.1 |\n",
      "\n",
      "BPO-aligned models achieve over 60%:40% preference ratio (20% win rate increase), with some even reaching 70%:30% win rates (40% win rate increase). This suggests that BPO can achieve greater alignment gain on open-ended instructions. BPO can significantly enhance the comprehensiveness of responses in these open-ended tasks (§5). However, the benefits of BPO are not limited to these tasks. In closed tasks within these evaluation sets, such as mathematics, reasoning, and coding, BPO also demonstrates excellent performance, achieving an average improvement in win rate of over 10%.\n",
      "\n",
      "Furthermore, we conduct a scaling experiment, as shown in Figure 7. We compare LLaMA2-chat models of varying sizes with our optimized instructions against the original llama2-70b-chat model. Remarkably, BPO boosts smaller model llama2-7b-chat to match or even outperform the 10x larger model on some datasets. And under Claude's evaluation, llama2-7b-chat with BPO alignment nearly reaches the performance of llama2-70b-chat . For the llama2-13b-chat model, BPO enables it to substantially surpass the 70b model, demonstrating the potential of BPO to boost smaller models beyond much larger ones.\n",
      "\n",
      "## 4.3 RLHF Results\n",
      "\n",
      "As shown in Table 5, PPO, DPO, and BPO all successfully improve the performance of vicuna-7b and vicuna-13b . Moreover, the SFT model with BPO outperforms PPO and DPO aligned models, which highlights BPO's advantage. As mentioned before, BPO is model-agnostic and can be applied to LLMs with different capabilities. Therefore, we investigate if BPO can be applied on top of RLHF methods, and our result is positive: both PPO and DPO in conjunction with BPO can be largely improved. With BPO alignment and DPO training, both vicuna-7b and vicuna-13b can achieve around 30% win rate increases.\n",
      "\n",
      "## 4.4 BPO for Data Augmentation\n",
      "\n",
      "BPO can also be applied to construct high-quality data by leveraging the optimized prompts to get high-quality responses. We validate its applicability on the Alpaca (Taori et al., 2023) dataset: we first optimize the original instructions with BPO and use these optimized instructions as inputs for text-davinci-003 to generate responses. This gives us a refined Alpaca dataset, and we train llama-7b and llama-13b with this new dataset. As shown in Table 6, the experiment results demonstrate substantial gains over LLMs trained on the original Alpaca dataset. Notably, on Vicuna Eval, llama-13b trained with 52k BPO reproduced data can achieve 93.8%:1.2% win rate against the one trained with the original dataset. Furthermore, using just 1k reproduced data, the trained model can surpass the original model, which is trained with\n",
      "\n",
      "| Method   | Method   | Vicuna Eval   | Vicuna Eval   | Vicuna Eval   | Self-instruct Eval   | Self-instruct Eval   | Self-instruct Eval   | Dolly Eval   | Dolly Eval   | Dolly Eval   | BPO-test Eval   | BPO-test Eval   | BPO-test Eval   | ∆ WR   |\n",
      "|----------|----------|---------------|---------------|---------------|----------------------|----------------------|----------------------|--------------|--------------|--------------|-----------------|-----------------|-----------------|--------|\n",
      "| A        | B        | A win         | tie           | B win         | A win                | tie                  | B win                | A win        | tie          | B win        | A win           | tie             | B win           | ∆ WR   |\n",
      "| PPO      | ori.     | 47.5          | 10.0          | 42.5          | 49.6                 | 10.3                 | 40.1                 | 46.0         | 13.9         | 38.5         | 42.0            | 19.5            | 36.0            | +7.0   |\n",
      "| BPO      | PPO      | 61.3          | 6.2           | 32.5          | 49.6                 | 11.9                 | 38.5                 | 49.0         | 12.5         | 41.5         | 47.5            | 13.0            | 39.5            | +13.8  |\n",
      "| BPO+PPO  | ori.     | 55.0          | 7.5           | 37.5          | 50.0                 | 10.3                 | 39.7                 | 52.5         | 9.0          | 38.5         | 54.5            | 10.0            | 35.5            | +15.2  |\n",
      "| BPO+PPO  | PPO      | 56.3          | 11.2          | 32.5          | 44.4                 | 20.7                 | 34.9                 | 43.0         | 29.0         | 28.0         | 44.0            | 23.0            | 33.0            | +14.8  |\n",
      "| DPO      | ori.     | 58.8          | 6.2           | 35.0          | 53.6                 | 11.5                 | 34.9                 | 50.0         | 19.0         | 31.0         | 51.0            | 18.0            | 31.0            | +20.4  |\n",
      "| BPO      | DPO      | 53.8          | 3.7           | 42.5          | 40.1                 | 8.3                  | 51.6                 | 45.0         | 10.0         | 45.0         | 45.0            | 11.0            | 44.0            | +0.2   |\n",
      "| BPO+DPO  | ori.     | 65.0          | 5.0           | 30.0          | 60.3                 | 10.7                 | 29.0                 | 54.0         | 17.0         | 29.0         | 56.0            | 13.0            | 31.0            | +29.1  |\n",
      "| BPO+DPO  | DPO      | 63.8          | 2.5           | 33.7          | 49.6                 | 9.9                  | 40.5                 | 46.0         | 14.0         | 40.0         | 45.0            | 16.0            | 39.0            | +12.8  |\n",
      "| PPO      | ori.     | 53.8          | 3.7           | 42.5          | 49.2                 | 11.1                 | 39.7                 | 49.0         | 14.5         | 36.5         | 42.0            | 17.5            | 40.5            | +8.7   |\n",
      "| BPO      | PPO      | 52.5          | 3.7           | 43.7          | 44.4                 | 6.4                  | 49.2                 | 50.0         | 9.0          | 41.0         | 53.5            | 11.5            | 35.0            | +7.9   |\n",
      "| BPO+PPO  | ori.     | 55.0          | 7.5           | 37.5          | 49.6                 | 9.9                  | 40.5                 | 54.0         | 11.0         | 35.0         | 55.5            | 11.5            | 33.0            | +17.0  |\n",
      "| BPO+PPO  | PPO      | 55.0          | 5.0           | 40.0          | 49.6                 | 5.6                  | 44.8                 | 49.5         | 9.5          | 41.0         | 55.0            | 11.0            | 34.0            | +12.3  |\n",
      "| DPO      | ori.     | 50.0          | 3.7           | 46.3          | 55.6                 | 6.3                  | 38.1                 | 58.5         | 6.5          | 35.0         | 58.0            | 11.5            | 30.5            | +18.1  |\n",
      "| BPO      | DPO      | 53.8          | 2.5           | 43.7          | 44.0                 | 8.4                  | 47.6                 | 45.0         | 5.0          | 50.0         | 43.0            | 16.0            | 41.0            | +0.9   |\n",
      "| BPO+DPO  | ori.     | 71.3          | 2.5           | 26.2          | 61.1                 | 7.2                  | 31.7                 | 58.0         | 9.0          | 33.0         | 62.0            | 8.0             | 30.0            | +32.9  |\n",
      "| BPO+DPO  | DPO      | 60.0          | 2.5           | 37.5          | 48.8                 | 9.1                  | 42.1                 | 48.0         | 8.5          | 43.5         | 50.0            | 11.0            | 39.0            | +11.2  |\n",
      "\n",
      "Table 5: Win rates between PPO, DPO, and BPO-aligned vicuna-v1.3 series LLMs, evaluated by gpt-4 (Cf. Table 10 for claude-v1.3 's evaluation). BPO not only outperforms both PPO and DPO, and could yield additional bonus over PPO and DPO-aligned LLMs. ('ori.' denotes 'original', and 'WR' denotes 'win rates').\n",
      "\n",
      "| Base LLM   | Method         | Method            | Vicuna Eval   | Vicuna Eval   | Vicuna Eval   | Self-instruct Eval   | Self-instruct Eval   | Self-instruct Eval   | Dolly Eval   | Dolly Eval   | Dolly Eval   | BPO-test Eval   | BPO-test Eval   | BPO-test Eval   | ∆ WR        |\n",
      "|------------|----------------|-------------------|---------------|---------------|---------------|----------------------|----------------------|----------------------|--------------|--------------|--------------|-----------------|-----------------|-----------------|-------------|\n",
      "| Base LLM   | A              | B                 | A win         | tie           | B win         | A win                | tie                  | B win                | A win        | tie          | B win        | A win           | tie             | B win           | ∆ WR        |\n",
      "| llama-7b   | BPO-1k BPO-52k | ori.-52k          | 72.5          | 10.0          | 17.5          | 45.2 47.2            | 14.7                 | 40.1                 | 57.0         | 13.0         | 30.0         | 44.5 50.0       | 13.5 20.0       | 42.0            | +22.4 +26.7 |\n",
      "|            |                | ori.-52k          | 75.0          | 7.5           | 17.5          | 55.2                 | 13.9                 | 38.9                 | 58.0         | 5.0          | 37.0         | 58.5            | 16.0            | 30.0            |             |\n",
      "| llama-13b  | BPO-1k BPO-52k | ori.-52k ori.-52k | 78.8 93.8     | 6.2 5.0       | 15.0 1.2      | 68.7                 | 10.7 8.3             | 34.1 23.0            | 56.5 56.0    | 15.0 12.0    | 28.5 32.0    | 67.0            | 19.0            | 25.5 14.0       | +36.5 +53.8 |\n",
      "\n",
      "Table 6: Win rates between BPO reproduced and original alpaca dataset tuned llama-1 series LLMs, evaluated by gpt-4 (Cf. Table 11 for claude-v1.3 's evaluation). -1k means training the LLM with 1k randomly sampled data, -52k means using the whole dataset. ('ori.' denotes 'original', and 'WR' denotes 'win rates').\n",
      "\n",
      "52k samples. These results underscore the importance of high-quality data and verify that BPO can assist in producing high-quality training data.\n",
      "\n",
      "## 4.5 Iterative Prompt Optimization\n",
      "\n",
      "Since BPO can optimize the user prompt for better response, a natural idea is whether we can iteratively improve a prompt, progressively enhancing an LLM's output. We thus conduct this experiment with gpt-3.5-turbo on the Vicuna Eval dataset. Specifically, we iteratively optimize the original instruction five times and compare the win rate against the original instruction. As shown in Figure 3, ∆ WR achieves noticeable improvement through four iterations, with a small decline on the fifth iteration. Appendix G presents a case study of a prompt after each optimization iteration. Furthermore, we also find that BPO exhibits good retention, which has a high probability of preserving the input prompt when it is already good enough. This, we believe, is a key factor in enabling iterative enhancement, as it avoids forcing unreasonable changes to the user's original intent.\n",
      "\n",
      "Figure 3: Difference of win rate and lose rate in each iteration (iteration 0 means the original) scored by gpt-4 and claude-v1.3 .\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## 4.6 Ablation Study\n",
      "\n",
      "One critical component of BPO is to leverage feedback to optimize user instructions. To investigate how much feedback contributes to BPO's prompt optimization, we conduct an ablation experiment to compare feedback-learned optimization (BPO) and directly using gpt-3.5-turbo for prompt optimization. As shown in Table 7, direct optimization can improve model performance, which val-\n",
      "\n",
      "Table 7: Win rates between BPO and directly using gpt-3.5-turbo for prompt optimization (w/o FDBK), evaluated by gpt-4 (Cf. Table 12 for claude-v1.3 's evaluation). While BPO largely improves model performance, w/o FDBK improves little. ('ori.' denotes 'original', and 'WR' denotes 'win rates', 'FDBK' denotes 'feedback').\n",
      "\n",
      "| Base LLM       | Method           | Method             | Vicuna Eval    | Vicuna Eval   | Vicuna Eval    | Self-instruct Eval   | Self-instruct Eval   | Self-instruct Eval   | Dolly Eval     | Dolly Eval     | Dolly Eval     | BPO-test Eval   | BPO-test Eval   | BPO-test Eval   | ∆ WR             |\n",
      "|----------------|------------------|--------------------|----------------|---------------|----------------|----------------------|----------------------|----------------------|----------------|----------------|----------------|-----------------|-----------------|-----------------|------------------|\n",
      "| Base LLM       | A                | B                  | A win          | tie           | B win          | A win                | tie                  | B win                | A win          | tie            | B win          | A win           | tie             | B win           | ∆ WR             |\n",
      "| gpt-3.5 -turbo | BPO w/o FDBK BPO | ori. ori. w/o FDBK | 60.0 58.8 52.5 | 8.7 8.7 6.2   | 31.3 32.5 41.3 | 50.4 36.9 57.9       | 12.3 7.5 5.6         | 37.3 55.6 36.5       | 55.0 43.5 52.0 | 16.0 16.0 16.0 | 29.0 40.5 32.0 | 51.0 46.0 49.0  | 18.0 16.0 13.0  | 31.0 38.0 38.0  | +22.0 +4.6 +15.9 |\n",
      "\n",
      "Figure 4: BPO Optimization types and examples. Due to space limitations, we omit some examples and refer to Figure 11 for the complete results.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "idates the potential for LLMs to be good prompt engineers. BPO provides further improvements beyond direct optimization. The results suggest that incorporating feedback allows LLMs to refine prompts in line with demonstrated user preferences, enabling more effective prompt optimization.\n",
      "\n",
      "## 5 Interpretability of BPO\n",
      "\n",
      "Compared with model-training-based alignment methods like PPO or DPO, BPO has a distinct advantage in its strong interpretability, as we can directly compare the instructions before and after optimization to find out how BPO works. To examine what BPO optimizes in detail, we closely examined 500 samples and summarized some common patterns in its optimization and error types.\n",
      "\n",
      "As shown in Figure 4, we summarize four common optimization strategies exhibited in BPO's results, including Explanation Generation (green box), Prompt Elaboration (orange box), Providing Hint (blue box) and Safety Enhancement (pink box). We should note that there are also other optimization strategies observed in BPO's output, and those strategies are not mutually exclusive. These presented examples are only typical instances in these four categories.\n",
      "\n",
      "- Explanation Generation is a common way that BPO employs to instruct LLMs to generate rea-\n",
      "\n",
      "soning steps or detailed explanations, which helps to form a more logical and understandable response.\n",
      "\n",
      "- Prompt Elaboration includes various methods to help models better understand user intentions and generate comprehensive responses, as users often give unclear, over-concise instructions and even with errors.\n",
      "- Providing Hint adds specific hints to the user's prompt. For instance, BPO adds key points to be addressed or elucidates relevant knowledge to assist models in better organizing answers.\n",
      "- Safety Enhancement is critical in alignment. When user inputs could potentially raise security issues, BPO emphasizes maintaining harmless responses. Moreover, BPO enables interpretable security enhancements, as it can refine the unsafe request to require the model to output relevant harmless advice. In this way, we can better prevent safety issues while still keeping responses helpful.\n",
      "\n",
      "Error analysis is shown in Appendix I.\n",
      "\n",
      "## 6 Conclusion\n",
      "\n",
      "In this work, we present BPO, a black-box alignment method that automatically optimizes user inputs to better suit LLMs' preference for improved responses. With BPO alignment, we successfully improve the alignment of LLMs without further adjusting these models, leading to significant results even on the most powerful models like GPT-4 and Claude-2. Moreover, extensive experiments show that BPO can reach or surpass the performance of current mainstream alignment techniques on Vicuna models and further improve these alignment methods. Our findings demonstrate that tailoring inputs to best suit LLMs is a promising technical direction to obtain interpretable and controllable alignment in parallel to existing model-trainingbased solutions, and there is still great room to further explore in depth.\n",
      "\n",
      "## Limitations\n",
      "\n",
      "Despite BPO's effectiveness and strong potential for wider applications, we want to discuss some known limitations of this work, which require further research and efforts to improve.\n",
      "\n",
      "Require more data and training. Though we show that BPO can effectively improve alignment on established benchmarks including Vicuna Eval (Chiang et al., 2023), Self-Instruct Eval (Wang et al., 2022), and our sampled Dolly Eval (Conover et al., 2023), BPO-test Eval, our prompt preference optimizer is only trained on 14k pairs of optimized prompts deriving from the combination of few existing academic feedback datasets. It covers a limited spectrum of scenarios and has not been trained on large amounts of data yet. Thus, the currently released optimizer may not be as good as expected for very general usage.\n",
      "\n",
      "Adaptation to long-context and math-related inputs. Another thing we notice is that due to the few academic feedback datasets we adopt, there is an imbalance in the prompt's topic distribution and length. One is the lack of long-context prompts. Take the summarization task as an example; due to the lack of related training data, our prompt optimizer tends to alter the instructional prompt as well as the original passage for summarization (which should not be changed). Another case is mathrelated problems. Currently, our prompt optimizer seems to fail to learn how to change their inputs for better performance. We believe such a problem could be improved if we pay more attention to related topics in the dataset construction.\n",
      "\n",
      "## Ethical Considerations\n",
      "\n",
      "In this work, we leveraged several available datasets for training BPO. The OASST1 (Köpf et al., 2023) dataset is under Apache license; the HH-RLHF (Bai et al., 2022a) dataset is under MIT license; Chatbot Arena Conversations (Zheng et al., 2023) dataset and Alpaca-GPT4 (Peng et al., 2023) dataset is under Creative Commons license. In these datasets, there exists some instructions with security issues. However, in BPO training, we constructed optimized prompt pairs that provide safety enhancements to these unsafe instructions, further mitigating the security issues.\n",
      "\n",
      "## ACKNOWLEDGEMENT\n",
      "\n",
      "This work was supported by the National Key Research and Development Program of China (No. 2021ZD0113304). This work was supported by the National Science Foundation for Distinguished Young Scholars (with No. 62125604). This work was supported by the NSFC projects (with No. 62306160). This work was also supported by China National Postdoctoral Program for Innovative Talents (No. BX20230194) and China Postdoctoral Science Foundation (No. 2023M731952). We would also like to thank Zhipu AI for sponsoring GPU computing and API cost consumed in this study.\n",
      "\n",
      "## References\n",
      "\n",
      "Anthropic. 2023a. Claude 2.\n",
      "\n",
      "Anthropic. 2023b. Introducing claude.\n",
      "\n",
      "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 .\n",
      "\n",
      "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022b. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073 .\n",
      "\n",
      "BELLEGroup. 2023. Belle: Be everyone's large language model engine. https://github.com/ LianjiaTech/BELLE .\n",
      "\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020a. Language models are few-shot learners. Advances in neural information processing systems , 33:1877-1901.\n",
      "\n",
      "- Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020b. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems , NIPS'20, Red Hook, NY, USA. Curran Associates Inc.\n",
      "- Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality.\n",
      "- Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 .\n",
      "- Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023. Free dolly: Introducing the world's first truly open instructiontuned llm.\n",
      "- Iason Gabriel. 2020. Artificial intelligence, values, and alignment. Minds and machines , 30(3):411-437.\n",
      "- Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, and Yaodong Yang. 2024a. Aligner: Achieving efficient alignment through weak-to-strong correction. arXiv preprint arXiv:2402.02416 .\n",
      "- Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 2024b. Beavertails: Towards improved safety alignment of llm via a humanpreference dataset. Advances in Neural Information Processing Systems , 36.\n",
      "- Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et al. 2023. Ai alignment: A comprehensive survey. arXiv preprint arXiv:2310.19852 .\n",
      "- Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, et al. 2023. Openassistant conversations-democratizing large language model alignment. arXiv preprint arXiv:2304.07327 .\n",
      "- Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang,\n",
      "- Xiaohan Zhang, Yuxiao Dong, et al. 2024. Autowebglm: Bootstrap and reinforce a large language model-based web navigating agent. arXiv preprint arXiv:2404.03648 .\n",
      "- Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. 2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267 .\n",
      "- Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691 .\n",
      "- Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and William B Dolan. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 110-119.\n",
      "- Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 45824597.\n",
      "- Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca\\_eval .\n",
      "- Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, and Xifeng Yan. 2024. Guiding large language models via directional stimulus prompting. Advances in Neural Information Processing Systems , 36.\n",
      "- Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt understands, too. arXiv:2103.10385 .\n",
      "- Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 .\n",
      "- OpenAI. 2022. Introducing chatgpt.\n",
      "- OpenAI. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774 .\n",
      "- Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35:27730-27744.\n",
      "- Rui Pan, Shuo Xing, Shizhe Diao, Xiang Liu, Kashun Shum, Jipeng Zhang, and Tong Zhang. 2023. Plum: Prompt learning using metaheuristic. arXiv preprint arXiv:2311.08364 .\n",
      "- Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277 .\n",
      "- Reid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang Zhu, and Michael Zeng. 2023. Automatic prompt optimization with 'gradient descent' and beam search. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 7957-7968.\n",
      "- Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290 .\n",
      "- Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining , pages 3505-3506.\n",
      "- John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 .\n",
      "- Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 4222-4235.\n",
      "- Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. Advances in Neural Information Processing Systems , 33:30083021.\n",
      "- Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford\\_alpaca .\n",
      "- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 .\n",
      "- Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. 2023. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087 .\n",
      "- Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560 .\n",
      "- Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pages 38-45, Online. Association for Computational Linguistics.\n",
      "- Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang, Zihan Wang, Aohan Zeng, Zhengxiao Du, Wenyi Zhao, et al. 2024. Chatglmmath: Improving math problem-solving in large language models with a self-critique pipeline. arXiv preprint arXiv:2404.02893 .\n",
      "- Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. 2023. Large language models as optimizers. arXiv preprint arXiv:2309.03409 .\n",
      "- Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, et al. 2023. Deepspeedchat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales. arXiv preprint arXiv:2308.01320 .\n",
      "- Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024. Self-rewarding language models. arXiv preprint arXiv:2401.10020 .\n",
      "- Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 .\n",
      "- Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 .\n",
      "- Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685 .\n",
      "- Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910 .\n",
      "\n",
      "## A Datasets for Traning\n",
      "\n",
      "The training data construction includes four preference-annotated datasets.\n",
      "\n",
      "- The OASST1 (Köpf et al., 2023) dataset is a crowd-sourced instruction dataset with human-annotated response quality ratings. Under each instruction, we choose the response with the highest score as the good response and the one with the lowest score as the bad response.\n",
      "- The HH-RLHF (Bai et al., 2022a) dataset contains human preference over the responses' helpfulness and harmfulness.\n",
      "- The Chatbot Arena Conversations (Zheng et al., 2023) dataset is collected from human on the Chatbot Arena leaderboard 1 platform.\n",
      "- In addition, we use the comparison data subset of the Alpaca-GPT4 (Peng et al., 2023) dataset, where the preference is generated by GPT4 (OpenAI, 2023). To ensure data quality, we only keep samples where gpt-4 outperforms text-davinci-003 .\n",
      "\n",
      "## B Data Construction Prompts\n",
      "\n",
      "Since our data construction process involves four datasets and the data formats are not the same, we design two prompts to construct the optimized prompts as shown in Figure 5. For OASST1, HHRLHF, and Chatbot Arena Conversations, we adopt the prompt without context; for Alpaca-GPT4, we adopt the prompt with context.\n",
      "\n",
      "## C Implementation Details\n",
      "\n",
      "For BPO, we use Llama-2-7b-chat-hf 2 as backbone model, trained for three epochs on our dataset. And we simply take the final checkpoint. In the training stage, we utilize AdamW (Loshchilov and Hutter, 2017) optimizer with β 1 = 0 . 9 and β 2 = 0 . 999 . We set the learning rate to 2e-5, with 0.1 ratio warm-up steps and linear decay. The training batch size is 4 per GPU, and we leverage Huggingface Transformers (Wolf et al., 2020) and DeepSpeed (Rasley et al., 2020) framework for the Zero-2 strategy. For the RLHF training, we employed the\n",
      "\n",
      "1\n",
      "\n",
      "https://huggingface.co/spaces/lmsys/\n",
      "\n",
      "chatbot-arena-leaderboard\n",
      "\n",
      "2\n",
      "\n",
      "https://huggingface.co/meta-llama/\n",
      "\n",
      "DeepSpeed-Chat (Yao et al., 2023) framework, running just one epoch for reward model learning and PPO optimization as recommended. Our reward model achieves 80% accuracy on the in-distribution test set. The 16k data for PPO optimization is also from the combined OASST1 (Köpf et al., 2023), HH-RLHF (Bai et al., 2022a), Chatbot Area Conversations (Zheng et al., 2023) and Alpaca-GPT4 (Peng et al., 2023). All experiments are conducted on 8 × 80GB NVIDIA A800 GPUs. BPO adopts Top-p 0.9 and temperature 0.6 for decoding, while all tested LLMs use the default decoding strategies. In LLM-based evaluation, we set the temperature to 0.\n",
      "\n",
      "## D Evaluation Prompts\n",
      "\n",
      "As existing works demonstrated (Zheng et al., 2023; Li et al., 2023), strong LLMs can be good evaluators and show high consistency with human. Therefore we adopt gpt-4 and claude-v1.3 for evaluation, evaluation prompt for gpt-4 is from MT-bench (Zheng et al., 2023), and the one for claude-v1.3 is from Alpaca Eval (Li et al., 2023), as shown in Figure 6.\n",
      "\n",
      "## E Model Scaling Experiments\n",
      "\n",
      "As shown in Figure 7, BPO-aligned llama2-13b-chat model outperforms the 70b version, and this shows the great potential of BPO to boost smaller LLMs to surpass much larger ones.\n",
      "\n",
      "## F Experimental Results of Claude Evaluation\n",
      "\n",
      "As shown in Table 8 and Table 9, the evaluation results of claude-v1.3 are consistent with the results of gpt-4 . For each model with vs. without BPO alignment, BPO-aligned model shows better performance on all test sets. For the scaling setting ( llama-2-chat series with BPO alignment vs. llama-2-70b-chat ), BPO-aligned llama-2-7b-chat nearly achieves the same performance as 10x larger llama-2-70b-chat , and BPO-aligned 13b version can surpass llama-2-70b-chat .\n",
      "\n",
      "Table 10 shows the results compared to RLHF through PPO and DPO. BPO outperforms both PPO and DPO and can further improve the PPO or DPO aligned models. For both vicuna-7b and vicuna-13b , BPO with DPO achieves over 20% win rate increases.\n",
      "\n",
      "Figure 5: Our data construction prompt for dataset with (like Alpaca) or without context (like Chatbot Area Conversations).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Claude Pairwise Scoring Prompt\n",
      "\n",
      "```\n",
      "Human: I want you to create a leaderboard of different of large-language models. To do so, I will give you the instructions (prompts) given to the models, and the responses of two models. Please rank the models based on which responses would be preferred by humans. All inputs and outputs should be python dictionaries. Here is the prompt: { \"instruction\": \"\"\"{instruction}\"\"\", } Here are the outputs of the models: [ { \"model\": \"model_1\", \"answer\": \"\"\"{output_1}\"\"\" }, { \"model\": \"model_2\", \"answer\": \"\"\"{output_2}\"\"\" } ] Now please rank the models by the quality of their answers, so that the model with rank 1 has the best output. Then return a list of the model names and ranks, i.e., produce the following output: [ {'model': <model-name>, 'rank': <model-rank>}, {'model': <model-name>, 'rank': <model-rank>} ] Your response must be a valid Python dictionary and should contain nothing else because we will directly execute it in Python. Please provide the ranking that the majority of humans would give. Assistant:\n",
      "```\n",
      "\n",
      "Figure 6: Pairwise scoring prompt for gpt-4 and claude-v1.3 .\n",
      "\n",
      "|                    | Method   | Vicuna Eval   | Vicuna Eval   | Self-inst. Eval   | Self-inst. Eval   | Dolly Eval   | Dolly Eval   | BPO-test Eval   | BPO-test Eval   | ∆ WR   |\n",
      "|--------------------|----------|---------------|---------------|-------------------|-------------------|--------------|--------------|-----------------|-----------------|--------|\n",
      "| Base LLM           | A B      | A win         | B win         | A win             | B win             | A win        | B win        | A win           | B win           | ∆ WR   |\n",
      "| gpt-3.5-turbo      | BPO ori. | 63.8          | 36.2          | 56.3              | 43.7              | 60.0         | 40.0         | 58.5            | 41.5            | +19.3  |\n",
      "| gpt-4              | BPO ori. | 53.8          | 46.2          | 51.2              | 48.8              | 62.0         | 38.0         | 51.5            | 48.5            | +9.2   |\n",
      "| claude-instant-1.2 | BPO ori. | 56.3          | 43.7          | 56.7              | 43.3              | 51.5         | 48.5         | 52.5            | 47.5            | +8.5   |\n",
      "| claude-2           | BPO ori. | 60.0          | 40.0          | 51.6              | 48.4              | 50.5         | 49.5         | 52.0            | 48.0            | +7.1   |\n",
      "| text-bison         | BPO ori. | 58.8          | 41.2          | 56.3              | 43.7              | 60.5         | 39.5         | 53.0            | 47.0            | +14.3  |\n",
      "\n",
      "Table 8: Win rates between BPO-aligned and original LLM APIs, evaluated by claude-v1.3 . Without training these LLMs, BPO can significantly improve block-box LLM APIs' alignment. ('Self-inst.' denotes 'Self-instruct', 'ori.' denotes 'original', and 'WR' denotes 'win rates').\n",
      "\n",
      "Table 9: Win rates between BPO-aligned and original llama-2-chat and vicuna-v1.3 LLMs, evaluated by claude-v1.3 . Training-free BPO improves alignment substantially, even making llama-2-13b-chat outperform llama-2-70b-chat . ('Self-inst.' denotes 'Self-instruct, and 'WR' denotes 'win rates').\n",
      "\n",
      "| Base LLM      | Method    | Method   | Vicuna Eval    | Vicuna Eval              | Self-inst. Eval   | Self-inst. Eval   | Dolly Eval               | Dolly Eval   | BPO-test Eval   | BPO-test Eval   | ∆ WR                  |\n",
      "|---------------|-----------|----------|----------------|--------------------------|-------------------|-------------------|--------------------------|--------------|-----------------|-----------------|-----------------------|\n",
      "| Base LLM      | A         | B        | A win          | B win                    | A win             | B win             | A win                    | B win        | A win           | B win           | ∆ WR                  |\n",
      "| llama-2 -chat | 7B + BPO  | 7B       | 55.0 52.5 48.8 | 45.0 47.5 51.2 53.7 47.5 | 52.0              | 48.0 43.7 52.0    | 56.0 57.0 51.0 62.0 56.0 | 44.0 43.0    | 58.0            | 42.0            | +10.5 +11.7 -0.6 +8.7 |\n",
      "| llama-2 -chat | 13B + BPO | 13B      |                |                          | 56.3              |                   |                          |              | 57.5            | 42.5            |                       |\n",
      "| llama-2 -chat | 7B + BPO  | 70B      |                |                          | 48.0              |                   |                          | 49.0         | 51.0            | 49.0            |                       |\n",
      "| llama-2 -chat | 13B + BPO | 70B      | 46.3           |                          | 55.6              | 44.4              |                          | 38.0         | 53.5            | 46.5            |                       |\n",
      "| llama-2 -chat | 70B + BPO | 70B      | 52.5           |                          | 52.4              | 47.6              |                          | 44.0         | 52.5            | 47.5            | +6.7                  |\n",
      "| vicuna        | 7B + BPO  | 7B       | 65.0           | 35.0                     | 56.7              | 43.3              | 54.0                     | 46.0         | 53.0            | 47.0            | +14.4                 |\n",
      "| -v1.3         | 13B + BPO | 13B      | 57.5           | 42.5                     | 54.0              | 46.0              | 56.5                     | 43.5         | 57.5            | 42.5            | +12.8                 |\n",
      "\n",
      "Table 10: Win rates between PPO, DPO, and BPO-aligned vicuna-v1.3 series LLMs, evaluated by claude-v1.3 . BPO not only outperforms both PPO and DPO, and could yield additional bonus over PPO and DPO-aligned LLMs. ('Self-inst.' denotes 'Self-instruct', 'ori.' denotes 'original', and 'WR' denotes 'win rates').\n",
      "\n",
      "| Method   | Method   | Vicuna Eval   | Vicuna Eval   | Self-inst. Eval   | Self-inst. Eval   | Dolly Eval   | Dolly Eval   | BPO-test Eval   | BPO-test Eval   | ∆ WR   |\n",
      "|----------|----------|---------------|---------------|-------------------|-------------------|--------------|--------------|-----------------|-----------------|--------|\n",
      "| A        | B        | A win         | B win         | A win             | B win             | A win        | B win        | A win           | B win           | ∆ WR   |\n",
      "| PPO      | ori.     | 53.8          | 46.2          | 48.8              | 51.2              | 52.5         | 47.5         | 52.5            | 47.5            | +3.8   |\n",
      "| BPO      | PPO      | 53.8          | 46.2          | 54.8              | 45.2              | 52.0         | 48.0         | 51.5            | 48.5            | +6.0   |\n",
      "| BPO+PPO  | ori.     | 57.5          | 42.5          | 51.2              | 48.8              | 57.5         | 42.5         | 56.5            | 43.5            | +11.4  |\n",
      "| BPO+PPO  | PPO      | 53.8          | 46.2          | 55.2              | 44.8              | 52.5         | 47.5         | 52.0            | 48.0            | +6.7   |\n",
      "| DPO      | ori.     | 53.8          | 46.2          | 54.8              | 45.2              | 55.0         | 45.0         | 58.0            | 42.0            | +10.8  |\n",
      "| BPO      | DPO      | 51.3          | 48.7          | 49.2              | 50.8              | 52.0         | 48.0         | 50.0            | 50.0            | +1.2   |\n",
      "| BPO+DPO  | ori.     | 62.5          | 37.5          | 62.3              | 37.7              | 57.5         | 42.5         | 62.0            | 38.0            | +22.2  |\n",
      "| BPO+DPO  | DPO      | 56.3          | 43.7          | 52.4              | 47.6              | 52.5         | 47.5         | 60.0            | 40.0            | +10.6  |\n",
      "| PPO      | ori.     | 47.5          | 52.5          | 55.2              | 44.8              | 61.5         | 38.5         | 51.0            | 49.0            | +7.6   |\n",
      "| BPO      | PPO      | 52.5          | 47.5          | 52.0              | 48.0              | 58.0         | 42.0         | 55.5            | 44.5            | +9.0   |\n",
      "| BPO+PPO  | ori.     | 57.5          | 42.5          | 60.3              | 39.7              | 62.0         | 38.0         | 57.5            | 42.5            | +18.7  |\n",
      "| BPO+PPO  | PPO      | 51.3          | 48.7          | 52.8              | 47.2              | 58.0         | 42.0         | 53.5            | 46.5            | +7.8   |\n",
      "| DPO      | ori.     | 48.8          | 51.2          | 54.0              | 46.0              | 58.0         | 42.0         | 58.0            | 42.0            | +9.4   |\n",
      "| BPO      | DPO      | 55.0          | 45.0          | 48.8              | 51.2              | 49.0         | 51.0         | 50.0            | 50.0            | +1.4   |\n",
      "| BPO+DPO  | ori.     | 57.5          | 42.5          | 60.7              | 39.3              | 60.5         | 39.5         | 62.0            | 38.0            | +20.4  |\n",
      "| BPO+DPO  | DPO      | 63.8          | 36.2          | 56.7              | 43.3              | 53.5         | 46.5         | 54.0            | 46.0            | +14.0  |\n",
      "\n",
      "| Base LLM   | Method   | Method            | Vicuna Eval   | Vicuna Eval   | Self-inst. Eval   | Self-inst. Eval   | Dolly Eval   | Dolly Eval   | BPO-test Eval   | BPO-test Eval   | ∆ WR        |\n",
      "|------------|----------|-------------------|---------------|---------------|-------------------|-------------------|--------------|--------------|-----------------|-----------------|-------------|\n",
      "| Base LLM   | A        | B                 | A win         | B win         | A win             | B win             | A win        | B win        | A win           | B win           | ∆ WR        |\n",
      "| llama-7b   | BPO-1k   | ori.-52k ori.-52k | 72.5          | 27.5          | 52.4              | 47.6              | 58.5         | 41.5         | 54.5            | 45.5            | +19.0 +22.2 |\n",
      "|            | BPO-52k  |                   | 76.3          | 23.7          | 53.2              | 46.8              | 57.0         | 43.0         | 58.0            | 42.0            |             |\n",
      "| llama-13b  | BPO-1k   | ori.-52k          | 77.5          | 22.5          | 61.1              | 38.9              | 61.5         | 38.5         | 64.0            | 36.0            | +32.1 +41.1 |\n",
      "|            | BPO-52k  | ori.-52k          | 86.3          | 13.7          | 69.0              | 31.0              | 57.5         | 42.5         | 69.5            | 30.5            |             |\n",
      "\n",
      "Table 11: Win rates between BPO reproduced and original alpaca dataset tuned llama-1 series LLMs, evaluated by claude-v1.3 . -1k means training the LLM with 1k randomly sampled data, -52k means using the whole dataset. ('Self-inst.' denotes 'Self-instruct, 'ori.' denotes 'original', and 'WR' denotes 'win rates').\n",
      "\n",
      "Table 12: Win rates between BPO optimization and directly using gpt-3.5-turbo for prompt optimization (w/o feedback), evaluated by claude-v1.3 . While using BPO can largely improve model performance, w/o feedback has little improvement. ('Self-inst.' denotes 'Self-instruct, 'ori.' denotes 'original', and 'WR' denotes 'win rates').\n",
      "\n",
      "|               | Method       | Method       | Method   | Vicuna Eval Self-inst.   | Vicuna Eval Self-inst.   | Eval   | Eval   | Dolly Eval   | Dolly Eval   | BPO-test Eval   | BPO-test Eval   | ∆ WR   |\n",
      "|---------------|--------------|--------------|----------|--------------------------|--------------------------|--------|--------|--------------|--------------|-----------------|-----------------|--------|\n",
      "|               | A            | B            | A win    | B win                    | A win                    | B win  | A win  | B win        | win          | A               | B win           | ∆ WR   |\n",
      "|               | BPO          | ori.         | 63.8     | 36.2                     | 56.3                     | 43.7   | 60.0   | 40.0         | 58.5         |                 | 41.5            | +19.3  |\n",
      "| gpt-3.5-turbo | w/o feedback | ori.         | 57.5     | 42.5                     | 44.4                     | 52.6   | 52.0   | 48.0         | 57.5         |                 | 42.5            | +6.5   |\n",
      "| gpt-3.5-turbo | BPO          | w/o feedback | 55.0     | 45.0                     | 53.6                     | 43.7   | 63.5   | 36.5         |              | 59.0            | 41.0            | +16.2  |\n",
      "\n",
      "Figure 7: Difference of win-lose rate of various versions of LLaMA-2-chat with BPO alignment v.s. LLaMA-2chat-70B scored by gpt-4 and claude-v1.3 .\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "The result of BPO for SFT data construction is shown in Table 11. Fine-tuning with BPO reproduced Alpaca dataset can largely enhance the alignment performance, with more than 40% win rate increase on llama-13b .\n",
      "\n",
      "As shown in Table 12, feedback is a critical component in BPO alignment. Optimization without feedback may bring a decline in some datasets, while BPO achieves significant gains on each test set.\n",
      "\n",
      "## G Iterative Prompt Optimization\n",
      "\n",
      "To show how the prompts are iteratively optimized, we cherry-pick an example in Figure 8. Comparing iteration 5 with the original prompt, we can see that the optimized prompt is more specific and complete, containing more possible scenarios about the question, which can prompt the LLM to give a more comprehensive and well-considered response.\n",
      "\n",
      "## H OPRO Experiments\n",
      "\n",
      "We compare BPO with one of the most recent prompt engineering methods, OPRO (Yang et al., 2023). OPRO, like other existing automated prompt engineering methods, requires a training dataset to perform its search for improved prompts; we sample 250 examples from each category of the Dolly (Conover et al., 2023) dataset, totaling 2000 instances. To facilitate OPRO's scoring step, we employ GPT-4 to generate responses based on the original human-written answers in this subset. Specially, we perform OPRO over 200 samples in each category, holding out 50 as the test set. Both scoring and the generation model used gpt-3.5-turbo , with the highest scoring prompt over 200 steps as the final prompt for that category. Leveraging the reproduced Dolly dataset, we adopt reference-based evaluation with gpt-4 . The scoring prompt is from (Zheng et al., 2023), shown in Figure 9. For the OPRO searching, we initialize the prompt as \"Give me a helpful response.\" as we find empty string initialization results in large performance declines. We should note BPO does not use any instances from the Dolly dataset for training,\n",
      "\n",
      "Figure 8: An example of iterative optimization. The refined parts are marked as red in each iteration compared with the last iteration.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "which also indicates BPO's better applicability in new tasks without the need for specific searching like OPRO.\n",
      "\n",
      "As shown in Figure 10, BPO achieves stable improvements across most categories, while OPRO degrades compared to the original performance on more than half the tasks with an average negative improvement across all tasks. In addition, BPO shows noticeable gains on General QA, which is an open-ended, topically diverse task, while OPRO exhibits largely performance declines. Our conjecture is that BPO performs sample-specific optimization and thus provides more tailored enhancement, while OPRO or other prompt engineering methods are task-specific and thus may be hurting the performance of some samples, which may also be one of the reasons why these methods are mostly un- stable. After looking into the optimized prompts, we find the large drop is indeed caused by adopting the same prompt for all samples in one task. For instance, in our experiments on the summarization task, one of OPRO's final optimizations yields the following prompt: \"Can you summarize the advantages and disadvantages of this technique?\" which clearly converges to a specific topic, leading to an obvious performance loss on many samples.\n",
      "\n",
      "## I Error Analysis\n",
      "\n",
      "Another advantage of strong interpretability is the ability to facilitate error analysis since iterative improvements can be made quickly from optimization failures. As shown in Figure 11, we present three illustrative examples of common errors (grey box). Error case 1 is over-specification, where the user's\n",
      "\n",
      "Figure 9: Reference-based evaluation prompt for gpt-4 .\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Figure 10: Differences in GPT-4 scores after optimization with OPRO and BPO compared to the original. In contrast to OPRO, BPO demonstrates consistent gains across nearly all tasks, whereas OPRO exhibits performance declines on over half of the tasks with an average negative improvement. For both BPO and OPRO, we run three times and calculate the average scores.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "instruction only provides general topics, but BPO turns the prompt into more specific ones. Such overspecification limits the LLM's output too much. Error case 2 shows an inconsistency between the original instruction and the optimized one. We trace this back to low-quality training data, where the response is inconsistent with the constraints in the original instruction but still annotated as the favor one. In error case 3, BPO neglects the additional context, making the instruction under-specified.\n",
      "\n",
      "Figure 11: BPO Optimization types and examples (above the line), as well as error cases (below the line).\n",
      "\n",
      "<!-- image -->\n",
      "Document 4:\n",
      "## Effective scalar-tensor description of regularized Lovelock gravity in four dimensions\n",
      "\n",
      "Tsutomu Kobayashi 1, ∗\n",
      "\n",
      "1 Department of Physics, Rikkyo University, Toshima, Tokyo 171-8501, Japan\n",
      "\n",
      "We reformulate the recently proposed regularized version of Lovelock gravity in four dimensions as a scalar-tensor theory. By promoting the warp factor of the internal space to a scalar degree of freedom by means of Kaluza-Klein reduction, we show that regularized Lovelock gravity can be described effectively by a certain subclass of the Horndeski theory. Cosmological aspects of this particular scalar-tensor theory are studied. It is found that the background with a scalar charge is generically allowed. The consequences of this scalar charge are briefly discussed.\n",
      "\n",
      "## I. INTRODUCTION\n",
      "\n",
      "Lovelock gravity [1] is the most general metric theory of gravity in higher dimensions retaining the second-order nature of field equations for the metric. The action for Lovelock gravity in D dimensions is given by\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where α p is the coupling constant and\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "The first three terms are written more explicitly as\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "In D = 4 dimensions, the Lovelock Lagrangian uniquely reduces to the Einstein-Hilbert term ( L 1 ) plus a cosmological constant ( L 0 ). This puts strong limitations on constructing the metric theory of gravity other than Einstein in four dimensions.\n",
      "\n",
      "Recently, a trick has been proposed to circumvent this limitation [2, 3]. (See also [4, 5] for earlier works.) The trick amounts to rescaling the coupling constants α p ( p ≥ 2) as\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "and then taking the D → 4 limit. This procedure leaves nonvanishing contributions in the gravitational field equations, and thus one ends up with a seemingly novel theory of gravity in four dimensions. Although how this 'regularization' works is not so evident at the level of the action or the covariant field equations, an explicit analysis of cosmological solutions, black hole solutions, and perturbations [2, 3, 6-11] shows that the Lovelock terms yield the factor of ( D -4) in the field equations to cancel ( D -4) in the denominator. See also Refs. [12-16] for aspects of black hole solutions in this regularized version of Lovelock gravity.\n",
      "\n",
      "Let us take a look at the cosmological spacetime studied in [2, 3]. The D -dimensional cosmological metric is assumed to take the form\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where i, j = 1 , 2 , 3 and indices a, b run through 4 , 5 , · · · , 3 + n with n = D -4. Substituting this metric to the gravitational field equations and then taking the D → 4 limit along with rescaling the coupling constants as Eq. (4), one obtains the modified background equations in four dimensions [2, 3]. Here some questions arise. What happens if one assumes a different scale factor of the n -dimensional internal space, b 2 ( t ) δ ab d x a d x b ? And then, what is the role of the ( a, b ) (i.e., the internal-space) components of the field equations in the D → 4 limit? More generically, what is\n",
      "\n",
      "∗ Email: tsutomu'at'rikkyo.ac.jp\n",
      "\n",
      "the explicit form of covariant equations to determine the four-dimensional metric after taking the D → 4 limit? Can the D → 4 limit be taken consistently for any metric? Concerning these points, some criticisms on the validity of taking the D → 4 limit have been raised [17-21].\n",
      "\n",
      "To address these issues, in this short paper we study the dynamics of regularized Lovelock gravity by assuming a general four-dimensional metric and a general warp factor of the internal space. We show that taking the D → 4 limit after the Kaluza-Klein reduction leaves a dynamical scalar degree of freedom in four dimensions, yielding a particular class of scalar-tensor theories within the Horndeski family. Although the validity of the original formulation is questioned, regularized Lovelock gravity can thus be reformulated in a consistent way as a well-defined fourdimensional theory. Employing this scalar-tensor reformulation, we revisit the dynamics of a cosmological spacetime in regularized Einstein-Gauss-Bonnet gravity in four dimensions.\n",
      "\n",
      "The rest of the paper is organized as follows. In the next section we clarify the relation between the Horndeski theory and the D → 4 limit of the Lovelock theory to provide the scalar-tensor reformulation of the latter theory. We then study the background dynamics and linear perturbations of a cosmological spacetime based on the scalar-tensor reformulation in Sec. III. A brief summary of the paper is presented in Sec. IV.\n",
      "\n",
      "## II. HORNDESKI AND REGULARIZED LOVELOCK\n",
      "\n",
      "For simplicity, we focus on the case of Einstein-Gauss-Bonnet gravity for the moment. Let us consider the (4 + n )-dimensional metric of the form\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where µ, ν = 0 , 1 , 2 , 3 and d σ 2 K is the line element of a n -dimensional maximally symmetric space with constant curvature K . We perform a Kaluza-Klein reduction starting from the metric (6). Substituting Eq. (6) to the GaussBonnet term and doing integration by parts, we obtain [22, 23]\n",
      "\n",
      "where χ µ = ∇ µ χ , X = -χ µ χ µ / 2, and O ( n 2 ) stands for the terms proportional to n 2 (or higher powers of n ). Here, R and G µν are the four-dimensional Ricci scalar and Einstein tensor, respectively, and G is the Gauss-Bonnet combination of the four-dimensional curvature tensors, G = R 2 -4 R µν R µν + R µνρσ R µνρσ . By rescaling the coupling constant α 2 as α 2 = α ′ 2 /n and taking the n → 0 limit, we arrive at the following alternative description of regularized Gauss-Bonnet gravity,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "This theory can be viewed as a particular subclass of the Horndeski theory [27] (see [28] for a review). The Lagrangian of the Horndeski theory is the sum of the following four Lagrangians [29, 30],\n",
      "\n",
      "As long as the (4 + n )-dimensional metric in underlying Einstein-Gauss-Bonnet gravity is assumed to be of the form of Eq. (6), the n → 0 ( D → 4) limit can be taken consistently and straightforwardly for any four-dimensional metric g µν , giving the above effective Lagrangian. 1 This is one of the main result of this paper. In the case of K = 0, the theory has the invariance under χ → χ +const.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "The Lagrangian (8) corresponds to the case with\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "1 After the first version of this paper was submitted, it was pointed out that the same scalar-tensor theory (with K = 0) is obtained also by introducing a counter term to the action to cancel the divergence in the D → 4 limit [24, 25]. Our result is also consistent with the analysis of amplitudes [26].\n",
      "\n",
      "Here, we used the fact that G µν χ µ χ ν can be written equivalently as XR + δ µ 1 µ 2 ν 1 ν 2 χ ν 1 µ 1 χ ν 2 µ 2 up to total derivatives. Note also that the nonminimal coupling to the Gauss-Bonnet term can be reproduced from the Horndeski functions including the ln X terms [30].\n",
      "\n",
      "The gravitational field equations derived from the Lagrangian (8) take the form\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where H ν µ is a χ -dependent tensor and T ν µ is the energy-momentum tensor of matter. One of the interesting properties of the above scalar-tensor theory is that a particular linear combination of the χ -field equation of motion and the trace of Eq. (17) reduce to\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "One can thus remove χ from the trace part of the gravitational field equations. Equation (18) itself is as expected from the original formulation of the theory [2], and our scalar-tensor reformulation can correctly reproduce the same result, though this equation is not trivial from the scalar-tensor viewpoint.\n",
      "\n",
      "Rewriting explicitly the p ≥ 3 Lovelock terms as the Horndeski theory is much more involved, though they must reduce to the form of the second-order scalar-tensor theory anyway [22]. For example, substituting the metric (6) with K = 0 to L 3 yields\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "The first line is of the form of the generalized Galileon [29],\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "which is a total derivative in four dimensions. The second line, the derivative coupling to the double dual Riemann tensor, can be written equivalently as L 5 {-48 X } [31]. It is easy to rearrange the other terms and we finally obtain, in the n → 0 limit,\n",
      "\n",
      "Similarly, the p -th Lovelock term yields L H 2 { X p } , L H 3 { X p -1 } , L H 5 { X p -1 } , and L H 2 { X p -2 } with particular coefficients. This confirms that the contributions from the higher-order Lovelock terms are high-energy corrections to Eq. (8). It is straightforward to include the curvature K .\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "## III. REVISITING COSMOLOGY IN REGULARIZED EINSTEIN-GAUSS-BONNET GRAVITY\n",
      "\n",
      "Let us study the cosmological dynamics, focusing again on the case of regularized Einstein-Gauss-Bonnet gravity and its scalar-tensor reformulation (8) with K = 0 for simplicity. Since χ is promoted to be a dynamical field in our scalar-tensor reformulation, we will emphasize its consequences on the background and perturbation dynamics.\n",
      "\n",
      "## A. Background Cosmology\n",
      "\n",
      "For the flat Friedmann-Lemaˆ ıtre-Robertson-Walker metric, d s 2 = -d t 2 + a 2 ( t ) δ ij d x i d x j , and χ = χ ( t ), we have\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where H = ˙ a/a is the Hubble parameter and a dot stands for differentiation with respect to t . The χ -field equation, or, equivalently, α ′ 2 ∇ ν H ν µ = 0, reduces to\n",
      "\n",
      "This can be integrated to give\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where the integration constant C is the scalar charge associated with the shift symmetry χ → χ +const. The case with C = 0 corresponds to the isotropically expanding solution, d s 2 = -d t 2 + a 2 δ ij d x i d x j + a 2 δ ab d x a d x b , which is assumed from the beginning in [2]. The scalar-tensor reformulation reveals that the background with the nonvanishing scalar charge is in fact allowed. In an accelerating universe, the second term in Eq. (25) decays quickly compared to the first, so that ˙ χ = H is a dynamical attractor. However, this is not the case in a decelerating universe.\n",
      "\n",
      "Substituting the solution (25) to Eqs. (22) and (23), we now have the following background cosmological equations,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "and ρ and P are the energy density and pressure of matter, respectively. It can be seen that the nonvanishing scalar charge gives rise to an extra radiation-like component in the background equations. This is a specific nature of the particular scalar-tensor theory (8) with K = 0. In the case of C = 0 the background equations derived in [2] are reproduced correctly.\n",
      "\n",
      "## B. Cosmological Perturbations\n",
      "\n",
      "Let us move to the perturbation dynamics. 2 As a matter field we simply add a canonical scalar field described by the Lagrangian L φ = -φ µ φ µ / 2 -V ( φ ). We fix the temporal gauge degree of freedom by imposing that φ ( t, x i ) = φ ( t ). The remaining spatial gauge degrees of freedom can be used to write the metric as\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "with ( e h ) ij = δ ij + h ij + h ik h kj / 2 + · · · . The χ -field also fluctuates,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Since all the gauge degrees of freedom have already been fixed, we cannot gauge away δχ . One would therefore expect that there are two dynamical modes in the scalar sector.\n",
      "\n",
      "It is straightforward to compute the quadratic Lagrangian for the tensor modes h ij :\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "2 It is worth emphasizing that we consider the perturbation dynamics within our scalar-tensor reformulation of the original theory. This effectively kills some of the perturbation modes associated to the internal space such as the Kaluza-Klein vector modes. In the presence of such modes, there is no guarantee that one can take the D → 4 limit consistently after perturbing the full D -dimensional metric.\n",
      "\n",
      "where where\n",
      "\n",
      "Clearly, this Lagrangian reproduces the equation of motion presented in [2] for C = 0. One might worry about ghost instabilities for a sufficiently large scalar charge. However, even if the scalar charge is as large as C /a /greaterorsimilar H , we have, from the background equations, that α 1 Γ H 2 ∼ α ′ 2 C 4 /a 4 /greaterorsimilar α ′ 2 H 2 C 2 /a 2 , which implies that ghost instabilities in the tensor sector are generically avoided.\n",
      "\n",
      "The propagation speed of tensor modes c GW is strongly constrained at low redshifts by the observation of GW170817 and its electromagnetic counterparts [32, 33] as | c 2 GW -1 | /lessorsimilar 10 -15 . From Eq. (32) we see that the propagation speed is given by\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "which deviates from 1 in general, | c 2 GW -1 | ∼ α ′ 2 ˙ H/α 1 , α ′ 2 C 2 /α 1 a 2 ( /lessorsimilar ( α ′ 2 ) 1 / 2 H/α 1 / 2 1 ). However, since H/α 1 / 2 1 is as small as 10 -60 in the present Universe (where we substituted the Planck mass to α 1 / 2 1 ), this does not lead to a meaningful constraint on the dimensionless parameter α ′ 2 .\n",
      "\n",
      "The quadratic Lagrangian for the scalar perturbations reads\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where\n",
      "\n",
      "The variation with respect to ψ gives\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Substituting this back to the above Lagrangian, we obtain the quadratic Lagrangian written in terms of ζ and δχ . The general expression is messy, and hence we expand the Lagrangian in terms of C assuming that the contribution of the scalar charge to the background is subdominant, leading to\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where /epsilon1 := -˙ H/H 2 . One sees that the sound speed of δχ is given by 1 / √ 3, signaling its radiation-like nature. On the C = 0 background, δχ apparently drops off from the Lagrangian and thereby the equation of motion in [2] is reproduced from our Lagrangian. Note, however, that the C → 0 limit must be taken with care because the disappearance of the kinetic term of δχ would indicate a strong coupling. 3 To look into this issue, one may use the rescaled variable ˜ δχ = C δχ and expand the Lagrangian to higher order in perturbations. It would thus be interesting to explore the nonlinear dynamics of the χ mode, which is however beyond the scope of this paper.\n",
      "\n",
      "3 The analysis of amplitudes shows that the scalar degree of freedom is strongly coupled around a flat space [26]. This is consistent with our result.\n",
      "\n",
      "## IV. SUMMARY\n",
      "\n",
      "In this paper, we have proposed a scalar-tensor reformulation of the recently proposed regularized version of Lovelock gravity in four dimensions. The effective scalar-tensor theory is obtained by promoting the warp factor of the internal ( D -4)-dimensional space to a dynamical scalar field, which survives after performing the Kaluza-Klein reduction and taking the D → 4 limit while keeping the rescaled coupling constants α ′ p = ( D -4) α p finite. The resultant theory resides in a particular subclass of the Horndeski theory, whose covariant field equations can be used for any fourdimensional metric. Our result is consistent with the conclusions of the different attempts to derive the well-defined version of the four-dimensional Einstein-Gauss-Bonnet theory [24-26].\n",
      "\n",
      "Employing our scalar-tensor reformulation, we have studied cosmological aspects of regularized Einstein-GaussBonnet gravity. We have found that cosmological solutions generically admit a scalar charge C , which yields a radiation-like component. All the previous cosmological results [2] are reproduced at the level of the Lagrangian by taking C → 0. However, in this limit the gravitational scalar degree of freedom might be strongly coupled. This point needs further investigation.\n",
      "\n",
      "Note added: While we were in the final stage of this work, the paper by Lu and Pang [34] appeared in the arXiv, where the same idea of reformulating regularized Einstein-Gauss-Bonnet gravity is presented independently. Their main focus is on black hole solutions, while ours is on cosmological aspects. Our conclusion agrees with them where they overlap.\n",
      "\n",
      "## ACKNOWLEDGMENTS\n",
      "\n",
      "We are grateful to Chunshan Lin for discussions. The work of TK was supported by MEXT KAKENHI Grant Nos. JP16K17707, JP17H06359, and JP18H04355.\n",
      "\n",
      "- [1] D. Lovelock, The Einstein tensor and its generalizations , J. Math. Phys. 12 (1971) 498.\n",
      "- [2] D. Glavan and C. Lin, Einstein-Gauss-Bonnet gravity in 4-dimensional space-time , Phys. Rev. Lett. 124 (2020) 081301 [ 1905.03601 ].\n",
      "- [3] A. Casalino, A. Colleaux, M. Rinaldi and S. Vicentini, Regularized Lovelock gravity , 2003.07068 .\n",
      "- [4] Y. Tomozawa, Quantum corrections to gravity , 1107.1424 .\n",
      "- [5] G. Cognola, R. Myrzakulov, L. Sebastiani and S. Zerbini, Einstein gravity with Gauss-Bonnet entropic corrections , Phys. Rev. D 88 (2013) 024006 [ 1304.1878 ].\n",
      "- [6] P. G. S. Fernandes, Charged Black Holes in AdS Spaces in 4 D Einstein Gauss-Bonnet Gravity , 2003.05491 .\n",
      "- [7] R. A. Konoplya and A. Zhidenko, Black holes in the four-dimensional Einstein-Lovelock gravity , 2003.07788 .\n",
      "- [8] S.-W. Wei and Y.-X. Liu, Testing the nature of Gauss-Bonnet gravity by four-dimensional rotating black hole shadow , 2003.07769 .\n",
      "- [9] R. Kumar and S. G. Ghosh, Rotating black holes in the novel 4 D Einstein-Gauss-Bonnet gravity , 2003.08927 .\n",
      "- [10] D. D. Doneva and S. S. Yazadjiev, Relativistic stars in 4D Einstein-Gauss-Bonnet gravity , 2003.10284 .\n",
      "- [11] S. G. Ghosh and S. D. Maharaj, Radiating black holes in the novel 4D Einstein-Gauss-Bonnet gravity , 2003.09841 .\n",
      "- [12] R. A. Konoplya and A. F. Zinhailo, Quasinormal modes, stability and shadows of a black hole in the novel 4D Einstein-Gauss-Bonnet gravity , 2003.01188 .\n",
      "- [13] M. Guo and P.-C. Li, The innermost stable circular orbit and shadow in the novel 4 D Einstein-Gauss-Bonnet gravity , 2003.02523 .\n",
      "- [14] K. Hegde, A. N. Kumara, C. L. A. Rizwan, A. K. M. and M. S. Ali, Thermodynamics, Phase Transition and Joule Thomson Expansion of novel 4-D Gauss Bonnet AdS Black Hole , 2003.08778 .\n",
      "- [15] Y.-P. Zhang, S.-W. Wei and Y.-X. Liu, Spinning test particle in four-dimensional Einstein-Gauss-Bonnet Black Hole , 2003.10960 .\n",
      "- [16] D. V. Singh and S. Siwach, Thermodynamics and P-v criticality of Bardeen-AdS Black Hole in 4-D Einstein-Gauss-Bonnet Gravity , 2003.11754 .\n",
      "- [17] W.-Y. Ai, A note on the novel 4D Einstein-Gauss-Bonnet gravity , 2004.02858 .\n",
      "- [18] M. Gurses, T. C. Sisman and B. Tekin, Is there a novel Einstein-Gauss-Bonnet theory in four dimensions? , 2004.03390 .\n",
      "- [19] S. Mahapatra, A note on the total action of 4 D Gauss-Bonnet theory , 2004.09214 .\n",
      "- [20] S. Tian and Z.-H. Zhu, Comment on 'Einstein-Gauss-Bonnet Gravity in Four-Dimensional Spacetime' , 2004.09954 .\n",
      "- [21] J. Arrechea, A. Delhom and A. Jimnez-Cano, Yet another comment on four-dimensional Einstein-Gauss-Bonnet gravity , 2004.12998 .\n",
      "- [22] K. Van Acoleyen and J. Van Doorsselaere, Galileons from Lovelock actions , Phys. Rev. D83 (2011) 084025 [ 1102.0487 ].\n",
      "\n",
      "- [23] C. Charmousis, B. Gouteraux and E. Kiritsis, Higher-derivative scalar-vector-tensor theories: black holes, Galileons, singularity cloaking and holography , JHEP 09 (2012) 011 [ 1206.1499 ].\n",
      "- [24] P. G. Fernandes, P. Carrilho, T. Clifton and D. J. Mulryne, Derivation of Regularized Field Equations for the Einstein-Gauss-Bonnet Theory in Four Dimensions , 2004.08362 .\n",
      "- [25] R. A. Hennigar, D. Kubiznak, R. B. Mann and C. Pollack, On Taking the D → 4 limit of Gauss-Bonnet Gravity: Theory and Solutions , 2004.09472 .\n",
      "- [26] J. Bonifacio, K. Hinterbichler and L. A. Johnson, Amplitudes and 4D Gauss-Bonnet Theory , 2004.10716 .\n",
      "- [27] G. W. Horndeski, Second-order scalar-tensor field equations in a four-dimensional space , Int. J. Theor. Phys. 10 (1974) 363.\n",
      "- [28] T. Kobayashi, Horndeski theory and beyond: a review , Rept. Prog. Phys. 82 (2019) 086901 [ 1901.07183 ].\n",
      "- [29] C. Deffayet, X. Gao, D. A. Steer and G. Zahariade, From k-essence to generalised Galileons , Phys. Rev. D84 (2011) 064039 [ 1103.3260 ].\n",
      "- [30] T. Kobayashi, M. Yamaguchi and J. Yokoyama, Generalized G-inflation: Inflation with the most general second-order field equations , Prog. Theor. Phys. 126 (2011) 511 [ 1105.5723 ].\n",
      "- [31] T. Kobayashi, N. Tanahashi and M. Yamaguchi, Multifield extension of G inflation , Phys. Rev. D 88 (2013) 083504 [ 1308.4798 ].\n",
      "- [32] LIGO Scientific, Virgo collaboration, B. Abbott et al., GW170817: Observation of Gravitational Waves from a Binary Neutron Star Inspiral , Phys. Rev. Lett. 119 (2017) 161101 [ 1710.05832 ].\n",
      "- [33] LIGO Scientific, Virgo, Fermi-GBM, INTEGRAL collaboration, B. Abbott et al., Gravitational Waves and Gamma-rays from a Binary Neutron Star Merger: GW170817 and GRB 170817A , Astrophys. J. Lett. 848 (2017) L13 [ 1710.05834 ].\n",
      "- [34] H. Lu and Y. Pang, Horndeski Gravity as D → 4 Limit of Gauss-Bonnet , 2003.11552 .\n",
      "Document 5:\n",
      "## SEMANTIC UNCERTAINTY: LINGUISTIC INVARIANCES FOR UNCERTAINTY ESTIMATION IN NATURAL LANGUAGE GENERATION\n",
      "\n",
      "Lorenz Kuhn, Yarin Gal, Sebastian Farquhar\n",
      "\n",
      "OATML Group, Department of Computer Science, University of Oxford lorenz.kuhn@cs.ox.ac.uk\n",
      "\n",
      "## ABSTRACT\n",
      "\n",
      "We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of 'semantic equivalence'-different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to 'off-the-shelf' language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.\n",
      "\n",
      "## 1 INTRODUCTION\n",
      "\n",
      "Despite progress in natural language generation (NLG) tasks like question answering or abstractive summarisation (Brown et al., 2020; Hoffmann et al., 2022; Chowdhery et al., 2022), there is little understanding of uncertainty in foundation models. Without measures of uncertainty in transformerbased systems it is hard to use generated language as a reliable source of information. Reliable measures of uncertainty have been identified as a key problem in building safer AI systems (Amodei et al., 2016; Hendrycks et al., 2022).\n",
      "\n",
      "Unfortunately, uncertainty in free-form NLG faces unique challenges. This limits how much we can learn from uncertainty estimation techniques in other applications of deep learning (Gal et al., 2016; Lakshminarayanan et al., 2017; Ovadia et al., 2019) which focuses especially on image classification (Kendall &amp; Gal, 2017) or regression in low-dimensional data spaces (Kuleshov et al., 2018).\n",
      "\n",
      "The key challenges come from the importance in language of meanings and form . This corresponds to what linguists and philosophers call the semantic content of a sentence and its syntactic or lexical form. Foundation models output token -likelihoods-representing lexical confidence. But for almost all applications we care about meanings! For example, a model which is uncertain about whether to generate 'France's capital is Paris' or 'Paris is France's capital' is not uncertain in any important sense. Yet, at a token-level the model is uncertain between two forms of the same meaning . Existing unsupervised methods (e.g., Malinin &amp; Gales (2020)) ignore this distinction.\n",
      "\n",
      "To address semantic equivalence, we estimate semantic likelihoods-probabilities attached to meanings of text rather than standard sequence-likelihoods. We introduce an algorithm for clustering sequences that mean the same thing based on the principle that two sentences mean the same thing if you can infer each from the other. We then use these semantic-likelihoods to estimate semantic uncertainty-uncertainty over different meanings. In particular, we compute the entropy of the probability distribution over meanings. Adjusting for semantic equivalence in this way offers better uncertainty estimation than standard entropy and also greatly improves over methods for model self-evaluation (Kadavath et al., 2022). In addition, semantic entropy scales better with model size and makes better use of increasing numbers of samples than baselines.\n",
      "\n",
      "We further analyse major challenges for measuring uncertainty in NLG. We show empirically how sampling a set of model answers to estimate entropies in NLG must balance sample accuracy and diversity, which significantly strengthens the baselines we compare against relative to prior imple-\n",
      "\n",
      "⋃˜⌉∐{√]̂(˜{√√}√glyph[arrowbt]({}√√√}\n",
      "\n",
      "\n",
      "\n",
      "⊗√⌉̂˜√(}˜(√∐√∐⌉˜√˜√√(}˜(⌉}̂˜⌈\n",
      "\n",
      "Figure 1: (a) Our semantic entropy (blue) predicts model accuracy better than baselines on the free-form question answering data set TriviaQA (30B parameter OPT model). Normalised entropy reimplements single-model variant of Malinin &amp; Gales (2020), lexical similarity measures the average Rouge-L in a sampled set of answers for a given question analogously to Fomicheva et al. (2020), entropy and p ( True ) reimplement Kadavath et al. (2022). (b) Our method's outperformance increases with model size while also doing well for smaller models.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "mentations. We also examine the situational heuristic of length-normalising predictive entropies. Our main contributions are thus as follows:\n",
      "\n",
      "- We explain why uncertainty in free-form NLG is different from other settings (Section 3).\n",
      "- We introduce semantic entropy -a novel entropy-based uncertainty measure which uses our algorithm for marginalising over semantically-equivalent samples (Section 4) and show that it outperforms comparable baselines in extensive ablations with both open- and closedbook free-form question answering using TriviaQA and CoQA (Section 6).\n",
      "- Through hyperparameter ablations we suggest how to balance the trade-off between sampling diverse and accurate generations for our method as well as baselines (Section 6.2) and show that far fewer samples are needed for effective uncertainty than prior work presumes.\n",
      "\n",
      "We focus on free-form question answering (QA) because it is a difficult and important use of NLG with high-stakes applications. At the same time, it is easier to establish a ground truth without expensive human evaluation than more nebulous tasks like summarisation.\n",
      "\n",
      "Ultimately, we show that semantic entropy is an effective unsupervised way to estimate uncertainty in NLG. As an unsupervised method, it requires no further training or data-gathering, unlike supervised methods including Lin et al. (2022a); Kadavath et al. (2022). Semantic entropy is designed to work with existing foundation and large language models with no modifications 'out-of-the-box'. Our experiments use OPT (Zhang et al., 2022) but semantic entropy works with any similar model.\n",
      "\n",
      "## 2 BACKGROUND ON UNCERTAINTY ESTIMATION\n",
      "\n",
      "Our method draws inspiration from probabilistic tools for uncertainty estimation, which have been extensively employed in settings like deep image classification (Gal et al., 2016). Although these methods are often used in Bayesian models, we emphasise that our method does not require any special training or architectural modifications and is not limited to Bayesian settings.\n",
      "\n",
      "The total uncertainty of a prediction can be understood as the predictive entropy of the output distribution. This measures the information one has about the output given the input. This entropy is highest when the output is minimally informative-predicting the same probability for all possible outcomes. The predictive entropy for a point x is the conditional entropy of the output random variable Y with realisation y given x\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "One can further distinguish aleatoric uncertainty-uncertainty in the underlying data distributionand epistemic uncertainty-resulting from missing information (Kendall &amp; Gal, 2017). Epistemic\n",
      "\n",
      "uncertainty, measured using a mutual information, can be useful but is hard to estimate, especially for very large models, requiring special methods and computational expense. Instead of estimating the epistemic uncertainty based on the model variance, the epistemic uncertainty can also be predicted directly using a second model (see e.g. Jain et al. (2021)). We do not use mutual information in this work, because our focus is on existing foundation models 'off-the-shelf'. Similarly, while, e.g., Malinin &amp; Gales (2020) use ensembles of models to estimate the integral in Eq. (1) we use samples from a single model's output distribution. Prior networks (Malinin &amp; Gales, 2018; Malinin et al., 2020) estimate model uncertainty by emulating an ensemble with a single model. This could be important for NLG because of large model sizes.\n",
      "\n",
      "For sequence-prediction tasks like NLG, the probability of the entire sequence, s , is the product of the conditional probabilities of new tokens given past tokens, whose resulting log-probability is log p ( s | x ) = ∑ i log p ( s i | s &lt;i ) , where s i is the i 'th output token and s &lt;i denotes the set of previous tokens. Sometimes, instead of the entropy of these probabilities, the geometric mean tokenprobability is used instead (Malinin &amp; Gales, 2020) becoming an arithmetic mean log-probability 1 N ∑ N i log p ( s i | s &lt;i ) . Despite empirical success Murray &amp; Chiang (2018), so far this has little theoretical justification.\n",
      "\n",
      "Direct application of language models to uncertainty. In contrast to our approach using probabilistic methods, recent work has sought to use the generating language model itself to estimate its own uncertainty. For example, Lin et al. (2022a) finetune language models to verbally describe their confidence. Meanwhile, Kadavath et al. (2022) sample multiple generations and return the completion to an NLG prompt asking if a proposed answer is true (further detail in Appendix B.5). Both Lin et al. (2022a) and Kadavath et al. (2022) also propose ways to finetune predictors on the embeddings of generating models to predict models uncertainty. While promising, these approaches need task-specific labels, additional training, and seem to be unreliable out-of-distribution (as shown in Figures 13 and 14 in Kadavath et al. (2022)).\n",
      "\n",
      "## 3 CHALLENGES IN UNCERTAINTY ESTIMATION FOR NLG\n",
      "\n",
      "Approaches to NLG uncertainty might treat the language model as a black-box (e.g., asking it if its answer is correct) or alternatively focus on the probabilistic model without accounting for the special characteristics of language (e.g., measuring predictive entropy).\n",
      "\n",
      "Our unsupervised approach instead uses the powerful tools of probabilistic modelling, but also recognises the unique challenges posed by free-form NLG. In this section, we critically analyse the probabilistic interpretation of language models in order to ground both our method and future exploration of the field.\n",
      "\n",
      "## 3.1 SEMANTIC EQUIVALENCE IN LANGUAGE OUTPUTS\n",
      "\n",
      "Most machine learning problems have mutually exclusive outputs. An image in class 17 is not class 29 as well; a regression output of 23.1 is not anything else; an RL agent going left does not go right. In contrast, for free-form text generation an output usually means the same thing as many other outputs. For example, 'The capital of France is Paris' means the same thing as 'France's capital is Paris'. Linguists and philosophers distinguish text's meaning-its semantic content-from its syntactic and lexical form. The syntax is the grammatical structure while its lexical form is the specific words used. Lexical equivalence entails the other two, but not the reverse.\n",
      "\n",
      "We almost always care about the semantic content of a sentence. For decision-problems relying on NLG, meaning is usually an invariance in output-space which is not present in the model specification. This is true for question answering, summarisation, artificial assistants. Meanings are especially important for trustworthiness: a system can be reliable even with many different ways to say the same thing but answering with inconsistent meanings shows poor reliability.\n",
      "\n",
      "We can formalize semantic equivalence mathematically. Let the space of tokens in a language be T . The space of all possible sequences of tokens of length N is then S N ≡ T N . For some sentence s ∈ S N , a sequence of tokens s i ∈ T there is an associated meaning. 1\n",
      "\n",
      "Let us introduce a placeholder semantic equivalence relation , E ( · , · ) , which holds of any two sentences that mean the same thing-we operationalise this in Section 4. Recall that an equivalence\n",
      "\n",
      "1 Theories of meaning are contested (Speaks, 2021). However, for specific models and deployment contexts many considerations can be set aside. Care should be taken comparing very different models and contexts.\n",
      "\n",
      "Table 1: Answers to the question 'What is the capital of France?' (a) When all generations from the model mean different things, semantic clustering has no effect-the entropy and semantic entropy are identical. (b) When some of the answers are semantically equivalent ('Paris' and 'It's Paris') the semantic entropy does a better job of capturing the actually low uncertainty.\n",
      "\n",
      "(a) Scenario 1: No semantic equivalence\n",
      "\n",
      "(b) Scenario 2: Some semantic equivalence\n",
      "\n",
      "| Answer s   |   Likelihood p ( s | x ) |   Semantic likelihood ∑ s ∈ c p ( s | x ) | Answer s   | Likelihood p ( s | x )   | Semantic likelihood ∑ s ∈ c p ( s | x )   |\n",
      "|------------|--------------------------|-------------------------------------------|------------|--------------------------|-------------------------------------------|\n",
      "| Paris      |                     0.5  |                                      0.5  | Paris      | 0.5 }                    | 0.9                                       |\n",
      "| Rome       |                     0.4  |                                      0.4  | It's Paris | 0.4                      |                                           |\n",
      "| London     |                     0.1  |                                      0.1  | London     | 0.1                      | 0.1                                       |\n",
      "| Entropy    |                     0.94 |                                      0.94 | Entropy    | 0.94                     | 0.33                                      |\n",
      "\n",
      "relation is any reflexive, symmetric, and transitive relation, and that any equivalence relation on a set corresponds to a set of equivalence classes. Each semantic equivalence class corresponds to one possible meaning that our text can have. That is, for the space of semantic equivalence classes C the sentences in the set c ∈ C all share a meaning such that ∀ s, s ′ ∈ c : E ( s, s ′ ) .\n",
      "\n",
      "Ordinarily, large language models produce conditional distributions over tokens and their resulting sequences. That is, the probability of the sequence conditioned on the context comes from conditional token probabilities p ( s | x ) = ∏ i p ( s i | s &lt;i , x ) . Instead, we focus on the probability of the model generating any sequence that shares some meaning. This can be written as\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Formally, this treats the output random variable whose event-space is C , a subσ -algebra of the standard event-space S .\n",
      "\n",
      "## 3.2 SAMPLING THE EXTREMELY HIGH-DIMENSIONAL LANGUAGE-SPACE\n",
      "\n",
      "Recall from Eq. (1) that estimating predictive entropy requires taking an expectation in output-space. However, the output-space of natural language has O ( |T | N ) dimensions. Moreover, while we can sample from our autoregressive token-model, we lack a normalized probability density function over sentences. The expectation must be approximated by Monte Carlo integration-sampling a finite set of sentences from the output distribution and averaging their likelihoods to compute the entropy. For entropies the average is dominated by low-probability sentences (whose logs are large and negative) making Monte Carlo integration difficult (Mackay, 2003).\n",
      "\n",
      "## 3.3 VARIABLE LENGTH GENERATIONS\n",
      "\n",
      "Sentences of natural language have different lengths. As is widely noted (Murray &amp; Chiang, 2018) and especially in the context of NLG uncertainty by Malinin &amp; Gales (2020), in expectation longer sequences have lower joint likelihoods because of the conditional independence of the token probabilities. The joint likelihood of a sequence of length N shrinks exponentially in N . Its negative log-probability therefore grows linearly in N , so longer sentences tend to contribute more to entropy.\n",
      "\n",
      "We therefore interpret length-normalising the log-probabilities when estimating the entropy as asserting that the expected uncertainty of generations is independent of sentence length. Sometimes, this is approximately valid. Other times, longer sentences may well be usually more uncertain (e.g., when the goal is to exactly match a typically short reference answer, such as for TriviaQA). In these cases, the advantages of length-normalisation become less clear-cut, as we show empirically in Section 6.1. This offers some guidance a priori on cases when length-normalisation is appropriate.\n",
      "\n",
      "## 4 SEMANTIC UNCERTAINTY\n",
      "\n",
      "We have introduced the idea that uncertainty over meanings is more important for most situations than uncertainty over the exact tokens used to express those meanings. Our method examines uncertainty in meaning-space-the entropy of the random variable representing the output distribution in the semantic event-space. This is in contrast to entropy in the usual token event-space. To do this we introduce a novel algorithm for estimating the semantic equivalence relation as well as a novel uncertainty estimation algorithm for semantic entropy. At a high level this involves three steps:\n",
      "\n",
      "1. Generation: Sample M sequences { s (1) , . . . , s ( M ) } from the predictive distribution of a large language model given a context x .\n",
      "2. Clustering: Cluster the sequences which mean the same thing using our bi-directional entailment algorithm.\n",
      "3. Entropy estimation: Approximate semantic entropy by summing probabilities that share a meaning following Eq. (2) and compute resulting entropy. This is illustrated in Table 1.\n",
      "\n",
      "## Step 1: Generating a set of answers from the model\n",
      "\n",
      "First we sample M sequences { s (1) , . . . , s ( M ) } which we will use later to estimate the uncertainty. These sequences must be sampled according to the distribution p ( s | x ) . In this paper, we sample these sequences only from a single model using either multinomial sampling or multinomial beam sampling. We show in Section 6.2, that the choice of sampling temperature and sampling method can have a significant impact on the performance of both our method and the baselines. Unlike Malinin &amp; Gales (2020), we do not use an ensemble of models. Ensembling would probably improve performance, but the cost of training multiple independent foundation models is often prohibitive.\n",
      "\n",
      "## Step 2: Clustering by semantic equivalence\n",
      "\n",
      "In Section 3.1, we formalised semantic equivalence by introducing the semantic equivalence relation, E ( · , · ) , which induces semantic equivalence classes which are sets of sequences that share a meaning. Recall that the equivalence class c is a set of sequences s such that ∀ s, s ′ ∈ c : E ( s, s ′ ) . We operationalise E ( · , · ) using the idea of bi-directional entailment. A sequence, s , means the same thing as a second sequence, s ′ , if and only if they entail (i.e. logically imply) each other. E.g., 'The capital of France is Paris.' entails 'Paris is the capital of France.' because they mean the same thing.\n",
      "\n",
      "Importantly, we require that the sequences mean the same thing with respect to the context-key meaning is sometimes contained within the context. For example, 'Paris.' does not entail 'The capital of France is Paris.' because 'Paris.' is not a declarative sentence without context. But within the context of the question, the one-word answer does entail the fuller answer.\n",
      "\n",
      "In general, any natural language inference classification system (NLI) can be used for our bidirectional entailment clustering algorithm. In our case, we use a Deberta-large model (He et al., 2020a) that is fine-tuned on the NLI data set MNLI (Williams et al., 2017). For each pair of sequences in our set of samples, s and s ′ , we detect whether it is possible to infer the concatenation of the context and s from the concatenation of the context and s ′ and vice versa. To do this we concatenate each of the two question/answer pairs, and then concatenate them both together separated by a special token. The Deberta model then classifies this sequence into one of: entailment , neutral , contradiction . We compute both directions, and the algorithm returns equivalent if and only if both directions were entailment . Algorithm pseudocode is provided in Appendix A.2.\n",
      "\n",
      "Because this component is novel, we confirm in Appendix B.2 that the bidirectional entailment classifier works by manually labelling 300 generations for semantic equivalence, finding an accuracy of 92.7% on TriviaQA and 95.5% on CoQA.\n",
      "\n",
      "Computational cost. The bidirectional equivalence algorithm is combinatorially complex in M , it requires ( M 2 ) -many comparisons in the worst-case. In practice, however, the computational cost is small compared to the cost of generating sequences. First, as we show in Section 6.2, M &lt; 20 is often sufficient for good uncertainty. Second, because the Deberta-large model is so much smaller than the main language model (1.5B parameters), each pair comparison is much faster than generating even one token from the main model. Third, because semantic equivalence is transitive we only need to compare one member of each equivalence class to remaining sequences (see Algorithm 1). Additionally, the number of semantic clusters in our tasks is empirically quite low, see Table 2.\n",
      "\n",
      "## Step 3: Computing the semantic entropy\n",
      "\n",
      "Having determined the clusters of generated sequences that mean the same thing, we add their likelihoods following Eq. (2) as a way of determining the likelihood of each meaning, rather than each sequence. We then compute the semantic entropy (SE) as the entropy over the meaning-distribution\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "We do not have access to every possible meaning-class c . Instead, we can only sample c from the sequence-generating distribution induced by the model. To handle this, we estimate the expectation in Eq. (3) using Monte Carlo integration over the semantic equivalence classes C from Algorithm 1\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "This is an unbiased estimator of the entropy in Eq. (3). In addition, in some cases we use lengthnormalisation as described in Section 3.3.\n",
      "\n",
      "## 4.1 HOW THE SEMANTIC ENTROPY ADDRESSES THE CHALLENGES OF NLG\n",
      "\n",
      "The main inspiration of semantic entropy is to address the semantic invariance of natural language head-on by converting the problem of uncertainty estimation into meaning-space. In addition, semantic entropy goes some way towards addressing unequal token importance. Generations whose meanings are the same but differ on unimportant tokens will be added together, which we expect will reduce the effect of the likelihoods of unimportant tokens although we do not demonstrate this empirically. However, this challenge is only partially addressed: semantic entropy will still pay too much attention to non-keyword likelihoods. This is one area where supervised language-modelbased uncertainty tools (Lin et al., 2022a; Kadavath et al., 2022) might enable future improvements that handle this challenge better. We address the challenges of sampling and variable-length generation using specific details of our sampling procedure in Section 4.\n",
      "\n",
      "## 5 RELATED WORK\n",
      "\n",
      "Prior work on uncertainty in foundation models for NLP has largely focused on the calibration of classifiers (Jiang et al., 2021; Desai &amp; Durrett, 2020) and text regressors (Glushkova et al., 2021; Wang et al., 2022). These settings, are analogous to classification or regression settings in other modalities like vision, and conventional uncertainty measures like MC dropout or Deep Ensembles can be applied without modification (see Section 2 for a discussion of uncertainty in deep learning in general). As we argue in Section 3, generative natural language poses important further challenges. Jiang et al. (2021) do examine calibration in generative question answering and find only a weak correlation between the log-likelihood models assign to their answer and the answer's correctness. In Section 6 we explain however why semantic equivalence in natural language makes calibration a problematic evaluation for generative language models. Reliable uncertainty can be useful on downstream tasks such as graph semantic parsing (Lin et al., 2022b).\n",
      "\n",
      "Some research has addressed uncertainty or calibration in NLG either by prompting the models to evaluate their own generations or by fine-tuning the generating model to predict its uncertainty (Mielke et al., 2020; Lin et al., 2022a; Kadavath et al., 2022). These methods need further training and supervision. Because they need additional training and supervision, they are hard to reproduce, expensive to create, and have been shown to be sensitive to distribution shift. For example, we were unable to implement one proposal by Kadavath et al. (2022) to train a language model to directly predict confidence due to hardware limitations. Our unsupervised method which uses models 'offthe-shelf' avoids these limitations.\n",
      "\n",
      "Many of the issues that make probabilistic uncertainty estimation in NLG difficult also make automatic evaluation of NLG difficult. Ott et al. (2018), for instance, study how the performance of machine translation models suffers because one sentence can be translated in multiple ways. Similarly, Sai et al. (2022) discuss how paraphrase detection can be used to evaluate NLG and other related methods might transfer to uncertainty estimation.\n",
      "\n",
      "Automatic paraphrase identification can be based on comparing lexical features of two given sequences (Fernando &amp; Stevenson, 2008; Issa et al., 2018) or on measuring the similarity between the embeddings of the two sequences (Yu et al., 2014; Socher et al., 2011). Recently, however, SotA paraphrase identification approaches have primarily used BERT-based models to classify pairs of sequences into the classes paraphrases and not paraphrases (He et al., 2020b; Tay et al., 2021). The idea of formalising semantic equivalence via textual entailment has a long history in linguistics (Culicover, 1968) and NLP (Pad´ o et al., 2009; Androutsopoulos &amp; Malakasiotis, 2010). Transformer-based paraphrase detection models such as EFL (Wang et al., 2021) achieve SotA performance on paraphrase detection benchmarks such as Quora Question Pairs Wang et al. (2017).\n",
      "\n",
      "Figure 2: (a) On CoQA open-book question answering semantic entropy demonstrates better uncertainty than ordinary predictive entropy with and without normalisation at larger model sizes. It also performs significantly better than p ( True ) . (b) TriviaQA shows similar results. Identical to Fig. 1b with the addition of p ( True ) , which was previously omitted to avoid stretching the scale.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## 6 EMPIRICAL EVALUATION\n",
      "\n",
      "We demonstrate that semantic entropy is an effective way to quantify the uncertainty of NLG on free-form QA tasks. Effective uncertainty measures should offer information about how reliable the model's answers are-that is, very uncertain generations should be less likely to be correct .\n",
      "\n",
      "Performance evaluation. Following prior work (e.g. Filos et al. (2019)), we evaluate uncertainty by treating uncertainty estimation as the problem of predicting whether to rely on a model generation for a given context-whether to trust an answer to a question. The area under the receiver operator characteristic curve (AUROC) metric is equivalent to the probability that a randomly chosen correct answer has a higher uncertainty score than a randomly chosen incorrect answer. Higher scores are better, with perfect uncertainty scoring 1 while a random uncertainty measure would score 0.5.\n",
      "\n",
      "The AUROC is a better measure of uncertainty for free-form question answering and NLG than calibration measures like the Brier score, which are often used in classification or for multiple choice QA. This is because the language model outputs a likelihood for a given token-sequence, but not for an entire meaning. In order to estimate the Brier score, we would need to estimate the entire probability mass assigned to any possible way of saying the correct answer. This is intractable for free form text where we do not have access to probabilities about meanings. In contrast, we can estimate the entropy because it is structured as an expected information, which makes Monte Carlo integration suitable.\n",
      "\n",
      "Model. We use the GPT-like OPT models (Zhang et al., 2022). We vary the size of the model between 2.7B, 6.7B, 13B and 30B parameters, while our headline results are all reported using the largest computationally feasible model, with 30B parameters. In all cases we use only a single unmodified model. There is no ensembling and no stochastic or Bayesian modification. We chose this because in most cases cutting-edge foundation models are not available as ensembles and are too large to efficiently perform approximate Bayesian inference with. We do not fine-tune these models on TriviaQA or CoQA but use them in their pre-trained form.\n",
      "\n",
      "Datasets. We use CoQA Reddy et al. (2019) as an open-book conversational question answering problem (in which the model answers a question using a supporting paragraph). We use the development split ( ∼ 8000 questions). We also use TriviaQA (Joshi et al., 2017) as a closed-book QA problem (in which the model must answer a question without access to a supporting paragraph). We use a subset of 8000 questions of the training split to match the size of CoQA.\n",
      "\n",
      "We evaluate correctness of our model's generations on the underlying dataset using the a fuzzy matching criterion: L ( s , s ′ ) = 1 RougeL ( s , s ′ ) &gt; 0 . 3 . That is, we consider an answer s to be correct if its Rouge-L (Lin &amp; Och, 2004) - a measure of the longest common subsequence - with regards to the reference answer is larger than 0.3. In Appendix B.3 we study other objective functions such as exact matching and Rouge-1 and find our method to be robust to these choices.\n",
      "\n",
      "Baselines. We compare our method against predictive entropy, length-normalised predictive entropy (Malinin &amp; Gales, 2020), p ( True ) (Kadavath et al., 2022), and lexical similarity (similar to (Fomicheva et al., 2020)). Predictive entropy is a widely used measure of uncertainty in other\n",
      "\n",
      "Table 2: Incorrectly answered questions have more semantically distinct answers than correct ones. On its own, this count is a reasonable uncertainty measure, though semantic entropy is better.\n",
      "\n",
      "| Dataset   | Average # of semantically distinct answers   | Average # of semantically distinct answers   | AUROC            | AUROC              |\n",
      "|-----------|----------------------------------------------|----------------------------------------------|------------------|--------------------|\n",
      "|           | Correctly answered                           | Incorrectly answered                         | Semantic entropy | # distinct answers |\n",
      "| CoQA      | 1.27                                         | 1.77                                         | 0.77             | 0.66               |\n",
      "| TriviaQA  | 1.89                                         | 3.89                                         | 0.83             | 0.79               |\n",
      "\n",
      "domains, and has been used as a baseline without length-normalisation in, e.g., Kadavath et al. (2022). Here, the score is just the predictive entropy of the output distribution as described in Eq. (1). Length-normalised predictive entropy divides the joint log-probability of each sequence by the length of the sequence, as proposed by Malinin &amp; Gales (2020) in the case of NLG uncertainty and further discussed in Section 3.3. Note that unlike Malinin &amp; Gales (2020), we use only a single model, not an ensemble, and use multinomial sampling as we do for all other methods. p ( True ) proposed by (Kadavath et al., 2022) as a way to estimate the probability that a model's generation is correct by 'asking' the model if its answer is correct. They propose sampling M answers and constructing a new natural language question using these possible answers as context before asking whether the proposed answer is correct and measuring the probability of the completion being True . An example of the format is provided in Appendix B. Note that our implementation of this uses OPT models with up to 30B parameters, while Kadavath et al. (2022) use a proprietary 52B parameter model. We are also limited computationally to 10-shot prompting while the original paper uses 20-shot prompting. Lexical similarity uses the average similarity of the answers in the answer set A : 1 C ∑ | A | i =1 ∑ | A | j =1 sim ( s i , s j ) , where C = | A | ∗ ( | A | -1) / 2 , and sim is Rouge-L. Additionally, we evaluate a margin-probability baseline (Lin et al., 2022b) in Appendix B.6, and study why it is not very predictive of model accuracy in this setting. All code and data used in our experiments is available at https://github.com/lorenzkuhn/semantic\\_uncertainty .\n",
      "\n",
      "## 6.1 SEMANTIC ENTROPY UNCERTAINTY\n",
      "\n",
      "For both TriviaQA and CoQA, semantic entropy improves over baselines in predicting whether a model's answer to a question is correct. For TriviaQA, using the largest model we show in Fig. 1a we show that semantic entropy has a significantly higher AUROC than entropy in sequence-probabilityspace with and without length-normalisation, as well as the lexical similarity baseline. At the same time, it performs dramatically better than p ( True ) . Similarly, we find in Fig. 1b that our method outperforms more for larger model sizes and continues to steadily improve as the model size increases, with the performance of the p ( True ) baseline added in Fig. 2b (not shown in the opening figure for visual clarity). For CoQA, in Fig. 2a we show that semantic entropy predicts model correctness significantly better than the baselines at larger model sizes.\n",
      "\n",
      "The ground truth answers for TriviaQA are generally single words or very short phrases, while CoQA contains both longer and shorter ground truth answers. This is why performing lengthnormalisation has a large effect for CoQA but no effect for TriviaQA (compare Fig. 2a and Fig. 2b). TriviaQA is also a more challenging dataset: accuracy of 50.6% against 82.3% for CoQA.\n",
      "\n",
      "We can better understand the mechanism of action for semantic entropy by examining the difference between incorrect and correct answers generated by the model. In Table 2 we show that the average number of semantically distinct clusters of answers ( | C | ) from the 30B parameter model is significantly greater for incorrectly answered questions than correctly answered ones. This fits our predictions, which is that the model is more likely to generate incorrect answers when it is uncertain about the most likely generation. There are 10 answers generated per question, so there is substantial overlap in meaning. We also show that simply using the number of semantically distinct answers as an uncertainty measure on its own performs reasonably well. Semantic entropy has a higher AUROC than the number of distinct answers, especially for CoQA whose difficulty causes greater spread in predicted probabilities between possible answers.\n",
      "\n",
      "Finally, we can see that much of the performance gain comes from making better use of more samples. In Fig. 3a we show that for both CoQA (top) and TriviaQA (bottom) the gap between semantic entropy and length-normalised entropy widens as the number of samples increases.\n",
      "\n",
      "## 6.2 HYPERPARAMETERS FOR EFFECTIVE SAMPLING\n",
      "\n",
      "Adjusting the temperature used for multinomial sampling has two competing effects on the generated sequences produced by the model. Increasing the temperature increases the diversity of samples\n",
      "\n",
      "〈]√˜√√]√glyph[arrowbt]\n",
      "\n",
      "Figure 3: (a) Semantic entropy makes better use of additional samples because it handles duplication better, the performance gap therefore continues to improve. (b) (bottom) Higher temperatures result in more diversity but less accurate generations. (top) The best performing uncertainty comes from an intermediate temperature that balances these two forces. Results on TriviaQA.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "glyph[negationslash]\n",
      "\n",
      "(Fig. 3b, bottom figure, solid line). One would expect more diverse generations to cover the space of possible meanings more fully. Here we measure the diversity using the average overlap of the longest sub-sequence among sampled answers ( 1 -( M 2 ) -1 ∑ s = s ′ ∈ C Rouge-L ( s , s ′ ) ). At the same time, reducing the temperature improves the average correctness of the answer (Fig. 3b, bottom figure, dashed line). Typically, more accurate models are also better at estimating uncertainty.\n",
      "\n",
      "In fact, we find that these two effects compete and the highest AUROC for semantic entropy and length-normalised entropy is optimised by an intermediate temperature of 0.5 (Fig. 3b, top figure). A lower temperature would improve accuracy, while a higher temperature would improve diversity. A similar figure for CoQA can be found in Appendix B. Note that prior work using predictive entropy as a baseline uses a temperature of 1.0 (Kadavath et al., 2022), which our evaluation suggests would significantly weaken the baseline relative to our implementation.\n",
      "\n",
      "## 7 DISCUSSION\n",
      "\n",
      "Many natural language problems display a crucial invariance: sequences of distinct tokens mean the same thing. Addressing this directly, we introduce semantic entropy-the entropy of the distribution over meanings rather than sequences-and show that this is more predictive of model accuracy on QA than strong baselines. Our unsupervised approach using 'out-of-the-box' models improves reproducibility and is easier to deploy. Unsupervised uncertainty may also help address the observation raised in prior work that supervised uncertainty measures struggle with distribution shift.\n",
      "\n",
      "For semantic entropy, we introduce a novel bidirectional entailment clustering algorithm which uses a smaller natural language inference model. Our method therefore represents a middle ground between fully probabilistic methods and methods that use language models to exploit aspects of natural language that are not transparently present in the model activations. We believe that this sort of joint approach is more promising than relying on either perspective on its own, especially as language models continue to improve. This will become more important in cases where language models are capable of deception, something which our method does not protect against, rather than merely being uncertain between many possible meaningful options. By combining model internals with model prediction one can hope to stay a step ahead of model capabilities.\n",
      "\n",
      "We focus on question answering because this is a particularly important free-form NLG problem with relatively clear ground truths. In the future, however, we hope our work on semantic equivalence can pave the way towards progress in settings like summarisation where correctness requires more human evaluation although additional progress on paraphrase identification in these settings is likely required first. Semantic likelihoods could also be extended to other tools for probabilistic uncertainty like mutual information, potentially offering new strategies for NLG uncertainty.\n",
      "\n",
      "## ACKNOWLEDGMENTS\n",
      "\n",
      "We are grateful to Geoffrey Irving, Kuba Perlin, Laura Rimell, and Miles Turpin for their advice and feedback on earlier drafts of this paper. We are also grateful to the members of the OATML group for helpful discussions about this project.\n",
      "\n",
      "## ETHICS STATEMENT\n",
      "\n",
      "Our aim is to work towards safer AI systems by enabling users to understand the confidence and reliability of language model generations. In principle, this could help mitigate many of the potential harms of NLG from foundation models such as generating false and harmful information in response to genuine questions about important topics like medical questions. However, this potential benefit comes with the risk that systematic mistakes in the assessment of uncertainty or its communication could cause unfounded and misplaced confidence. While this paper represents research progress in identifying new considerations and methods for uncertainty quantification in NLG, before deployment we advise that practitioners conduct extensive evaluations specific to the deployment context to make sure that uncertainty is communicated in a way that empowers users and is not misleading or confusing.\n",
      "\n",
      "## REPRODUCIBILITY STATEMENT\n",
      "\n",
      "Because of the computational cost of experimentation with foundation models, most of the relatively small amount of existing research into NLG uncertainty relies on proprietary models, finetuning of expensive models, and human evaluation. These factors put this kind of research out of reach for many academic groups. Our work takes advantage of the recently released, publicly available OPT models, and builds on this to provide an uncertainty quantification pipeline for NLG that uses entirely open source tools. Meanwhile our method requires no finetuning or training of foundation models and can work with 'off-the-shelf' existing models. We hope that this can facilitate more research on these important topics in the academic community as well as making our methods easier to replicate. We make all of our code, as well as the hand-labelled semantic equivalence dataset drawn from TriviaQA and CoQA, available under an MIT license.\n",
      "\n",
      "## REFERENCES\n",
      "\n",
      "- Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´ e. Concrete Problems in AI Safety. arXiv , 2016. 1\n",
      "- Ion Androutsopoulos and Prodromos Malakasiotis. A survey of paraphrasing and textual entailment methods. Journal of Artificial Intelligence Research , 38:135-187, 2010. 6\n",
      "- Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877-1901, 2020. 1\n",
      "- Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022. 1\n",
      "- Peter W Culicover. Paraphrase generation and information retrieval from stored text. Comput. Linguistics , 11(3-4):78-88, 1968. 6\n",
      "- Mech. Transl.\n",
      "- Shrey Desai and Greg Durrett. Calibration of pre-trained transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 295302, 2020. 6\n",
      "- Samuel Fernando and Mark Stevenson. A semantic similarity approach to paraphrase detection. In Proceedings of the 11th annual research colloquium of the UK special interest group for computational linguistics , pp. 45-52. Citeseer, 2008. 6\n",
      "- Angelos Filos, Sebastian Farquhar, Aidan N Gomez, Tim G J Rudner, Zachary Kenton, Lewis Smith, Milad Alizadeh, Arnoud de Kroon, and Yarin Gal. Benchmarking Bayesian Deep Learning with Diabetic Retinopathy Diagnosis. Bayesian Deep Learning Workshop at NeurIPS , 2019. 7\n",
      "\n",
      "- Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Fr´ ed´ eric Blain, Francisco Guzm´ an, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. Unsupervised quality estimation for neural machine translation. Transactions of the Association for Computational Linguistics , 8: 539-555, 2020. 2, 7\n",
      "- Yarin Gal et al. Uncertainty in deep learning. PhD thesis , 2016. 1, 2\n",
      "- Taisiya Glushkova, Chrysoula Zerva, Ricardo Rei, and Andr´ e FT Martins. Uncertainty-aware machine translation evaluation. arXiv preprint arXiv:2109.06352 , 2021. 6\n",
      "- Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654 , 2020a. 5\n",
      "- Ruining He, Anirudh Ravula, Bhargav Kanagal, and Joshua Ainslie. Realformer: Transformer likes residual attention. arXiv preprint arXiv:2012.11747 , 2020b. 6\n",
      "- Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved Problems in ML Safety. arXiv , 2022. 1\n",
      "- Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022. 1\n",
      "- Fuad Issa, Marco Damonte, Shay B Cohen, Xiaohui Yan, and Yi Chang. Abstract meaning representation for paraphrase detection. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pp. 442-452, 2018. 6\n",
      "- Moksh Jain, Salem Lahlou, Hadi Nekoei, Victor Butoi, Paul Bertin, Jarrid Rector-Brooks, Maksym Korablyov, and Yoshua Bengio. Deup: Direct epistemic uncertainty prediction. arXiv preprint arXiv:2102.08501 , 2021. 3\n",
      "- Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics , 9:962-977, 2021. 6\n",
      "- Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551 , 2017. 7\n",
      "- Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221 , 2022. 1, 2, 3, 6, 7, 8, 9, 18\n",
      "- Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? Advances in neural information processing systems , 30, 2017. 1, 2\n",
      "- Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning using calibrated regression. In International conference on machine learning , pp. 2796-2804. PMLR, 2018. 1\n",
      "- Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems , 30, 2017. 1\n",
      "- Chin-Yew Lin and Franz Josef Och. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04) , pp. 605-612, 2004. 7\n",
      "- Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334 , 2022a. 2, 3, 6\n",
      "\n",
      "- Zi Lin, Jeremiah Zhe Liu, and Jingbo Shang. Towards collaborative neural-symbolic graph semantic parsing via uncertainty. In Findings of the Association for Computational Linguistics: ACL 2022 , pp. 4160-4173, 2022b. 6, 8, 19\n",
      "- David Mackay. Information Theory, Inference and Learning Algorithms . Cambridge University Press, 2003. 4\n",
      "- Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. Advances in neural information processing systems , 31, 2018. 3\n",
      "- Andrey Malinin and Mark Gales. Uncertainty estimation in autoregressive structured prediction. arXiv preprint arXiv:2002.07650 , 2020. 1, 2, 3, 4, 5, 7, 8\n",
      "- Andrey Malinin, Sergey Chervontsev, Ivan Provilkov, and Mark Gales. Regression prior networks. arXiv preprint arXiv:2006.11590 , 2020. 3\n",
      "- Sabrina J Mielke, Arthur Szlam, Y-Lan Boureau, and Emily Dinan. Linguistic calibration through metacognition: aligning dialogue agent responses with expected correctness. arXiv preprint arXiv:2012.14983 , 2020. 6\n",
      "- Kenton Murray and David Chiang. Correcting length bias in neural machine translation. arXiv preprint arXiv:1808.10006 , 2018. 3, 4\n",
      "- Myle Ott, Michael Auli, David Grangier, and Marc'Aurelio Ranzato. Analyzing uncertainty in neural machine translation. In International Conference on Machine Learning , pp. 3956-3965. PMLR, 2018. 6\n",
      "- Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. Advances in neural information processing systems , 32, 2019. 1\n",
      "- Sebastian Pad´ o, Daniel Cer, Michel Galley, Dan Jurafsky, and Christopher D Manning. Measuring machine translation quality as semantic equivalence: A metric based on entailment features. Machine Translation , 23(2):181-193, 2009. 6\n",
      "- Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question answering challenge. Transactions of the Association for Computational Linguistics , 7:249-266, 2019. 7\n",
      "- Ananya B Sai, Akash Kumar Mohankumar, and Mitesh M Khapra. A survey of evaluation metrics used for nlg systems. ACM Computing Surveys (CSUR) , 55(2):1-39, 2022. 6\n",
      "- Richard Socher, Eric Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Ng. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. Advances in neural information processing systems , 24, 2011. 6\n",
      "- Jeff Speaks. Theories of Meaning. In Edward N. Zalta (ed.), The Stanford Encyclopedia of Philosophy . Metaphysics Research Lab, Stanford University, 2021. 3\n",
      "- Yi Tay, Vinh Q Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin, Simon Baumgartner, Cong Yu, and Donald Metzler. Charformer: Fast character transformers via gradient-based subword tokenization. arXiv preprint arXiv:2106.12672 , 2021. 6\n",
      "- Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao, and Hao Ma. Entailment as few-shot learner. arXiv preprint arXiv:2104.14690 , 2021. 6\n",
      "- Yuxia Wang, Daniel Beck, Timothy Baldwin, and Karin Verspoor. Uncertainty estimation and reduction of pre-trained models for text regression. Transactions of the Association for Computational Linguistics , 10:680-696, 2022. 6\n",
      "- Zhiguo Wang, Wael Hamza, and Radu Florian. Bilateral multi-perspective matching for natural language sentences. arXiv preprint arXiv:1702.03814 , 2017. 6\n",
      "- Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426 , 2017. 5\n",
      "\n",
      "- Lei Yu, Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman. Deep learning for answer sentence selection. arXiv preprint arXiv:1412.1632 , 2014. 6\n",
      "- Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022. 2, 7\n",
      "\n",
      "Table 3: Illustration of semantic, syntactic, and lexical equivalence. Work with foundation models implicitly focuses on lexical equivalence, which entails the others, but we usually care about semantic equivalence.\n",
      "\n",
      "|                                 |                                                                                             | Equivalence   | Equivalence   | Equivalence   |\n",
      "|---------------------------------|---------------------------------------------------------------------------------------------|---------------|---------------|---------------|\n",
      "| Sentence A                      | Sentence B                                                                                  | Lexical       | Syntactic     | Semantic      |\n",
      "| Paris is the capital of France. | Paris is the capital of France. Berlin is the capital of France. France's capital is Paris. | !             | ! !           | ! !           |\n",
      "\n",
      "## A FURTHER DETAILS ON SEMANTIC ENTROPY\n",
      "\n",
      "## A.1 FURTHER DISCUSSION OF SEMANTIC EQUIVALENCE\n",
      "\n",
      "We illustrate the distinction between different kinds of equivalence in Table 3. Lexically equivalent sequences use exactly the same symbols. They are always also semantically and syntactically equivalent (in a given context). Syntactically equivalent sentences have the same grammatical form. But they can have different meanings (not semantically equivalent) and can use different symbols (not lexically equivalent). Semantically equivalent sentences mean the same thing, but they might have different grammatical form (not syntactically equivalent) or symbols (not lexically equivalent). Two sentences can also be both syntactically and semantically equivalent but not lexically equivalent if they match up to a synonym.\n",
      "\n",
      "Soft equivalence and transitivity. Formally, semantic equivalence is transitive. That is, if E ( s , s ′ ) and E ( s ′ , s ′′ ) then it follows that E ( s , s ′′ ) . However, the implementation of our bidirectional equivalence algorithm permits some classification errors and it is slightly 'soft'-it will sometimes return equivalent for pairs that are not quite equivalent. As a result, it is not strictly true that our equivalence relation is transitive, and therefore not strictly true that there is a unique set of equivalence classes. For example, the clusters might depend on the order in which the comparisons are made. In practice, however, we find that this does not pose a noticeable problem-usually, inspecting the outputs shows that the equivalence appears clear cut. However, we acknowledge this potential issue as an area for improvement in future clustering algorithms.\n",
      "\n",
      "Unequal token importance. From the perspective of meaning, some tokens can matter more than others-key words. Naive methods like predictive entropy do distinguish between key words or unimportant tokens. Supervised uncertainty methods that make use of language models in the uncertainty evaluation can potentially take this into account better. In addition, our semantic entropy approach partly adjusts for this, as discussed in Section 4.1.\n",
      "\n",
      "## A.2 FURTHER ALGORITHMIC DETAILS\n",
      "\n",
      "In addition to the description of the method provided in the main body, in Algorithm 1 we provide the pseudocode for our bi-directional entailment algorithm.\n",
      "\n",
      "## A.3 IMPACT OF SAMPLING METHOD ON QUALITY OF UNCERTAINTY ESTIMATE\n",
      "\n",
      "In Section 4, we study the impact of the temperature hyper-parameter on the performance of the uncertainty measures. Here, we show a variant of Fig. 3b for the CoQA dataset showing an almost identical pattern. Like TriviaQA, the optimal temperature is 0.5 despite a significantly harder problem with lower accuracy, suggesting that this choice hyperparameter may generalize well. Unlike TriviaQA, normalised entropy outperforms semantic entropy at high temperatures.\n",
      "\n",
      "Beyond the temperature, there are a number of other design choices to be made when sampling: the sampling method and hyper-parameters such as top-p and top-k . Our contribution in this paper is to show the importance of these choices for uncertainty estimation which has been overlooked previously, and study the temperature in particular. While we leave the detailed study of these hyperparameters to future work, we do compare our default multinomial sampling method, to multinomial beam search sampling which focuses more on high-likelihood regions of the output space.\n",
      "\n",
      "## Algorithm 1 Bidirectional Entailment Clustering\n",
      "\n",
      "```\n",
      "Require: context x , set of seqs. { s (2) , . . . , s ( M ) } , NLI classifier M , set of meanings C = {{ s (1) }} for 2 ≤ m ≤ M do for c ∈ C do glyph[triangleright] Compare to already-processed meanings. s ( c ) ← c 0 glyph[triangleright] Use first sequence for each semantic-class. left ←M (cat( x, s ( c ) , ' < g/ > ' , x, s ( m ) )) glyph[triangleright] Does old sequence entail new one? right ←M (cat( x, s ( m ) , ' < g/ > ' , x, s ( c ) )) glyph[triangleright] Vice versa? if left is entailment and right is entailment then c ← c ⋃ s ( m ) glyph[triangleright] Put into existing class. end if end for C ← C ⋃ { s ( m ) } glyph[triangleright] Semantically distinct, gets own class. end for return C\n",
      "```\n",
      "\n",
      "〈]√˜√√]√glyph[arrowbt]\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "⋂˜⌉√˜√∐√√√˜\n",
      "\n",
      "Figure 4: CoQA temperature ablation. (bottom) Similar to TriviaQA, higher temperatures mean higher diversity and lower accuracy. (top) The best performance for both methods comes at a temperature of 0.5. Unlike TriviaQA, normalised entropy outperforms semantic entropy at high temperatures.\n",
      "\n",
      "In Table 4 we show that multinomial beam search sampling yields uncertainty measures that are less predictive of model accuracy than multinomial sampling. Beam search also generates much less diverse samples. We conjecture that multinomial beam search sampling focuses too much on the most likely sequences. The diversity of this beam search corresponds to the lowest temperature result in Fig. 4. As in the main body of the paper, we measure diversity as the average lexical overlap of the answers in the answer set. Additionally, we investigate, why the semantic entropy underperforms the length-normalised entropy at high temperatures. To that end, we manually inspect and label 100 classifications of our semantic equivalence method at T=1.5, and we find that at these temperatures, many of the generated model answers are nonsensical combinations of words from the context that is provided for the question. While the likelihood of these sequences still seems somewhat predictive of the model's accuracy, semantic clustering becomes very difficult and an unreliable signal for uncertainty estimation. At this temperature, the accuracy of the semantic equivalence methods is only 61%, whereas it is over 92% at lower temperatures (see Appendix B.2)\n",
      "\n",
      "Note, that at low-temperatures, where one does gets plausible and well-formed model generations, semantic entropy does clearly outperform the baselines. This finding further underlines the importance of choosing appropriate sampling hyper-parameters when using entropy-based uncertainty measures in NLG.\n",
      "\n",
      "Table 4: Multinomial beam search sampling produces sampled answers that are less diverse and thus less useful for uncertainty estimation than multinomial sampling.\n",
      "\n",
      "| Sampling method                  |   Semantic Entropy AUROC |   Diversity of answers |\n",
      "|----------------------------------|--------------------------|------------------------|\n",
      "| Multinomial sampling             |                    0.758 |                  0.49  |\n",
      "| Multinomial beam search sampling |                    0.735 |                  0.258 |\n",
      "\n",
      "## B EXPERIMENTAL DETAILS AND ABLATIONS\n",
      "\n",
      "We use both the OPT models 2 and the Deberta-large model 3 via the HuggingFace transformers library which can be easily adopted for reproducibility. All of our code is open-source and relies on no proprietary models.\n",
      "\n",
      "We use the following functions of the HuggingFace API to sample the most likely answers, and the set of answers:\n",
      "\n",
      "- To obtain the answer which is compared to the reference answer, which determines whether the question is correctly answered, we use beam search using the generate() function with num beams = 5 and do sample = True .\n",
      "- To obtain the answer set for uncertainty estimation, by default we use multinomial sampling, that is generate() using do sample = True and num beams = 1 . If indicated explicitly, we use beam multinomial sampling, that is generate() using num beams = 5 and do sample = True .\n",
      "\n",
      "We run all of our experiments on 80GB NVIDIA A100s.\n",
      "\n",
      "Testing up to 20 samples per answer on the 2.7B, 6.7B and 13B CoQA experiments, we conclude that using more than 10 samples does not significantly improve the performance of the uncertainty measure, we use 10 sampled answers per question in the remaining experiments on TriviaQA. Note, that in Table 2 we compare the 30B model on CoQA and TriviaQA where in both settings we use answer sets of size 10.\n",
      "\n",
      "We use the following prompts on CoQA and TriviaQA. We find that on CoQA, we obtain accurate model results with zero-shot prompting. While we have to use few-shot prompting to obtain accurate answers on closed-book TriviaQA. We use the following prompts for each of the settings:\n",
      "\n",
      "## CoQA:\n",
      "\n",
      "```\n",
      "[The provided context paragraph] [additional question-answer pairs] Q: [Provided question] A:\n",
      "```\n",
      "\n",
      "where additional question-answer pairs are preceding turns of the conversation about the paragraph consisting of questions and reference answers.\n",
      "\n",
      "## TriviaQA:\n",
      "\n",
      "For TriviaQA, we use a 10-shot prompt of the format:\n",
      "\n",
      "Q: Which Oscar-nominated film had You Sexy Thing as its theme song? A: The Full Monty Q: Which Joan's career revived in Whatever Happened to Baby Jane? A: Crawford Q: Which much-loved actor won the Best Actor Oscar for The Philadelphia Story? A: James Stewart (...) Q: In which river is the Boulder Dam? A:\n",
      "\n",
      "To account for generations where the model continues the Q:...A:... pattern after providing an answer to the given question, we trim all generations by pattern matching for a selection of stopwords that we observe in the generations: Q: , Question: , QUESTION: and questions: .\n",
      "\n",
      "2 https://huggingface.co/docs/transformers/model doc/opt\n",
      "\n",
      "3 https://huggingface.co/docs/transformers/model doc/opt\n",
      "\n",
      "Table 5: Automatic evaluation of question answering is highly accurate as compared to human evaluation. We evaluate how accurate the automatic evaluation metric. The predictions, in this settings are the automatically determined accuracy labels on our question answering task, and the ground truth are human labels for the accuracy of the provided model generation given the reference answer\n",
      "\n",
      "| Data set   |   Accuracy of automatic evaluation |\n",
      "|------------|------------------------------------|\n",
      "| CoQA       |                               0.89 |\n",
      "| TriviaQA   |                               0.96 |\n",
      "\n",
      "Table 6: TriviaQA: the exact choice of accuracy metric for the free-form QA task has little effect on the assessment of the quality of the uncertainty measure.\n",
      "\n",
      "| Metric                     | AUROC            | AUROC              | Accuracy   |\n",
      "|----------------------------|------------------|--------------------|------------|\n",
      "|                            | Semantic entropy | Normalised entropy |            |\n",
      "| Rouge-L ( y, y ′ ) > 0 . 3 | 0.828            | 0.802              | 0.506      |\n",
      "| Rouge-L ( y, y ′ ) > 0 . 5 | 0.835            | 0.810              | 0.456      |\n",
      "| Rouge-1 ( y, y ′ ) > 0 . 5 | 0.835            | 0.810              | 0.457      |\n",
      "| Exact matching             | 0.828            | 0.808              | 0.394      |\n",
      "\n",
      "## B.1 RELIABILITY OF ACCURACY METRIC AS COMPARED TO HUMAN EVALUATION\n",
      "\n",
      "In our experiments, we evaluate how well our uncertainty measures predict the model's accuracy when answering a given question. The choice of accuracy metric is thus a crucial component of our experimental setup. Generally, it has been shown to be difficult to develop automatic metrics for free-form generation that correlate well with human evaluations. We thus verify our choice of accuracy criterion: Rouge-L ( y, y ′ ) &gt; 0 . 3 , for a given reference answer y and a model generation y ′ . We manually evaluate the accuracy of 200 answers of the 30B parameter model on both COQA and on TriviaQA, and evaluate how closely the human evaluation matches the automatic evaluation. We find that on both data sets, the accuracy of the automatic labels as compared to the human labels as the ground truth is high, see Table 5.\n",
      "\n",
      "## B.2 TESTING THE BI-DIRECTIONAL ENTAILMENT CLASSIFIER\n",
      "\n",
      "To the best of our knowledge, this paper is the first application of the bi-directional entailment approach to identifying answers with the same meaning in question answering. Since this is a core component of our approach, we verify how accurately this approach identifies model answers with the same meaning. To this end, we manually label 300 samples for each of TriviaQA and CoQA produced by the 13B parameter model to provide a ground truth as to whether or not they mean the same thing. We find that the model achieves an accuracy of 92.7% and 95.3% respectively.\n",
      "\n",
      "## B.3 SENSITIVITY OF RESULTS TO ACCURACY METRIC\n",
      "\n",
      "In principle, the choice of metric to decide whether or not an answer is 'correct' might have a large effect on the assessment of our method and baselines. However, we find empirically that our results are relatively insensitive to the choice of accuracy metric.\n",
      "\n",
      "In Table 6 we show that for TriviaQA the choice of accuracy metric for the question answering has almost no effect on the measured AUROC of the uncertainty estimation, despite making the measured accuracy of the model's generation significantly different. In particular, the exact matching requirement reduces the accuracy significantly but has little effect on the AUROCs.\n",
      "\n",
      "For CoQA, which is an open-book QA task with greater answer variability and longer answers the results are broadly similar (see Table 7) except for the exact matching accuracy criterion which is too demanding because of the much larger variety of possible answers for this task.\n",
      "\n",
      "Table 7: CoQA: the exact choice of the accuracy metric for the free-form open-book QA task has little effect on the assessment of the quality of the uncertainty measure except for the use of exact matching. For CoQA, getting an exact match is significantly harder.\n",
      "\n",
      "| Metric                     | AUROC            | AUROC              | Accuracy   |\n",
      "|----------------------------|------------------|--------------------|------------|\n",
      "|                            | Semantic entropy | Normalised entropy |            |\n",
      "| Rouge-L ( y, y ′ ) > 0 . 3 | 0.7672           | 0.7533             | 0.8239     |\n",
      "| Rouge-L ( y, y ′ ) > 0 . 5 | 0.7379           | 0.7290             | 0.7657     |\n",
      "| Rouge-1 ( y, y ′ ) > 0 . 3 | 0.7672           | 0.7533             | 0.8239     |\n",
      "| Rouge-1 ( y, y ′ ) > 0 . 5 | 0.7397           | 0.7309             | 0.7677     |\n",
      "| Exact matching             | 0.6749           | 0.6727             | 0.6459     |\n",
      "\n",
      "Figure 5: Accuracy improves with model size, as does semantic entropy's uncertainty performance. At the smallest model size, both accuracy and uncertainty diminish.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## B.4 ACCURACY ABLATIONS WITH MODEL SIZE\n",
      "\n",
      "We confirm that increasing the model size improves the accuracy of the generations on both QA datasets (see Fig. 5a and Fig. 5b). Semantic entropy's uncertainty performance is also shown for context.\n",
      "\n",
      "## B.5 EXAMPLE P(TRUE) FORMAT\n",
      "\n",
      "The format of the prompt, reproduced here for convenient reference from the original source Kadavath et al. (2022), is:\n",
      "\n",
      "```\n",
      "Question: Who was the third president of the United States? Here are some brainstormed ideas: James Monroe Thomas Jefferson John Adams Thomas Jefferson George Washington Possible Answer: James Monroe Is the possible answer: (A) True (B) False The possible answer is:\n",
      "```\n",
      "\n",
      "where the 'brainstormed answers' are from the set of sampled answers A and P(True), i.e. the likelihood of the next token being True is taken as the uncertainty measure. The authors note that doing the above needs to be done in a few-shot manner and does not work well as in a zero-shot format. In our experiments, we use a few-shot prompt with 10 examples.\n",
      "\n",
      "\n",
      "\n",
      "⊗√⌉̂˜√(}˜(√∐√∐⌉˜√˜√√(}˜(⌉}̂˜⌈\n",
      "\n",
      "Figure 6: The margin probability, i.e. the difference between the likelihood of the most likely answer and the likelihood of the second most likely answer, is not very predictive of models' accuracy on CoQA open-book question answering (a) nor on TriviaQA (b). Identical to Fig. 2 with the addition of Margin probability which was previously omitted to avoid stretching the scale.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## B.6 MARGIN-PROBABILITY BASELINE\n",
      "\n",
      "We additionally compare our method to the margin probability method used for neural-symbolic parsing in Lin et al. (2022b):\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where y (1) is the top-1 beam search result and y (2) is the top-2 beam search result.\n",
      "\n",
      "Initially, running the method as proposed in Lin et al. (2022b) using a 13B parameter model on CoQA, we find that H margin is not very predictive of the model's accuracy on answering questions in CoQA achieving an AUROC of 0.54.\n",
      "\n",
      "We hypothesise that two factors contribute to this poor performance. First, since this measure only looks at the difference of likelihoods, the information about the magnitude of the likelihood of a given answer is lost. Second-analogously to the predictive entropy-it would be important to take semantic uncertainty into account when computing H margin . Manually inspecting model answers on CoQA, and the corresponding H margin , we see that the margin between two semantically equivalent answers and two semantically distinct answers is often similar. That is, this measure does not distinguish between uncertainty between paraphrases of the same meaning (in which case the model might actually be confident about meaning of the answer), and the model's uncertainty about which semantically distinct meaning is correct.\n",
      "\n",
      "We find that if instead of obtaining y (1) and y (2) by multinomial sampling (as in our other experiments) instead of by beam search, this second problem becomes less pronounced and H margin performs better while still being clearly outperformed by the other methods we study. We report our full results in Fig. 6.\n",
      "\n",
      "Table 8: Example of challenges for H margin . H margin does not distinguish between lexical and semantic uncertainty and thus can not distinguish cases where the model is certain about the correct answer (but uncertain about the precise formulation) as in row 1, and cases where the model is uncertain about the correct answer as in row 2. The semantic entropy correctly indicates low uncertainty in the first case and high uncertainty in the second case.\n",
      "\n",
      "| y (1)          | y (2)   |   H margin |   Semantic entropy |\n",
      "|----------------|---------|------------|--------------------|\n",
      "| Thomas Edison. | Edison. |       0.9  |               0.1  |\n",
      "| Thomas.        | George. |       0.36 |               0.87 |\n",
      "Document 6:\n",
      "## Formalizing Timing Diagram Requirements in Discrete Duration Calulus\n",
      "\n",
      "Raj Mohan Matteplackel 1 , Paritosh K. Pandya 1 , and Amol Wakankar 2\n",
      "\n",
      "1\n",
      "\n",
      "Tata Institute of Fundamental Research, Mumbai 400005, India. { raj.matteplackel,pandya } @tifr.res.in 2 Bhabha Atomic Research Centre, Mumbai, India.\n",
      "\n",
      "amolk@barc.gov.in\n",
      "\n",
      "Abstract. Several temporal logics have been proposed to formalise timing diagram requirements over hardware and embedded controllers. These include LTL [CF05], discrete time MTL [AH93] and the recent industry standard PSL [EF16]. However, succintness and visual structure of a timing diagram are not adequately captured by their formulae [CF05]. Interval temporal logic QDDC is a highly succint and visual notation for specifying patterns of behaviours [Pan00].\n",
      "\n",
      "In this paper, we propose a practically useful notation called SeCeNL which enhances negation free fragment of QDDC with features of nominals and limited liveness . We show that timing diagrams can be naturally (compositionally) and succintly formalized in SeCeNL as compared with PSL-Sugar and MTL. We give a linear time translation from timing diagrams to SeCeNL. As our second main result, we propose a linear time translation of SeCeNL into QDDC. This allows QDDC tools such as DCVALID [Pan00,Pan01] and DCSynth to be used for checking consistency of timing diagram requirements as well as for automatic synthesis of property monitors and controllers. We give examples of a minepump controller and a bus arbiter to illustrate our tools. Giving a theoretical analysis, we show that for the proposed SeCeNL, the satisfiability and model checking have elementary complexity as compared to the nonelementary complexity for the full logic QDDC.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "A timing diagram is a collection of binary signals and a set of timing constraints on them. It is a widely used visual formalism in the realm of digital hardware design, communication protocol specification and embedded controller specification. The advantages of timing diagrams in hardware design are twofold, one, since designers can visualize waveforms of signals they are easy to comprehend and two, they are very convenient for specifying ordering and timing constraints between events (see figures Fig. 1 and Fig. 2 below).\n",
      "\n",
      "There have been numerous attempts at formalizing timing diagram constraints in the framework of temporal logics such as the timing diagram logic [Fis99], with LTL formulas [CF05], and as synchronous regular timing diagrams\n",
      "\n",
      "[AEKN00]. Moreover, there are industry standard property specification languages such as PSL/Sugar and OVA for associating temporal assertions to hardware designs [EF16]. The main motivation for these attempts was to exploit automatic verification techniques that these formalisms support for validation and automatic circuit synthesis. However, commenting on their success, Fisler et. al. state that the less than satisfactory adoption of formal methods in timing diagram domain can be partly attributed to the gulf that exists between graphical timing diagrams and textual temporal logic - expressing various timing dependencies that can exist among signals that can be illustrated so naturally in timing diagrams is rather tedious in temporal logics [CF05]. As a result, hardware designers use timing diagrams informally without any well defined semantics which make them unamenable to automatic design verification techniques.\n",
      "\n",
      "In this paper, we take a fresh look at formalizing timing diagram requirements with emphasis on the following three features of the formalism that we propose here.\n",
      "\n",
      "Firstly, we propose the use of an interval temporal logic QDDC to specify patterns of behaviours. QDDC is a highly succinct and visual notation for specifying regular patterns of behaviours [Pan00,Pan01,KP05]. We identify a quantifier and negation-free subset SeCe of QDDC which is sufficient for formalizing timing diagram patterns. It includes generalized regular expression like syntax with counting constructs. Constraints imposed by timing diagrams are straightforwardly and compactly stated in this logic. For example, the timing diagram in Fig. 1 stating that P transits from 0 to 1 somewhere in interval u to u +3 cycles is captured by the SeCe formula [ ¬ P]^&lt;u&gt;^(slen=3 ∧ [ ¬ P]^[[P]])^[[P]] . The main advantage of SeCe is that it has elementary satisfiability as compared to the non-elementary satisfiability of general QDDC .\n",
      "\n",
      "Fig. 1. Timing diagram with a marked position u and a timing constraint.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Secondly, it is very typical for timing diagrams to have partial ordering and synchronization constraints between distinct events. Emphasizing this aspect, formalisms such as two dimensional regular expressions [Fis07] have been proposed for timing diagrams. We find that synchronization in timing diagram may even extend across different patterns of limited liveness properties. In order to handle such synchronization, we extend our logic SeCe with nominals from hybrid temporal logics [FdRS03]. Nominals are temporal variables which 'freeze' the positions of occurrences of events. They naturally allow synchronization across formulae.\n",
      "\n",
      "Thirdly, we enhance the timing diagram specifications (as well as logic SeCe) with limited liveness operators . While timing diagrams visually specify patterns of occurrence of signals, they do not make precise the modalities of occurrences of such patterns. We explicitly introduce modalities such as a ) initially, a specified pattern must occur, or that b ) every occurrence of pattern1 is necessarily and immediately followed by an occurrence of pattern2 , or that c ) occurrence of a specified pattern is forbidden anywhere within a behaviour. In this, we are inspired by Allen's Interval Algebra relations [All83] as well as the LSC operators of Harel for message sequence charts [DH01]. We confine ourselves to limited liveness properties where good things are achieved within specified bounds. For example, in specifying a modulo 6 counter, we can say that the counter will stabilize before completion of first 15 cycles. Astute readers will notice that, technically, our limited liveness operators only give rise to 'safety' properties (in the sense of Alpern and Schneider [AS87]). However, from a designer's perspective they do achieve the practical goal of forcing good things to happen.\n",
      "\n",
      "Putting all these together, we define a logic SeCeNL which includes negationfree QDDC together with limited liveness operators as well as nominals. The formal syntax and semantics of SeCeNL formulas is given in § 2.3. We claim that SeCeNL provides a natural and convenient formalism for encoding timing diagram requirements. Substantiating this, we formulate a translation of timing diagrams into SeCeNL formulae in § 3 . The translation is succinct, in fact, linear time computable in the size of the timing diagram. (A textual syntax is used for timing diagrams. The textual syntax of timing diagrams used is inspired by the tool WaveDrom [CP16], which is also used for graphical rendering of our timing diagram specifications.) Moreover, the translation is compositional, i.e. it translates each element of the timing diagram as one small formula and overall specification is just the conjunction of such constraints. Hence, the translation preserves the structure of the diagram.\n",
      "\n",
      "With several examples of timing diagrams, we compare its SeCeNL formula with the formula in logics such as PSL-Sugar and MTL. Logic PSL-Sugar is amongst the most expressive notations for requirements. Logic PSL-Sugar is syntactically a superset of MTL and LTL. It extends LTL with SERE (regular expressions with intersection) which are similar to our SeCe. In spite of this, we a show natural examples where SeCeNL formula is at least one exponent more succinct as compared to PSL-Sugar.\n",
      "\n",
      "As the second main contribution of this paper, we consider formal verification and controller synthesis from SeCeNL specifications. In § 3.1, we formulate a reduction from a SeCeNL formula to an equivalent QDDC formula. This allows QDDC tools to be used for SeCeNL. It may be noted that, though expressively no more powerful than QDDC, logic SeCeNL considerably more efficient for satisfiability and model checking. We show that these problems have elementary complexity as compared with full QDDC which exhibits non-elementary complexity. Also, the presence of limited liveness and nominals makes it more convenient as compared to QDDC for practical use.\n",
      "\n",
      "By implementing the above reductions, we have constructed a Python based translator which converts a requirement consisting of a boolean combination of timing diagram specifications (augmented with limited liveness) and SeCeNL formulae into an equivalent QDDC formula. We can analyze the resulting formula using the QDDC tools DCVALID [Pan00,Pan01] as well as DCSynthG for model checking and controller synthesis, respectively (see Fig. 11 for the tool chain). We illustrate the use of our tools by the case studies of a synchronous bus arbiter and a minepump controller in § 4. Readers may note that we specify rather rich quantitative requirements not commonly considered, and our tools are able to automatically synthesize monitors and controllers for such specifications.\n",
      "\n",
      "## 2 Logic QDDC\n",
      "\n",
      "Let Σ be a finite non empty set of propositional variables. A word σ over Σ is a finite sequence of the form P 0 · · · P n where P i ⊆ Σ for each i ∈ { 0 , . . . , n } . Let len ( σ ) = n +1, dom ( σ ) = { 0 , . . . , n } and ∀ i ∈ dom ( σ ) : σ ( i ) = P i .\n",
      "\n",
      "The syntax of a propositional formula over Σ is given by:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "and operators such as ⇒ and ⇔ are defined as usual. Let Ω Σ be the set of all propositional formulas over Σ .\n",
      "\n",
      "Let σ = P 0 · · · P n be a word and ϕ ∈ Ω Σ . Then, for an i ∈ dom ( σ ) the satisfaction relation σ, i | = ϕ is defined inductively as expected: σ, i | = 1 ; σ, i | = p iff p ∈ σ ( i ); σ, i | = ¬ p iff σ, i glyph[negationslash]| = p , and the satisfaction relation for the rest of the boolean combinations defined in a natural way.\n",
      "\n",
      "The syntax of a QDDC formula over Σ is given by:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where ϕ ∈ Ω Σ , p ∈ Σ , c ∈ N and glyph[triangleright] glyph[triangleleft] ∈ { &lt;, ≤ , = , ≥ , &gt; } .\n",
      "\n",
      "An interval over a word σ is of the form [ b, e ] where b, e ∈ dom ( σ ) and b ≤ e . An interval [ b 1 , e 1 ] is a sub interval of [ b, e ] if b ≤ b 1 and e 1 ≤ e . Let Intv ( σ ) be the set of all intervals over σ .\n",
      "\n",
      "Let σ be a word over Σ and let [ b, e ] ∈ Intv ( σ ) be an interval. Then the satisfaction relation of a QDDC formula D over Σ , written σ, [ b, e ] | = D , is defined inductively as follows:\n",
      "\n",
      "```\n",
      "σ, [ b, e ] | = 〈 ϕ 〉 iff σ, b | = ϕ, σ, [ b, e ] | = [ ϕ ] iff ∀ b ≤ i < e : σ, i | = ϕ, σ, [ b, e ] | = [[ ϕ ]] iff ∀ b ≤ i ≤ e : σ, i | = ϕ, σ, [ b, e ] | = {{ ϕ }} iff e = b +1 and σ, b | = ϕ, σ, [ b, e ] | = ¬ D iff σ, [ b, e ] glyph[negationslash]| = D, σ, [ b, e ] | = D 1 ∨ D 2 iff σ, [ b, e ] | = D 1 or σ, [ b, e ] | = D 2 , σ, [ b, e ] | = D 1 ∧ D 2 iff σ, [ b, e ] | = D 1 and σ, [ b, e ] | = D 2 , σ, [ b, e ] | = D 1 ^ D 2 iff ∃ b ≤ i ≤ e : σ, [ b, i ] | = D 1 and σ, [ i, e ] | = D 2 .\n",
      "```\n",
      "\n",
      "glyph[negationslash]\n",
      "\n",
      "We call word σ ′ a p -variant, p ∈ Σ , of a word σ if ∀ i ∈ dom ( σ ) , ∀ q = p : σ ′ ( i )( q ) = σ ( i )( q ). Then σ, [ b, e ] | = ∃ p. D ⇔ σ ′ , [ b, e ] | = D for some p -variant σ ′ of σ and, σ, [ b, e ] | = ∀ p. D ⇔ σ, [ b, e ] glyph[negationslash]| = ∃ p. ¬ D . We define σ | = D iff σ, [0 , len ( σ )] | = D .\n",
      "\n",
      "Example 1. Let Σ = { p, q } and let σ = P 0 · · · P 7 be such that ∀ 0 ≤ i &lt; 7 : P i = { p } and P 7 = { q } . Then σ, [0 , 7] | = [ p ] but not σ, [0 , 7] | = [[ p ]] as p glyph[negationslash]∈ P 7 .\n",
      "\n",
      "Example 2. Let Σ = { p, q, r } and let σ = P 0 · · · P 10 be such that ∀ 0 ≤ i &lt; 4 : P i = { p } , ∀ 4 ≤ i &lt; 8 : P i = { p, q, r } and ∀ 8 ≤ i ≤ 10 : P i = { q, r } . Then\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "because for i ∈ { 8 , 9 , 10 } the condition ∃ 0 ≤ i ≤ 10 : σ, [0 , i ] | = [ p ] and σ, [ i, 10] | = [[ ¬ p ∧ r ]] is met. But σ, [0 , 7] glyph[negationslash]| = [ p ] ^ [[ ¬ p ∧ r ]] as ¬∃ 0 ≤ i ≤ 7 : σ, [0 , i ] | = [ p ] and σ, [ i, 7] | = [[ ¬ p ∧ r ]].\n",
      "\n",
      "Entities slen , scount , and sdur are called terms in QDDC. The term slen gives the length of the interval in which it is measured, scount ϕ where ϕ ∈ Ω Σ , counts the number of positions including the last point in the interval under consideration where ϕ holds, and sdur ϕ gives the number of positions excluding the last point in the interval where ϕ holds. Formally, for ϕ ∈ Ω Σ { }\n",
      "\n",
      "we have slen\n",
      "\n",
      "(\n",
      "\n",
      "σ,\n",
      "\n",
      "[\n",
      "\n",
      "b, e\n",
      "\n",
      "]) =\n",
      "\n",
      "e\n",
      "\n",
      "-\n",
      "\n",
      "b\n",
      "\n",
      ",\n",
      "\n",
      "scount\n",
      "\n",
      "(\n",
      "\n",
      "σ, ϕ,\n",
      "\n",
      "[\n",
      "\n",
      "b, e\n",
      "\n",
      "]) =\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "e\n",
      "\n",
      "b\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      ",\n",
      "\n",
      ",\n",
      "\n",
      "if\n",
      "\n",
      "σ, i\n",
      "\n",
      "|\n",
      "\n",
      "=\n",
      "\n",
      "ϕ, otherwise.\n",
      "\n",
      "and\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "lowing derived constructs: σ, [ b, e ] | = pt iff b = e ; σ, [ b, e ] | = ext iff b &lt; e ; σ, [ b, e ] | = ♦ D iff true ^ D ^ true and σ, [ b, e ] | = glyph[square] D iff σ, [ b, e ] glyph[negationslash]| = ♦ ¬ D .\n",
      "\n",
      "A formula automaton for a QDDC formula D is a deterministic finite state automaton which accepts precisely language L = { σ | σ | = D } .\n",
      "\n",
      "Theorem 1. [Pan01] For every QDDC formula D over Σ we can construct a DFA A ( D ) for D such L ( D ) = L ( A ( D )) . The size of A ( D ) is non elementary in the size of D in the worst case.\n",
      "\n",
      "## 2.1 Chop expressions: Ce and SeCe\n",
      "\n",
      "Definition 1. The logic Semi extended Chop expressions (SeCe) is a syntactic subset of QDDC in which the operators ∃ p. D , ∀ p. D and negation are not allowed. The logic Chop expressions (Ce) is a sublogic of SeCe in which conjuction is not allowed.\n",
      "\n",
      "Lemma 1. For any chop expression D of size n we can effectively construct a language equivalent DFA A of size Ω (2 2 n ) .\n",
      "\n",
      "Proof. We observe that for any chop expression D we can construct a language equivalent NFA which is at most exponential in size of D including the constants appearing in it (for a detailed proof see [BP12] wherein a similar result has been proved). But this implies there exists a DFA of size 2 2 n which accepts exactly the set of words σ such that σ | = D .\n",
      "\n",
      "∑\n",
      "\n",
      "Corollary 1. For any SeCe D of size n we can effectively construct a language equivalent DFA A of size Ω (2 2 2 n ) .\n",
      "\n",
      "Proof. Proof follows from the definition of SeCe, lemma 1 and from the fact that the size of the product of DFA 's can be atmost exponential in the size of individual DFA 's.\n",
      "\n",
      "## 2.2 DCVALID and DCSynthG\n",
      "\n",
      "The reduction from a QDDC formula to its formula automaton has been implemented into the tool DCVALID [Pan00,Pan01]. The formula automaton it generates is total, deterministic and minimal automaton for the formula. DCVALID can also translate the formula automaton into Lustre/SCADE, Esterel, SMV and Verilog observer module . By connecting this observer module to run synchronously with a system we can reduce model checking of QDDC property to reachability checking in observer augmented system. See [Pan00,Pan01] for details. A further use of formula automata can be seen in the tool called DCSynthG which synthesizes synchronous dataflow controller in SCADE/NuSMV/Verilog from QDDC specification.\n",
      "\n",
      "## 2.3 Logic SeCeNL: Syntax and Semantics\n",
      "\n",
      "We can now introduce our logic SeCeNL which builds upon SeCe by augmenting it with nominals and limited liveness operators .\n",
      "\n",
      "Syntax : The syntax of SeCeNL atomic formula is as follows. Let D , D 1 , D 2 and D 3 range over SeCe formulae and let Θ , Θ 1 , Θ 2 and Θ 3 range over subset of propositional variables occurring in SeCe formula. The notation D : Θ , called a nominated formula , denotes that Θ is the set of variables used as nominals in the formula D .\n",
      "\n",
      "```\n",
      "init ( D 1 : Θ 1 / D 2 : Θ 2 ) | anti ( D : Θ ) | pref ( D : Θ ) | implies ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 ) | follows ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 /D 3 : Θ 3 ) | triggers ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 /D 3 : Θ 3 )\n",
      "```\n",
      "\n",
      "An SeCeNL formula is a boolean combination of atomic SeCeNL formulae of the form above. As a convention, D : {} is abbreviated as D when the set of nominals Θ is empty.\n",
      "\n",
      "Limited Liveness Operators : Given an word σ and a position i ∈ dom ( σ ), we state that σ, i | = D iff σ [0 : i ] | = D . Thus, the interpretation is that the past of the position i in execution satisfies D .\n",
      "\n",
      "For a SeCe formula D we let Ξ ( D ) = D ∧ ¬ ( D ^ ext ), which says that if σ, [ b, e ] | = Ξ ( D ) then σ, [ b, e ] | = D and there exists no proper prefix interval [ b, e 1 ], (i. e. [ b, e 1 ] ∈ Intv ( σ ) and b ≤ e 1 &lt; e ) such that σ, [ b, e 1 ] | = D . We say σ ′ ≤ prefix σ if σ ′ is a prefix of σ , and σ ′ &lt; prefix σ if σ ′ is a proper prefix of σ .\n",
      "\n",
      "We first explain the semantics of limited liveness operators assuming that no nominals are used in the specification, i.e. Θ , Θ 1 , Θ 2 and Θ 3 are all empty. A set S ⊆ Σ ∗ is prefix closed if σ ∈ S then ∀ σ ′ : σ ′ ≤ prefix σ ⇒ σ ′ ∈ S . We observe that each atomic liveness formula denotes a prefix closed subset of (2 Σ ) + .\n",
      "\n",
      "- -L ( pref ( D ) ) = { σ | ∀ σ ′ ≤ prefix σ : σ ′ | = D } . Operator pref ( D ) denotes that D holds invariantly throughout the execution.\n",
      "- -L ( init ( D 1 /D 2 )) = { σ | ∀ j : σ, [0 , j ] | = D 2 ⇒ ∃ k ≤ j : σ, [0 , k ] | = D 1 } . Operator init ( D 1 /D 2 ) basically states that if j is the first position which satisfies D 2 in the execution then there exists an i ≤ j such that i satisfies D 1 . Thus, initially D 1 holds before D 2 unless the execution (is too short and hence) does not satisfy D 2 anywhere.\n",
      "- -L ( anti ( D )) = { σ | ∀ i, j : σ, [ i, j ] glyph[negationslash]| = D } . Operator anti ( D ) states that there is no observation sub interval of the execution which satisfies D .\n",
      "- -L ( implies ( D 1 glyph[squiggleright] D 2 )) = { σ | ∀ i, j : ( σ, [ i, j ] | = D 1 ⇒ σ, [ i, j ] | = D 2 ) } . Operator implies ( D 1 glyph[squiggleright] D 2 ) states all observation intervals which satisfy D 1 will also satisfy D 2 .\n",
      "- -\n",
      "\n",
      "L ( follows ( D 1 glyph[squiggleright] D 2 /D 3 )) = { σ | ∀ i, j : ( σ, [ i, j ] | = D 1 ⇒ ( ∀ k : σ, [ j, k ] | = Ξ ( D 3 ) ⇒∃ l ≤ k : σ, [ j, l ] | = D 2 )) } . Operator follows ( D 1 glyph[squiggleright] D 2 /D 3 ) states that if any observation interval [ i, j ] satisfies D 1 and there is a following shortest interval [ j, k ] which satisfies D 3 then there exists a prefix interval of [ j, k ] which satisfies D 2 .\n",
      "\n",
      "-\n",
      "\n",
      "L\n",
      "\n",
      "(\n",
      "\n",
      "triggers\n",
      "\n",
      "(\n",
      "\n",
      "D\n",
      "\n",
      "1\n",
      "\n",
      "glyph[squiggleright]\n",
      "\n",
      "D\n",
      "\n",
      "2\n",
      "\n",
      "/D\n",
      "\n",
      "3\n",
      "\n",
      ")) =\n",
      "\n",
      "{\n",
      "\n",
      "σ\n",
      "\n",
      "| ∀\n",
      "\n",
      "i, j\n",
      "\n",
      ": (\n",
      "\n",
      "σ,\n",
      "\n",
      "[\n",
      "\n",
      "i, j\n",
      "\n",
      "]\n",
      "\n",
      "|\n",
      "\n",
      "=\n",
      "\n",
      "D\n",
      "\n",
      "1\n",
      "\n",
      "⇒\n",
      "\n",
      "( ∀ k : σ, [ i, k ] | = Ξ ( D 3 ) ⇒∃ l ≤ k : σ, [ i, l ] | = D 2 )) } . Operator triggers ( D 1 glyph[squiggleright] D 2 /D 3 ) states that if any observation interval [ i, j ] satisfies D 1 and if [ i, k ] is the shortest interval which satisfies D 3 then D 2 holds for a prefix interval of [ i, k ].\n",
      "\n",
      "Based on this semantics, we can translate an atomic SeCeNL formula ζ without nominals into equivalent SeCe formula ℵ ( ζ ) as follows.\n",
      "\n",
      "1. ℵ ( pref ( D ) ) def ≡ ¬ (( ¬ D ) ^ true ).\n",
      "2. ℵ ( init ( D 1 /D 2 )) def ≡ pref ( Ξ ( D 2 ) ⇒ D 1 ^ true ).\n",
      "3. ℵ ( anti ( D )) def ≡ ¬ ( true ^ D ^ true ).\n",
      "4. ℵ ( implies ( D 1 glyph[squiggleright] D 2 )) def ≡ glyph[square] ( D 1 ⇒ D 2 ).\n",
      "5. ℵ ( follows ( D 1 glyph[squiggleright] D 2 /D 3 )) def ≡ glyph[square] ( ¬ ( D 1 ^ ( Ξ ( D 3 ) ∧ ¬ ( D 2 ^ true )))).\n",
      "6. ℵ ( triggers ( D 1 glyph[squiggleright] D 2 /D 3 )) def ≡ glyph[square] ( D 1 ^ true ⇒ ( Ξ ( D 3 ) ⇒ D 2 ^ true )) ∧ glyph[square] ( D 1 ⇒ pref ( Ξ ( D 3 ) ⇒ D 2 ^ true )).\n",
      "\n",
      "Lemma 2. For any ζ ∈ SeCeNL , if ζ does not use nominals then σ ∈ L ( ζ ) iff σ ∈ L ( ℵ ( ζ )) .\n",
      "\n",
      "The proof follows from examination of the semantics of ζ and the definition of ℵ ( ζ ). We omit the details.\n",
      "\n",
      "Nominals : Consider a nominated formula D : Θ where D is a SeCe formula over propositional variables Σ ∪ Θ . As we shall see later, the propositional variables in Θ are treated as 'place holders' - variables which are meant to be true exactly at one point - and we call them nominals following [FdRS03].\n",
      "\n",
      "Given an interval [ b, e ] ∈ Intv ( N ) we define a nominal valuation over [ b, e ] to be a map ν : Θ → { i | b ≤ i ≤ e } . It assigns a unique position within [ b, e ] to each nominal variable. We can then straightforwardly define σ, [ b, e ] | = ν D by constructing a word σ ν over Σ ∪ Θ such that ∀ p ∈ Σ : p ∈ σ ν ( i ) ⇔ p ∈ σ ( i ) and ∀ u ∈ Θ : u ∈ σ ν ( i ) ⇔ ν ( u ) = i . Then σ ν , [ b, e ] | = D ⇔ σ, [ b, e ] | = ν D . We state that ν 1 over Θ 1 and ν 2 over Θ 2 are consistent if ν 1 ( u ) = ν 2 ( u ) for all u ∈ Θ 1 ∩ Θ 2 . We denote this by ν 1 ‖ ν 2 .\n",
      "\n",
      "Semantics of SeCeNL : Now we consider the semantics of SeCeNL where nominals are used and shared between different parts D 1 , D 2 and D 3 of an atomic formula such as implies ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 ).\n",
      "\n",
      "Example 3 (lags). Let D 1 : { u, v } be the formula (&lt;u&gt; ^ [[P]] ∧ ((slen=n) ^ &lt;v&gt; ^ true) which holds for an interval where P is true throughout the interval and v marks the n +1 position from u denoting the start of the interval. Let D 2 : { v } be the formula true ^ &lt;v&gt; ^ [[Q]] . Then, implies ( D 1 : { u, v } glyph[squiggleright] D 2 : { v } ) states that for all observation intervals [ i, j ] and all nominal valuations ν over [ i, j ] if σ, [ i, j ] | = ν D 1 then σ, [ i, j ] | = ν D 2 . This formula is given by live timing diagram in Fig. 2 below. 3\n",
      "\n",
      "Fig. 2. Live timing diagram.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "We now give the semantics of SeCeNL.\n",
      "\n",
      "- -L ( pref ( D : Θ ) ) = { σ | ∀ σ ′ ≤ prefix σ : ∃ ν. σ ′ | = ν D } .\n",
      "- -L ( init ( D 1 : Θ 1 / D 2 : Θ 2 )) = { σ | ∀ j ∀ ν : σ, [0 , j ] | = ν D 2 ⇒ ∃ k ≤ j ∃ ν 2 : ν 1 ‖ ν 2 ∧ σ, [0 , k ] | = ν 2 D 1 } .\n",
      "- -L ( anti ( D : Θ )) = { σ | ∀ i, j ∀ ν : σ, [ i, j ] glyph[negationslash]| = ν D } .\n",
      "- -L ( implies ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 )) = { σ | ∀ i, j ∀ ν 1 : ( σ, [ i, j ] | = ν 1 D 1 ⇒ ∃ ν 2 : ν 1 ‖ ν 2 ∧ σ, [ i, j ] | = ν 2 D 2 ) } .\n",
      "\n",
      "3 Here we wish to point out that the illustration was made with the timing diagram editor WaveDrom and due to its limitation on naming nominals we were forced to rename the nominal v appearing in D 2 as a .\n",
      "\n",
      "- -L ( follows ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 /D 3 : Θ 3 )) = { σ | ∀ i, j ∀ ν 1 : ( σ, [ i, j ] | = ν 1 D 1 ⇒ ( ∀ k ∀ ν 2 ‖ ν 1 : σ, [ j, k ] | = ν 2 Ξ ( D 3 ) ⇒ ∃ l ≤ k ∃ ν 3 : ν 3 ‖ ν 1 ∧ ν 3 ‖ ν 2 ∧ σ, [ j, l ] | = ν 3 D 2 )) } .\n",
      "- ⇒\n",
      "\n",
      "```\n",
      "-L ( triggers ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 /D 3 : Θ 3 )) = { σ | ∀ i, j ∀ ν 1 : ( σ, [ i, j ] | = ν 1 D 1 ⇒ ( ∀ k ∀ ν 2 ‖ ν 1 : σ, [ i, k ] | = ν 2 Ξ ( D 3 ) ∃ l ≤ k ∃ ν 3 : ν 3 ‖ ν 1 ∧ ν 3 ‖ ν 2 ∧ σ, [ i, l ] | = D 2 )) } .\n",
      "```\n",
      "\n",
      "Based on the above semantics, we now formulate a QDDC formula equivalent to a SeCeNL formula. We define the following useful notations ∀ 1 Θ : D and ∃ 1 Θ : D as derived operators. These operators are essentially relativize quantifiers to restrict variables to singletons.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "From SeCeNL to QDDC : We now define the translation ℵ from SeCeNL to QDDC.\n",
      "\n",
      "1. ℵ ( pref ( D : Θ ) ) def ≡ ¬ ( ∃ 1 : ¬ D ^ true\n",
      "2. Θ ). 2. ℵ ( init ( D 1 : Θ 1 / D 2 : Θ 2 )) def ≡ pref ( ∀ 1 Θ 2 : ( D 2 ⇒∃ 1 Θ 1 -Θ 2 : D 1 ^ true )). 3. ℵ ( ¬∃ D : Θ ) def ≡ ¬ ( ∃ 1 Θ : true ^ D ^ true ). 4. ℵ ( implies ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 )) def ≡ glyph[square] ( ∀ 1 Θ 1 : ( D 1 ⇒∃ 1 Θ 2 -Θ 1 : D 2 )). 5. ℵ ( follows ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 /D 3 : Θ 3 )) def ≡ glyph[square] ( ∀ 1 Θ 1 : ∀ 1 Θ 3 -Θ 1 : ∃ 1 Θ 2 -( Θ 1 ∪ Θ 3 ) : ¬ ( D 1 ^ ( Ξ ( D 3 ) ∧ ¬ ( D 2 ^ true )))). 6. ℵ ( triggers ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 /D 3 : Θ 3 )) def ≡ glyph[square] ( ∀ 1 Θ 1 : ( D 1 ^ true ⇒ ( ∀ 1 Θ 3 -Θ 1 : ( Ξ ( D 3 ) ⇒∃ 1 Θ 2 -( Θ 1 ∪ Θ 3 ) : D 2 ^ true )))) ∧ glyph[square] ( ∀ 1 Θ 1 : ( D 1 ⇒ pref ( ∀ 1 Θ 3 -Θ 1 : ( Ξ ( D 3 ) ⇒∃ 1 Θ 2 -( Θ 1 ∪ Θ 3 ) : D 2 ^ true )))).\n",
      "\n",
      "Theorem 2. For any word σ over Σ and any ζ ∈ SeCeNL we have that σ ∈ L ( ζ ) iff σ ∈ L ( ℵ ( ζ )) . Moreover, the translation ℵ ( ζ ) can be computed in time linear in the size of ζ .\n",
      "\n",
      "The proof follows from the semantics of ζ and the definition of ℵ ( ζ ).\n",
      "\n",
      "Lemma 3. Let ζ = implies ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 ) and let |A ( D i ) | = m i for i ∈ { 1 , 2 } . Then there exists a DFA A ( ζ ) of size at most 2 2 m 1 m 2 for ζ .\n",
      "\n",
      "Proof. The formula ζ can be written in terms of a negation and two existential quantifiers. Note that each application of existential quantifier will result in an NFA and each time we determinize we get a DFA which is at most exponential in the size of NFA . Since that both A ( D 1 ) and A ( D 2 ) are DFA 's to start with, this implies we can construct a DFA A ( ζ ) of size at most 2 2 m 1 m 2 for ζ .\n",
      "\n",
      "In an similar way we can show that the size of formula automata for other SeCeNL atomic formulae are also elementary.\n",
      "\n",
      "Lemma 4. For any ζ ∈ SeCeNL the size of the automaton A ( ζ ) for ζ is elementary.\n",
      "\n",
      "## 3 Formalizing timing diagrams\n",
      "\n",
      "In this section we give a formal semantics to timing diagrams and formula translation from timing diagrams to SeCeNL. We begin by giving a textual syntax for timing diagrams which is derived from the timing diagram format of WaveDrom [CP16,Wav16].\n",
      "\n",
      "The symbols in a waveform come from Λ = { 0 , 1 , 2 , x , 0 | , 1 | , 2 | , x |} and Θ , an atomic set of nominals. Let Γ = Θ ∪ Λ . The syntax of a waveform over Γ is given by the grammar:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where u ∈ Θ and π ∈ Λ . We call the elements in Θ the nominals . As we shall see later, when we convert a waveform to a SeCeNL formula the nominals that appear in the formula are exactly the nominals in the waveform and hence the name. Let Wf be the set of all waveforms over Γ .\n",
      "\n",
      "An example of a waveform is 01a:2x011xb:x2 | 220c:00 with Θ = { a,b,c } . Intuitively, in a waveform 0 denotes low , 1 high , 2 and x don't care s (there is a subtle difference between 2 and x though) and ' | ' the stuttering operator.\n",
      "\n",
      "Let Σ be a set of propositional variables. A timing diagram over Σ is a tuple 〈W , Σ, C, Θ 〉 where W = { W p ∈ Wf | p ∈ Σ } and C ⊂ Θ × Θ × Intv ( N ) a set of timing constraints.\n",
      "\n",
      "Fig. 3 shows an example timing diagram T = 〈{ W p , W q } , { ( a, b, [10 : 10]) , ( a, d, [1 : 8]) , ( c, d, [20 : 30]) } , { a, b, c, d, e, f }〉 along with its rendering in WaveDrom. The shared nominals have to be renamed in WaveDrom as commented in § 2.3, in this case a and c in W q have been renamed g and h respectively. As in the case with SeCeNL formulas, nominals act as place holders in timing diagrams which can be shared among multiple waveforms. For example, in the figure W p and W q share the nominals a and c . As a result a timing constraint in one timing diagram can implicitly induce a timing constraint in the other. For instance, even though there is no direct timing constraint between a and c in W p the constraints between a and d , and d and c together impose one on them.\n",
      "\n",
      "waveform W p - 01a : 2x011xb : x2 | 220c : 00 waveform W q - 00a : 0 | d : 11 | e : xxx | f : 01c : 11 timing constraints: d-a ∈ [1:8], c-d ∈ [20:30], b-a ∈ [10:10]\n",
      "\n",
      "Fig. 3. Timing diagram T and its WaveDrom rendering.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Let T = 〈W , Σ, C, Θ 〉 , W = { W p ∈ Wf | p ∈ Σ } , be a timing diagram. Let ν : Θ → [ b, e ] be a nominal valuation. Let σ : [0 , n ] → 2 Σ be a word over Σ and for all p ∈ Σ let σ p : [0 , n ] → { 0 , 1 } given by σ p ( i ) = 1 iff p ∈ σ ( i ). Then the satisfaction relation σ p over a waveform W under the valuation ν is defined as follows.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "We say ν | = C iff ∀ ( a, b, 〈 l, r 〉 ) ∈ C : ν ( b ) -ν ( a ) ∈ 〈 l, r 〉 . We define σ, [ b, e ] | = ν 〈W , Σ, C, Θ 〉 iff ∀ p ∈ Σ : σ p , [ b, e ] | = ν W p and ν | = C .\n",
      "\n",
      "## 3.1 Waveform to SeCeNL translation\n",
      "\n",
      "We translate a waveform W p to SeCeNL as follows: every 0 occurring in P is translated to {{¬ P }} , 1 to {{ P }} , 2 and x to slen=1 , 0 | to pt ∨ [ ¬ P] , 1 | to pt ∨ [P] , 2 | to true , and x | to pt ∨ [P] ∨ [ ¬ P] . A nominal u that is appearing in W p is translated to &lt; u &gt; . For instance, the waveform W p = 01a:2x011xb:x2 | 220c:00 in T of Fig. 3 will be translated to SeCeNL formula as below.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "We denote the translated SeCeNL formula by ξ ( T, W p ). Similarly we can translate W q to get the formula ξ ( T, W q ). The timing constraints in C is roughly translated to the SeCeNL formula ξ ( T, C ) as follows.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "We define ξ ( T ) = ξ ( T, W p ) ∧ ξ ( T, W q ) ∧ ξ ( T, C ). For a timing diagram T = 〈W , Σ, C, Θ 〉 , W = { W p | p ∈ Σ } we define ξ ( T ) = ∧ p ∈ Σ ξ ( T, W p ) ∧ ∧ ξ ( T, C ).\n",
      "\n",
      "Theorem 3. Let T be a timing diagram. Then, for all σ ∈ Σ ∗ , for all [ b, e ] ∈ Intv ( σ ) and for all nominal valuation ν over [ b, e ] , σ, [ b, e ] | = ν T iff σ, [ b, e ] | = ν ξ ( T ) : Θ . Also, the translation ξ ( T ) : Θ is linear in the size of T .\n",
      "\n",
      "Proof. Proof is not difficult and is by induction on the length of the waveform.\n",
      "\n",
      "Due above theorem we can now use timing diagrams in place of nominated formulas with liveness operators. We call such timing diagrams live timing diagrams . For an example of a live timing diagram see Fig. 2.\n",
      "\n",
      "## 3.2 Comparision with other temporal logics\n",
      "\n",
      "In previous section, Lemma 3 showed that timing diagrams can be translated to equivalent SeCeNL formulas with only linear blowup in size. In this section we compare our logic SeCeNL with other relevent logics in the literature viz, LTL, discrete time MTL, and PSL-Sugar. Of these, PSL-Sugar is the most expressive and discrete time MTL and LTL are its syntactic subset. We show by examples that SeCeNL formulae are more succint (smaller in size) than PSL-Sugar and we believe that they capture the diagrams more directly. Appendix A gives several more examples which could not be included due to lack of space.\n",
      "\n",
      "Example (Ordered Stack) Let us now consider the timing diagram in Fig. 4 adapted from [CF05]. Rise and fall of successive signals follow a stack discipline. The language described by it is given by the SeCeNL formula:\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Fig. 4. Example 1.\n",
      "\n",
      "```\n",
      "([ ¬ a ] ˆ <ua> ˆ [ a ] ˆ <va> ˆ [ ¬ a ]) ∧ ([ ¬ b ] ˆ <ub> ˆ [ b ] ˆ <vb> ˆ [ ¬ b ]) ∧ ([ ¬ c ] ˆ <uc> ˆ [ c ] ˆ <vc> ˆ [ ¬ c ]) ∧ ([ ¬ d ] ˆ <ud> ˆ [ d ] ˆ <vd> ˆ [ ¬ d ]) ∧ ([ ¬ e ] ˆ <ue> ˆ [ e ] ˆ <ve> ˆ [ ¬ e ]) ∧ ( ext ˆ <ua> ˆ ext ˆ <ub> ˆ ext ) ∧ ( ext ˆ <ub> ˆ ext ˆ <uc> ˆ ext ) ∧ ( ext ˆ <uc> ˆ ext ˆ <ud> ˆ ext ) ∧ ( ext ˆ <ud> ˆ ext ˆ <ue> ˆ ext ) ∧ ( ext ˆ <va> ˆ ext ˆ <vb> ˆ ext ) ∧ ( ext ˆ <vb> ˆ ext ˆ <vc> ˆ ext ) ∧ ( ext ˆ <vc> ˆ ext ˆ <vd> ˆ ext ) ∧ ( ext ˆ <vd> ˆ ext ˆ <ve> ˆ ext ) .\n",
      "```\n",
      "\n",
      "Note that first five conjuncts exactly correspond to the five waveforms. The last constraint enforces the ordering constraints between waveforms. In general, if n signals are stacked, its SeCeNL specification has size O ( n ).\n",
      "\n",
      "An equivalent MTL (or LTL) formula is given by:\n",
      "\n",
      "```\n",
      "[ ¬ a ∧ ¬ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ] UU [ a ∧ ¬ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ] UU [ a ∧ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ] UU [ a ∧ b ∧ c ∧ ¬ d ∧ ¬ e ] UU [ a ∧ b ∧ c ∧ d ∧ ¬ e ] UU [ a ∧ b ∧ c ∧ d ∧ e ] UU [ a ∧ b ∧ c ∧ d ∧ ¬ e ] UU [ a ∧ b ∧ c ∧ ¬ d ∧ ¬ e ] UU [ a ∧ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ] UU [ a ∧ ¬ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ] UU [ ¬ a ∧ ¬ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ]\n",
      "```\n",
      "\n",
      "where a UU b is the derived modality a ∧ X ( a U b ). For a stack of n signals, the size of the MTL formula is O ( n 2 ).\n",
      "\n",
      "Above formula is also a PSL-Sugar formula. We attempt to specify the pattern as a PSL-Sugar regular expression as follows:\n",
      "\n",
      "```\n",
      "(( ¬ a ∧ ¬ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ; )[+]; ( a ∧ ¬ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ; )[+]; ( a ∧ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ; )[+]; ( a ∧ b ∧ c ∧ ¬ d ∧ ¬ e ; )[+]; ( a ∧ b ∧ c ∧ d ∧ ¬ e ; )[+]; ( a ∧ b ∧ c ∧ d ∧ e ; )[+]; ( a ∧ b ∧ c ∧ d ∧ ¬ e ; )[+]; ( a ∧ b ∧ c ∧ ¬ d ∧ ¬ e ; )[+]; ( a ∧ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ; )[+]; ( a ∧ ¬ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ; )[+]; ( ¬ a ∧ ¬ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ; )[+]\n",
      "```\n",
      "\n",
      "For a stack of n signals, the size of the PSL-Sugar SERE expression is O ( n 2 ). We believe that there is no formula of size O ( n ) in PSL-Sugar which can express the above property. Compare this with size O ( n ) formula of SeCeNL.\n",
      "\n",
      "Example (Unordered Stack) In ordered stack signal a turns on first and turns off last followed by signals b, c, d, e in that order. We consider a variation of the ordered stack example above where signals turn on and off in first-on-last-off order but there is no restriction on which signal becomes high first. This can be compactly specified in SeCeNL as follows.\n",
      "\n",
      "```\n",
      "([ ¬ a ] ˆ <ua> ˆ [ a ] ˆ <va> ˆ [ ¬ a ]) ∧ ([ ¬ b ] ˆ <ub> ˆ [ b ] ˆ <vb> ˆ [ ¬ b ]) ∧ ([ ¬ c ] ˆ <uc> ˆ [ c ] ˆ <vc> ˆ [ ¬ c ]) ∧ ([ ¬ d ] ˆ <ud> ˆ [ d ] ˆ <vd> ˆ [ ¬ d ]) ∧ ([ ¬ e ] ˆ <ue> ˆ [ e ] ˆ <ve> ˆ [ ¬ e ]) ∧ ( ext ˆ <u 1 > ˆ ext ˆ <u 2 > ˆ ext ) ∧ ( ext ˆ <u 2 > ˆ ext ˆ <u 3 > ˆ ext ) ∧ ( ext ˆ <u 3 > ˆ ext ˆ <u 4 > ˆ ext ) ∧ ( ext ˆ <u 4 > ˆ ext ˆ <u 5 > ˆ ext ) ∧ ( ext ˆ <v 5 > ˆ ext ˆ <v 4 > ˆ ext ) ∧ ( ext ˆ <v 4 > ˆ ext ˆ <v 3 > ˆ ext ) ∧ ( ext ˆ <v 3 > ˆ ext ˆ <v 2 > ˆ ext ) ∧ ( ext ˆ <v 2 > ˆ ext ˆ <v 1 > ˆ ext ) ∧ Bijection ( ua, ub, uc, ud, ue, va, vb, vc, vd, ve, u 1 , u 2 , u 3 , u 4 , u 5 , v 1 , v 2 , v 3 , v 4 , v 5)\n",
      "```\n",
      "\n",
      "where formula Bijection below states that there is one to one correspondence between positions marked by ua, ub, uc, ud, ue, va, vb, vc, vd, de and positions marked by u 1 , u 2 , u 3 , u 4 , u 5 , v 1 , v 2 , v 3 , v 4 , v 5. Moreover, it states that if u a maps to say u 3 than v a must map to v 3 and so on.\n",
      "\n",
      "glyph[negationslash]\n",
      "\n",
      "```\n",
      "[[( u 1 ∨ u 2 ∨ u 3 ∨ u 4 ∨ u 5) ⇔ ( ua ∨ ub ∨ uc ∨ ud ∨ ue )]] ∧ [[ ∧ 1 ≤ i,j ≤ 5 ,i = j ¬ ( u i ∧ u j )]] [[( v 1 ∨ v 2 ∨ v 3 ∨ v 4 ∨ v 5) ⇔ ( va ∨ vb ∨ vc ∨ vd ∨ ve )]] ∧ [[ ∧ 1 ≤ i,j ≤ 5 ,i = j ¬ ( v i & v j )]] ∧ 1 ≤ i ≤ 5 ,j ∈ a,b,c,d,e ( true ˆ <u i ∧ u j > ˆ true ⇔ true ˆ <v i ∧ v j > ˆ true )\n",
      "```\n",
      "\n",
      "glyph[negationslash]\n",
      "\n",
      "Note that, in general, if n signals are stacked, then the above SeCeNL specification has size O ( n 2 ).\n",
      "\n",
      "Now we discuss encoding of unordered stack in PSL-Sugar. In absence of nominals, it is difficult to state the above behaviour succinctly in logics PSLSugar even using its SERE regular expressions. Each order of occurrence of signals has to be enumerated as a disjunction where each disjunct is as in the example ordered stack (where the order was a, b, c, d, e ). As there are n ! orders possible between n signals, the size of the PSL-Sugar formula is also O ( n !). We believe that there is no polynomially sized formula in PSL-Sugar encoding this property. This shows that SeCeNL is exponentially more succint as compared to PSL-Sugar.\n",
      "\n",
      "In general, presence of nominals distinguishes SeCeNL from logics like PSLSugar. In formalizing behaviour of hardware circuits it has been proposed that regular expressions are not enough and operators such as pipelining have been introduced [CF05]. These are a form of synchronization and they can be easily expressed using nominals too.\n",
      "\n",
      "## 4 Case study: Minepump Specification\n",
      "\n",
      "We first specify some useful generic timing diagram properties which would used for requirement specification in this (and many other) case studies.\n",
      "\n",
      "- lags ( P, Q, n ): it is defined by Fig. 5. It specifies that in any observation interval if P holds continuously for n +1 cycles and persists then Q holds from ( n +1) th cycle onwards and persists till P persists.\n",
      "- tracks ( P, Q, n ): defined Fig. 6. In any observation interval if P becomes true then Q sustains as long as P sustains or upto n cycles whichever is shorter.\n",
      "- sep ( P, n ): Fig. 7 defines this property. Any interval which begins with a falling edge of P and ends with a rising edge of P then the length of the interval should be at least n cycles.\n",
      "- ubound ( P, n ): Fig. 8 defines the property. In any observation interval P can be continuously true for at most n cycles.\n",
      "\n",
      "Note that we have presented these formulae diagrammatically. The textual version of these live timing diagrams can be found in Appendix C.\n",
      "\n",
      "We now state the minepump problem. Imagine a minepump which keeps the water level in a mine under control for the safety of miners. The pump is driven by a controller which can switch it on and off . Mines are prone to methane leakage trapped underground which is highly flammable. So as a safety measure if a methane leakage is detected the controller is not allowed to switch on the pump under no circumstances.\n",
      "\n",
      "The controller has two input sensors - HH2O which becomes 1 when water level is high, and HCH4 which is 1 when there is a methane leakage; and can generate two output signals - ALARM which is set to 1 to sound/persist the alarm, and PUMPON which is set to 1 to switch on the pump. The objective of the controller is to safely operate the pump and the alarm in such a way that the water level is never dangerous, indicated by the indicator variable DH2O, whenever certain assumptions hold. We have the following assumptions on the mine and the pump.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "- -Sensor reliability assumption: pref ([[ DH 2 O ⇒ HH 2 O ]]) . If HH2O is false then so is DH2O.\n",
      "- -Water seepage assumptions: tracks ( HH 2 O,DH 2 O,κ 1 ). The minimum no. of cycles for water level to become dangerous once it becomes high is κ 1 .\n",
      "- -Pump capacity assumption: lags ( PUMPON, ¬ HH 2 O,κ 2 ). If pump is switched on for at least κ 2 +1 cycles then water level will not be high after κ 2 cycles.\n",
      "- -Methane release assumptions: sep ( HCH 4 , κ 3 ) and ubound ( HCH 4 , κ 4 ). The minimum separation between the two leaks of methane is κ 3 cycles and the methane leak cannot persist for more than κ 4 cycles.\n",
      "- -Initial condition assumption: init ( &lt; ¬ HH 2 O&gt; ∧ &lt; ¬ HCH 4 &gt;,slen = 0). Initially neither the water level is high nor there is a methane leakage.\n",
      "\n",
      "Let the conjunction of these SeCeNL formulas be denoted as MINEASSUME . The commitments are:\n",
      "\n",
      "- -Alarm control: lags ( HH 2 O,ALARM,κ 5 ) and lags ( HCH 4 , ALARM,κ 6 ) and lags ( ¬ HH 2 O ∧ ¬ HCH 4 , ¬ ALARM,κ 7 ). If the water level is dangerous then alarm will be high after κ 5 cycles and if there is a methane leakage then alarm will be high after κ 6 cycles. If neither the water level is dangerous nor there is a methane leakage then alarm should be off after κ 7 cycle.\n",
      "- -Safety condition: pref ([[ ¬ DH 2 O ∧ ( HCH 4 ⇒¬ PUMPON )]]) . The water level should never become dangerous and whenever there is a methane leakage pump should be off.\n",
      "\n",
      "Let the conjunction of these commitments be denoted as MINECOMMIT . Then the requirement over the minepump controller is given by the formula MINEASSUME ⇒ MINECOMMIT . A textual version of this full minepump specification, which can be input to our tools is given in Appendix C. Note that the require consists of a mixture of timing diagram constraints (such as pump capacity assumption above) as well as SeCeNL formulas (such as Safety condition above).\n",
      "\n",
      "We can automatically synthesize a controller for the values say κ 1 = 10, κ 2 = 2, κ 3 = 14, κ 4 = 2, and κ 5 = κ 6 = κ 7 = 1. The tool outputs a SCADE/SMV\n",
      "\n",
      "controller meeting the specification. A snapshot of SCADE code for the controller synthesized by DCSynthG for minepump can be found in Appendix D. If the specification is not realizable we output an explanation.\n",
      "\n",
      "A second case study of synchronous bus arbiter specification can be found in Appendix. E. We can automatically synthesize a property monitor for such requirement and use it to model check a given arbiter design; or we can directly synthesize a controller meeting the requirement. The appendix gives results of both these experiments.\n",
      "\n",
      "## References\n",
      "\n",
      "| AEKN00.   | Nina Amla, E. Allen Emerson, Robert P. Kurshan, and Kedar S. Namjoshi. Model checking synchronous timing diagrams. In Warren A. Hunt Jr. and Steven D. Johnson, editors, Formal Methods in Computer-Aided De- sign, Third International Conference, FMCAD 2000, Austin, Texas, USA, November 1-3, 2000, Proceedings , volume 1954 of Lecture Notes in Computer Science , pages 283-298. Springer, 2000.                  |\n",
      "|-----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| AH93.     | Rajeev Alur and Thomas A. Henzinger. Real-time logics: Complexity and expressiveness. Inf. Comput. , 104(1):35-77, 1993.                                                                                                                                                                                                                                                                                                 |\n",
      "| All83.    | James F. Allen. Maintaining knowledge about temporal intervals. Commun. ACM , 26(11):832-843, 1983.                                                                                                                                                                                                                                                                                                                      |\n",
      "| AS87.     | Bowen Alpern and Fred B. Schneider. Recognizing safety and liveness. Distributed Computing , 2(3):117-126, 1987.                                                                                                                                                                                                                                                                                                         |\n",
      "| BP12.     | Ajesh Babu and Paritosh K. Pandya. Chop expressions and discrete dura- tion calculus. In Modern Applications of Automata Theory , pages 229-256. 2012.                                                                                                                                                                                                                                                                   |\n",
      "| CF05.     | Hana Chockler and Kathi Fisler. Temporal modalities for concisely captur- ing timing diagrams. In Dominique Borrione and Wolfgang J. Paul, editors, Correct Hardware Design and Verification Methods, 13th IFIP WG10.5 Ad- vanced Research Working Conference, CHARME 2005, Saarbr¨ ucken, Ger- many, October 3-6, 2005, Proceedings , volume 3725 of Lecture Notes in Computer Science , pages 176-190. Springer, 2005. |\n",
      "| CP16.     | Aliaksei Chapyzhenka and Jonah Probell. Wavedrom: Rendering beautiful waveforms from plain text. Synopsys User Group , 2016.                                                                                                                                                                                                                                                                                             |\n",
      "| DH01.     | Werner Damm and David Harel. Lscs: Breathing life into message sequence charts. Formal Methods in System Design , 19(1):45-80, 2001.                                                                                                                                                                                                                                                                                     |\n",
      "| EF16.     | Cindy Eisner and Dana Fisman. Temporal logic made practical. Handbook of Model Checking. Springer (Expected 2016), http://www. cis. upenn. edu/˜ fisman/documents/EF HBMC14. pdf , 2016.                                                                                                                                                                                                                                 |\n",
      "| FdRS03.   | Massimo Franceschet, Maarten de Rijke, and Bernd-Holger Schlingloff. Hy- brid logics on linear structures: Expressivity and complexity. In 10th In- ternational Symposium on Temporal Representation and Reasoning / 4th International Conference on Temporal Logic (TIME-ICTL 2003), 8-10 July 2003, Cairns, Queensland, Australia , pages 166-173. IEEE Computer Soci- ety, 2003.                                      |\n",
      "| Fis99.    | Kathi Fisler. Timing diagrams: Formalization and algorithmic verification. Journal of Logic, Language and Information , 8(3):323-361, 1999.                                                                                                                                                                                                                                                                              |\n",
      "\n",
      "- Fis07. Kathi Fisler. Two-dimensional regular expressions for compositional bus protocols. In Formal Methods in Computer-Aided Design, 7th International Conference, FMCAD 2007, Austin, Texas, USA, November 11-14, 2007, Proceedings , pages 154-157. IEEE Computer Society, 2007.\n",
      "- KP05. Yonit Kesten and Amir Pnueli. A compositional approach to CTL* verification. Theor. Comput. Sci. , 331(2-3):397-428, 2005.\n",
      "- Pan00. Paritosh K. Pandya. Specifying and deciding quantified discrete-time duration calculus formulae using DCVALID. Technical report, Tata Institute of Fundamental Research, Mumbai, 2000.\n",
      "- Pan01. Paritosh K. Pandya. Model checking ctl*[dc]. In Tiziana Margaria and Wang Yi, editors, Tools and Algorithms for the Construction and Analysis of Systems, 7th International Conference, TACAS 2001 Held as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS 2001 Genova, Italy, April 2-6, 2001, Proceedings , volume 2031 of Lecture Notes in Computer Science , pages 559-573. Springer, 2001.\n",
      "- Wav16. WaveDrom. Wavedrom user manual. http://wavedrom.com/tutorial.html , 2016.\n",
      "\n",
      "## A Examples of Comparision with other logics\n",
      "\n",
      "Example 1 (Ordering with timing) Consider the timing diagram in Fig. 9 which says that a holds invariantly in the interval [0 , i ] where i ≥ 1, b holds invariantly in the interval [ i, j ], j ≥ i +1, and c holds at j and j ≤ n .\n",
      "\n",
      "Fig. 9. Example 1.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "- -The language described by the above timing diagram is given by the SeCeNL formula ([ a ∧¬ b ] ˆ [ b ∧¬ a ∧¬ c ] ˆ &lt;c&gt; ) ∧ ( slen ≤ n ) which is of size O (log( n )). It is assumed that all timing constants such as n are encoded in binary and hence they contribute size log( n ).\n",
      "- -An equivalent MTL formula is ∨ i = n -1 i =1 ( a ∧ ¬ b U [ i, i ]( b ∧ ¬ a U [1 , n -i ] c )) whose size is O ( n log( n )).\n",
      "- -Equivalent LTL formula is ∨ i = n -1 i =1 ∨ j = n -i j =1 ( a UX i ( b UX j c )) where X k = X · . . . · X ︸ ︷︷ ︸ , whose size is O ( n 2 ).\n",
      "\n",
      "k\n",
      "\n",
      "times\n",
      "\n",
      "- -Equivalent PSL-Sugar formula is ( a ∧¬ b [+]; b ∧¬ a ∧¬ c [+]; c ) ∧ (( a | b )[ &lt; n ]; c ) with size O (log( n )).\n",
      "\n",
      "We also give examples of complex dependancy constraints. Consider the timing diagram in Fig. 10. In this diagram, ua occurs before ub and uc , and uc occurs before ud and ue . The point vc occurs after vd and ve , and va occurs after vb and vb .\n",
      "\n",
      "Fig. 10. Example 3.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "The behaviour is described straightforwardly by the SeCeNL formula:\n",
      "\n",
      "```\n",
      "([ ¬ a ] ˆ <ua> ˆ [ a ] ˆ <va> ˆ [ ¬ a ]) ∧ ([ ¬ b ] ˆ <ub> ˆ [ b ] ˆ <vb> ˆ[ ¬ b ]) ∧ ([ ¬ c ] ˆ <uc> ˆ [ c ] ˆ <vc> ˆ [ ¬ c ]) ∧ ([ ¬ d ] ˆ <ud> ˆ [ d ] ˆ <vd> ˆ [ ¬ d ]) ∧ ([ ¬ e ] ˆ <ue> ˆ [ e ] ˆ <ve> ˆ [ ¬ e ]) ∧ ( ext ˆ <ua> ˆ ext ˆ <ub> ˆ true ) ∧ ( ext ˆ <ua> ˆ ext ˆ <uc> ˆ true ) ∧ ( ext ˆ <uc> ˆ ext ˆ <ud> ˆ true ) ∧ ( ext ˆ <uc> ˆ ext ˆ <ue> ˆ true ) ∧ ( ext ˆ <ve> ˆ ext ˆ <vc> ˆ true ) ∧ ( ext ˆ <vd> ˆ ext ˆ <vc> ˆ true ) ∧ ( ext ˆ <vc> ˆ ext ˆ <va> ˆ true ) ∧ ( ext ˆ <vb> ˆ ext ˆ <va> ˆ true ) .\n",
      "```\n",
      "\n",
      "This formula is linear in the size of the timing diagram. Unfortunately, specifying these dependancies in PSL-Sugar is complex and formula size blows up at least quadratically.\n",
      "\n",
      "## B Implementation\n",
      "\n",
      "We propose a textual framework with a well defined syntax and semantics for requirement specification (of the form assumptions ⇒ commitments). Our framework is heterogeneous in the sense that it supports both SeCeNL formulas and timing diagrams with nominals for system specification. It can also handle all of our limited liveness operators. (see Appendix. C for the code for minepump in our framework).\n",
      "\n",
      "We have also developed a Python based translator which takes requirements in our textual format as input and produces property monitors as well as controllers as output. Fig. 11 gives a broad picture of the current status of our tool chain.\n",
      "\n",
      "Fig. 11. Our tool chain.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## C Minepump Code\n",
      "\n",
      "```\n",
      "The example code for minepump is written using textual syntax for QDDC which can be found in [Pan00,Pan01]. #lhrs 'minepump' interface {\n",
      "```\n",
      "\n",
      "```\n",
      "input HH2O, HCH4; output ALARM monitor x, PUMPON monitor x; constant delta = 1, w = 10, epsilon=2 , zeta=14, kappa=2; auxvar DH2O; softreq (!YHCH4) || (!PUMPON); } #implies lag(P, Q, n) { td lagspeclet1(P, n) { P: < u > 1 | < v > 1 | ; @sync:(u, v, n); } td lagspeclet2(Q) { Q: 2 | < v > 1 | ; } } #implies tracks(P, Q, n) { td tracksspeclet1(P, n) { P: 0 < u > 1 | < v > 1 | ; @sync: (u,v,[n,)); } td tracksspeclet2(Q) Q: 2 < u > 1 | < v > 0 | ; } #implies tracks2(P, Q, n) td tracks2speclet1(P, n) { P: 0 < u > 1 | < v > 0 | ; @sync: (u,v,[,n]); } td tracks2speclet2(Q) Q:2 < u > 0 | < v > 2 | ; } #implies sep(P, n) { td sepspeclet1(P) P: 1 < u > 0 | < v > 1; td sepspeclet2(n) { @null: 2 < u > 2 | < v > 2;\n",
      "```\n",
      "\n",
      "```\n",
      "@sync: (u, v, (n,]); } } #implies ubound(P, n) { td boundspeclet1(P) P: < c > 1 | < d > 1; td boundspeclet2(n) { @null: < c > 2 | < d > 2; @sync: (c, d, [,n)); } } dc safe(DH2O) { pt || [!DH2O && ((HCH4 || !HH2O) = > !PUMPON)]; } main() { assume ( < !HH2O > ˆ true); assume (pt || [DH2O = > HH2O]); assume tracks(HH2O, !DH2O, w); assume tracks2 (HH2O, DH2O, w); assume lag(PUMPON, !HH2O, epsilon); assume sep(HCH4, zeta); assume ubound(HCH4, kappa); req ( < !ALARM > ˆ true); req lag(HH2O, ALARM, delta); req lag(HCH4, ALARM, delta); req lag(!HCH4 && !HH2O, !ALARM, delta); req safe(DH2O); }\n",
      "```\n",
      "\n",
      "## D Synthesized controller for minepump\n",
      "\n",
      "A snapshot of a controller synthesized from the minepump requirement in § 4. The controller had approximately 140 states and it took less than a second for synthesis.\n",
      "\n",
      "```\n",
      "node minepump ( HH2O, HCH4:bool) returns ( ALARM, PUMPON:bool) var cstate: int; let ALARM, PUMPON, cstate = ( if true and not HH2O and not HCH4 then ( false, false, 2) else if true and not HH2O and HCH4 then ( false, false, 4) else if true and HH2O and not HCH4 then ( false, false, 4) else if true and HH2O and HCH4 then ( false, false, 4) else ( dontCare, dontCare, 1)) ⇒ if pre cstate = 1 and not HH2O and not HCH4 then ( false, false, 2) else if pre cstate = 1 and not HH2O and HCH4 then ( false, false, 4) else if pre cstate = 1 and HH2O and not HCH4 then ( false, false, 4) else if pre cstate = 1 and HH2O and HCH4 then ( false, false, 4) else if pre cstate = 2 and not HH2O and not HCH4 then ( false, false, 2) else if pre cstate = 2 and not HH2O and HCH4 then ( false, false, 7) else if pre cstate = 2 and HH2O and not HCH4 then ( false, false, 9) else if pre cstate = 2 and HH2O and HCH4 then ( false, false, 11) else if pre cstate = 4 and not HH2O and not HCH4 then ( false, false, 4) else if pre cstate = 4 and not HH2O and HCH4 then ( false, false, 4) .............................. .............................. else if pre cstate = 309 and HH2O and not HCH4 then ( false, true, 4) else if pre cstate = 309 and HH2O and HCH4 then ( false, true, 4) else if pre cstate = 372 and not HH2O and not HCH4 then ( false, false, 2) else if pre cstate = 372 and not HH2O and HCH4 then ( false, false, 7) else if pre cstate = 372 and HH2O and not HCH4 then ( false, true, 4) else if pre cstate = 372 and HH2O and HCH4 then ( false, true, 4) else ( dontCare, dontCare, pre cstate) ; tel\n",
      "```\n",
      "\n",
      "## E Case study: 3-cell arbiter\n",
      "\n",
      "In this section we illustrate another application of our specification format and associated tools. For this we use the standard McMillan arbiter circuit given in NuSMV examples and do the model checking against the specification below.\n",
      "\n",
      "A synchronous 3-cell bus arbiter has 3 request lines req 1 , req 2 and req 3, and corresponding acknowledgement lines ack 1 , ack 2 and ack 3. At any time instance a subset of request lines can be high and arbiter decides which request should be granted permission to access the bus by making corresponding acknowledgement line high. The requirements for such a bus arbiter are as formulated below.\n",
      "\n",
      "glyph[negationslash]\n",
      "\n",
      "- -Exclusion: pref ([[( ∧ i = j ¬ ( ack i ∧ ack j ))]]) . At most 1 acknowledgement can be given at a time.\n",
      "- -No spurious acknowledgement: pref ([[( ∧ 1 ( ack i ⇒ req i ))]]) . A request should be granted access to the bus only if it has requested it.\n",
      "- -Response time: implies ([[ req ]] ∧ slen = n, true ^ &lt;ack&gt; ^ true ). One of the most important property of an arbiter is that it any request should be granted within n cycles, i. e. if a request is continuously true for sometime then it should be heard.\n",
      "- -Deadtime: to specify this property we first specify lost cycle as follows: Lost ≡ ( ∨ i req i ) ∧ ( ¬ ( ∨ i ack i )). Then Deadtime ≡ anti ([[ Lost ]] ∧ slen &gt; n ). This specifies the maximum number of consecutive cycles that can be lost by the arbiter is n .\n",
      "\n",
      "The requirement ARBREQ is a conjunction of above formulas.\n",
      "\n",
      "We ran the requirement through our tool chain to generate NuSMV module for the requirement monitor. This module was then instantiated synchronously with McMillan arbiter implementation in NuSMV and NuSMV model checker was called in to check the property G ( assumptions ⇒ commitments ).\n",
      "\n",
      "Model checking : Experimental results show that the deadtime for 3-cell McMillan arbiter is 3. If we specify the deadtime as 2 cycles then a counter example is generated by NuSMV as depicted in Fig. 12. This counter examples show that even though there is an request line high in 4 th , 5 th and 6 th cycle, but no acknowledgment is given by arbiter. Similarly, the response time for 1 st request is 3 cycles whereas for 2 nd and 3 rd cell it is 6 cycles. If we specify the response time of 2 and 5 cycles for 1 st and 2 nd then NuSMV generates counter examples in Fig. 13 and Fig. 14 respectively. Fig. 14 shows that the request line for cell 2 (i. e. req2) is high continuously for 5 cycles starting from 3 rd without an acknowledgement from the arbiter.\n",
      "\n",
      "Fig. 12. Counter Example Showing deadtime exceeding 2 cycles Fig. 13. Counter Example showing response time of 1st cell exceeding 2 cycles Fig. 14. Counter Example showing response time of 2nd cell exceeding 5 cycles\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Controller synthesis : We have also synthesized a controller for the arbiter specification using our tool DCSynthG. We have tightened the requirements by specifying the response time as 3 cycles uniformly for all three cells and deadtime as 0 cycles, i. e. there is no lost cycle. The tool could synthesize a controller in 0.03 seconds with 17 states.\n",
      "Document 7:\n",
      "## Structural and magnetic properties of 3d transition metal oxide chains on the (001) surfaces of Ir and Pt\n",
      "\n",
      "Martin Schmitt, 1 Chong H. Park, 1, 2 Paula Weber, 1 Andreas J¨ ager, 1 Jeannette Kemmer, 1 Matthias Vogt, 1 and Matthias Bode 1, 3, ∗\n",
      "\n",
      "1\n",
      "\n",
      "Physikalisches Institut, Experimentelle Physik II, Universit¨ at W¨ urzburg, Am Hubland, 97074 W¨ urzburg, Germany University of British Columbia, 2329 West Mall, Vancouver, BC Canada Wilhelm Conrad R¨ ontgen-Center for Complex Material Systems (RCCM), Universit¨ at W¨ urzburg, Am Hubland, 97074 W¨ urzburg, Germany (Dated: October 8, 2019)\n",
      "\n",
      "We present a survey of the structural and magnetic properties of submonolayer transition metal dioxides on the (001) surfaces of the heavy face-centered cubic (fcc) noble metals Ir and Pt performed by spin-averaged scanning tunneling microscopy (STM) and spin-polarized (SP-)STM. Our STM results confirm that deposition of Co, Fe, Mn, and Cr on the (2 × 1) oxygen-reconstructed Ir(001) surface leads to the formation of quasi one-dimensional chains with a (3 × 1) unit cell. As recently predicted by density functional theory [Ferstl et al. , Phys. Rev. Lett. 117 , 046101 (2016)], our SPSTM images of FeO 2 and MnO 2 on Ir(001) show a two-fold periodicity along the chains which is characteristic for an antiferromagnetic coupling along the chains. In addition, these two materials also exhibit spontaneous, permanent, and long-range magnetic coupling across the chains. Whereas we find a ferromagnetic inter-chain coupling for FeO 2 /Ir(001), the magnetic coupling of MnO 2 on Ir(001) appears to be a non-collinear 120 ◦ spin spiral, resulting in a (9 × 2) magnetic unit cell. On Pt(001) patches of (3 × 1)-reconstructed oxide chains could only be prepared by transition metal (Co, Fe, and Mn) deposition onto the cold substrate and subsequent annealing in an oxygen atmosphere. Again SP-STM on MnO 2 /Pt(001) reveals a very large, (15 × 2) magnetic unit cell which can tentatively be explained by a commensurate 72 ◦ spin spiral. Large scale SP-STM images reveal a long wavelength spin rotation along the MnO 2 chain.\n",
      "\n",
      "## I. INTRODUCTION\n",
      "\n",
      "Significant progress has been achieved towards a thorough understanding of magnetically ordered states in solid-state materials. 1 Over the past 40 years spin structures with increasing complexity were detected. Whereas collinear ferroor antiferromagnetism governed by the competition of exchange, magnetocrystalline anisotropy, and dipolar interactions initially dominated the scientific debate, we have witnessed a focussing on more complex non-collinear magnetic states since the advent of the current century. 2 This development was-at least partially-made possible by the development of advanced surface analysis and microscopy tools which allow for the detection of magnetic signals with unprecedented sensitivity and spatial resolution. In the context of this contribution spin-polarized scanning tunneling microscopy (SP-STM) will be of particular interest. This technique utilizes the tunnel magnetoresistance effect between a magnetic surface and a spin-polarized tip to obtain information about the sample's spin structure with atomic resolution. 48 SP-STM allowed for the first direct imaging of antiferromagnetic surfaces 4 and domain walls, 5 as well as of frustrated N´ eel spin states with antiphase domains. 6 Furthermore, it turned out that the spin-orbit-induced Dzyaloshinskii-Morija interaction (DMI), which has previously considered in some rare cases only, can be very significant at surfaces and interfaces. For example, it turned out that the Mn monolayer on W(110), which has initially been assumed to be a simple collinear and uniaxial antiferromagnet, 7,8 instead forms chiral spin cycloid. 9\n",
      "\n",
      "Recently a group of novel quasi one-dimensional 3 d transition metal oxides (TMO) was discovered which can conveniently be prepared by self-organized growth on the (001) surfaces of the heavy fcc metals Ir and Pt. 10,11 For Ni, Co, Fe, and Mn on Ir(001) and also for Co on Pt(001) a structural (3 × 1) unit cell was observed by low-energy electron diffraction (LEED). In either case scanning tunneling microscopy (STM) reveals a surface morphology which is characterized by long and highly periodic chains oriented along the [100] and [010] high symmetry directions of the (001) surface. The structure of the TMO chains on fcc(001) surfaces as proposed by Ferstl et al. 10 is schematically represented in Fig. 1. Within each chain we find two oxygen atoms (red) between nearest-neighbor transition metal atoms (yellow). The TMO chains sit\n",
      "\n",
      "FIG. 1. Structure model of transition metal oxide chains on Ir(001) as proposed in Ref. 10. The transition metal atoms chains sit above empty substrate rows, held in place by the oxygens atoms. The inter-chain spacing corresponds to 3 a Ir .\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "TABLE I. Magnetic moments and magnetic coupling of onedimensional TMOs along (intra-) and across (inter-) adjacent chains and corresponding energy gain as predicted by DFT in Refs. 10 and 11.\n",
      "\n",
      "| substrate                  | Ir(001)   | Ir(001)   | Ir(001)   | Ir(001)   | Pt(001)   |\n",
      "|----------------------------|-----------|-----------|-----------|-----------|-----------|\n",
      "| TMO chain                  | NiO 2     | CoO 2     | FeO 2     | MnO 2     | CoO 2     |\n",
      "| µ ( µ B )                  | 0.00      | 1.96      | 3.55      | 3.62      | -         |\n",
      "| intra-chain coupling ( ‖ ) | NM        | FM        | AFM       | AFM       | AFM       |\n",
      "| ∆ E ‖ (meV)                | -         | 25        | 44        | 27        | -         |\n",
      "| inter-chain coupling ( ⊥ ) | -         | FM        | AFM       | AFM       | -         |\n",
      "| ∆ E ⊥ (meV)                | -         | 4         | 9         | 0.4       | -         |\n",
      "\n",
      "above empty substrate rows, held in place by the oxygens atoms.\n",
      "\n",
      "Indeed, density functional theory (DFT) calculations reproduced the experimentally determined structural properties well. 10 These theoretical investigations also predicted highly interesting intra-chain magnetic couplings, ranging from non-magnetic (NM) NiO 2 via ferromagnetic (FM) coupling for CoO 2 and FeO 2 to an antiferromagnetic (AFM) interaction along MnO 2 chains on Ir(001). 10 Furthermore, an AFM coupling was predicted for CoO 2 chains on Pt(001). 11 In general, the coupling strength was found to be much stronger along the chains (up to 44 meV per TM atom) than across adjacent chains (a few meV). It should be kept in mind, however, that the calculations performed in Refs. 10 and 11, which are qualitatively summarized in Table I, were restricted to collinear spin configurations. Non-collinear magnetic structures, such as the N´ eel state, spin spirals, skyrmions, or helical spins structures, which can potentially arise from frustration, 6 higher-order exchange, 12,13 or the DMI 9 have not been considered.\n",
      "\n",
      "To verify the predictions of Ferstl et al. 10,11 we recently studied the magnetic structure of MnO 2 chains on Ir(001) by means of spin-polarized scanning tunneling microscopy (SP-STM). 14 In addition to the AFM coupling along the chains predicted by Ferstl et al. 10 an indirect 120 ◦ magnetic coupling across the chains was observed. This surprising finding was rationalized in terms of an Dzyaloshinskii-Moriya-enhanced Ruderman-KittelKasuya-Yosida (RKKY) interaction. 14 These earlier results obtained on MnO 2 /Ir(001) showed that this indirect magnetic coupling mechanism which was previously only observed for assemblies of single atoms or clusters on Pt(111) surfaces 15,16 can also result in chiral magnetic order in laterally extended structures.\n",
      "\n",
      "The purpose of this paper is to investigate the magnetic structure of a broad range of (3 × 1)-ordered TMO chains on (001) surfaces of the heavy face-centered cubic (fcc) noble metals Ir and Pt by SP-STM. The paper is organized as follows: The SP-STM technique and the experimental procedures applied for substrate cleaning, oxidation, and transition metal deposition are briefly described in Sect. II. Results for the two substrates, i.e., Ir(001) and\n",
      "\n",
      "Pt(001), will be presented separately in Sect. III A and Sect. III B, respectively. SP-STM measurements were performed on the oxides of Co, Fe, Mn, and Cr on Ir(001) and for Co and Mn on Pt(001). Whereas no magnetic contrast could be detected for Co and Cr, the magnetic intra-chain coupling observed for the other transition metals is in agreement with DFT predictions. 10,11 In addition, our results also reveal magnetic ordering across the chains. Whereas we find a collinear coupling across the chains for FeO 2 on Ir(001), the indirect inter-chain coupling of MnO 2 on both, Ir(001) and Pt(001), is found to be helical, resulting in complex spin structures with surprisingly large magnetic unit cells.\n",
      "\n",
      "## II. EXPERIMENTAL PROCEDURES\n",
      "\n",
      "STM experiments were performed in a two-chamber ultra-high vacuum (UHV) system with a base pressure p ≤ 5 × 10 -11 mbar. Clean Ir(001) and Pt(001) surfaces were prepared by annealing cycles in an oxygen atmosphere followed by cycles of sputtering and annealing without oxygen. After this procedure the well known (5 × 1) reconstruction of Ir(001) as well as the (26 × 118) structural unit cell of Pt(001) was confirmed. 17-20\n",
      "\n",
      "Closely following the procedures described by Ferstl and co-workers, 10 the clean Ir(001) surface was then exposed to molecular oxygen resulting in a (2 × 1)reconstructed surface. The oxygen pressure indicated by our quadrupole mass spectrometer was p O 2 = 1 × 10 -8 mbar, but the local pressure is assumed to be about two orders of magnitude higher since the gas nozzle is located just a few cm above the sample surface. On this oxygen-reconstructed Ir surface we deposited one third of a monolayer (ML) of either, Co, Fe, Mn, or Cr. All 3 d transition metals were vaporized with commercial e-beam evaporation sources (EFM3). Whereas Co and Fe were evaporated from wires with a diameter of 2 mm, Mn and Cr lumps were loaded into Mo crucibles. Upon 3 d transition metal deposition the sample was again annealed in an oxygen atmosphere, resulting in the (3 × 1) structure of TMO chains. 10 Since the annealing temperature T ann required for optimal TMO chain quality was found to depend on the transition metal element, the specific values will be given below. Due to the higher stability of the Pt(001) reconstruction to oxygen exposure the preparation of TMO chains on Pt(001) is slightly different. 11,21 Namely, the 3 d transition metal was directly evaporated onto the reconstructed surface and only subsequently annealed in an oxygen atmosphere. 11\n",
      "\n",
      "To verify the structural properties of the TMO chains the samples were transferred into a home-built lowtemperature scanning tunneling microscope (LT-STM) where they were scanned with an electro-chemically etched polycrystalline W tip at an operation temperature of T ≈ 5 . 5 K. All images were obtained in the constantcurrent mode with bias voltage ( U ) applied to the sample. When using spin-polarized tips in SP-STM measure-\n",
      "\n",
      "FIG. 2. (a) Overview scan of a sample with coexisting (3 × 1)-ordered CoO 2 chains and (2 × 1) oxygen-reconstructed areas on Ir(001). (b) Higher magnification image of the area marked by a white square in (a) revealing an orthogonal orientation row orientation of (2 × 1)- and (3 × 1)-ordered areas. The line profile in the bottom panel confirms a 3 a Ir periodicity perpendicular to the CoO 2 chains. (c) Atomic resolution scan of the area marked by a box in (b). The line profile verifies the × 1 periodicity along the CoO 2 chains. Scan parameters: (a), (b) U = 1V, I = 300pA; (c) U = 50mV, I = 1nA. All scans performed with a non-magnetic W tip.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "ments the recorded tunneling current I can be written as\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where I 0 is the spin-averaged contribution to the tunneling current. The second term in Eq. 1 represents the magnetization direction-dependent variation of the total current which depends on the angle θ between tip P t and sample P s polarization. For SP-STM the W tips were flashed by electron bombardment and coated with either Fe or Cr. To unambiguously determine the inplane or out-of-plane sensitivity of the magnetic SP-STM tips they were characterized on a reference system. In the present case we used the Fe double-layer (DL) on W(110) which exhibits a well-known inhomogeneous spin spiral, thereby offering the possibility to identify the in-plane and out-of-plane components of the tip magnetization in a single measurement. 22-24\n",
      "\n",
      "As we will discuss below, we could not obtain magnetic contrast on some TMOs even though earlier DFT calculations predict them to order magnetically. This raises the question of the detection limit of SP-STM. In fact, the surface spin structures of numerous elements have successfully been imaged in the past, including rareearth metals, such as Gd 25 and Dy, 26 or the antiferromagnetic 3 d metal Cr. 27 The Gd magnetic moment is largely carried by the 4 f shell (7 µ B ) which is energetically located well below the Fermi level and therefore cannot contribute to the tunneling current. For Gd(0001) it has been shown that the SP-STM contrast originates from a d z 2 -like surface state which carries a relatively low magnetic moment µ ≈ 0 . 35 µ B only. 28 In the case of Cr measurements were performed at room temperature, i.e. at a relatively high reduced temperature T/T N ≈ 0 . 94 (N´ eel temperature T N = 311K). At this temperature the magnetic moment only amounts to about 40% of its ground state value. Nevertheless, for both Gd(0001) and for Cr(001) the surface magnetic structure could clearly be imaged. Considering these earlier results we assume that the detection limit of SP-STM is well below a surface moment of 1 µ B .\n",
      "\n",
      "## III. RESULTS\n",
      "\n",
      "## A. TMO chains on Ir(001)\n",
      "\n",
      "## 1. CoO 2 /Ir(001)\n",
      "\n",
      "After evaporation of Co onto the oxidized Ir(001) surface at room temperature the sample was annealed at T ann ≈ 870 K at an oxygen partial pressure p O 2 = 1 × 10 -8 mbar. During the TMO chain growth process every third Ir surface atom is expelled from the surface layer. It has previously observed for MnO 2 on Ir(001) 1014 that-at sufficiently high annealing temperature-these atoms form extended islands or even recombine at step edges with existing surface terraces. A similar behavior can be observed in Fig. 2(a), where monatomic rectangular shaped islands occur with step edges along the [110] and [110] high-symmetry directions of the substrate.\n",
      "\n",
      "Closer inspection of the terraces in Fig. 2(b) show the coexistence of the (2 × 1) oxygen reconstruction and the (3 × 1) TMO structure along the high-symmetry direc-\n",
      "\n",
      "tions. Furthermore, Fig. 2(b) reveals that the stripes of the (2 × 1) reconstruction are generally oriented perpendicular to the (3 × 1)-ordered CoO 2 chains, possibly due to the incommensurability of (2 × 1) oxygenreconstructed and (3 × 1)-ordered TMO chains. The presence of oxygen-reconstructed areas without Co possibly indicates that the amount of deposited Co was slightly below one third of a ML. With a density about 0.03 nm -2 the most frequent defects are point-like protrusions on the TMO chains. Their height amounts to about 80 pm, consistent with typical values for transition metal adatoms. Therefore, we assume that these protrusions are caused by either excess Co or Ir atoms which were expelled from the surface layer but remained on the CoO 2 chains. Furthermore, a few depressions in the chains can be recognized (one of which is marked by the green arrow). Since these depressions are centered where one would expect a maximum in a periodic chain in the absence of a defect, it appears reasonable to preliminarily assign them to Co vacancies. Both types of defects with similar characteristics will also appear for the other transition metals studied. In addition, we occasionally observe weak circular depressions (see blue arrow) which have also been observed on the bare Ir(001) and are assigned to sub-surface defects.\n",
      "\n",
      "To verify the structural properties of the CoO 2 on Ir(001) we measured a line profile perpendicular to the chains in between the two black arrows in the bottom right corner of the STM image displayed in Fig. 2(b). It is plotted at the bottom of Fig. 2(b). The periodicity of (828 ± 30) pm agrees well with the expected value of 3 a Ir = 813pm. 29 Additionally, the atomic resolution scan shown in Fig. 2(c) and the corresponding line section shown in the bottom panel also confirm the × 1 periodicity with an atomic distance of (278 ± 10) pm along the TMO chains.\n",
      "\n",
      "After structural analysis we prepared magnetic Cr/W tips and Fe/W tips for SP-STM measurements. As documented in the Supplementary Information, these out-ofplane and in-plane sensitive tips were thoroughly tested by imaging the domain and domain wall structure the Fe DL on W(110), respectively. 30 Although these tests clearly confirmed the magnetic imaging capabilities of our SP-STM tips before and after the measurements on the CoO 2 chains on Ir(001), we never observed any magnetic contrast on the (3 × 1) structure of CoO 2 chains (not shown). This result is not necessarily in contradiction with the ferromagnetic order predicted in Ref. 10 because we have to keep in mind that the imaging of magnetic spin structures by SP-STM relies on the existence of domains or other spatial variations of the projection of the local sample magnetization onto the fixed magnetization of the tip. Therefore, the ferromagnetic domains could remain undetected if their size was much larger than the scan size. In this context future spatially averaging techniques, such as the magneto-optical Kerr effect, might be useful to clarify this open issue.\n",
      "\n",
      "## 2. FeO 2 /Ir(001)\n",
      "\n",
      "The next transition metal element with one electron less in the 3 d shell is Fe. It has been predicted that FeO 2 on Ir(001) exhibits an AFM order along the chains. 10 Just like for the preparation of CoO 2 chains described in Sect. III A 1, Fe was evaporated at room temperature on the (2 × 1)-reconstructed O/Ir(001) surface and immediately annealed at T ann ≈ 970 K in an oxygen atmosphere ( p O 2 = 1 × 10 -7 mbar). The overview scan in Fig. 3(a) shows two flat terraces separated by a monatomic substrate step edge. The absence of rectangular islands indicates that all Ir atoms expelled from the substrate had the chance to diffuse to step edges. Close inspection reveals that the chains are oriented along the [110] direction on the upper (left) terrace whereas they are oriented in the [110] direction on the lower (right) terrace. The density of the point-like defects on top of the chains amount to about 0.06 nm -2 . Similar to what was discussed in the preceding section III A 1, we believe that the most likely\n",
      "\n",
      "FIG. 3. (a) Overview scan of FeO 2 /Ir(001) with chains running into [110] direction on the upper and [110] direction on the lower terrace. (b) Higher resolution scan on the lower terrace. (c) Line profile along the blue line in (b). The periodicity of the stripes amounts to (855 ± 50) pm. Scan parameters: (a) &amp; (b) U = -1 V, I = 300pA, non-magnetic W tip.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "origin of these protrusions is a slight overdosing of the surface with Fe such that some atoms cannot be accommodated within the resulting Fe oxide chain structure. We would like to note that the teardrop-shaped appearance of the protrusions is an imaging artifact caused by an unusual shape of the tip used in this experiment.\n",
      "\n",
      "For structural analysis of the FeO 2 chains a higher resolution scan on the lower terrace is shown in Fig. 3(b). The periodicity across the chains along the blue line is determined by the line profile shown in Fig. 3(c). Again we find periodicity of (855 ± 50) pm which is in good agreement with the value expected for a (3 × 1) reconstruction, i.e., 3 a Ir = 813pm. To complete the structural analysis an atomic resolution scan of FeO 2 is presented in Fig. 4(a). Line sections drawn between the black arrows in Fig. 4(a) which are presented in Fig. 4(c) (black lines) show a periodicity of (278 ± 10) pm, consistent with the Ir lattice constant a Ir = 271pm. Furthermore, an additional weak stripe in the center of the (3 × 1) unit cell in Fig. 4(a) can be recognized in between the chains. A similar observation was reported by Ferstl et al. 10 and assigned to an electronic signal of the Ir double-rows separating adjacent TMO chains.\n",
      "\n",
      "The magnetic structure of this surface was investigated by SP-STM with an out-of-plane sensitive Cr-coated W tip. The result is presented in Fig. 4(b). Direct comparison with Fig. 4(a) reveals a doubled period along the chains. This impression is corroborated by the line profiles presented in Fig. 4(c) which have been taken in between the three pairs of colored arrows and clearly show a period 2 a Ir = 542pm along the chains. The doubling is caused by the magnetoresistance effect which leads to a tunneling current which scales with the projection of the sample magnetization onto the tip magnetization (cf. Eq. 1). The most obvious explanation for the observed doubling of the period along the chain is an AFM coupling. Furthermore, the maxima and minima of adjacent FeO 2 chains in Fig. 4(b,c) are aligned, thereby clearly indicating a FM coupling across the chains. The fact that all magnetic line profiles show the same corrugation essentially excludes any non-collinear magnetic order, such as spin spirals or frustrated N´ eel spin states, but rather supports that FeO 2 /Ir(001) exhibits collinear magnetism.\n",
      "\n",
      "Our experimental results obtained on FeO 2 chains on Ir(001) indicate a (3 × 2) magnetic unit cell which is marked as a box in Fig. 4(b) and schematically illustrated in Fig. 4(d). The AFM coupling observed along the chains is in agreement with recent theoretical predictions. 10 To our opinion it is quite reasonable to assume that this AFM along the chains is caused by superexchange mediated by the fully occupied oxygen 2 p orbitals. 31 Although a much weaker AFM coupling across the chains was predicted in Ref. 10, we experimentally observe spontaneous, permanent, and long-range FM interchain coupling at T = 5K. At the moment we can only speculate why theory and experiment deviate. Since adjacent TMO chains are separated by two non-magnetic Ir rows this order cannot be mediated by direct exchange.\n",
      "\n",
      "FIG. 4. (a) Atomic resolution scan of FeO 2 chains on Ir(001) obtained with a non-magnetic W tip. The atoms along the chains are separated by one Ir lattice constant a Ir = 271pm. (b) SP-STM image obtained with an out-of-plane Cr/W tip. Comparison to (a) reveals period along the chains lead corresponding to 2 a Ir = 542pm resulting in a (3 × 2) magnetic unit cell. (c) Line profiles measured along the chains at the position indicated by the arrows in (a) and (b). (d) Schematic illustration of the (3 × 2) collinear magnetic order. While the magnetic coupling is AFM along the chains, the interaction of adjacent chains is FM. Scan parameters: (a) U = -10 mV, I = 20nA; (b) U = 50mV, I = 5nA.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "As we will point out below for MnO 2 chains on Ir(001), it appears that the strong spin-orbit interaction of the Ir substrate plays a decisive role for the formation of the magnetic ground state. Potentially, similar effects are also relevant for FeO 2 /Ir(001). More advanced theoretical considerations will be necessary to fully elucidate the coupling mechanism.\n",
      "\n",
      "## 3. MnO 2 /Ir(001)\n",
      "\n",
      "Our SP-STM results presented in the preceding section were in good agreement with DFT predictions regarding the intra-chain magnetic coupling, 10 but also indicate that some aspect of the inter-chain coupling may not have been captured with sufficient accuracy. Since DFT qualitatively predicts the same intra-chain magnetic coupling for MnO 2 /Ir(001) as for FeO 2 it is highly interesting to experimentally investigate also this sample system. The preparation of MnO 2 chains is very similar to the TMOs discussed so far. Also in this case Mn is deposited onto the oxidized Ir(001) surface at room temperature. To our experience the best surface quality can be achieved when choosing a slightly higher annealing temperature T ann ≈ 1020 K. This results in a sample topography with roughly rectangular islands, as exemplarily presented in the overview scan of Fig. 5(a). The (3 × 1)-ordered MnO 2 chain structure covers almost the entire sample surface such that no regions with the oxygen-induced (2 × 1) reconstruction of Ir(001) remain visible. Typical (3 × 1) domains are about 20 to 40 nm wide and often hundreds of nm long. Adjacent domains are separated by either orientational and anti-phase domain boundaries.\n",
      "\n",
      "Similar to other TMO chains on Ir(001) we observe several defects which are located on top of the chains. For example, in Fig. 5(b) 12 single protrusions, one dimer, a vacancy in a MnO 2 row can be recognized. This corresponds to a defect density of only 0.015 nm -2 , well below what has been determined for CoO 2 and FeO 2 in Sect. III A 1 and III A 2, respectively. The periodicity of the stripes can be determined by the line profile presented in Fig. 5(c). In agreement with the expected (3 × 1) structure it amounts to (840 ± 50) pm.\n",
      "\n",
      "As shown in Fig. 6(a) this (3 × 1) superstructure has been imaged with atomic resolution in a spin-averaging STM experiments by using a non-magnetic W tip. The corrugation and periodicity along the chains in Fig. 6(a) can be determined from the line profiles presented in Fig. 6(c) (black lines). These data sets clearly show that any chains exhibits a corrugation of z ≈ 2 . 2 pm only and a periodicity of (287 ± 20) pm. In addition, Fig. 6(a) also reveals a small corrugation in between the chains which can possibly be assigned to Ir pairs.\n",
      "\n",
      "In Fig. 6(b) we present spin-polarized measurements of MnO 2 chains on Ir(001) which were performed with an in-plane sensitive magnetic Cr/W tip. We would like to note that these results, which were obtained in a different experimental run on a newly prepared sample with a different SP-STM tip, qualitatively reproduce earlier results. 14 Quantitative differences, such as a different corrugation, are attributed to the fact that the two tips probably differ in their spin polarization and quantization axes. Similar to the results on FeO 2 discussed in Sect. III A 2 the use of a magnetic probe tip leads to a doubling of the measured corrugation period which now corresponds to 2 a Ir along the chains. Furthermore, comparison of adjacent chains reveals a periodic variation of\n",
      "\n",
      "FIG. 5. (a) Overview scan of MnO 2 chains on Ir(001). Domains with stripes of the (3 × 1) structure orientated along the [110] and the [110] direction can be recognized. (b) Spinaveraged STM image of an atomically flat surface area showing a few defects only. (c) Line profile measured along the blue line in (b) confirming an inter-chain distance of 3 a Ir . Scan parameters: U = 1V, I = 300pA, non-magnetic W tip.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "the corrugation. This can qualitatively be realized by comparing the chain marked with red arrows in Fig. 6(b) with the chains on the left and right which are marked green and blue, respectively. The green chain on the left obviously exhibits a much weaker corrugation. In contrast, no obvious difference between the contrast strength of the red and blue line can be recognized, but the positions of maxima and minima of the chain marked with blue arrows are interchanged with respect to the red line.\n",
      "\n",
      "The colored line profiles presented in Fig. 6(c) allow for a more quantitative comparison. In agreement to our qualitative assessment the green chain exhibit the smallest corrugation of 2 . 2 pm whereas both, the blue and red line have a much higher corrugation of about 5 pm. In addition, we can also recognize the phase shift of a Ir between the red and blue line. Detailed comparison shows that the blue chain has a slightly higher corrugation than the red chain. Together with the above-mentioned periodicity 2 a Ir along the chains these SP-STM observations suggest a (9 × 2) magnetic unit cell.\n",
      "\n",
      "FIG. 6. (a) Atomic resolution scan of the MnO 2 chains on Ir(001) measured with a non-magnetic W tip showing the (3 × 1) unit cell. (b) SP-STM image of MnO 2 /Ir(001) obtained with an in-plane sensitive Cr-coated W tip. The period along the stripes corresponds to 2 a Ir indicating AFM order along the chain. The magnetic contrast of adjacent chains is modulated by phase and corrugation changes resulting in a (9 × 2) magnetic unit cell. (c) Line profile measured along three adjacent chains in (a) without (black) and in (b) with magnetically sensitive tips (colored). Scan parameters: (a) U = -300 mV, I = 3nA; (b) U = 200mV, I = 300pA.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "The experimental observations on MnO 2 chains can be explained by a spin structure as schematically illustrated in Fig. 7(a). Each MnO 2 chain is AFM ordered whereby the easy axis always lies within the surface plane. The azimuthal orientation rotates by 120 ◦ between adjacent MnO 2 chains such that the magnetic structure repeats every third chain. These two ingredients, i.e., the AFM order along and the tripled magnetic period perpendicular to the MnO 2 chains, result in a (9 × 2) magnetic unit cell as indicated in Fig. 6(b). As sketched in Fig. 7(b), this magnetic structure can explain the corrugation and the phase relation observed by SP-STM in Fig. 6(b,c).\n",
      "\n",
      "FIG. 7. (a) Schematic illustration of the (9 × 2) magnetic unit cell of MnO 2 chains on Ir(001). The arrows indicate the spin direction of the corresponding Mn atoms. (b) Sketch of the SP-STM signal expected for the proposed chiral 120 ◦ spin spiral. The corrugation and the phase shift observed in Fig. 6(b) can be explained by an angle Θ = (15 ± 6) ◦ between tip polarization (black arrow) and the TMO chain with highest corrugation (blue line).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "In this representation the black arrow represents the tip magnetization and the colored arrows represent the quantization axes of the respective chains.\n",
      "\n",
      "Proceeding from right to left in the (9 × 2) magnetic unit cell in Fig. 7(a) we can recognize that the magnetic structure is governed by an in-plane counterclockwise spin spiral with a rotation angle of 120 ◦ between adjacent chains. Since the blue arrow exhibits the largest projection onto the black arrow it will-according to Eq. 1give the strongest magnetic contrast in SP-STM measurements, symbolized by the dot-dashed line in Fig. 7(a). As a result of the spin spiral the green and red arrows are aligned antiparallel with respect to the tip. In case of an AFM intra-chain coupling this will unavoidably lead to a phase shift between the blue spin chain as compared to the green and red spin chain. Based on this representation we can nicely reproduce the corrugations extracted from Fig. 6(c) by an angle between the blue chain and the tip polarization of θ = 15 ± 6 ◦ , indicated by a grey region in Fig. 7(b).\n",
      "\n",
      "In a recently published paper the observation of a chiral 120 ◦ spin spiral which is mediated across two atomic rows of the non-magnetic Ir(001) substrate was explained by a Dzyaloshinskii-Moriya type (DM) enhancement of the RKKY interaction. 14 This interaction, which has already been predicted in 1980 32 , leads to a magnetic coupling which is at the same time indirect and chiral. Evidence for DM-enhanced indirect magnetic coupling had previously been observed on some surfacedeposited clusters 15,16,33,34 and Dy/Y superlattices. 35 Indeed, state-of-the-art density functional theory calcula-\n",
      "\n",
      "、\n",
      "\n",
      "FIG. 8. (a) Overview and (b) higher resolution scan on CrO 2 chains on Ir(001). (c)-(g) Bias-dependent STM images showing the same sample area. A cluster (marked by an arrow) serves as a position marker. A corrugation reversal is observed between panels (e) and (f), i.e., between U = 200mV and U = -200 mV. (h) Line profiles measured along lines corresponding to the blue line in (c). Scan parameters: (a),(b) U = 1V, I = 300pA; (d) I = 2nA; (d),(g) I = 1 . 5 nA; (e),(f) I = 0 . 5 nA.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "tions could successfully reproduce not only the antiferromagnetic coupling along the chains but also explain the period of the chiral magnetic structure across the chains. 14 However, the experimental data suggest a spin spiral which predominantly rotates within the surface plane, corresponding to a Dzyaloshinskii-Moriya vector D oriented along the surface normal. This surprising result cannot be explained within the structural model deduced from quantitative low-energy electron diffraction data, 10 which would result in a D vector within the surface. The discrepancy might be caused by yet unconsidered effects of structural domain boundaries or relaxation effects. Additional DFT calculations and field-dependent SP-STM measurements will be necessary to fully resolve this open question.\n",
      "\n",
      "## 4. CrO 2 /Ir(001)\n",
      "\n",
      "Motivated by the observation of an unexpected noncollinear magnetism in (3 × 1)-ordered MnO 2 chains on Ir(001) we tried to extent the series of self-organized TMO chains to Cr which is the left neighbor of Mn in the periodic table of elements. Since the growth of CrO 2 chains was not part of the study by Ferstl et al. 10 it will be discussed here in detail. The preparation follows the principle of the known TMO chains and comprises the evaporation of Cr onto the (2 × 1) O/Ir(001) surface at room temperature. Subsequently, the sample was an- nealed in an oxygen atmosphere ( p O 2 = 1 × 10 -8 mbar) at a temperature T ann ≈ 1070 K.\n",
      "\n",
      "The resulting sample topography and its structural analysis are shown in Fig. 8. Already in the overview scan presented in Fig. 8(a) we can recognize stripes orientated along the [110] direction. We count 145 point-like defects with a typical height of several 10 up to 100 pm on top of the stripes, corresponding to a defect density below 0.015 per nm 2 . Moreover, some protrusions are visible that we already identified on clean (5 × 1) reconstructed Ir(001) surfaces. The periodic arrangement of the stripes with an inter-stripe distance of (846 ± 40) pm is shown at higher resolution in Fig. 8(b). This value is comparable to the inter-chain distance observed on the other quasi one-dimensional TMO systems and consistent with the (3 × 1) structural unit cell.\n",
      "\n",
      "To study the electronic properties of this stripe pattern we performed the bias-dependent measurements presented in Fig. 8(c)-(f). All scans show the same sample area as can be recognized by the cluster of unknown origin. An arrow marks the central region of this cluster and serves as a reference point during the following comparison. At a bias voltage U = 1V, Fig. 8(c), the arrow is aligned with the top of a stripe (A-stripe). The next stripe to the right can be found about 1 nm apart just right of the bright cluster. When the bias voltage is reduced to U = 600mV, Fig. 8(d), another stripe (B-stripe) appears between the A-stripes. Interestingly, the corrugation is not mirror symmetric with respect to the plane\n",
      "\n",
      "defined by the [ ¯ 110] direction, i.e., along the stripes, and the (001) surface normal. This can best be recognized in the line sections shown in Fig. 8(h). The data have been extracted from the images shown in Fig. 8(c)-(f) along traces corresponding to the blue line in panel (c). Again the arrow (and the hatched line) indicates the central position of the bright cluster. At U = 600mV the two minima left and right of the hatched line clearly exhibit different depth, with the left minimum being about 1 pm deeper than the right minimum. This trend becomes even stronger as the bias voltage is changed to U = 200meV. Furthermore, when decreasing the bias voltage further to U = -200 meV, Fig. 8(f), and U = -500 meV, Fig. 8(g), we observe that the corrugation of the B-stripes becomes much larger than that of the A-stripes. For example, at U = -500 meV the corrugation of B-stripes reaches (15 ± 1) pm, whereas the corrugation of the A-stripes only amounts to (2 . 6 ± 0 . 3) pm.\n",
      "\n",
      "Our bias-dependent experimental results on CrO 2 /Ir(001) are strikingly different as compared to similar results on the other TMO chains. We find an asymmetric corrugation and A- and B-stripes which are the dominant features in topographic STM measurements performed at U &gt; 200 meV and U ≤ -200 meV, respectively. Furthermore, the change in corrugation is more pronounced for B-stripes. Combined with the fact that the B-stripes almost vanish for U = 1V we suppose that A-stripes correspond to the CrO 2 chains whereas the B-stripes are assigned to the intermediate Ir double row. As detailed in Ref. 30 we performed SP-STM experiments on CrO 2 /Ir(001) with both, out-of-plane sensitive Cr/W and in-plane sensitive Fe/W tips. These measurements confirmed the structural (3 × 1) unit cell of the CrO 2 chains but did not provide any hint of a magnetically ordered state.\n",
      "\n",
      "## B. TMO chains on Pt(001)\n",
      "\n",
      "## 1. CoO 2 /Pt(001)\n",
      "\n",
      "Our SP-STM experiments of TMO chains on Ir(001) presented so far revealed a wide variety of spin structures. Whereas we could not detect any magnetic signal on CoO 2 chains, suggesting a non-magnetic state, for other 3 d elements we found collinear (anti)ferromagnetic (Fe) and a helical spin spiral order (Mn). It appears to be an obvious idea to extend the search for similar indirect DM-enhanced RKKY interactions by growing TMO chains on other heavy noble metals which also crystallize in the fcc structure. One candidate material is Pt, the direct neighbor of Ir in the periodic table of elements. In fact, it has already been shown that CoO 2 chains can be grown by similar procedures on Pt(001). 11\n",
      "\n",
      "Since the Pt(001) reconstruction is stable against oxidation we first studied the growth of Co on the clean surface. 11 In our STM images the complex hexagonal reconstructions of clean Pt(001) shows up as stripes with a\n",
      "\n",
      "FIG. 9. Growth of CoO 2 chains on Pt(001). (a) Topography of Co/Pt(001) after evaporation on the warm substrate. (b) After oxidation of sample (a) no (3 × 1) structure is observed. (c) Topography of Co/Pt(001) after evaporation on the cold substrate. (d) After oxidation of sample (c) a coexistence of Pt reconstruction and domains of CoO 2 chains is observed. (e) Higher resolution scan of the chains with line profile across the stripes. (f) Atomic resolution scan with a non-magnetic W tip confirms the structural (3 × 1) unit cell. Scan parameters: (a)-(d) U = 1V, I = 300pA; (e) U = -100 mV, I = 1 . 5 nA; (e) U = 50mV, I = 5nA, all data measured with a non-magnetic W tip.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "periodicity of (1 . 36 ± 15) nm. 36 Fig. 9(a) shows the topography of a sample prepared by Co evaporation onto the warm surface resulting in the formation of (163 ± 15) pm high islands which are elongated along the [110] direction. This orientation correlates with the stripe direction of the Pt(001) surface and indicates preferential diffusion along the trenches of the reconstruction.\n",
      "\n",
      "Annealing this sample in an oxygen atmosphere ( p O 2 = 1 × 10 -7 mbar) at temperature T ann ≈ 920 K results in the topography shown in Fig. 9(b). Two surface areas can be distinguished in these data which were measured with non-magnetic W tips. In the upper right of Fig. 9(b) we\n",
      "\n",
      "again recognize the clean Pt(001) surface which-similar to our observations on clean Ir(001) [cf. blue arrow in Fig. 9(b)]-exhibits occasional circular depressions (density ≈ 0 . 02 nm -2 ). The persistence of this reconstruction confirms the stability of the Pt(001) surface against oxidation. The remaining part of the surface is covered by islands without any oxygen-induced reconstruction (nr). Together with the unusual step height of (90 ± 15) pm we interpret this as evidence for the formation of a CoPt surface alloy.\n",
      "\n",
      "In a second attempt we initially cooled the clean Pt(001) surface in the LT-STM down to T ≈ 5 . 5 K. Subsequently the crystal was transferred to the preparation chamber for Co deposition onto the cold surface. Comparison of the resulting surface topography, Fig. 9(c), with the Co film grown at elevated temperature, Fig. 9(a), shows that low-temperature growth leads to smaller Co islands and the nucleation of very few second layer Co islands. In contrast to the Co islands shown in Fig. 9(a) these low-temperature-deposited islands exhibit no stripes. Annealing this sample at the same parameters, T ann ≈ 920 K, leads to the topography presented in Fig. 9(d), which is comparable to the earlier results of Ref. 11. We can distinguish areas of reconstructed clean Pt(001) from extended and well-ordered regions showing the (3 × 1) structure which is characteristic for CoO 2 chains. The step height between domains of the same structure amounts to (199 ± 10) pm, in perfect agreement with the value expected for Pt(001). To complete the structural analysis we performed a higher resolution scan which are presented in Fig. 9(e). Again point-like defects are located on the stripes. The line profile taken at the position of the blue line confirms the periodicity of (830 ± 20) pm. The inner structure of the stripes can be resolved by the atomic resolution image shown in Fig. 9(f). Along the chains we measure a periodicity of (278 ± 15) pm which agrees well with the cubic lattice vector expected for Pt, a Pt = 277pm. 21\n",
      "\n",
      "After confirmation of the (3 × 1) structure of CoO 2 chains on Pt(001) 11 we attempted to resolve the spin or magnetic domain structure by means of SP-STM experiments using out-of-plane and in-plane polarized tips. However, these magnetically sensitive experiments 30 showed neither a hint of the theoretically proposed AFM structure 11 nor could we detect any hint of other magnetically ordered states.\n",
      "\n",
      "## 2. FeO 2 /Pt(001)\n",
      "\n",
      "For the preparation of FeO 2 chains on Pt(001) we followed the same procedure as for Co, i.e., we started with the growth of Fe on the clean reconstructed Pt(001) surface. Indeed, Fe evaporation onto the warm Pt(001) substrate leads to a surface morphology very similar to Co/Pt(001) [cf. 9(a,b), results for Fe not shown here] which we interpret as evidence for alloying with the substrate. To overcome this issue Fe was evaporated onto the\n",
      "\n",
      "FIG. 10. (a) Overview scan of oxidized Fe on Pt(001). (b) A higher resolution scan shows that terraces and islands exhibit a stripe pattern with the expected 3 a Pt periodicity. (c) Atomic scale image of oxidized Fe on Pt(001). Along the stripes the corrugation has a period of twice the atomic lattice vector. Within the circle the order across the stripes changes from aligned to a shifted position. (d) Line profiles across (top) and along the stripes (bottom) at the positions of the blue line in (b) and at the position of the arrows in (c). Scan parameters: (a),(b) U = 1V, I = 300pA; (c) U = 100mV, I = 1nA, non-magnetic W tip.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "cold sample and immediately oxidized at T ≈ 870 K. The resulting topography which is presented in the overview scan of Fig. 10(a) (scan size: 300 nm × 300 nm) shows plateaus and valleys with a typical lateral size of about 50 nm. They are separated by (199 ± 15) pm high step edges preferentially oriented along high symmetry directions of the substrate. Scanning a similar surface area at higher resolution, Fig. 10(b), reveals a stripe pattern with a periodicity of (844 ± 40) pm, as determined from the line profile in Fig. 10(d), in agreement with the (3 × 1) reconstruction expected for FeO 2 chains on Pt(001). Regions with stripes oriented in the [110] or the [110] directions can be recognized.\n",
      "\n",
      "Surprisingly, atomic scale images performed with nonmagnetic tips within a single domain are inconsistent with the expected structural (3 × 1) unit cell. This can clearly be recognized by the image presented in Fig. 10(c) and the line section shown Fig. 10(e) which was measured in between the two black arrows. The periodicity extracted from this line profile along the chains amounts to (543 ± 25) pm. This value is in good agreement with twice the lattice constant of Pt, 2 a Pt . Furthermore, the maxima of adjacent stripes are out of phase in most cases. Therefore, our spin-averaged experimental data suggest\n",
      "\n",
      "the existence of a (6 × 2) unit cell for the oxidized Fe on Pt(001) [see white box in Fig. 10(c)]. Since the positions of maxima and minima in a given chain are not completely static but slowly fluctuate, occasional exceptions from this rule can be observed. For example, the chain in the right part of the white circle in Fig. 10(c) shows such a switching event. Whereas the two chains marked by the circle are in phase in the lower part of the image, a phase shifts by a Pt in the right chain causes that the two chains are out of phase in the upper part of the image. Since the measurement of Fig. 10(c) was executed with a nonmagnetic tip and since the use of out-of-plane or in plane sensitive magnetic probe tips didn't result in any additional contrast, we have to conclude that the observed reconstruction has no magnetic but either a structural or an electronic origin. One possible explanation could be a Peierls instability due to a metal-isolator transition at low temperatures. 37 Further investigations are required to figure out the origin of the iron oxide phase.\n",
      "\n",
      "## 3. MnO 2 /Pt(001)\n",
      "\n",
      "Finally we investigated MnO 2 chains on Pt(001). In a first attempt we evaporated Mn while the Pt(001) substrate was held at room temperature and subsequently oxidized the sample at an oxygen pressure p O 2 = 1 × 10 -7 mbar at T ≈ 920 K. Unfortunately, this preparation procedure resulted in very small TMO domains with size of (10 ... 20) nm only. In order to improve surface homogeneity we reduced the oxygen pressure to p O 2 = 1 × 10 -8 mbar. The resulting topography measured with a non-magnetic W tip is shown in Fig. 11(a). A significant fraction of the surface still exhibits the familiar reconstruction of the clean Pt(001) surface. In addition, two other kinds of surfaces could be identified. The first one no longer shows any reconstruction (not reconstructed; nr) but is relatively rough. Similar findings have been reported for Cu/Ir(001) and were assigned to alloying. 38 Second, we found (3 × 1)-reconstructed areas of MnO 2 . For structural determination a higher resolution scan measured on a homogeneously striped domain is shown in Fig. 11(b). Adjacent stripes are separated by (866 ± 45) pm, as determined from the corresponding line profile in Fig. 11(c). We would like to emphasize that this particular structural domain exhibits a lateral size well above 50 nm, thereby allowing for the clear identification even of large magnetic unit cells. Moreover the surface exhibits very few defects.\n",
      "\n",
      "To exclude a behaviour similar to what we have observed on oxidized Fe on Pt(001) we performed a detailed analysis of the observed contrasts. For example, line profiles measured along the chains marked with black arrows in Fig. 12(a) are presented in the upper part of Fig. 12(c). With a corrugation between 8 . 9 and 9 . 3 pm and an atomic spacing of (281 ± 19) pm the data are in good agreement with a (3 × 1) structure of MnO 2 chains.\n",
      "\n",
      "Such a sample surface was investigated by means of\n",
      "\n",
      "FIG. 11. (a) Overview scan of Pt(001) after Mn deposition and subsequent annealing in oxygen. In addition to reconstructed clean Pt(001) one also observes disordered (nr) and (3 × 1)-reconstructed surface regions. (b) Higher resolution scan at the position of the stripes. (c) Line profile measured along the blue line in (b) confirming the 3 a Pt periodicity. Scan parameters: U = 1V, I = 300pA, non-magnetic W tip.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "SP-STM using out-of-plane sensitive Cr-coated W tips. The resulting SP-STM image presented in Fig. 12(b) exhibits striking similarities if compared with the results obtained on MnO 2 chains on Ir(001) [cf. Fig. 6(b)]. To analyze the spin-polarized measurement five line profiles which have been taken in between the colored arrows are plotted in the lower part of Fig. 12(c). Comparison with the spin-averaged data (black) reveals that-identical to our results obtained on MnO 2 /Ir(001)-a doubling of the period along the chains can be recognized. This is a clear sign for an AFM intra-chain coupling of the Mn atoms. In further analogy to the spin spiral system of MnO 2 on Ir(001) the magnetic corrugation measured on the respective chains for MnO 2 /Pt(001) is not constant but modulates periodically. For example, the line profiles clearly show that the green chain exhibits the lowest, almost vanishing corrugation. The other four chains constitute\n",
      "\n",
      "FIG. 12. (a) Atomic resolution scan of MnO 2 chains on Pt(001) performed with a W tip. (b) Spin-polarized measurement taken with a Cr-coated W tip. A periodic modulation of the chain-specific corrugation can clearly be seen. (c) Line profiles measured along five adjacent chains marked by correspondingly colored arrows as measured with a non-magnetic (top) and a magnetic tip (bottom). Whereas a doubling of the period characteristic for an intra-chain AFM coupling is observed on most of the chains, a pronounced modulation of the contrast is also visible across the chains. Scan parameters: (a) U = -5 mV, I = 20nA; (c) U = 1V, I = 300pA.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "two groups which can be distinguished on the basis of the phase of their magnetic contrast. Whenever the two upper colored line profiles (orange, light blue) exhibit a maximum, the lower line profiles (red, dark blue) show a minimum. Since all line profiles possess a periodicity 2 a Pt due to their AFM order along the chains, the phase difference shifts the magnetic contrast by a Pt .\n",
      "\n",
      "This behaviour can be explained by the model illus-\n",
      "\n",
      "FIG. 13. (a) Schematic representation of a (15 × 2) magnetic unit cell. A cycloidal spin spiral with out-of-plane rotating spins. The model in (b) is based on the magnetic corrugation in Fig. 12(c). From this a tilt Θ = ( -15 ± 5) ◦ of the tip polarization with respect to the light blue chain with highest corrugation is seen.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "trated in Fig. 13(a). The coupling along the chains is assumed to be collinear AFM within each individual MnO 2 chain. We initially assume a periodic modulation by a commensurate (15 × 2) magnetic unit cell where the spin quantization axis of adjacent stripes rotates by 72 ◦ . Due to the dependence of the spin-polarized current on the angle between the tip magnetization and the spin quantization axis of the Mn atoms described by Eq.1 this leads to a characteristic behavior of the magnetic corrugation. It is highest for the TMO chains which exhibit the highest projection onto the tip magnetization vector symbolized by a black arrow in Fig. 13(b), i.e., the light and dark blue chains. Their opposite perpendicular orientations will result in a π phase shift. A lower magnetic corrugation can be expected for the chains symbolized by orange and red arrows. The corrugation vanishes if the spin quantization axis of a chain is perpendicular to the tip polarization (green arrow). As a consequence of this rotating spin structure two chains will exhibit a parallel (light blue, orange) and two an antiparallel (blue, red) alignment with respect to the tip, thereby explaining the phase shift of a Pt observed in Fig. 12(b). On the basis of this model we are able to determine the angle between the magnetization direction of the tip and the light blue MnO 2 chain to Θ = ( -15 ± 5) ◦ .\n",
      "\n",
      "SP-STM data measured with an in-plane sensitive Fecoated W tip across a structural domain boundary indicate that the spin spiral possesses in-plane and outof-plane contributions to the magnetization. 30 A similar spin spiral driven by the Dzyaloshinskii-Moriya interaction has already been found by DFT calculations for MnO 2 chains on Ir(001). 14 In agreement with the socalled Moriya rules 39 it is most natural to assume a cy-\n",
      "\n",
      "FIG. 14. Large SP-STM overview scan performed with a magnetic tip on MnO 2 chains on Pt(001). The intra-chain coupling is not strictly collinear, as can be recognized by inspecting the MnO 2 chains marked by black arrows. Whereas no significant magnetic contrast is detected in the bottom part, a strong magnetic contrast with a 2 a Pt periodicity is clearly visible in the upper part of the image.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "cloidal spin spiral with a Dzyaloshinskii-Moriya vector oriented along the chain direction. While the DMI is expected to be very similar for Ir and Pt due to their very similar atomic number, the higher occupation of the Pt 5 d shell causes significant differences in the respective Fermi surfaces. Indeed, it has been found that the RKKY-mediated oscillation period for (001)-oriented magnetic superlattices is about 50% longer for Co/Pt 40 than for Fe/Ir, 41 in reasonable agreement with our observation of a periodicity which is also about 1.5 times as long.\n",
      "\n",
      "As we will show below deviations from the (15 × 2) magnetic unit cell can be detected by imaging MnO 2 chains on Pt(001) over larger surface areas. For example, Fig. 14 shows an SP-STM image with a scan area of 30 nm × 30 nm. For better visibility, the data set was filtered by using a band pass. As described above SPSTM images of the (15 × 2) MnO 2 /Pt(001) magnetic unit cell are characterized by five-fold periodicity of the structural (3 × 1) unit cell. In general, the magnetization direction-dependent variation of the total current gives rise to corrugation maxima if tip and sample are magnetized in the same direction, corrugation minima for an antiparrallel alignment, or a very low or even vanishing corrugation if the angle between the tip and sample magnetization direction is close to 90 ◦ (cf. Eq. 1). At the y -position marked by a dash-dotted white line at the bot- tom of Fig. 14 we find a sequence consisting of two MnO 2 chains which exhibit corrugation maxima (colored green, labelled 2A), then one chain with a low contrast, one MnO 2 chains with corrugation minima (B), and finally a chain with no recognizable spin contrast (marked by black arrows). Close inspection of the data set presented in Fig. 14 reveals that the contrast of the specific MnO 2 chains is not constant but slowly varies. For example, the MnO 2 chains marked by black arrows which showed a vanishingly low magnetic contrast at the bottom of Fig. 14 develop a sizable magnetic contrast towards the top part of the scan. This observation indicates that the magnetic order along the MnO 2 chains is not strictly collinear but slightly canted. Within our field of view of 30 nm along the [ ¯ 110] direction, however, we only observe about a quarter of a 2 π rotation. Therefore, we can conclude that the wavelength of the magnetic structure along the MnO 2 chains on Pt(001) must be very long, probably of the order of 100 nm.\n",
      "\n",
      "As a result of the spin rotation along the MnO 2 chains and the existence of occasional structural defects the magnetic order is not strictly periodic. In fact, comparison of the bottom part of the SP-STM image with the top part of Fig. 14 reveals that the contrast changed. Whereas the sequence of contrasts obtained within the five TMO chains of a magnetic unit cell was 2A-low-Blow in the bottom part, the dominating sequence in the upper part is A-low-2B-low. At the moment we can only speculate about the origin of this non-collinearity along the chains. Possibly, DM-type interactions triggered by the overlap of the O 2 p with Ir 5 d orbitals also influence the superexchange along the TMO chains.\n",
      "\n",
      "## IV. CONCLUSION\n",
      "\n",
      "In summary, we systematically investigated the structural and magnetic properties of transition metal oxides (TMOs) on the fcc(001) surfaces of Ir and Pt. We find that Co, Fe, and Mn form quasi one-dimensional TMO chains with a (3 × 1) unit cell. Similar (3 × 1)ordered chains were observed for Cr on Ir(001). Whereas no magnetic signal was found for Coand Cr-based chains, our SP-STM measurements confirm the theoretically predicted antiferromagnetic intra-chain coupling for FeO 2 /Ir(001) and for MnO 2 chains on both substrates. A ferromagnetic inter-chain coupling is found for FeO 2 /Ir(001). In the case of MnO 2 the SP-STM images reveal a complex helical intra-chain magnetic coupling, resulting in a (9 × 2) magnetic unit cell on Ir(001) and a (15 × 2) magnetic unit cell for Pt(001). Furthermore, large scale SP-STM images of MnO 2 chains on Pt(001) unveil that also the intra-chain magnetic coupling is not perfectly collinear but slightly canted, resulting in a spin spiral with a periodicity corresponding to several hundreds of substrate atoms. Our results highlight the relevance of spin-orbit-related effects for magnetic coupling phenomena in systems with broken inversion symmetry,\n",
      "\n",
      "such as surfaces or nanoparticles.\n",
      "\n",
      "## ACKNOWLEDGMENTS\n",
      "\n",
      "Experimental work was supported by DFG through FOR 1700 (project E6), SPP 2137 'Skyrmionics' (BO\n",
      "\n",
      "- ∗ corresponding author: bode@physik.uni-wuerzburg.de\n",
      "- 1 C. A. F. Vaz, J. A. C. Bland, and G. Lauhoff, 'Magnetism in ultrathin film structures,' Reports on Progress in Physics 71 , 056501 (2008).\n",
      "- 2 H.-B. Braun, 'Topological effects in nanomagnetism: from superparamagnetism to chiral quantum solitons,' Advances in Physics 61 , 1 (2012).\n",
      "- 48 M. Bode, 'Spin-polarized scanning tunnelling microscopy,' Reports on Progress in Physics 66 , 523 (2003).\n",
      "- 4 A. Kubetzka, P. Ferriani, M. Bode, S. Heinze, G. Bihlmayer, K. von Bergmann, O. Pietzsch, S. Bl¨ ugel, and R. Wiesendanger, 'Revealing antiferromagnetic order of the Fe monolayer on W(001): Spin-polarized scanning tunneling microscopy and first-principles calculations,' Phys. Rev. Lett. 94 , 087204 (2005).\n",
      "- 5 M. Bode, E. Y. Vedmedenko, K. von Bergmann, A. Kubetzka, P. Ferriani, S. Heinze, and R. Wiesendanger, 'Atomic spin structure of antiferromagnetic domain walls,' Nature Materials 5 , 477 (2006).\n",
      "- 6 C. L. Gao, W. Wulfhekel, and J. Kirschner, 'Revealing the 120 ◦ antiferromagnetic N´ eel structure in real space: One monolayer Mn on Ag(111),' Phys. Rev. Lett. 101 , 267205 (2008).\n",
      "- 7 M. Bode, S. Heinze, A. Kubetzka, O. Pietzsch, M. Hennefarth, M. Getzlaff, R. Wiesendanger, X. Nie, G. Bihlmayer, and S. Bl¨ ugel, 'Structural, electronic, and magnetic properties of a Mn monolayer on W(110),' Phys. Rev. B 66 , 014425 (2002).\n",
      "- 8 S. Heinze, M. Bode, A. Kubetzka, O. Pietzsch, X. Nie, S. Bl¨ ugel, and R. Wiesendanger, 'Real-space imaging of two-dimensional antiferromagnetism on the atomic scale,' Science 288 , 1805 (2000).\n",
      "- 9 M. Bode, M. Heide, K. von Bergmann, P. Ferriani, S. Heinze, G. Bihlmayer, A. Kubetzka, O. Pietzsch, S. Bl¨ ugel, and R. Wiesendanger, 'Chiral magnetic order at surfaces driven by inversion asymmetry,' Nature 447 , 190 (2007).\n",
      "- 10 P. Ferstl, L. Hammer, C. Sobel, M. Gubo, K. Heinz, M. A. Schneider, F. Mittendorfer, and J. Redinger, 'Selforganized growth, structure, and magnetism of monatomic transition-metal oxide chains,' Phys. Rev. Lett. 117 , 046101 (2016).\n",
      "- 11 P. Ferstl, F. Mittendorfer, J. Redinger, M. A. Schneider, and L. Hammer, 'Monatomic Co, CoO 2 , and CoO 3 nanowires on Ir(100) and Pt(100) surfaces: Formation, structure, and energetics,' Phys. Rev. B 96 , 085407 (2017).\n",
      "- 12 A. Kr¨ onlein, M. Schmitt, M. Hoffmann, J. Kemmer, N. Seubert, M. Vogt, J. K¨ uspert, M. B¨ ohme, B. Alonazi, J. K¨ ugel, H. A. Albrithen, M. Bode, G. Bihlmayer, and S. Bl¨ ugel, 'Magnetic ground state stabilized by three-site interactions: Fe / Rh(111),' Phys. Rev. Lett. 120 , 207202 (2018).\n",
      "\n",
      "1468/26-1) and by the Dresden-W¨ urzburg Center for Topological Quantum Matter Research (ct.qmat).. The authors would like to thank Paolo Moras (Trieste, Italy) for bringing one-dimensional TMOs to our attention and Jens K¨ ugel (W¨ urzburg, Germany) for critically reading the manuscript.\n",
      "\n",
      "- 13 N. Romming, H. Pralow, A. Kubetzka, M. Hoffmann, S. von Malottki, S. Meyer, B. Dup´ e, R. Wiesendanger, K. von Bergmann, and S. Heinze, 'Competition of Dzyaloshinskii-Moriya and higher-order exchange interactions in Rh/Fe atomic bilayers on Ir(111),' Phys. Rev. Lett. 120 , 207201 (2018).\n",
      "- 14 M. Schmitt, P. Moras, G. Bihlmayer, R. Cotsakis, M. Vogt, J. Kemmer, A. Belabbes, P. M. Sheverdyaeva, A. K. Kundu, C. Carbone, S. Bl¨ ugel, and M. Bode, 'Indirect chiral magnetic exchange through Dzyaloshinskii-Moriyaenhanced RKKY interactions in manganese oxide chains on Ir(100),' Nature Communications 10 , 2610 (2019).\n",
      "- 15 A. A. Khajetoorians, M. Steinbrecher, M. Ternes, M. Bouhassoune, M. dos Santos Dias, S. Lounis, J. Wiebe, and R. Wiesendanger, 'Tailoring the chiral magnetic interaction between two individual atoms,' Nature Communications 7 , 10620 (2016).\n",
      "- 16 J. Hermenau, J. Iba˜ nez-Azpiroz, C. H¨ ubner, A. Sonntag, B. Baxevanis, K. T. Ton, M. Steinbrecher, A. A. Khajetoorians, M. dos Santos Dias, S. Bl¨ ugel, R. Wiesendanger, S. Lounis, and J. Wiebe, 'A gateway towards non-collinear spin processing using three-atom magnets with strong substrate coupling,' Nature Communications 8 , 642 (2017).\n",
      "- 17 J. T. Grant, 'A LEED study of the Ir(100) surface,' Surface Science 18 , 228 (1969).\n",
      "- 18 T. N. Rhodin and G. Brod´ en, 'Preparation and chemisorptive properties of the clean normal and reconstructed surfaces of Ir(100): role of multiplets,' Surface Science 60 , 466 (1976).\n",
      "- 19 A. Schmidt, W. Meier, L. Hammer, and K. Heinz, 'Deepgoing reconstruction of Ir(100)-5 × 1,' Journal of Physics: Condensed Matter 14 , 12353 (2002).\n",
      "- 20 R. Hammer, K. Meinel, O. Krahn, and W. Widdra, 'Surface reconstruction of Pt(001) quantitatively revisited,' Phys. Rev. B 94 , 195406 (2016).\n",
      "- 21 A. E. Morgan and G. A. Somorjai, 'Low energy electron diffraction studies of gas adsorption on the platinum (100) single crystal surface,' Surface Science 12 , 405 (1968).\n",
      "- 22 O. Pietzsch, A. Kubetzka, M. Bode, and R. Wiesendanger, 'Real-space observation of dipolar antiferromagnetism in magnetic nanowires by spin-polarized scanning tunneling spectroscopy,' Phys. Rev. Lett. 84 , 5212 (2000).\n",
      "- 23 M. Bode, O. Pietzsch, A. Kubetzka, S. Heinze, and R. Wiesendanger, 'Experimental evidence for intra-atomic noncollinear magnetism at thin film probe tips,' Phys. Rev. Lett. 86 , 2142-2145 (2001).\n",
      "- 24 S. Meckler, N. Mikuszeit, A. Preßler, E. Y. Vedmedenko, O. Pietzsch, and R. Wiesendanger, 'Real-space observation of a right-rotating inhomogeneous cycloidal spin spiral by spin-polarized scanning tunneling microscopy in a triple axes vector magnet,' Phys. Rev. Lett. 103 , 157201 (2009).\n",
      "\n",
      "- 25 M. Bode, M. Getzlaff, and R. Wiesendanger, 'Spinpolarized vacuum tunneling into the exchange-split surface state of Gd(0001),' Phys. Rev. Lett. 81 , 4256-4259 (1998).\n",
      "- 26 L. Berbil-Bautista, S. Krause, M. Bode, and R. Wiesendanger, 'Spin-polarized scanning tunneling microscopy and spectroscopy of ferromagnetic Dy(0001)/W(110) films,' Phys. Rev. B 76 , 064411 (2007).\n",
      "- 27 R. Ravli´ c, M. Bode, A. Kubetzka, and R. Wiesendanger, 'Correlation of dislocation and domain structure of Cr(001) investigated by spin-polarized scanning tunneling microscopy,' Phys. Rev. B 67 , 174411 (2003).\n",
      "- 28 Ph. Kurz, G. Bihlmayer, and S. Bl¨ ugel, 'Magnetism and electronic structure of hcp Gd and the Gd(0001) surface,' Journal of Physics: Condensed Matter 14 , 63536371 (2002).\n",
      "- 29 J. W. Arblaster, 'Crystallographic properties of iridium,' Platinum Metals Review 54 , 93-102 (2010).\n",
      "- 30 See supplemental material [url] for detailed information regarding the preparation of clean, oxygen-reconstructed, and TMO-covered fcc(001) surfaces, the characterization of magnetic probe tips, and SP-STM investigations of TMOs not discussed in the main text, including Refs. [4250].\n",
      "- 31 P. W. Anderson, 'Antiferromagnetism. theory of superexchange interaction,' Phys. Rev. 79 , 350-356 (1950).\n",
      "- 32 A. Fert and Peter M. Levy, 'Role of anisotropic exchange interactions in determining the properties of spin-glasses,' Phys. Rev. Lett. 44 , 1538-1541 (1980).\n",
      "- 33 B. Dup´ e, J.E. Bickel, Y. Mokrousov, F. Otte, K. von Bergmann, A. Kubetzka, S. Heinze, and R. Wiesendanger, 'Giant magnetization canting due to symmetry breaking in zigzag Co chains on Ir(001),' New Journal of Physics 17 , 023014 (2015).\n",
      "- 34 J. Bouaziz, M. dos Santos Dias, A. Ziane, M. Benakki, S. Bl¨ ugel, and S. Lounis, 'Chiral magnetism of magnetic adatoms generated by Rashba electrons,' New Journal of Physics 19 , 023010 (2017).\n",
      "- 35 S. V. Grigoriev, Yu. O. Chetverikov, D. Lott, and A. Schreyer, 'Field induced chirality in the helix structure of Dy / Y multilayer films and experimental evidence for Dzyaloshinskii-Moriya interaction on the interfaces,' Phys. Rev. Lett. 100 , 197203 (2008).\n",
      "- 36 R. Hammer, K. Meinel, O. Krahn, and W. Widdra, 'Surface reconstruction of pt(001) quantitatively revisited,' Phys. Rev. B 94 , 195406 (2016).\n",
      "- 37 A. van Houselt, T. Gnielka, J. M. J. Aan de Brugh, N. Oncel, D. Kockmann, R. Heid, K.-P. Bohnen, B. Poelsema, and H. J. W. Zandvliet, 'Peierls instability in Pt chains on Ge(001),' Surface Science 602 , 1731 (2008).\n",
      "- 38 Gerhard Gilarowski, Javier M´ endez, and Horst Niehus, 'Initial growth of cu on ir(100)-(5 × 1),' Surface Science 448 , 290 (2000).\n",
      "- 39 T. Moriya, 'Anisotropic superexchange interaction and weak ferromagnetism,' Phys. Rev. 120 , 91-98 (1960).\n",
      "- 40 J. Moritz, F. Garcia, J. C. Toussaint, B. Dieny, and J. P. Nozi` eres, 'Orange peel coupling in multilayers with perpendicular magnetic anisotropy: Application to (co/pt)based exchange-biased spin-valves,' Europhys. Lett. 65 , 123-129 (2004).\n",
      "- 41 D. Stoeffler, 'Ab initio study of the fe intra- and inter-layer magnetic order in fe/ir(001) superlattices,' The European Physical Journal B - Condensed Matter and Complex Systems 37 , 311-320 (2004).\n",
      "- 42 K. Johnson, Q. Ge, S. Titmuss, and D. A. King, 'Unusual bridged site for adsorbed oxygen adatoms: Theory and experiment for Ir { 100 } -(1 × 2)-O,' The Journal of Chemical Physics 112 , 10460-10466 (2000).\n",
      "- 43 P. Ferstl, T. Schmitt, M. A. Schneider, L. Hammer, A. Michl, and S. M¨ uller, 'Structure and ordering of oxygen on unreconstructed Ir(100),' Phys. Rev. B 93 , 235406 (2016).\n",
      "- 44 H. J. Elmers, J. Hauschild, and U. Gradmann, 'Onset of perpendicular magnetization in nanostripe arrays of Fe on stepped W(110) surfaces,' Phys. Rev. B 59 , 3688-3695 (1999).\n",
      "- 45 M. Bode, S. Heinze, A. Kubetzka, O. Pietzsch, X. Nie, G. Bihlmayer, S. Bl¨ ugel, and R. Wiesendanger, 'Magnetization-direction-dependent local electronic structure probed by scanning tunneling spectroscopy,' Phys. Rev. Lett. 89 , 237205 (2002).\n",
      "- 46 O. Pietzsch, A. Kubetzka, M. Bode, and R. Wiesendanger, 'Observation of magnetic hysteresis at the nanometer scale by spin-polarized scanning tunneling spectroscopy,' Science 292 , 2053-2056 (2001).\n",
      "- 47 A. Kubetzka, O. Pietzsch, M. Bode, and R. Wiesendanger, 'Spin-polarized scanning tunneling microscopy study of 360 ◦ walls in an external magnetic field,' Phys. Rev. B 67 , 020401 (2003).\n",
      "- 48 M. Bode, A. Kubetzka, S. Heinze, O. Pietzsch, R. Wiesendanger, M. Heide, X. Nie, G. Bihlmayer, and S. Bl¨ ugel, 'Spin-orbit induced local band structure variations revealed by scanning tunnelling spectroscopy,' Journal of Physics: Condensed Matter 15 , S679-S692 (2003).\n",
      "- 49 E. Y. Vedmedenko, A. Kubetzka, K. von Bergmann, O. Pietzsch, M. Bode, J. Kirschner, H. P. Oepen, and R. Wiesendanger, 'Domain wall orientation in magnetic nanowires,' Phys. Rev. Lett. 92 , 077207 (2004).\n",
      "- 50 H. J. Elmers, J. Hauschild, H. Fritzsche, G. Liu, U. Gradmann, and U. K¨ ohler, 'Magnetic frustration in ultrathin Fe films,' Phys. Rev. Lett. 75 , 2031 (1995).\n",
      "Document 8:\n",
      "## Deep contextualized word representations\n",
      "\n",
      "Matthew E. Peters † , Mark Neumann † , Mohit Iyyer † , Matt Gardner † ,\n",
      "\n",
      "{ matthewp,markn,mohiti,mattg } @allenai.org\n",
      "\n",
      "Christopher Clark ∗ , Kenton Lee ∗ , Luke Zettlemoyer †∗\n",
      "\n",
      "{ csquared,kentonl,lsz } @cs.washington.edu\n",
      "\n",
      "†\n",
      "\n",
      "Paul G. Allen School of Computer Science &amp; Engineering, University of Washington\n",
      "\n",
      "Allen Institute for Artificial Intelligence ∗\n",
      "\n",
      "## Abstract\n",
      "\n",
      "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014) are a key component in many neural language understanding models. However, learning high quality representations can be challenging. They should ideally model both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). In this paper, we introduce a new type of deep contextualized word representation that directly addresses both challenges, can be easily integrated into existing models, and significantly improves the state of the art in every considered case across a range of challenging language understanding problems.\n",
      "\n",
      "Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence. We use vectors derived from a bidirectional LSTM that is trained with a coupled lan- guage model (LM) objective on a large text corpus. For this reason, we call them ELMo (Embeddings from Language Models) representations. Unlike previous approaches for learning contextualized word vectors (Peters et al., 2017; McCann et al., 2017), ELMo representations are deep, in the sense that they are a function of all of the internal layers of the biLM. More specifically, we learn a linear combination of the vectors stacked above each input word for each end task, which markedly improves performance over just using the top LSTM layer.\n",
      "\n",
      "Combining the internal states in this manner allows for very rich word representations. Using intrinsic evaluations, we show that the higher-level LSTM states capture context-dependent aspects of word meaning (e.g., they can be used without modification to perform well on supervised word sense disambiguation tasks) while lowerlevel states model aspects of syntax (e.g., they can be used to do part-of-speech tagging). Simultaneously exposing all of these signals is highly beneficial, allowing the learned models select the types of semi-supervision that are most useful for each end task.\n",
      "\n",
      "Extensive experiments demonstrate that ELMo representations work extremely well in practice. We first show that they can be easily added to existing models for six diverse and challenging language understanding problems, including textual entailment, question answering and sentiment analysis. The addition of ELMo representations alone significantly improves the state of the art in every case, including up to 20% relative error reductions. For tasks where direct comparisons are possible, ELMo outperforms CoVe (McCann et al., 2017), which computes contextualized representations using a neural machine translation encoder. Finally, an analysis of both ELMo and CoVe reveals that deep representations outperform those derived from just the top layer of an LSTM. Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems. 1\n",
      "\n",
      "## 2 Related work\n",
      "\n",
      "Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word.\n",
      "\n",
      "Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes.\n",
      "\n",
      "Other recent work has also focused on learning context-dependent representations. context2vec (Melamud et al., 2016) uses a bidirectional Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word. Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation (MT) system (CoVe; McCann et al., 2017) or an unsupervised language model (Peters et al., 2017). Both of these approaches benefit from large datasets, although the MT approach is limited by the size of parallel corpora. In this paper, we take full advantage of access to plentiful monolingual data, and train our biLM on a corpus with approximately 30 million sentences (Chelba et al., 2014). We also generalize these approaches to deep contextual representations, which we show work well across a broad range of diverse NLP tasks.\n",
      "\n",
      "1 http://allennlp.org/elmo\n",
      "\n",
      "Previous work has also shown that different layers of deep biRNNs encode different types of information. For example, introducing multi-task syntactic supervision (e.g., part-of-speech tags) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing (Hashimoto et al., 2017) or CCGsuper tagging (Søgaard and Goldberg, 2016). In an RNN-based encoder-decoder machine translation system, Belinkov et al. (2017) showed that the representations learned at the first layer in a 2layer LSTM encoder are better at predicting POS tags then second layer. Finally, the top layer of an LSTMfor encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense. We show that similar signals are also induced by the modified language model objective of our ELMo representations, and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision.\n",
      "\n",
      "Dai and Le (2015) and Ramachandran et al. (2017) pretrain encoder-decoder pairs using language models and sequence autoencoders and then fine tune with task specific supervision. In contrast, after pretraining the biLM with unlabeled data, we fix the weights and add additional taskspecific model capacity, allowing us to leverage large, rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model.\n",
      "\n",
      "## 3 ELMo: Embeddings from Language Models\n",
      "\n",
      "Unlike most widely used word embeddings (Pennington et al., 2014), ELMo word representations are functions of the entire input sentence, as described in this section. They are computed on top of two-layer biLMs with character convolutions (Sec. 3.1), as a linear function of the internal network states (Sec. 3.2). This setup allows us to do semi-supervised learning, where the biLM is pretrained at a large scale (Sec. 3.4) and easily incorporated into a wide range of existing neural NLP architectures (Sec. 3.3).\n",
      "\n",
      "## 3.1 Bidirectional language models\n",
      "\n",
      "Given a sequence of N tokens, ( t 1 , t 2 , ..., t N ) , a forward language model computes the probability of the sequence by modeling the probability of to- ken t k given the history ( t 1 , ..., t k -1 ) :\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Recent state-of-the-art neural language models (J´ ozefowicz et al., 2016; Melis et al., 2017; Merity et al., 2017) compute a context-independent token representation x LM k (via token embeddings or a CNN over characters) then pass it through L layers of forward LSTMs. At each position k , each LSTM layer outputs a context-dependent representation - → h LM k,j where j = 1 , . . . , L . The top layer LSTM output, - → h LM k,L , is used to predict the next token t k +1 with a Softmax layer.\n",
      "\n",
      "Abackward LM is similar to a forward LM, except it runs over the sequence in reverse, predicting the previous token given the future context:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "It can be implemented in an analogous way to a forward LM, with each backward LSTM layer j in a L layer deep model producing representations ← -h LM k,j of t k given ( t k +1 , . . . , t N ) .\n",
      "\n",
      "AbiLMcombines both a forward and backward LM. Our formulation jointly maximizes the log likelihood of the forward and backward directions:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "We tie the parameters for both the token representation ( Θ x ) and Softmax layer ( Θ s ) in the forward and backward direction while maintaining separate parameters for the LSTMs in each direction. Overall, this formulation is similar to the approach of Peters et al. (2017), with the exception that we share some weights between directions instead of using completely independent parameters. In the next section, we depart from previous work by introducing a new approach for learning word representations that are a linear combination of the biLM layers.\n",
      "\n",
      "## 3.2 ELMo\n",
      "\n",
      "ELMo is a task specific combination of the intermediate layer representations in the biLM. For each token t k , a L -layer biLM computes a set of 2 L +1 representations\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where h LM k, 0 is the token layer and h LM k,j = [ - → h LM k,j ; ← -h LM k,j ] , for each biLSTM layer.\n",
      "\n",
      "For inclusion in a downstream model, ELMo collapses all layers in R into a single vector, ELMo k = E ( R k ; Θ e ) . In the simplest case, ELMo just selects the top layer, E ( R k ) = h LM k,L , as in TagLM (Peters et al., 2017) and CoVe (McCann et al., 2017). More generally, we compute a task specific weighting of all biLM layers:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "In (1), s task are softmax-normalized weights and the scalar parameter γ task allows the task model to scale the entire ELMo vector. γ is of practical importance to aid the optimization process (see supplemental material for details). Considering that the activations of each biLM layer have a different distribution, in some cases it also helped to apply layer normalization (Ba et al., 2016) to each biLM layer before weighting.\n",
      "\n",
      "## 3.3 Using biLMs for supervised NLP tasks\n",
      "\n",
      "Given a pre-trained biLM and a supervised architecture for a target NLP task, it is a simple process to use the biLM to improve the task model. We simply run the biLM and record all of the layer representations for each word. Then, we let the end task model learn a linear combination of these representations, as described below.\n",
      "\n",
      "First consider the lowest layers of the supervised model without the biLM. Most supervised NLP models share a common architecture at the lowest layers, allowing us to add ELMo in a consistent, unified manner. Given a sequence of tokens ( t 1 , . . . , t N ) , it is standard to form a context-independent token representation x k for each token position using pre-trained word embeddings and optionally character-based representations. Then, the model forms a context-sensitive representation h k , typically using either bidirectional RNNs, CNNs, or feed forward networks.\n",
      "\n",
      "To add ELMo to the supervised model, we first freeze the weights of the biLM and then concatenate the ELMo vector ELMo task k with x k and pass the ELMo enhanced representation [ x k ; ELMo task k ] into the task RNN. For some tasks (e.g., SNLI, SQuAD), we observe further improvements by also including ELMo at the output of the task RNN by introducing another set of output specific linear weights and replacing h k with [ h k ; ELMo task k ] . As the remainder of the supervised model remains unchanged, these additions can happen within the context of more complex neural models. For example, see the SNLI experiments in Sec. 4 where a bi-attention layer follows the biLSTMs, or the coreference resolution experiments where a clustering model is layered on top of the biLSTMs.\n",
      "\n",
      "Finally, we found it beneficial to add a moderate amount of dropout to ELMo (Srivastava et al., 2014) and in some cases to regularize the ELMo weights by adding λ ‖ w ‖ 2 2 to the loss. This imposes an inductive bias on the ELMo weights to stay close to an average of all biLM layers.\n",
      "\n",
      "## 3.4 Pre-trained bidirectional language model architecture\n",
      "\n",
      "The pre-trained biLMs in this paper are similar to the architectures in J´ ozefowicz et al. (2016) and Kim et al. (2015), but modified to support joint training of both directions and add a residual connection between LSTM layers. We focus on large scale biLMs in this work, as Peters et al. (2017) highlighted the importance of using biLMs over forward-only LMs and large scale training.\n",
      "\n",
      "To balance overall language model perplexity with model size and computational requirements for downstream tasks while maintaining a purely character-based input representation, we halved all embedding and hidden dimensions from the single best model CNN-BIG-LSTM in J´ ozefowicz et al. (2016). The final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second layer. The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers (Srivastava et al., 2015) and a linear projection down to a 512 representation. As a result, the biLM provides three layers of representations for each input token, including those outside the training set due to the purely character input. In contrast, traditional word embedding methods only provide one layer of representation for tokens in a fixed vocabulary.\n",
      "\n",
      "After training for 10 epochs on the 1B Word Benchmark (Chelba et al., 2014), the average forward and backward perplexities is 39.7, compared to 30.0 for the forward CNN-BIG-LSTM . Generally, we found the forward and backward perplexities to be approximately equal, with the backward value slightly lower.\n",
      "\n",
      "Once pretrained, the biLM can compute representations for any task. In some cases, fine tuning the biLM on domain specific data leads to significant drops in perplexity and an increase in downstream task performance. This can be seen as a type of domain transfer for the biLM. As a result, in most cases we used a fine-tuned biLM in the downstream task. See supplemental material for details.\n",
      "\n",
      "## 4 Evaluation\n",
      "\n",
      "Table 1 shows the performance of ELMo across a diverse set of six benchmark NLP tasks. In every task considered, simply adding ELMo establishes a new state-of-the-art result, with relative error reductions ranging from 6 - 20% over strong base models. This is a very general result across a diverse set model architectures and language understanding tasks. In the remainder of this section we provide high-level sketches of the individual task results; see the supplemental material for full experimental details.\n",
      "\n",
      "Question answering The Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) contains 100K+ crowd sourced questionanswer pairs where the answer is a span in a given Wikipedia paragraph. Our baseline model (Clark and Gardner, 2017) is an improved version of the Bidirectional Attention Flow model in Seo et al. (BiDAF; 2017). It adds a self-attention layer after the bidirectional attention component, simplifies some of the pooling operations and substitutes the LSTMs for gated recurrent units (GRUs; Cho et al., 2014). After adding ELMo to the baseline model, test set F 1 improved by 4.7% from 81.1% to 85.8%, a 24.9% relative error reduction over the baseline, and improving the overall single model state-of-the-art by 1.4%. A 11 member ensemble pushes F 1 to 87.4, the overall state-of-the-art at time of submission to the leaderboard. 2 The increase of 4.7% with ELMo is also significantly larger then the 1.8% improvement from adding CoVe to a baseline model (McCann et al., 2017).\n",
      "\n",
      "2 As of November 17, 2017.\n",
      "\n",
      "Table 1: Test set comparison of ELMo enhanced neural models with state-of-the-art single model baselines across six benchmark NLP tasks. The performance metric varies across tasks - accuracy for SNLI and SST-5; F 1 for SQuAD, SRL and NER; average F 1 for Coref. Due to the small test sizes for NER and SST-5, we report the mean and standard deviation across five runs with different random seeds. The 'increase' column lists both the absolute and relative improvements over our baseline.\n",
      "\n",
      "| TASK   | PREVIOUS SOTA        | PREVIOUS SOTA   |   OUR BASELINE | ELMO + BASELINE   | INCREASE (ABSOLUTE/ RELATIVE)   |\n",
      "|--------|----------------------|-----------------|----------------|-------------------|---------------------------------|\n",
      "| SQuAD  | Liu et al. (2017)    | 84.4            |          81.1  | 85.8              | 4.7 / 24.9%                     |\n",
      "| SNLI   | Chen et al. (2017)   | 88.6            |          88    | 88.7 ± 0.17       | 0.7 / 5.8%                      |\n",
      "| SRL    | He et al. (2017)     | 81.7            |          81.4  | 84.6              | 3.2 / 17.2%                     |\n",
      "| Coref  | Lee et al. (2017)    | 67.2            |          67.2  | 70.4              | 3.2 / 9.8%                      |\n",
      "| NER    | Peters et al. (2017) | 91.93 ± 0.19    |          90.15 | 92.22 ± 0.10      | 2.06 / 21%                      |\n",
      "| SST-5  | McCann et al. (2017) | 53.7            |          51.4  | 54.7 ± 0.5        | 3.3 / 6.8%                      |\n",
      "\n",
      "Textual entailment Textual entailment is the task of determining whether a 'hypothesis' is true, given a 'premise'. The Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) provides approximately 550K hypothesis/premise pairs. Our baseline, the ESIM sequence model from Chen et al. (2017), uses a biLSTM to encode the premise and hypothesis, followed by a matrix attention layer, a local inference layer, another biLSTM inference composition layer, and finally a pooling operation before the output layer. Overall, adding ELMo to the ESIM model improves accuracy by an average of 0.7% across five random seeds. A five member ensemble pushes the overall accuracy to 89.3%, exceeding the previous ensemble best of 88.9% (Gong et al., 2018).\n",
      "\n",
      "Semantic role labeling A semantic role labeling (SRL) system models the predicate-argument structure of a sentence, and is often described as answering 'Who did what to whom'. He et al. (2017) modeled SRL as a BIO tagging problem and used an 8-layer deep biLSTM with forward and backward directions interleaved, following Zhou and Xu (2015). As shown in Table 1, when adding ELMo to a re-implementation of He et al. (2017) the single model test set F 1 jumped 3.2% from 81.4% to 84.6% - a new state-of-the-art on the OntoNotes benchmark (Pradhan et al., 2013), even improving over the previous best ensemble result by 1.2%.\n",
      "\n",
      "Coreference resolution Coreference resolution is the task of clustering mentions in text that refer to the same underlying real world entities. Our baseline model is the end-to-end span-based neural model of Lee et al. (2017). It uses a biLSTM\n",
      "\n",
      "and attention mechanism to first compute span representations and then applies a softmax mention ranking model to find coreference chains. In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task (Pradhan et al., 2012), adding ELMo improved the average F 1 by 3.2% from 67.2 to 70.4, establishing a new state of the art, again improving over the previous best ensemble result by 1.6% F 1 .\n",
      "\n",
      "Named entity extraction The CoNLL 2003 NER task (Sang and Meulder, 2003) consists of newswire from the Reuters RCV1 corpus tagged with four different entity types ( PER , LOC , ORG , MISC ). Following recent state-of-the-art systems (Lample et al., 2016; Peters et al., 2017), the baseline model uses pre-trained word embeddings, a character-based CNN representation, two biLSTM layers and a conditional random field (CRF) loss (Lafferty et al., 2001), similar to Collobert et al. (2011). As shown in Table 1, our ELMo enhanced biLSTM-CRF achieves 92.22% F 1 averaged over five runs. The key difference between our system and the previous state of the art from Peters et al. (2017) is that we allowed the task model to learn a weighted average of all biLM layers, whereas Peters et al. (2017) only use the top biLM layer. As shown in Sec. 5.1, using all layers instead of just the last layer improves performance across multiple tasks.\n",
      "\n",
      "Sentiment analysis The fine-grained sentiment classification task in the Stanford Sentiment Treebank (SST-5; Socher et al., 2013) involves selecting one of five labels (from very negative to very positive) to describe a sentence from a movie review. The sentences contain diverse linguistic phenomena such as idioms and complex syntac-\n",
      "\n",
      "Table 2: Development set performance for SQuAD, SNLI and SRL comparing using all layers of the biLM (with different choices of regularization strength λ ) to just the top layer.\n",
      "\n",
      "| Task   |   Baseline |   Last Only |   All layers λ =1 λ |   =0.001 |\n",
      "|--------|------------|-------------|---------------------|----------|\n",
      "| SQuAD  |       80.8 |        84.7 |                85   |     85.2 |\n",
      "| SNLI   |       88.1 |        89.1 |                89.3 |     89.5 |\n",
      "| SRL    |       81.6 |        84.1 |                84.6 |     84.8 |\n",
      "\n",
      "Table 3: Development set performance for SQuAD, SNLI and SRL when including ELMo at different locations in the supervised model.\n",
      "\n",
      "| Task   |   Input Only |   Input& Output |   Output Only |\n",
      "|--------|--------------|-----------------|---------------|\n",
      "| SQuAD  |         85.1 |            85.6 |          84.8 |\n",
      "| SNLI   |         88.9 |            89.5 |          88.7 |\n",
      "| SRL    |         84.7 |            84.3 |          80.9 |\n",
      "\n",
      "tic constructions such as negations that are difficult for models to learn. Our baseline model is the biattentive classification network (BCN) from McCann et al. (2017), which also held the prior state-of-the-art result when augmented with CoVe embeddings. Replacing CoVe with ELMo in the BCN model results in a 1.0% absolute accuracy improvement over the state of the art.\n",
      "\n",
      "## 5 Analysis\n",
      "\n",
      "This section provides an ablation analysis to validate our chief claims and to elucidate some interesting aspects of ELMo representations. Sec. 5.1 shows that using deep contextual representations in downstream tasks improves performance over previous work that uses just the top layer, regardless of whether they are produced from a biLM or MT encoder, and that ELMo representations provide the best overall performance. Sec. 5.3 explores the different types of contextual information captured in biLMs and uses two intrinsic evaluations to show that syntactic information is better represented at lower layers while semantic information is captured a higher layers, consistent with MT encoders. It also shows that our biLM consistently provides richer representations then CoVe. Additionally, we analyze the sensitivity to where ELMo is included in the task model (Sec. 5.2), training set size (Sec. 5.4), and visualize the ELMo learned weights across the tasks (Sec. 5.5).\n",
      "\n",
      "## 5.1 Alternate layer weighting schemes\n",
      "\n",
      "There are many alternatives to Equation 1 for combining the biLM layers. Previous work on contextual representations used only the last layer, whether it be from a biLM (Peters et al., 2017) or an MT encoder (CoVe; McCann et al., 2017). The choice of the regularization parameter λ is also important, as large values such as λ = 1 effectively reduce the weighting function to a simple average over the layers, while smaller values (e.g., λ = 0 . 001 ) allow the layer weights to vary.\n",
      "\n",
      "Table 2 compares these alternatives for SQuAD, SNLI and SRL. Including representations from all layers improves overall performance over just using the last layer, and including contextual representations from the last layer improves performance over the baseline. For example, in the case of SQuAD, using just the last biLM layer improves development F 1 by 3.9% over the baseline. Averaging all biLM layers instead of using just the last layer improves F 1 another 0.3% (comparing 'Last Only' to λ =1 columns), and allowing the task model to learn individual layer weights improves F 1 another 0.2% ( λ =1 vs. λ =0.001). A small λ is preferred in most cases with ELMo, although for NER, a task with a smaller training set, the results are insensitive to λ (not shown).\n",
      "\n",
      "The overall trend is similar with CoVe but with smaller increases over the baseline. For SNLI, averaging all layers with λ =1 improves development accuracy from 88.2 to 88.7% over using just the last layer. SRL F 1 increased a marginal 0.1% to 82.2 for the λ =1 case compared to using the last layer only.\n",
      "\n",
      "## 5.2 Where to include ELMo?\n",
      "\n",
      "All of the task architectures in this paper include word embeddings only as input to the lowest layer biRNN. However, we find that including ELMo at the output of the biRNN in task-specific architectures improves overall results for some tasks. As shown in Table 3, including ELMo at both the input and output layers for SNLI and SQuAD improves over just the input layer, but for SRL (and coreference resolution, not shown) performance is highest when it is included at just the input layer. One possible explanation for this result is that both the SNLI and SQuAD architectures use attention layers after the biRNN, so introducing ELMo at this layer allows the model to attend directly to the biLM's internal representations. In the SRL case,\n",
      "\n",
      "Table 4: Nearest neighbors to 'play' using GloVe and the context embeddings from a biLM.\n",
      "\n",
      "|       | Source                                                                | Nearest Neighbors                                                                                                                                                |\n",
      "|-------|-----------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| GloVe | play                                                                  | playing, game, games, played, players, plays, player, Play, football, multiplayer                                                                                |\n",
      "| biLM  | Chico Ruiz made a spec- tacular play on Alusik 's grounder { . . . }  | Kieffer , the only junior in the group , was commended for his ability to hit in the clutch , as well as his all-round excellent play .                          |\n",
      "| biLM  | Olivia De Havilland signed to do a Broadway play for Garson { . . . } | { . . . } they were actors who had been handed fat roles in a successful play , and had talent enough to fill the roles competently , with nice understatement . |\n",
      "\n",
      "Table 5: All-words fine grained WSD F 1 . For CoVe and the biLM, we report scores for both the first and second layer biLSTMs.\n",
      "\n",
      "| Model                      |   F 1 |\n",
      "|----------------------------|-------|\n",
      "| WordNet 1st Sense Baseline |  65.9 |\n",
      "| Raganato et al. (2017a)    |  69.9 |\n",
      "| Iacobacci et al. (2016)    |  70.1 |\n",
      "| CoVe, First Layer          |  59.4 |\n",
      "| CoVe, Second Layer         |  64.7 |\n",
      "| biLM, First layer          |  67.4 |\n",
      "| biLM, Second layer         |  69   |\n",
      "\n",
      "the task-specific context representations are likely more important than those from the biLM.\n",
      "\n",
      "## 5.3 What information is captured by the biLM's representations?\n",
      "\n",
      "Since adding ELMo improves task performance over word vectors alone, the biLM's contextual representations must encode information generally useful for NLP tasks that is not captured in word vectors. Intuitively, the biLM must be disambiguating the meaning of words using their context. Consider 'play', a highly polysemous word. The top of Table 4 lists nearest neighbors to 'play' using GloVe vectors. They are spread across several parts of speech (e.g., 'played', 'playing' as verbs, and 'player', 'game' as nouns) but concentrated in the sportsrelated senses of 'play'. In contrast, the bottom two rows show nearest neighbor sentences from the SemCor dataset (see below) using the biLM's context representation of 'play' in the source sentence. In these cases, the biLM is able to disambiguate both the part of speech and word sense in the source sentence.\n",
      "\n",
      "These observations can be quantified using an\n",
      "\n",
      "Table 6: Test set POS tagging accuracies for PTB. For CoVe and the biLM, we report scores for both the first and second layer biLSTMs.\n",
      "\n",
      "| Model                   |   Acc. |\n",
      "|-------------------------|--------|\n",
      "| Collobert et al. (2011) |   97.3 |\n",
      "| Ma and Hovy (2016)      |   97.6 |\n",
      "| Ling et al. (2015)      |   97.8 |\n",
      "| CoVe, First Layer       |   93.3 |\n",
      "| CoVe, Second Layer      |   92.8 |\n",
      "| biLM, First Layer       |   97.3 |\n",
      "| biLM, Second Layer      |   96.8 |\n",
      "\n",
      "intrinsic evaluation of the contextual representations similar to Belinkov et al. (2017). To isolate the information encoded by the biLM, the representations are used to directly make predictions for a fine grained word sense disambiguation (WSD) task and a POS tagging task. Using this approach, it is also possible to compare to CoVe, and across each of the individual layers.\n",
      "\n",
      "Word sense disambiguation Given a sentence, we can use the biLM representations to predict the sense of a target word using a simple 1nearest neighbor approach, similar to Melamud et al. (2016). To do so, we first use the biLM to compute representations for all words in SemCor 3.0, our training corpus (Miller et al., 1994), and then take the average representation for each sense. At test time, we again use the biLM to compute representations for a given target word and take the nearest neighbor sense from the training set, falling back to the first sense from WordNet for lemmas not observed during training.\n",
      "\n",
      "Table 5 compares WSD results using the evaluation framework from Raganato et al. (2017b) across the same suite of four test sets in Raganato et al. (2017a). Overall, the biLM top layer rep- resentations have F 1 of 69.0 and are better at WSD then the first layer. This is competitive with a state-of-the-art WSD-specific supervised model using hand crafted features (Iacobacci et al., 2016) and a task specific biLSTM that is also trained with auxiliary coarse-grained semantic labels and POS tags (Raganato et al., 2017a). The CoVe biLSTM layers follow a similar pattern to those from the biLM (higher overall performance at the second layer compared to the first); however, our biLM outperforms the CoVe biLSTM, which trails the WordNet first sense baseline.\n",
      "\n",
      "POS tagging To examine whether the biLM captures basic syntax, we used the context representations as input to a linear classifier that predicts POS tags with the Wall Street Journal portion of the Penn Treebank (PTB) (Marcus et al., 1993). As the linear classifier adds only a small amount of model capacity, this is direct test of the biLM's representations. Similar to WSD, the biLM representations are competitive with carefully tuned, task specific biLSTMs (Ling et al., 2015; Ma and Hovy, 2016). However, unlike WSD, accuracies using the first biLM layer are higher than the top layer, consistent with results from deep biLSTMs in multi-task training (Søgaard and Goldberg, 2016; Hashimoto et al., 2017) and MT (Belinkov et al., 2017). CoVe POS tagging accuracies follow the same pattern as those from the biLM, and just like for WSD, the biLM achieves higher accuracies than the CoVe encoder.\n",
      "\n",
      "Implications for supervised tasks Taken together, these experiments confirm different layers in the biLM represent different types of information and explain why including all biLM layers is important for the highest performance in downstream tasks. In addition, the biLM's representations are more transferable to WSD and POS tagging than those in CoVe, helping to illustrate why ELMo outperforms CoVe in downstream tasks.\n",
      "\n",
      "## 5.4 Sample efficiency\n",
      "\n",
      "Adding ELMo to a model increases the sample efficiency considerably, both in terms of number of parameter updates to reach state-of-the-art performance and the overall training set size. For example, the SRL model reaches a maximum development F 1 after 486 epochs of training without ELMo. After adding ELMo, the model exceeds the baseline maximum at epoch 10, a 98% relative decrease in the number of updates needed to reach\n",
      "\n",
      "Figure 1: Comparison of baseline vs. ELMo performance for SNLI and SRL as the training set size is varied from 0.1% to 100%.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Figure 2: Visualization of softmax normalized biLM layer weights across tasks and ELMo locations. Normalized weights less then 1 / 3 are hatched with horizontal lines and those greater then 2 / 3 are speckled.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "the same level of performance.\n",
      "\n",
      "In addition, ELMo-enhanced models use smaller training sets more efficiently than models without ELMo. Figure 1 compares the performance of baselines models with and without ELMo as the percentage of the full training set is varied from 0.1% to 100%. Improvements with ELMo are largest for smaller training sets and significantly reduce the amount of training data needed to reach a given level of performance. In the SRL case, the ELMo model with 1% of the training set has about the same F 1 as the baseline model with 10% of the training set.\n",
      "\n",
      "## 5.5 Visualization of learned weights\n",
      "\n",
      "Figure 2 visualizes the softmax-normalized learned layer weights. At the input layer, the task model favors the first biLSTM layer. For coreference and SQuAD, the this is strongly favored, but the distribution is less peaked for the other tasks. The output layer weights are relatively balanced, with a slight preference for the lower layers.\n",
      "\n",
      "## 6 Conclusion\n",
      "\n",
      "We have introduced a general approach for learning high-quality deep context-dependent representations from biLMs, and shown large improvements when applying ELMo to a broad range of NLPtasks. Through ablations and other controlled experiments, we have also confirmed that the biLM layers efficiently encode different types of syntactic and semantic information about wordsin-context, and that using all layers improves overall task performance.\n",
      "\n",
      "## References\n",
      "\n",
      "- Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization. CoRR abs/1607.06450.\n",
      "- Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James R. Glass. 2017. What do neural machine translation models learn about morphology? In ACL .\n",
      "- Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. TACL 5:135-146.\n",
      "- Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics.\n",
      "- Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2014. One billion word benchmark for measuring progress in statistical language modeling. In INTERSPEECH .\n",
      "- Qian Chen, Xiao-Dan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced lstm for natural language inference. In ACL .\n",
      "- Jason Chiu and Eric Nichols. 2016. Named entity recognition with bidirectional LSTM-CNNs. In TACL .\n",
      "- Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder approaches. In SSST@EMNLP .\n",
      "- Christopher Clark and Matthew Gardner. 2017. Simple and effective multi-paragraph reading comprehension. CoRR abs/1710.10723.\n",
      "- Kevin Clark and Christopher D. Manning. 2016. Deep reinforcement learning for mention-ranking coreference models. In EMNLP .\n",
      "- Ronan Collobert, Jason Weston, L´ eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. 2011. Natural language processing (almost) from scratch. In JMLR .\n",
      "- Andrew M. Dai and Quoc V. Le. 2015. Semisupervised sequence learning. In NIPS .\n",
      "- Greg Durrett and Dan Klein. 2013. Easy victories and uphill battles in coreference resolution. In EMNLP .\n",
      "- Yarin Gal and Zoubin Ghahramani. 2016. A theoretically grounded application of dropout in recurrent neural networks. In NIPS .\n",
      "- Yichen Gong, Heng Luo, and Jian Zhang. 2018. Natural language inference over interaction space. In ICLR .\n",
      "- Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. 2017. A joint many-task model: Growing a neural network for multiple nlp tasks. In EMNLP 2017 .\n",
      "- Luheng He, Kenton Lee, Mike Lewis, and Luke S. Zettlemoyer. 2017. Deep semantic role labeling: What works and what's next. In ACL .\n",
      "- Sepp Hochreiter and J¨ urgen Schmidhuber. 1997. Long short-term memory. Neural Computation 9.\n",
      "- Ignacio Iacobacci, Mohammad Taher Pilehvar, and Roberto Navigli. 2016. Embeddings for word sense disambiguation: An evaluation study. In ACL .\n",
      "- Rafal J´ ozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. 2016. Exploring the limits of language modeling. CoRR abs/1602.02410.\n",
      "- Rafal J´ ozefowicz, Wojciech Zaremba, and Ilya Sutskever. 2015. An empirical exploration of recurrent network architectures. In ICML .\n",
      "- Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. 2015. Character-aware neural language models. In AAAI 2016 .\n",
      "- Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR .\n",
      "- Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, Ishaan Gulrajani James Bradbury, Victor Zhong, Romain Paulus, and Richard Socher. 2016. Ask me anything: Dynamic memory networks for natural language processing. In ICML .\n",
      "- John D. Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML .\n",
      "- Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In NAACL-HLT .\n",
      "- Kenton Lee, Luheng He, Mike Lewis, and Luke S. Zettlemoyer. 2017. End-to-end neural coreference resolution. In EMNLP .\n",
      "- Wang Ling, Chris Dyer, Alan W. Black, Isabel Trancoso, Ramon Fermandez, Silvio Amir, Lu´ ıs Marujo, and Tiago Lu´ ıs. 2015. Finding function in form: Compositional character models for open vocabulary word representation. In EMNLP .\n",
      "- Xiaodong Liu, Yelong Shen, Kevin Duh, and Jianfeng Gao. 2017. Stochastic answer networks for machine reading comprehension. arXiv preprint arXiv:1712.03556 .\n",
      "- Xuezhe Ma and Eduard H. Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNsCRF. In ACL .\n",
      "- Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics 19:313-330.\n",
      "- Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Contextualized word vectors. In NIPS 2017 .\n",
      "- Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context embedding with bidirectional lstm. In CoNLL .\n",
      "- G´ abor Melis, Chris Dyer, and Phil Blunsom. 2017. On the state of the art of evaluation in neural language models. CoRR abs/1707.05589.\n",
      "- Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2017. Regularizing and optimizing lstm language models. CoRR abs/1708.02182.\n",
      "- Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS .\n",
      "- George A. Miller, Martin Chodorow, Shari Landes, Claudia Leacock, and Robert G. Thomas. 1994. Using a semantic concordance for sense identification. In HLT .\n",
      "- Tsendsuren Munkhdalai and Hong Yu. 2017. Neural tree indexers for text understanding. In EACL .\n",
      "- Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. 2014. Efficient nonparametric estimation of multiple embeddings per word in vector space. In EMNLP .\n",
      "- Martha Palmer, Paul Kingsbury, and Daniel Gildea. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics 31:71106.\n",
      "- Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In EMNLP .\n",
      "- Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL .\n",
      "- Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Bj¨ orkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. 2013. Towards robust linguistic analysis using ontonotes. In CoNLL .\n",
      "- Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes. In EMNLPCoNLL Shared Task .\n",
      "- Alessandro Raganato, Claudio Delli Bovi, and Roberto Navigli. 2017a. Neural sequence learning models for word sense disambiguation. In EMNLP .\n",
      "- Alessandro Raganato, Jose Camacho-Collados, and Roberto Navigli. 2017b. Word sense disambiguation: A unified evaluation framework and empirical comparison. In EACL .\n",
      "- Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. In EMNLP .\n",
      "- Prajit Ramachandran, Peter Liu, and Quoc Le. 2017. Improving sequence to sequence learning with unlabeled data. In EMNLP .\n",
      "- Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In CoNLL .\n",
      "- Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In ICLR .\n",
      "- Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP .\n",
      "- Anders Søgaard and Yoav Goldberg. 2016. Deep multi-task learning with low level tasks supervised at lower layers. In ACL 2016 .\n",
      "- Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research 15:1929-1958.\n",
      "- Rupesh Kumar Srivastava, Klaus Greff, and J¨ urgen Schmidhuber. 2015. Training very deep networks. In NIPS .\n",
      "- Joseph P. Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In ACL .\n",
      "- Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. 2017. Gated self-matching networks for reading comprehension and question answering. In ACL .\n",
      "- John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2016. Charagram: Embedding words and sentences via character n-grams. In EMNLP .\n",
      "- Sam Wiseman, Alexander M. Rush, and Stuart M. Shieber. 2016. Learning global features for coreference resolution. In HLT-NAACL .\n",
      "- Matthew D. Zeiler. 2012. Adadelta: An adaptive learning rate method. CoRR abs/1212.5701.\n",
      "- Jie Zhou and Wei Xu. 2015. End-to-end learning of semantic role labeling using recurrent neural networks. In ACL .\n",
      "- Peng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu, Hongyun Bao, and Bo Xu. 2016. Text classification improved by integrating bidirectional lstm with twodimensional max pooling. In COLING .\n",
      "\n",
      "## A Supplemental Material to accompany Deep contextualized word representations\n",
      "\n",
      "This supplement contains details of the model architectures, training routines and hyper-parameter choices for the state-of-the-art models in Section 4.\n",
      "\n",
      "All of the individual models share a common architecture in the lowest layers with a context independent token representation below several layers of stacked RNNs - LSTMs in every case except the SQuAD model that uses GRUs.\n",
      "\n",
      "## A.1 Fine tuning biLM\n",
      "\n",
      "As noted in Sec. 3.4, fine tuning the biLM on task specific data typically resulted in significant drops in perplexity. To fine tune on a given task, the supervised labels were temporarily ignored, the biLM fine tuned for one epoch on the training split and evaluated on the development split. Once fine tuned, the biLM weights were fixed during task training.\n",
      "\n",
      "Table 7 lists the development set perplexities for the considered tasks. In every case except CoNLL 2012, fine tuning results in a large improvement in perplexity, e.g., from 72.1 to 16.8 for SNLI.\n",
      "\n",
      "The impact of fine tuning on supervised performance is task dependent. In the case of SNLI, fine tuning the biLM increased development accuracy 0.6% from 88.9% to 89.5% for our single best model. However, for sentiment classification development set accuracy is approximately the same regardless whether a fine tuned biLM was used.\n",
      "\n",
      "## A.2 Importance of γ in Eqn. (1)\n",
      "\n",
      "The γ parameter in Eqn. (1) was of practical importance to aid optimization, due to the different distributions between the biLM internal representations and the task specific representations. It is especially important in the last-only case in Sec. 5.1. Without this parameter, the last-only case performed poorly (well below the baseline) for SNLI and training failed completely for SRL.\n",
      "\n",
      "## A.3 Textual Entailment\n",
      "\n",
      "Our baseline SNLI model is the ESIM sequence model from Chen et al. (2017). Following the original implementation, we used 300 dimensions for all LSTM and feed forward layers and pretrained 300 dimensional GloVe embeddings that were fixed during training. For regularization, we\n",
      "\n",
      "Table 7: Development set perplexity before and after fine tuning for one epoch on the training set for various datasets (lower is better). Reported values are the average of the forward and backward perplexities.\n",
      "\n",
      "| Dataset                | Dataset                |   Before tuning | After tuning   |\n",
      "|------------------------|------------------------|-----------------|----------------|\n",
      "| SNLI                   | SNLI                   |            72.1 | 16.8           |\n",
      "| CoNLL 2012 (coref/SRL) | CoNLL 2012 (coref/SRL) |            92.3 | -              |\n",
      "| CoNLL 2003 (NER)       | CoNLL 2003 (NER)       |           103.2 | 46.3           |\n",
      "| SQuAD                  | Context                |            99.1 | 43.5           |\n",
      "| SQuAD                  | Questions              |           158.2 | 52.0           |\n",
      "| SST                    | SST                    |           131.5 | 78.6           |\n",
      "\n",
      "added 50% variational dropout (Gal and Ghahramani, 2016) to the input of each LSTM layer and 50% dropout (Srivastava et al., 2014) at the input to the final two fully connected layers. All feed forward layers use ReLU activations. Parameters were optimized using Adam (Kingma and Ba, 2015) with gradient norms clipped at 5.0 and initial learning rate 0.0004, decreasing by half each time accuracy on the development set did not increase in subsequent epochs. The batch size was 32.\n",
      "\n",
      "The best ELMo configuration added ELMo vectors to both the input and output of the lowest layer LSTM, using (1) with layer normalization and λ = 0 . 001 . Due to the increased number of parameters in the ELMo model, we added glyph[lscript] 2 regularization with regularization coefficient 0.0001 to all recurrent and feed forward weight matrices and 50% dropout after the attention layer.\n",
      "\n",
      "Table 8 compares test set accuracy of our system to previously published systems. Overall, adding ELMo to the ESIM model improved accuracy by 0.7% establishing a new single model state-of-the-art of 88.7%, and a five member ensemble pushes the overall accuracy to 89.3%.\n",
      "\n",
      "## A.4 Question Answering\n",
      "\n",
      "Our QA model is a simplified version of the model from Clark and Gardner (2017). It embeds tokens by concatenating each token's case-sensitive 300 dimensional GloVe word vector (Pennington et al., 2014) with a character-derived embedding produced using a convolutional neural network followed by max-pooling on learned character embeddings. The token embeddings are passed through a shared bi-directional GRU, and then the bi-directional attention mechanism from BiDAF (Seo et al., 2017). The augmented con-\n",
      "\n",
      "Table 8: SNLI test set accuracy. 3 Single model results occupy the portion, with ensemble results at the bottom.\n",
      "\n",
      "| Model                               | Acc.        |\n",
      "|-------------------------------------|-------------|\n",
      "| Feature based (Bowman et al., 2015) | 78.2        |\n",
      "| DIIN (Gong et al., 2018)            | 88.0        |\n",
      "| BCN+Char+CoVe (McCann et al., 2017) | 88.1        |\n",
      "| ESIM (Chen et al., 2017)            | 88.0        |\n",
      "| ESIM+TreeLSTM (Chen et al., 2017)   | 88.6        |\n",
      "| ESIM+ELMo                           | 88.7 ± 0.17 |\n",
      "| DIIN ensemble (Gong et al., 2018)   | 88.9        |\n",
      "| ESIM+ELMo ensemble                  | 89.3        |\n",
      "\n",
      "text vectors are then passed through a linear layer with ReLU activations, a residual self-attention layer that uses a GRU followed by the same attention mechanism applied context-to-context, and another linear layer with ReLU activations. Finally, the results are fed through linear layers to predict the start and end token of the answer.\n",
      "\n",
      "Variational dropout is used before the input to the GRUs and the linear layers at a rate of 0.2. A dimensionality of 90 is used for the GRUs, and 180 for the linear layers. We optimize the model using Adadelta with a batch size of 45. At test time we use an exponential moving average of the weights and limit the output span to be of at most size 17. We do not update the word vectors during training.\n",
      "\n",
      "Performance was highest when adding ELMo without layer normalization to both the input and output of the contextual GRU layer and leaving the ELMo weights unregularized ( λ = 0 ).\n",
      "\n",
      "Table 9 compares test set results from the SQuAD leaderboard as of November 17, 2017 when we submitted our system. Overall, our submission had the highest single model and ensemble results, improving the previous single model result (SAN) by 1.4% F 1 and our baseline by 4.2%. A 11 member ensemble pushes F 1 to 87.4%, 1.0% increase over the previous ensemble best.\n",
      "\n",
      "## A.5 Semantic Role Labeling\n",
      "\n",
      "Our baseline SRL model is an exact reimplementation of (He et al., 2017). Words are represented using a concatenation of 100 dimensional vector representations, initialized using GloVe (Pennington et al., 2014) and a binary, per-word predicate feature, represented using an 100 dimensional em-\n",
      "\n",
      "3 A comprehensive comparison can be found at https: //nlp.stanford.edu/projects/snli/\n",
      "\n",
      "bedding. This 200 dimensional token representation is then passed through an 8 layer 'interleaved' biLSTM with a 300 dimensional hidden size, in which the directions of the LSTM layers alternate per layer. This deep LSTM uses Highway connections (Srivastava et al., 2015) between layers and variational recurrent dropout (Gal and Ghahramani, 2016). This deep representation is then projected using a final dense layer followed by a softmax activation to form a distribution over all possible tags. Labels consist of semantic roles from PropBank (Palmer et al., 2005) augmented with a BIO labeling scheme to represent argument spans. During training, we minimize the negative log likelihood of the tag sequence using Adadelta with a learning rate of 1.0 and ρ = 0 . 95 (Zeiler, 2012). At test time, we perform Viterbi decoding to enforce valid spans using BIO constraints. Variational dropout of 10% is added to all LSTM hidden layers. Gradients are clipped if their value exceeds 1.0. Models are trained for 500 epochs or until validation F1 does not improve for 200 epochs, whichever is sooner. The pretrained GloVe vectors are fine-tuned during training. The final dense layer and all cells of all LSTMs are initialized to be orthogonal. The forget gate bias is initialized to 1 for all LSTMs, with all other gates initialized to 0, as per (J´ ozefowicz et al., 2015).\n",
      "\n",
      "Table 10 compares test set F1 scores of our ELMo augmented implementation of (He et al., 2017) with previous results. Our single model score of 84.6 F1 represents a new state-of-the-art result on the CONLL 2012 Semantic Role Labeling task, surpassing the previous single model result by 2.9 F1 and a 5-model ensemble by 1.2 F1.\n",
      "\n",
      "## A.6 Coreference resolution\n",
      "\n",
      "Our baseline coreference model is the end-to-end neural model from Lee et al. (2017) with all hy-\n",
      "\n",
      "| Model                                  |   EM |   F 1 |\n",
      "|----------------------------------------|------|-------|\n",
      "| BiDAF (Seo et al., 2017)               | 68   |  77.3 |\n",
      "| BiDAF + Self Attention                 | 72.1 |  81.1 |\n",
      "| DCN+                                   | 75.1 |  83.1 |\n",
      "| Reg-RaSoR                              | 75.8 |  83.3 |\n",
      "| FusionNet                              | 76   |  83.9 |\n",
      "| r-net (Wang et al., 2017)              | 76.5 |  84.3 |\n",
      "| SAN (Liu et al., 2017)                 | 76.8 |  84.4 |\n",
      "| BiDAF + Self Attention + ELMo          | 78.6 |  85.8 |\n",
      "| DCN+ Ensemble                          | 78.9 |  86   |\n",
      "| FusionNet Ensemble                     | 79   |  86   |\n",
      "| Interactive AoA Reader+ Ensemble       | 79.1 |  86.5 |\n",
      "| BiDAF + Self Attention + ELMo Ensemble | 81   |  87.4 |\n",
      "\n",
      "Table 9: Test set results for SQuAD, showing both Exact Match (EM) and F 1 . The top half of the table contains single model results with ensembles at the bottom. References provided where available.\n",
      "\n",
      "Table 10: SRL CoNLL 2012 test set F 1 .\n",
      "\n",
      "| Model                       |   F 1 |\n",
      "|-----------------------------|-------|\n",
      "| Pradhan et al. (2013)       |  77.5 |\n",
      "| Zhou and Xu (2015)          |  81.3 |\n",
      "| He et al. (2017), single    |  81.7 |\n",
      "| He et al. (2017), ensemble  |  83.4 |\n",
      "| He et al. (2017), our impl. |  81.4 |\n",
      "| He et al. (2017) + ELMo     |  84.6 |\n",
      "\n",
      "Table 11: Coreference resolution average F 1 on the test set from the CoNLL 2012 shared task.\n",
      "\n",
      "| Model                        |   Average F 1 |\n",
      "|------------------------------|---------------|\n",
      "| Durrett and Klein (2013)     |          60.3 |\n",
      "| Wiseman et al. (2016)        |          64.2 |\n",
      "| Clark and Manning (2016)     |          65.7 |\n",
      "| Lee et al. (2017) (single)   |          67.2 |\n",
      "| Lee et al. (2017) (ensemble) |          68.8 |\n",
      "| Lee et al. (2017) + ELMo     |          70.4 |\n",
      "\n",
      "perparameters exactly following the original implementation.\n",
      "\n",
      "The best configuration added ELMo to the input of the lowest layer biLSTM and weighted the biLM layers using (1) without any regularization ( λ = 0 ) or layer normalization. 50% dropout was added to the ELMo representations.\n",
      "\n",
      "Table 11 compares our results with previously published results. Overall, we improve the single model state-of-the-art by 3.2% average F 1 , and our single model result improves the previous ensemble best by 1.6% F 1 . Adding ELMo to the output from the biLSTM in addition to the biLSTM input reduced F 1 by approximately 0.7% (not shown).\n",
      "\n",
      "## A.7 Named Entity Recognition\n",
      "\n",
      "Our baseline NER model concatenates 50 dimensional pre-trained Senna vectors (Collobert et al., 2011) with a CNN character based representation. The character representation uses 16 dimensional character embeddings and 128 convolutional filters of width three characters, a ReLU activation and by max pooling. The token representation is passed through two biLSTM layers, the first with 200 hidden units and the second with 100 hidden units before a final dense layer and softmax layer. During training, we use a CRF loss and at test time perform decoding using the Viterbi algorithm while ensuring that the output tag sequence is valid.\n",
      "\n",
      "Variational dropout is added to the input of both biLSTM layers. During training the gradients are rescaled if their glyph[lscript] 2 norm exceeds 5.0 and parameters updated using Adam with constant learning rate of 0.001. The pre-trained Senna embeddings are fine tuned during training. We employ early stopping on the development set and report the averaged test set score across five runs with different random seeds.\n",
      "\n",
      "ELMowasadded to the input of the lowest layer task biLSTM. As the CoNLL 2003 NER data set is relatively small, we found the best performance by constraining the trainable layer weights to be effectively constant by setting λ = 0 . 1 with (1).\n",
      "\n",
      "Table 12 compares test set F 1 scores of our ELMo enhanced biLSTM-CRF tagger with previous results. Overall, the 92.22% F 1 from our system establishes a new state-of-the-art. When compared to Peters et al. (2017), using representations\n",
      "\n",
      "Table 12: Test set F 1 for CoNLL 2003 NER task. Models with ♣ included gazetteers and those with ♦ used both the train and development splits for training.\n",
      "\n",
      "| Model                         | F 1 ± std.   |\n",
      "|-------------------------------|--------------|\n",
      "| Collobert et al. (2011) ♣     | 89.59        |\n",
      "| Lample et al. (2016)          | 90.94        |\n",
      "| Ma and Hovy (2016)            | 91.2         |\n",
      "| Chiu and Nichols (2016) ♣ , ♦ | 91.62 ± 0.33 |\n",
      "| Peters et al. (2017) ♦        | 91.93 ± 0.19 |\n",
      "| biLSTM-CRF + ELMo             | 92.22 ± 0.10 |\n",
      "\n",
      "Table 13: Test set accuracy for SST-5.\n",
      "\n",
      "| Model                               |   Acc. |\n",
      "|-------------------------------------|--------|\n",
      "| DMN(Kumar et al., 2016)             |   52.1 |\n",
      "| LSTM-CNN (Zhou et al., 2016)        |   52.4 |\n",
      "| NTI (Munkhdalai and Yu, 2017)       |   53.1 |\n",
      "| BCN+Char+CoVe (McCann et al., 2017) |   53.7 |\n",
      "| BCN+ELMo                            |   54.7 |\n",
      "\n",
      "from all layers of the biLM provides a modest improvement.\n",
      "\n",
      "## A.8 Sentiment classification\n",
      "\n",
      "We use almost the same biattention classification network architecture described in McCann et al. (2017), with the exception of replacing the final maxout network with a simpler feedforward network composed of two ReLu layers with dropout. A BCN model with a batch-normalized maxout network reached significantly lower validation accuracies in our experiments, although there may be discrepancies between our implementation and that of McCann et al. (2017). To match the CoVe training setup, we only train on phrases that contain four or more tokens. We use 300-d hidden states for the biLSTM and optimize the model parameters with Adam (Kingma and Ba, 2015) using a learning rate of 0.0001. The trainable biLM layer weights are regularized by λ = 0 . 001 , and we add ELMo to both the input and output of the biLSTM; the output ELMo vectors are computed with a second biLSTM and concatenated to the input.\n",
      "Document 9:\n",
      "1\n",
      "\n",
      "## INTRODUCTION\n",
      "\n",
      "Recent advances in object detection are driven by the success of region proposal methods ( e.g ., [4]) and region-based convolutional neural networks (RCNNs) [5]. Although region-based CNNs were computationally expensive as originally developed in [5], their cost has been drastically reduced thanks to sharing convolutions across proposals [1], [2]. The latest incarnation, Fast R-CNN [2], achieves near real-time rates using very deep networks [3], when ignoring the time spent on region proposals . Now, proposals are the test-time computational bottleneck in state-of-the-art detection systems.\n",
      "\n",
      "Region proposal methods typically rely on inexpensive features and economical inference schemes. Selective Search [4], one of the most popular methods, greedily merges superpixels based on engineered low-level features. Yet when compared to efficient detection networks [2], Selective Search is an order of magnitude slower, at 2 seconds per image in a CPU implementation. EdgeBoxes [6] currently provides the best tradeoff between proposal quality and speed, at 0.2 seconds per image. Nevertheless, the region proposal step still consumes as much running time as the detection network.\n",
      "\n",
      "· S. Ren is with University of Science and Technology of China, Hefei, China. This work was done when S. Ren was an intern at Microsoft Research. Email: sqren@mail.ustc.edu.cn\n",
      "\n",
      "· K. He and J. Sun are with Visual Computing Group, Microsoft Research. E-mail: { kahe,jiansun } @microsoft.com\n",
      "\n",
      "· R. Girshick is with Facebook AI Research. The majority of this work was done when R. Girshick was with Microsoft Research. E-mail: rbg@fb.com\n",
      "\n",
      "## Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\n",
      "\n",
      "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
      "\n",
      "Abstract -State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5fps ( including all steps ) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.\n",
      "\n",
      "Index Terms -Object Detection, Region Proposal, Convolutional Neural Network.\n",
      "\n",
      "F\n",
      "\n",
      "One may note that fast region-based CNNs take advantage of GPUs, while the region proposal methods used in research are implemented on the CPU, making such runtime comparisons inequitable. An obvious way to accelerate proposal computation is to reimplement it for the GPU. This may be an effective engineering solution, but re-implementation ignores the down-stream detection network and therefore misses important opportunities for sharing computation.\n",
      "\n",
      "In this paper, we show that an algorithmic changecomputing proposals with a deep convolutional neural network-leads to an elegant and effective solution where proposal computation is nearly cost-free given the detection network's computation. To this end, we introduce novel Region Proposal Networks (RPNs) that share convolutional layers with state-of-the-art object detection networks [1], [2]. By sharing convolutions at test-time, the marginal cost for computing proposals is small ( e.g ., 10ms per image).\n",
      "\n",
      "Our observation is that the convolutional feature maps used by region-based detectors, like Fast RCNN, can also be used for generating region proposals. On top of these convolutional features, we construct an RPN by adding a few additional convolutional layers that simultaneously regress region bounds and objectness scores at each location on a regular grid. The RPN is thus a kind of fully convolutional network (FCN) [7] and can be trained end-toend specifically for the task for generating detection proposals.\n",
      "\n",
      "RPNs are designed to efficiently predict region proposals with a wide range of scales and aspect ratios. In contrast to prevalent methods [8], [9], [1], [2] that use\n",
      "\n",
      "Figure 1: Different schemes for addressing multiple scales and sizes. (a) Pyramids of images and feature maps are built, and the classifier is run at all scales. (b) Pyramids of filters with multiple scales/sizes are run on the feature map. (c) We use pyramids of reference boxes in the regression functions.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "pyramids of images (Figure 1, a) or pyramids of filters (Figure 1, b), we introduce novel 'anchor' boxes that serve as references at multiple scales and aspect ratios. Our scheme can be thought of as a pyramid of regression references (Figure 1, c), which avoids enumerating images or filters of multiple scales or aspect ratios. This model performs well when trained and tested using single-scale images and thus benefits running speed.\n",
      "\n",
      "To unify RPNs with Fast R-CNN [2] object detection networks, we propose a training scheme that alternates between fine-tuning for the region proposal task and then fine-tuning for object detection, while keeping the proposals fixed. This scheme converges quickly and produces a unified network with convolutional features that are shared between both tasks. 1\n",
      "\n",
      "We comprehensively evaluate our method on the PASCAL VOC detection benchmarks [11] where RPNs with Fast R-CNNs produce detection accuracy better than the strong baseline of Selective Search with Fast R-CNNs. Meanwhile, our method waives nearly all computational burdens of Selective Search at test-time-the effective running time for proposals is just 10 milliseconds. Using the expensive very deep models of [3], our detection method still has a frame rate of 5fps ( including all steps ) on a GPU, and thus is a practical object detection system in terms of both speed and accuracy. We also report results on the MS COCO dataset [12] and investigate the improvements on PASCAL VOC using the COCO data. Code has been made publicly available at https://github.com/shaoqingren/faster\\_ rcnn (in MATLAB) and https://github.com/ rbgirshick/py-faster-rcnn (in Python).\n",
      "\n",
      "A preliminary version of this manuscript was published previously [10]. Since then, the frameworks of RPN and Faster R-CNN have been adopted and generalized to other methods, such as 3D object detection [13], part-based detection [14], instance segmentation [15], and image captioning [16]. Our fast and effective object detection system has also been built in com-\n",
      "\n",
      "1. Since the publication of the conference version of this paper [10], we have also found that RPNs can be trained jointly with Fast R-CNN networks leading to less training time.\n",
      "\n",
      "mercial systems such as at Pinterests [17], with user engagement improvements reported.\n",
      "\n",
      "In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the basis of several 1st-place entries [18] in the tracks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. RPNs completely learn to propose regions from data, and thus can easily benefit from deeper and more expressive features (such as the 101-layer residual nets adopted in [18]). Faster R-CNN and RPN are also used by several other leading entries in these competitions 2 . These results suggest that our method is not only a cost-efficient solution for practical usage, but also an effective way of improving object detection accuracy.\n",
      "\n",
      "## 2 RELATED WORK\n",
      "\n",
      "Object Proposals. There is a large literature on object proposal methods. Comprehensive surveys and comparisons of object proposal methods can be found in [19], [20], [21]. Widely used object proposal methods include those based on grouping super-pixels ( e.g ., Selective Search [4], CPMC [22], MCG [23]) and those based on sliding windows ( e.g ., objectness in windows [24], EdgeBoxes [6]). Object proposal methods were adopted as external modules independent of the detectors ( e.g ., Selective Search [4] object detectors, RCNN [5], and Fast R-CNN [2]).\n",
      "\n",
      "Deep Networks for Object Detection. The R-CNN method [5] trains CNNs end-to-end to classify the proposal regions into object categories or background. R-CNN mainly plays as a classifier, and it does not predict object bounds (except for refining by bounding box regression). Its accuracy depends on the performance of the region proposal module (see comparisons in [20]). Several papers have proposed ways of using deep networks for predicting object bounding boxes [25], [9], [26], [27]. In the OverFeat method [9], a fully-connected layer is trained to predict the box coordinates for the localization task that assumes a single object. The fully-connected layer is then turned\n",
      "\n",
      "Figure 2: Faster R-CNN is a single, unified network for object detection. The RPN module serves as the 'attention' of this unified network.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "into a convolutional layer for detecting multiple classspecific objects. The MultiBox methods [26], [27] generate region proposals from a network whose last fully-connected layer simultaneously predicts multiple class-agnostic boxes, generalizing the 'singlebox' fashion of OverFeat. These class-agnostic boxes are used as proposals for R-CNN [5]. The MultiBox proposal network is applied on a single image crop or multiple large image crops ( e.g ., 224 × 224), in contrast to our fully convolutional scheme. MultiBox does not share features between the proposal and detection networks. We discuss OverFeat and MultiBox in more depth later in context with our method. Concurrent with our work, the DeepMask method [28] is developed for learning segmentation proposals.\n",
      "\n",
      "Shared computation of convolutions [9], [1], [29], [7], [2] has been attracting increasing attention for efficient, yet accurate, visual recognition. The OverFeat paper [9] computes convolutional features from an image pyramid for classification, localization, and detection. Adaptively-sized pooling (SPP) [1] on shared convolutional feature maps is developed for efficient region-based object detection [1], [30] and semantic segmentation [29]. Fast R-CNN [2] enables end-to-end detector training on shared convolutional features and shows compelling accuracy and speed.\n",
      "\n",
      "## 3 FASTER R-CNN\n",
      "\n",
      "Our object detection system, called Faster R-CNN, is composed of two modules. The first module is a deep fully convolutional network that proposes regions, and the second module is the Fast R-CNN detector [2] that uses the proposed regions. The entire system is a single, unified network for object detection (Figure 2). Using the recently popular terminology of neural networks with 'attention' [31] mechanisms, the RPN module tells the Fast R-CNN module where to look. In Section 3.1 we introduce the designs and properties of the network for region proposal. In Section 3.2 we develop algorithms for training both modules with features shared.\n",
      "\n",
      "## 3.1 Region Proposal Networks\n",
      "\n",
      "A Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score. 3 We model this process with a fully convolutional network [7], which we describe in this section. Because our ultimate goal is to share computation with a Fast R-CNN object detection network [2], we assume that both nets share a common set of convolutional layers. In our experiments, we investigate the Zeiler and Fergus model [32] (ZF), which has 5 shareable convolutional layers and the Simonyan and Zisserman model [3] (VGG-16), which has 13 shareable convolutional layers.\n",
      "\n",
      "To generate region proposals, we slide a small network over the convolutional feature map output by the last shared convolutional layer. This small network takes as input an n × n spatial window of the input convolutional feature map. Each sliding window is mapped to a lower-dimensional feature (256-d for ZF and 512-d for VGG, with ReLU [33] following). This feature is fed into two sibling fullyconnected layers-a box-regression layer ( reg ) and a box-classification layer ( cls ). We use n = 3 in this paper, noting that the effective receptive field on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-network is illustrated at a single position in Figure 3 (left). Note that because the mini-network operates in a sliding-window fashion, the fully-connected layers are shared across all spatial locations. This architecture is naturally implemented with an n × n convolutional layer followed by two sibling 1 × 1 convolutional layers (for reg and cls , respectively).\n",
      "\n",
      "## 3.1.1 Anchors\n",
      "\n",
      "At each sliding-window location, we simultaneously predict multiple region proposals, where the number of maximum possible proposals for each location is denoted as k . So the reg layer has 4 k outputs encoding the coordinates of k boxes, and the cls layer outputs 2 k scores that estimate probability of object or not object for each proposal 4 . The k proposals are parameterized relative to k reference boxes, which we call\n",
      "\n",
      "3. 'Region' is a generic term and in this paper we only consider rectangular regions, as is common for many methods ( e.g ., [27], [4], [6]). 'Objectness' measures membership to a set of object classes vs . background.\n",
      "\n",
      "4. For simplicity we implement the cls layer as a two-class softmax layer. Alternatively, one may use logistic regression to produce k scores.\n",
      "\n",
      "Figure 3: Left : Region Proposal Network (RPN). Right : Example detections using RPN proposals on PASCAL VOC 2007 test. Our method detects objects in a wide range of scales and aspect ratios.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "anchors . An anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio (Figure 3, left). By default we use 3 scales and 3 aspect ratios, yielding k = 9 anchors at each sliding position. For a convolutional feature map of a size W × H (typically ∼ 2,400), there are WHk anchors in total.\n",
      "\n",
      "## Translation-Invariant Anchors\n",
      "\n",
      "An important property of our approach is that it is translation invariant , both in terms of the anchors and the functions that compute proposals relative to the anchors. If one translates an object in an image, the proposal should translate and the same function should be able to predict the proposal in either location. This translation-invariant property is guaranteed by our method 5 . As a comparison, the MultiBox method [27] uses k-means to generate 800 anchors, which are not translation invariant. So MultiBox does not guarantee that the same proposal is generated if an object is translated.\n",
      "\n",
      "The translation-invariant property also reduces the model size. MultiBox has a (4 + 1) × 800 -dimensional fully-connected output layer, whereas our method has a (4 + 2) × 9 -dimensional convolutional output layer in the case of k = 9 anchors. As a result, our output layer has 2 . 8 × 10 4 parameters ( 512 × (4 + 2) × 9 for VGG-16), two orders of magnitude fewer than MultiBox's output layer that has 6 . 1 × 10 6 parameters ( 1536 × (4 + 1) × 800 for GoogleNet [34] in MultiBox [27]). If considering the feature projection layers, our proposal layers still have an order of magnitude fewer parameters than MultiBox 6 . We expect our method to have less risk of overfitting on small datasets, like PASCAL VOC.\n",
      "\n",
      "5. As is the case of FCNs [7], our network is translation invariant up to the network's total stride.\n",
      "\n",
      "6. Considering the feature projection layers, our proposal layers' parameter count is 3 × 3 × 512 × 512 + 512 × 6 × 9 = 2 . 4 × 10 6 ; MultiBox's proposal layers' parameter count is 7 × 7 × (64 + 96 + 64 + 64) × 1536 + 1536 × 5 × 800 = 27 × 10 6 .\n",
      "\n",
      "## Multi-Scale Anchors as Regression References\n",
      "\n",
      "Our design of anchors presents a novel scheme for addressing multiple scales (and aspect ratios). As shown in Figure 1, there have been two popular ways for multi-scale predictions. The first way is based on image/feature pyramids, e.g ., in DPM [8] and CNNbased methods [9], [1], [2]. The images are resized at multiple scales, and feature maps (HOG [8] or deep convolutional features [9], [1], [2]) are computed for each scale (Figure 1(a)). This way is often useful but is time-consuming. The second way is to use sliding windows of multiple scales (and/or aspect ratios) on the feature maps. For example, in DPM [8], models of different aspect ratios are trained separately using different filter sizes (such as 5 × 7 and 7 × 5). If this way is used to address multiple scales, it can be thought of as a 'pyramid of filters' (Figure 1(b)). The second way is usually adopted jointly with the first way [8].\n",
      "\n",
      "As a comparison, our anchor-based method is built on a pyramid of anchors , which is more cost-efficient. Our method classifies and regresses bounding boxes with reference to anchor boxes of multiple scales and aspect ratios. It only relies on images and feature maps of a single scale, and uses filters (sliding windows on the feature map) of a single size. We show by experiments the effects of this scheme for addressing multiple scales and sizes (Table 8).\n",
      "\n",
      "Because of this multi-scale design based on anchors, we can simply use the convolutional features computed on a single-scale image, as is also done by the Fast R-CNN detector [2]. The design of multiscale anchors is a key component for sharing features without extra cost for addressing scales.\n",
      "\n",
      "## 3.1.2 Loss Function\n",
      "\n",
      "For training RPNs, we assign a binary class label (of being an object or not) to each anchor. We assign a positive label to two kinds of anchors: (i) the anchor/anchors with the highest Intersection-overUnion (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher than 0.7 with\n",
      "\n",
      "any ground-truth box. Note that a single ground-truth box may assign positive labels to multiple anchors. Usually the second condition is sufficient to determine the positive samples; but we still adopt the first condition for the reason that in some rare cases the second condition may find no positive sample. We assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes. Anchors that are neither positive nor negative do not contribute to the training objective.\n",
      "\n",
      "With these definitions, we minimize an objective function following the multi-task loss in Fast R-CNN [2]. Our loss function for an image is defined as:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Here, i is the index of an anchor in a mini-batch and p i is the predicted probability of anchor i being an object. The ground-truth label p ∗ i is 1 if the anchor is positive, and is 0 if the anchor is negative. t i is a vector representing the 4 parameterized coordinates of the predicted bounding box, and t ∗ i is that of the ground-truth box associated with a positive anchor. The classification loss L cls is log loss over two classes (object vs . not object). For the regression loss, we use L reg ( t i , t ∗ i ) = R ( t i -t ∗ i ) where R is the robust loss function (smooth L 1 ) defined in [2]. The term p ∗ i L reg means the regression loss is activated only for positive anchors ( p ∗ i = 1 ) and is disabled otherwise ( p ∗ i = 0 ). The outputs of the cls and reg layers consist of { p i } and { t i } respectively.\n",
      "\n",
      "The two terms are normalized by N cls and N reg and weighted by a balancing parameter λ . In our current implementation (as in the released code), the cls term in Eqn.(1) is normalized by the mini-batch size ( i.e ., N cls = 256 ) and the reg term is normalized by the number of anchor locations ( i.e ., N reg ∼ 2 , 400 ). By default we set λ = 10 , and thus both cls and reg terms are roughly equally weighted. We show by experiments that the results are insensitive to the values of λ in a wide range (Table 9). We also note that the normalization as above is not required and could be simplified.\n",
      "\n",
      "For bounding box regression, we adopt the parameterizations of the 4 coordinates following [5]:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where x , y , w , and h denote the box's center coordinates and its width and height. Variables x , x a, and x ∗ are for the predicted box, anchor box, and groundtruth box respectively (likewise for y, w, h ). This can be thought of as bounding-box regression from an anchor box to a nearby ground-truth box.\n",
      "\n",
      "Nevertheless, our method achieves bounding-box regression by a different manner from previous RoIbased (Region of Interest) methods [1], [2]. In [1], [2], bounding-box regression is performed on features pooled from arbitrarily sized RoIs, and the regression weights are shared by all region sizes. In our formulation, the features used for regression are of the same spatial size ( 3 × 3 ) on the feature maps. To account for varying sizes, a set of k bounding-box regressors are learned. Each regressor is responsible for one scale and one aspect ratio, and the k regressors do not share weights. As such, it is still possible to predict boxes of various sizes even though the features are of a fixed size/scale, thanks to the design of anchors.\n",
      "\n",
      "## 3.1.3 Training RPNs\n",
      "\n",
      "The RPN can be trained end-to-end by backpropagation and stochastic gradient descent (SGD) [35]. We follow the 'image-centric' sampling strategy from [2] to train this network. Each mini-batch arises from a single image that contains many positive and negative example anchors. It is possible to optimize for the loss functions of all anchors, but this will bias towards negative samples as they are dominate. Instead, we randomly sample 256 anchors in an image to compute the loss function of a mini-batch, where the sampled positive and negative anchors have a ratio of up to 1:1. If there are fewer than 128 positive samples in an image, we pad the mini-batch with negative ones.\n",
      "\n",
      "We randomly initialize all new layers by drawing weights from a zero-mean Gaussian distribution with standard deviation 0.01. All other layers ( i.e ., the shared convolutional layers) are initialized by pretraining a model for ImageNet classification [36], as is standard practice [5]. We tune all layers of the ZF net, and conv3 1 and up for the VGG net to conserve memory [2]. We use a learning rate of 0.001 for 60k mini-batches, and 0.0001 for the next 20k mini-batches on the PASCAL VOC dataset. We use a momentum of 0.9 and a weight decay of 0.0005 [37]. Our implementation uses Caffe [38].\n",
      "\n",
      "## 3.2 Sharing Features for RPN and Fast R-CNN\n",
      "\n",
      "Thus far we have described how to train a network for region proposal generation, without considering the region-based object detection CNN that will utilize these proposals. For the detection network, we adopt Fast R-CNN [2]. Next we describe algorithms that learn a unified network composed of RPN and Fast R-CNN with shared convolutional layers (Figure 2).\n",
      "\n",
      "Both RPN and Fast R-CNN, trained independently, will modify their convolutional layers in different ways. We therefore need to develop a technique that allows for sharing convolutional layers between the\n",
      "\n",
      "Table 1: the learned average proposal size for each anchor using the ZF net (numbers for s = 600 ). anchor 128 2 , 2:1 128 2 , 1:1 128 2 , 1:2 256 2 , 2:1 256 2 , 1:1 256 2 , 1:2 512 2 , 2:1 512 2 , 1:1 512 2 , 1:2 proposal 188 × 111 113 × 114 70 × 92 416 × 229 261 × 284 174 × 332 768 × 437 499 × 501 355 × 715\n",
      "\n",
      "two networks, rather than learning two separate networks. We discuss three ways for training networks with features shared:\n",
      "\n",
      "(i) Alternating training . In this solution, we first train RPN, and use the proposals to train Fast R-CNN. The network tuned by Fast R-CNN is then used to initialize RPN, and this process is iterated. This is the solution that is used in all experiments in this paper.\n",
      "\n",
      "- (ii) Approximate joint training . In this solution, the RPN and Fast R-CNN networks are merged into one network during training as in Figure 2. In each SGD iteration, the forward pass generates region proposals which are treated just like fixed, pre-computed proposals when training a Fast R-CNN detector. The backward propagation takes place as usual, where for the shared layers the backward propagated signals from both the RPN loss and the Fast R-CNN loss are combined. This solution is easy to implement. But this solution ignores the derivative w.r.t. the proposal boxes' coordinates that are also network responses, so is approximate. In our experiments, we have empirically found this solver produces close results, yet reduces the training time by about 25-50% comparing with alternating training. This solver is included in our released Python code.\n",
      "\n",
      "(iii) Non-approximate joint training . As discussed above, the bounding boxes predicted by RPN are also functions of the input. The RoI pooling layer [2] in Fast R-CNN accepts the convolutional features and also the predicted bounding boxes as input, so a theoretically valid backpropagation solver should also involve gradients w.r.t. the box coordinates. These gradients are ignored in the above approximate joint training. In a non-approximate joint training solution, we need an RoI pooling layer that is differentiable w.r.t. the box coordinates. This is a nontrivial problem and a solution can be given by an 'RoI warping' layer as developed in [15], which is beyond the scope of this paper.\n",
      "\n",
      "4-Step Alternating Training . In this paper, we adopt a pragmatic 4-step training algorithm to learn shared features via alternating optimization. In the first step, we train the RPN as described in Section 3.1.3. This network is initialized with an ImageNet-pre-trained model and fine-tuned end-to-end for the region proposal task. In the second step, we train a separate detection network by Fast R-CNN using the proposals generated by the step-1 RPN. This detection network is also initialized by the ImageNet-pre-trained model. At this point the two networks do not share convolutional layers. In the third step, we use the detector network to initialize RPN training, but we fix the shared convolutional layers and only fine-tune the layers unique to RPN. Now the two networks share convolutional layers. Finally, keeping the shared convolutional layers fixed, we fine-tune the unique layers of Fast R-CNN. As such, both networks share the same convolutional layers and form a unified network. A similar alternating training can be run for more iterations, but we have observed negligible improvements.\n",
      "\n",
      "## 3.3 Implementation Details\n",
      "\n",
      "We train and test both region proposal and object detection networks on images of a single scale [1], [2]. We re-scale the images such that their shorter side is s = 600 pixels [2]. Multi-scale feature extraction (using an image pyramid) may improve accuracy but does not exhibit a good speed-accuracy trade-off [2]. On the re-scaled images, the total stride for both ZF and VGG nets on the last convolutional layer is 16 pixels, and thus is ∼ 10 pixels on a typical PASCAL image before resizing ( ∼ 500 × 375). Even such a large stride provides good results, though accuracy may be further improved with a smaller stride.\n",
      "\n",
      "For anchors, we use 3 scales with box areas of 128 2 , 256 2 , and 512 2 pixels, and 3 aspect ratios of 1:1, 1:2, and 2:1. These hyper-parameters are not carefully chosen for a particular dataset, and we provide ablation experiments on their effects in the next section. As discussed, our solution does not need an image pyramid or filter pyramid to predict regions of multiple scales, saving considerable running time. Figure 3 (right) shows the capability of our method for a wide range of scales and aspect ratios. Table 1 shows the learned average proposal size for each anchor using the ZF net. We note that our algorithm allows predictions that are larger than the underlying receptive field. Such predictions are not impossible-one may still roughly infer the extent of an object if only the middle of the object is visible.\n",
      "\n",
      "The anchor boxes that cross image boundaries need to be handled with care. During training, we ignore all cross-boundary anchors so they do not contribute to the loss. For a typical 1000 × 600 image, there will be roughly 20000 ( ≈ 60 × 40 × 9 ) anchors in total. With the cross-boundary anchors ignored, there are about 6000 anchors per image for training. If the boundary-crossing outliers are not ignored in training, they introduce large, difficult to correct error terms in the objective, and training does not converge. During testing, however, we still apply the fully convolutional RPN to the entire image. This may generate crossboundary proposal boxes, which we clip to the image boundary.\n",
      "\n",
      "Table 2: Detection results on PASCAL VOC 2007 test set (trained on VOC 2007 trainval). The detectors are Fast R-CNN with ZF, but using various proposal methods for training and testing.\n",
      "\n",
      "| train-time region proposals       | train-time region proposals       | test-time region proposals        | test-time region proposals        |                                   |\n",
      "|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|\n",
      "| method                            | # boxes                           | method                            | # proposals                       | mAP (%)                           |\n",
      "| SS                                | 2000                              | SS                                | 2000                              | 58.7                              |\n",
      "| EB                                | 2000                              | EB                                | 2000                              | 58.6                              |\n",
      "| RPN+ZF, shared                    | 2000                              | RPN+ZF, shared                    | 300                               | 59.9                              |\n",
      "| ablation experiments follow below | ablation experiments follow below | ablation experiments follow below | ablation experiments follow below | ablation experiments follow below |\n",
      "| RPN+ZF, unshared                  | 2000                              | RPN+ZF, unshared                  | 300                               | 58.7                              |\n",
      "| SS                                | 2000                              | RPN+ZF                            | 100                               | 55.1                              |\n",
      "| SS                                | 2000                              | RPN+ZF                            | 300                               | 56.8                              |\n",
      "| SS                                | 2000                              | RPN+ZF                            | 1000                              | 56.3                              |\n",
      "| SS                                | 2000                              | RPN+ZF (no NMS)                   | 6000                              | 55.2                              |\n",
      "| SS                                | 2000                              | RPN+ZF (no cls )                  | 100                               | 44.6                              |\n",
      "| SS                                | 2000                              | RPN+ZF (no cls )                  | 300                               | 51.4                              |\n",
      "| SS                                | 2000                              | RPN+ZF (no cls )                  | 1000                              | 55.8                              |\n",
      "| SS                                | 2000                              | RPN+ZF (no reg )                  | 300                               | 52.1                              |\n",
      "| SS                                | 2000                              | RPN+ZF (no reg )                  | 1000                              | 51.3                              |\n",
      "| SS                                | 2000                              | RPN+VGG                           | 300                               | 59.2                              |\n",
      "\n",
      "Some RPN proposals highly overlap with each other. To reduce redundancy, we adopt non-maximum suppression (NMS) on the proposal regions based on their cls scores. We fix the IoU threshold for NMS at 0.7, which leaves us about 2000 proposal regions per image. As we will show, NMS does not harm the ultimate detection accuracy, but substantially reduces the number of proposals. After NMS, we use the topN ranked proposal regions for detection. In the following, we train Fast R-CNN using 2000 RPN proposals, but evaluate different numbers of proposals at test-time.\n",
      "\n",
      "## 4 EXPERIMENTS\n",
      "\n",
      "## 4.1 Experiments on PASCAL VOC\n",
      "\n",
      "We comprehensively evaluate our method on the PASCAL VOC 2007 detection benchmark [11]. This dataset consists of about 5k trainval images and 5k test images over 20 object categories. We also provide results on the PASCAL VOC 2012 benchmark for a few models. For the ImageNet pre-trained network, we use the 'fast' version of ZF net [32] that has 5 convolutional layers and 3 fully-connected layers, and the public VGG-16 model 7 [3] that has 13 convolutional layers and 3 fully-connected layers. We primarily evaluate detection mean Average Precision (mAP), because this is the actual metric for object detection (rather than focusing on object proposal proxy metrics).\n",
      "\n",
      "Table 2 (top) shows Fast R-CNN results when trained and tested using various region proposal methods. These results use the ZF net. For Selective Search (SS) [4], we generate about 2000 proposals by the 'fast' mode. For EdgeBoxes (EB) [6], we generate the proposals by the default EB setting tuned for 0.7\n",
      "\n",
      "IoU. SS has an mAP of 58.7% and EB has an mAP of 58.6% under the Fast R-CNN framework. RPN with Fast R-CNN achieves competitive results, with an mAP of 59.9% while using up to 300 proposals 8 . Using RPN yields a much faster detection system than using either SS or EB because of shared convolutional computations; the fewer proposals also reduce the region-wise fully-connected layers' cost (Table 5).\n",
      "\n",
      "Ablation Experiments on RPN. To investigate the behavior of RPNs as a proposal method, we conducted several ablation studies. First, we show the effect of sharing convolutional layers between the RPN and Fast R-CNN detection network. To do this, we stop after the second step in the 4-step training process. Using separate networks reduces the result slightly to 58.7% (RPN+ZF, unshared, Table 2). We observe that this is because in the third step when the detectortuned features are used to fine-tune the RPN, the proposal quality is improved.\n",
      "\n",
      "Next, we disentangle the RPN's influence on training the Fast R-CNN detection network. For this purpose, we train a Fast R-CNN model by using the 2000 SS proposals and ZF net. We fix this detector and evaluate the detection mAP by changing the proposal regions used at test-time. In these ablation experiments, the RPN does not share features with the detector.\n",
      "\n",
      "Replacing SS with 300 RPN proposals at test-time leads to an mAP of 56.8%. The loss in mAP is because of the inconsistency between the training/testing proposals. This result serves as the baseline for the following comparisons.\n",
      "\n",
      "Somewhat surprisingly, the RPN still leads to a competitive result (55.1%) when using the top-ranked\n",
      "\n",
      "8. For RPN, the number of proposals ( e.g ., 300) is the maximum number for an image. RPN may produce fewer proposals after NMS, and thus the average number of proposals is smaller.\n",
      "\n",
      "Table 3: Detection results on PASCAL VOC 2007 test set . The detector is Fast R-CNN and VGG-16. Training data: '07': VOC 2007 trainval, '07+12': union set of VOC 2007 trainval and VOC 2012 trainval. For RPN, the train-time proposals for Fast R-CNN are 2000. † : this number was reported in [2]; using the repository provided by this paper, this result is higher (68.1).\n",
      "\n",
      "| method            |   # proposals | data       | mAP (%)   |\n",
      "|-------------------|---------------|------------|-----------|\n",
      "| SS                |          2000 | 07         | 66.9 †    |\n",
      "| SS                |          2000 | 07+12      | 70.0      |\n",
      "| RPN+VGG, unshared |           300 | 07         | 68.5      |\n",
      "| RPN+VGG, shared   |           300 | 07         | 69.9      |\n",
      "| RPN+VGG, shared   |           300 | 07+12      | 73.2      |\n",
      "| RPN+VGG, shared   |           300 | COCO+07+12 | 78.8      |\n",
      "\n",
      "Table 4: Detection results on PASCAL VOC 2012 test set . The detector is Fast R-CNN and VGG-16. Training data: '07': VOC 2007 trainval, '07++12': union set of VOC 2007 trainval+test and VOC 2012 trainval. For RPN, the train-time proposals for Fast R-CNN are 2000. † : http://host.robots.ox.ac.uk:8080/anonymous/HZJTQA.html. ‡ : http://host.robots.ox.ac.uk:8080/anonymous/YNPLXB.html. § : http://host.robots.ox.ac.uk:8080/anonymous/XEDH10.html.\n",
      "\n",
      "| method            |   # proposals | data        |   mAP (%) |\n",
      "|-------------------|---------------|-------------|-----------|\n",
      "| SS                |          2000 | 12          |      65.7 |\n",
      "| SS                |          2000 | 07++12      |      68.4 |\n",
      "| RPN+VGG, shared † |           300 | 12          |      67   |\n",
      "| RPN+VGG, shared ‡ |           300 | 07++12      |      70.4 |\n",
      "| RPN+VGG, shared § |           300 | COCO+07++12 |      75.9 |\n",
      "\n",
      "Table 5: Timing (ms) on a K40 GPU, except SS proposal is evaluated in a CPU. 'Region-wise' includes NMS, pooling, fully-connected, and softmax layers. See our released code for the profiling of running time.\n",
      "\n",
      "| model   | system           |   conv |   proposal |   region-wise |   total | rate    |\n",
      "|---------|------------------|--------|------------|---------------|---------|---------|\n",
      "| VGG     | SS + Fast R-CNN  |    146 |       1510 |           174 |    1830 | 0.5 fps |\n",
      "| VGG     | RPN + Fast R-CNN |    141 |         10 |            47 |     198 | 5 fps   |\n",
      "| ZF      | RPN + Fast R-CNN |     31 |          3 |            25 |      59 | 17 fps  |\n",
      "\n",
      "100 proposals at test-time, indicating that the topranked RPN proposals are accurate. On the other extreme, using the top-ranked 6000 RPN proposals (without NMS) has a comparable mAP (55.2%), suggesting NMS does not harm the detection mAP and may reduce false alarms.\n",
      "\n",
      "Next, we separately investigate the roles of RPN's cls and reg outputs by turning off either of them at test-time. When the cls layer is removed at testtime (thus no NMS/ranking is used), we randomly sample N proposals from the unscored regions. The mAP is nearly unchanged with N = 1000 (55.8%), but degrades considerably to 44.6% when N = 100 . This shows that the cls scores account for the accuracy of the highest ranked proposals.\n",
      "\n",
      "On the other hand, when the reg layer is removed at test-time (so the proposals become anchor boxes), the mAP drops to 52.1%. This suggests that the highquality proposals are mainly due to the regressed box bounds. The anchor boxes, though having multiple scales and aspect ratios, are not sufficient for accurate detection.\n",
      "\n",
      "We also evaluate the effects of more powerful networks on the proposal quality of RPN alone. We use VGG-16 to train the RPN, and still use the above detector of SS+ZF. The mAP improves from 56.8%\n",
      "\n",
      "(using RPN+ZF) to 59.2% (using RPN+VGG). This is a promising result, because it suggests that the proposal quality of RPN+VGG is better than that of RPN+ZF. Because proposals of RPN+ZF are competitive with SS (both are 58.7% when consistently used for training and testing), we may expect RPN+VGG to be better than SS. The following experiments justify this hypothesis.\n",
      "\n",
      "Performance of VGG-16. Table 3 shows the results of VGG-16 for both proposal and detection. Using RPN+VGG, the result is 68.5% for unshared features, slightly higher than the SS baseline. As shown above, this is because the proposals generated by RPN+VGG are more accurate than SS. Unlike SS that is predefined, the RPN is actively trained and benefits from better networks. For the featureshared variant, the result is 69.9%-better than the strong SS baseline, yet with nearly cost-free proposals. We further train the RPN and detection network on the union set of PASCAL VOC 2007 trainval and 2012 trainval. The mAP is 73.2% . Figure 5 shows some results on the PASCAL VOC 2007 test set. On the PASCAL VOC 2012 test set (Table 4), our method has an mAP of 70.4% trained on the union set of VOC 2007 trainval+test and VOC 2012 trainval. Table 6 and Table 7 show the detailed numbers.\n",
      "\n",
      "Table 6: Results on PASCAL VOC 2007 test set with Fast R-CNN detectors and VGG-16. For RPN, the train-time proposals for Fast R-CNN are 2000. RPN ∗ denotes the unsharing feature version.\n",
      "\n",
      "| method   |   # box | data       |   mAP |   areo |   bike |   bird |   boat |   bottle |   bus |   car |   cat |   chair |   cow |   table |   dog |   horse | mbike     |   person |   plant |   sheep |   sofa |   train tv |\n",
      "|----------|---------|------------|-------|--------|--------|--------|--------|----------|-------|-------|-------|---------|-------|---------|-------|---------|-----------|----------|---------|---------|--------|------------|\n",
      "| SS       |    2000 | 07         |  66.9 |   74.5 |   78.3 |   69.2 |   53.2 |     36.6 |  77.3 |  78.2 |  82   |    40.7 |  72.7 |    67.9 |  79.6 |    79.2 | 73.0 69.0 |     30.1 |    65.4 |    70.2 |   75.8 |       65.8 |\n",
      "| SS       |    2000 | 07+12      |  70   |   77   |   78.1 |   69.3 |   59.4 |     38.3 |  81.6 |  78.6 |  86.7 |    42.8 |  78.8 |    68.9 |  84.7 |    82   | 76.6 69.9 |     31.8 |    70.1 |    74.8 |   80.4 |       70.4 |\n",
      "| RPN ∗    |     300 | 07         |  68.5 |   74.1 |   77.2 |   67.7 |   53.9 |     51   |  75.1 |  79.2 |  78.9 |    50.7 |  78   |    61.1 |  79.1 |    81.9 | 72.2 75.9 |     37.2 |    71.4 |    62.5 |   77.4 |       66.4 |\n",
      "| RPN      |     300 | 07         |  69.9 |   70   |   80.6 |   70.1 |   57.3 |     49.9 |  78.2 |  80.4 |  82   |    52.2 |  75.3 |    67.2 |  80.3 |    79.8 | 75.0 76.3 |     39.1 |    68.3 |    67.3 |   81.1 |       67.6 |\n",
      "| RPN      |     300 | 07+12      |  73.2 |   76.5 |   79   |   70.9 |   65.5 |     52.1 |  83.1 |  84.7 |  86.4 |    52   |  81.9 |    65.7 |  84.8 |    84.6 | 77.5 76.7 |     38.8 |    73.6 |    73.9 |   83   |       72.6 |\n",
      "| RPN      |     300 | COCO+07+12 |  78.8 |   84.3 |   82   |   77.7 |   68.9 |     65.7 |  88.1 |  88.4 |  88.9 |    63.6 |  86.3 |    70.8 |  85.9 |    87.6 | 80.1 82.3 |     53.6 |    80.4 |    75.8 |   86.6 |       78.9 |\n",
      "\n",
      "Table 7: Results on PASCAL VOC 2012 test set with Fast R-CNN detectors and VGG-16. For RPN, the train-time proposals for Fast R-CNN are 2000.\n",
      "\n",
      "| method   |   # box | data        |   mAP |   areo |   bike |   bird |   boat |   bottle |   bus |   car |   cat |   chair |   cow |   table |   dog |   horse | mbike     | person    |   plant |   sheep | sofa train tv   |\n",
      "|----------|---------|-------------|-------|--------|--------|--------|--------|----------|-------|-------|-------|---------|-------|---------|-------|---------|-----------|-----------|---------|---------|-----------------|\n",
      "| SS       |    2000 | 12          |  65.7 |   80.3 |   74.7 |   66.9 |   46.9 |     37.7 |  73.9 |  68.6 |  87.7 |    41.7 |  71.1 |    51.1 |  86   |    77.8 | 79.8      | 69.8 32.1 |    65.5 |    63.8 | 76.4 61.7       |\n",
      "| SS       |    2000 | 07++12      |  68.4 |   82.3 |   78.4 |   70.8 |   52.3 |     38.7 |  77.8 |  71.6 |  89.3 |    44.2 |  73   |    55   |  87.5 |    80.5 | 80.8      | 72.0 35.1 |    68.3 |    65.7 | 80.4 64.2       |\n",
      "| RPN      |     300 | 12          |  67   |   82.3 |   76.4 |   71   |   48.4 |     45.2 |  72.1 |  72.3 |  87.3 |    42.2 |  73.7 |    50   |  86.8 |    78.7 | 78.4 77.4 | 34.5      |    70.1 |    57.1 | 77.1 58.9       |\n",
      "| RPN      |     300 | 07++12      |  70.4 |   84.9 |   79.8 |   74.3 |   53.9 |     49.8 |  77.5 |  75.9 |  88.5 |    45.6 |  77.1 |    55.3 |  86.9 |    81.7 | 80.9 79.6 | 40.1      |    72.6 |    60.9 | 81.2 61.5       |\n",
      "| RPN      |     300 | COCO+07++12 |  75.9 |   87.4 |   83.6 |   76.8 |   62.9 |     59.6 |  81.9 |  82   |  91.3 |    54.9 |  82.6 |    59   |  89   |    85.5 | 84.7 84.1 | 52.2      |    78.9 |    65.5 | 85.4 70.2       |\n",
      "\n",
      "Table 8: Detection results of Faster R-CNN on PASCAL VOC 2007 test set using different settings of anchors . The network is VGG-16. The training data is VOC 2007 trainval. The default setting of using 3 scales and 3 aspect ratios (69.9%) is the same as that in Table 3.\n",
      "\n",
      "| settings           | anchor scales             | aspect ratios                       | mAP (%)   |\n",
      "|--------------------|---------------------------|-------------------------------------|-----------|\n",
      "| 1 scale, 1 ratio   | 128 2 256 2               | 1:1 1:1                             | 65.8 66.7 |\n",
      "| 1 scale, 3 ratios  | 128 2 256 2               | { 2:1, 1:1, 1:2 } { 2:1, 1:1, 1:2 } | 68.8 67.9 |\n",
      "| 3 scales, 1 ratio  | { 128 2 , 256 2 , 512 2 } | 1:1                                 | 69.8      |\n",
      "| 3 scales, 3 ratios | { 128 2 , 256 2 , 512 2 } | { 2:1, 1:1, 1:2 }                   | 69.9      |\n",
      "\n",
      "Table 9: Detection results of Faster R-CNN on PASCAL VOC 2007 test set using different values of λ in Equation (1). The network is VGG-16. The training data is VOC 2007 trainval. The default setting of using λ = 10 (69.9%) is the same as that in Table 3.\n",
      "\n",
      "| λ       |   0.1 |    1 |   10 |   100 |\n",
      "|---------|-------|------|------|-------|\n",
      "| mAP (%) |  67.2 | 68.9 | 69.9 |  69.1 |\n",
      "\n",
      "In Table 5 we summarize the running time of the entire object detection system. SS takes 1-2 seconds depending on content (on average about 1.5s), and Fast R-CNN with VGG-16 takes 320ms on 2000 SS proposals (or 223ms if using SVD on fully-connected layers [2]). Our system with VGG-16 takes in total 198ms for both proposal and detection. With the convolutional features shared, the RPN alone only takes 10ms computing the additional layers. Our regionwise computation is also lower, thanks to fewer proposals (300 per image). Our system has a frame-rate of 17 fps with the ZF net.\n",
      "\n",
      "Sensitivities to Hyper-parameters. In Table 8 we investigate the settings of anchors. By default we use\n",
      "\n",
      "3 scales and 3 aspect ratios (69.9% mAP in Table 8). If using just one anchor at each position, the mAP drops by a considerable margin of 3-4%. The mAP is higher if using 3 scales (with 1 aspect ratio) or 3 aspect ratios (with 1 scale), demonstrating that using anchors of multiple sizes as the regression references is an effective solution. Using just 3 scales with 1 aspect ratio (69.8%) is as good as using 3 scales with 3 aspect ratios on this dataset, suggesting that scales and aspect ratios are not disentangled dimensions for the detection accuracy. But we still adopt these two dimensions in our designs to keep our system flexible.\n",
      "\n",
      "In Table 9 we compare different values of λ in Equation (1). By default we use λ = 10 which makes the two terms in Equation (1) roughly equally weighted after normalization. Table 9 shows that our result is impacted just marginally (by ∼ 1% ) when λ is within a scale of about two orders of magnitude (1 to 100). This demonstrates that the result is insensitive to λ in a wide range.\n",
      "\n",
      "Analysis of Recall-to-IoU. Next we compute the recall of proposals at different IoU ratios with groundtruth boxes. It is noteworthy that the Recall-to-IoU metric is just loosely [19], [20], [21] related to the ultimate detection accuracy. It is more appropriate to use this metric to diagnose the proposal method than to evaluate it.\n",
      "\n",
      "In Figure 4, we show the results of using 300, 1000, and 2000 proposals. We compare with SS and EB, and the N proposals are the topN ranked ones based on the confidence generated by these methods. The plots show that the RPN method behaves gracefully when the number of proposals drops from 2000 to 300. This explains why the RPN has a good ultimate detection mAP when using as few as 300 proposals. As we analyzed before, this property is mainly attributed to the cls term of the RPN. The recall of SS and EB drops more quickly than RPN when the proposals are fewer.\n",
      "\n",
      "Figure 4: Recall vs . IoU overlap ratio on the PASCAL VOC 2007 test set.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Table 10: One-Stage Detection vs . Two-Stage Proposal + Detection . Detection results are on the PASCAL VOC 2007 test set using the ZF model and Fast R-CNN. RPN uses unshared features.\n",
      "\n",
      "|           | proposals                        |   proposals | detector                  |   mAP (%) |\n",
      "|-----------|----------------------------------|-------------|---------------------------|-----------|\n",
      "| Two-Stage | RPN + ZF, unshared               |         300 | Fast R-CNN + ZF, 1 scale  |      58.7 |\n",
      "| One-Stage | dense, 3 scales, 3 aspect ratios |       20000 | Fast R-CNN + ZF, 1 scale  |      53.8 |\n",
      "| One-Stage | dense, 3 scales, 3 aspect ratios |       20000 | Fast R-CNN + ZF, 5 scales |      53.9 |\n",
      "\n",
      "One-Stage Detection vs . Two-Stage Proposal + Detection. The OverFeat paper [9] proposes a detection method that uses regressors and classifiers on sliding windows over convolutional feature maps. OverFeat is a one-stage , class-specific detection pipeline, and ours is a two-stage cascade consisting of class-agnostic proposals and class-specific detections. In OverFeat, the region-wise features come from a sliding window of one aspect ratio over a scale pyramid. These features are used to simultaneously determine the location and category of objects. In RPN, the features are from square (3 × 3) sliding windows and predict proposals relative to anchors with different scales and aspect ratios. Though both methods use sliding windows, the region proposal task is only the first stage of Faster RCNN-the downstream Fast R-CNN detector attends to the proposals to refine them. In the second stage of our cascade, the region-wise features are adaptively pooled [1], [2] from proposal boxes that more faithfully cover the features of the regions. We believe these features lead to more accurate detections.\n",
      "\n",
      "To compare the one-stage and two-stage systems, we emulate the OverFeat system (and thus also circumvent other differences of implementation details) by one-stage Fast R-CNN. In this system, the 'proposals' are dense sliding windows of 3 scales (128, 256, 512) and 3 aspect ratios (1:1, 1:2, 2:1). Fast R-CNN is trained to predict class-specific scores and regress box locations from these sliding windows. Because the OverFeat system adopts an image pyramid, we also evaluate using convolutional features extracted from 5 scales. We use those 5 scales as in [1], [2].\n",
      "\n",
      "Table 10 compares the two-stage system and two variants of the one-stage system. Using the ZF model, the one-stage system has an mAP of 53.9%. This is lower than the two-stage system (58.7%) by 4.8%. This experiment justifies the effectiveness of cascaded region proposals and object detection. Similar observations are reported in [2], [39], where replacing SS\n",
      "\n",
      "region proposals with sliding windows leads to ∼ 6% degradation in both papers. We also note that the onestage system is slower as it has considerably more proposals to process.\n",
      "\n",
      "## 4.2 Experiments on MS COCO\n",
      "\n",
      "We present more results on the Microsoft COCO object detection dataset [12]. This dataset involves 80 object categories. We experiment with the 80k images on the training set, 40k images on the validation set, and 20k images on the test-dev set. We evaluate the mAP averaged for IoU ∈ [0 . 5 : 0 . 05 : 0 . 95] (COCO's standard metric, simply denoted as mAP@[.5, .95]) and mAP@0.5 (PASCAL VOC's metric).\n",
      "\n",
      "There are a few minor changes of our system made for this dataset. We train our models on an 8-GPU implementation, and the effective mini-batch size becomes 8 for RPN (1 per GPU) and 16 for Fast R-CNN (2 per GPU). The RPN step and Fast R-CNN step are both trained for 240k iterations with a learning rate of 0.003 and then for 80k iterations with 0.0003. We modify the learning rates (starting with 0.003 instead of 0.001) because the mini-batch size is changed. For the anchors, we use 3 aspect ratios and 4 scales (adding 64 2 ), mainly motivated by handling small objects on this dataset. In addition, in our Fast R-CNN step, the negative samples are defined as those with a maximum IoU with ground truth in the interval of [0 , 0 . 5) , instead of [0 . 1 , 0 . 5) used in [1], [2]. We note that in the SPPnet system [1], the negative samples in [0 . 1 , 0 . 5) are used for network fine-tuning, but the negative samples in [0 , 0 . 5) are still visited in the SVM step with hard-negative mining. But the Fast R-CNN system [2] abandons the SVM step, so the negative samples in [0 , 0 . 1) are never visited. Including these [0 , 0 . 1) samples improves mAP@0.5 on the COCO dataset for both Fast R-CNN and Faster R-CNN systems (but the impact is negligible on PASCAL VOC).\n",
      "\n",
      "Table 11: Object detection results (%) on the MS COCO dataset. The model is VGG-16.\n",
      "\n",
      "|                                  |           |               | COCO val   | COCO val      | COCO test-dev   | COCO test-dev   |\n",
      "|----------------------------------|-----------|---------------|------------|---------------|-----------------|-----------------|\n",
      "| method                           | proposals | training data | mAP@.5     | mAP@[.5, .95] | mAP@.5          | mAP@[.5, .95]   |\n",
      "| Fast R-CNN [2]                   | SS, 2000  | COCO train    | -          | -             | 35.9            | 19.7            |\n",
      "| Fast R-CNN [impl. in this paper] | SS, 2000  | COCO train    | 38.6       | 18.9          | 39.3            | 19.3            |\n",
      "| Faster R-CNN                     | RPN, 300  | COCO train    | 41.5       | 21.2          | 42.1            | 21.5            |\n",
      "| Faster R-CNN                     | RPN, 300  | COCO trainval | -          | -             | 42.7            | 21.9            |\n",
      "\n",
      "The rest of the implementation details are the same as on PASCAL VOC. In particular, we keep using 300 proposals and single-scale ( s = 600 ) testing. The testing time is still about 200ms per image on the COCO dataset.\n",
      "\n",
      "In Table 11 we first report the results of the Fast R-CNN system [2] using the implementation in this paper. Our Fast R-CNN baseline has 39.3% mAP@0.5 on the test-dev set, higher than that reported in [2]. We conjecture that the reason for this gap is mainly due to the definition of the negative samples and also the changes of the mini-batch sizes. We also note that the mAP@[.5, .95] is just comparable.\n",
      "\n",
      "Next we evaluate our Faster R-CNN system. Using the COCO training set to train, Faster R-CNN has 42.1% mAP@0.5 and 21.5% mAP@[.5, .95] on the COCO test-dev set. This is 2.8% higher for mAP@0.5 and 2.2% higher for mAP@[.5, .95] than the Fast RCNN counterpart under the same protocol (Table 11). This indicates that RPN performs excellent for improving the localization accuracy at higher IoU thresholds. Using the COCO trainval set to train, Faster RCNNhas 42.7% mAP@0.5 and 21.9% mAP@[.5, .95] on the COCO test-dev set. Figure 6 shows some results on the MS COCO test-dev set.\n",
      "\n",
      "Faster R-CNN in ILSVRC &amp; COCO 2015 competitions We have demonstrated that Faster R-CNN benefits more from better features, thanks to the fact that the RPN completely learns to propose regions by neural networks. This observation is still valid even when one increases the depth substantially to over 100 layers [18]. Only by replacing VGG-16 with a 101layer residual net (ResNet-101) [18], the Faster R-CNN system increases the mAP from 41.5%/21.2% (VGG16) to 48.4%/27.2% (ResNet-101) on the COCO val set. With other improvements orthogonal to Faster RCNN, He et al . [18] obtained a single-model result of 55.7%/34.9% and an ensemble result of 59.0%/37.4% on the COCO test-dev set, which won the 1st place in the COCO 2015 object detection competition. The same system [18] also won the 1st place in the ILSVRC 2015 object detection competition, surpassing the second place by absolute 8.5%. RPN is also a building block of the 1st-place winning entries in ILSVRC 2015 localization and COCO 2015 segmentation competitions, for which the details are available in [18] and [15] respectively.\n",
      "\n",
      "Table 12: Detection mAP (%) of Faster R-CNN on PASCAL VOC 2007 test set and 2012 test set using different training data. The model is VGG-16. 'COCO' denotes that the COCO trainval set is used for training. See also Table 6 and Table 7.\n",
      "\n",
      "| training data   | 2007 test   | 2012 test   |\n",
      "|-----------------|-------------|-------------|\n",
      "| VOC07           | 69.9        | 67.0        |\n",
      "| VOC07+12        | 73.2        | -           |\n",
      "| VOC07++12       | -           | 70.4        |\n",
      "| COCO (no VOC)   | 76.1        | 73.0        |\n",
      "| COCO+VOC07+12   | 78.8        | -           |\n",
      "| COCO+VOC07++12  | -           | 75.9        |\n",
      "\n",
      "## 4.3 From MS COCO to PASCAL VOC\n",
      "\n",
      "Large-scale data is of crucial importance for improving deep neural networks. Next, we investigate how the MS COCO dataset can help with the detection performance on PASCAL VOC.\n",
      "\n",
      "As a simple baseline, we directly evaluate the COCO detection model on the PASCAL VOC dataset, without fine-tuning on any PASCAL VOC data . This evaluation is possible because the categories on COCO are a superset of those on PASCAL VOC. The categories that are exclusive on COCO are ignored in this experiment, and the softmax layer is performed only on the 20 categories plus background. The mAP under this setting is 76.1% on the PASCAL VOC 2007 test set (Table 12). This result is better than that trained on VOC07+12 (73.2%) by a good margin, even though the PASCAL VOC data are not exploited.\n",
      "\n",
      "Then we fine-tune the COCO detection model on the VOC dataset. In this experiment, the COCO model is in place of the ImageNet-pre-trained model (that is used to initialize the network weights), and the Faster R-CNN system is fine-tuned as described in Section 3.2. Doing so leads to 78.8% mAP on the PASCAL VOC 2007 test set. The extra data from the COCO set increases the mAP by 5.6%. Table 6 shows that the model trained on COCO+VOC has the best AP for every individual category on PASCAL VOC 2007. Similar improvements are observed on the PASCAL VOC 2012 test set (Table 12 and Table 7). We note that the test-time speed of obtaining these strong results is still about 200ms per image .\n",
      "\n",
      "## 5 CONCLUSION\n",
      "\n",
      "We have presented RPNs for efficient and accurate region proposal generation. By sharing convolutional\n",
      "\n",
      "person : 0.918\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "cow : 0.995\n",
      "\n",
      "Figure 5: Selected examples of object detection results on the PASCAL VOC 2007 test set using the Faster R-CNN system. The model is VGG-16 and the training data is 07+12 trainval (73.2% mAP on the 2007 test set). Our method detects objects of a wide range of scales and aspect ratios. Each output box is associated with a category label and a softmax score in [0 , 1] . A score threshold of 0.6 is used to display these images. The running time for obtaining these results is 198ms per image, including all steps .\n",
      "\n",
      "features with the down-stream detection network, the region proposal step is nearly cost-free. Our method enables a unified, deep-learning-based object detection system to run at near real-time frame rates. The learned RPN also improves region proposal quality and thus the overall object detection accuracy.\n",
      "\n",
      "## REFERENCES\n",
      "\n",
      "- [1] K. He, X. Zhang, S. Ren, and J. Sun, 'Spatial pyramid pooling in deep convolutional networks for visual recognition,' in European Conference on Computer Vision (ECCV) , 2014.\n",
      "- [2] R. Girshick, 'Fast R-CNN,' in IEEE International Conference on Computer Vision (ICCV) , 2015.\n",
      "- [3] K. Simonyan and A. Zisserman, 'Very deep convolutional\n",
      "\n",
      "Figure 6: Selected examples of object detection results on the MS COCO test-dev set using the Faster R-CNN system. The model is VGG-16 and the training data is COCO trainval (42.7% mAP@0.5 on the test-dev set). Each output box is associated with a category label and a softmax score in [0 , 1] . A score threshold of 0.6 is used to display these images. For each image, one color represents one object category in that image.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "networks for large-scale image recognition,' in International Conference on Learning Representations (ICLR) , 2015.\n",
      "\n",
      "- [4] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders, 'Selective search for object recognition,' International Journal of Computer Vision (IJCV) , 2013.\n",
      "- [5] R. Girshick, J. Donahue, T. Darrell, and J. Malik, 'Rich feature hierarchies for accurate object detection and semantic segmentation,' in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2014.\n",
      "- [6] C. L. Zitnick and P. Doll´ ar, 'Edge boxes: Locating object proposals from edges,' in European Conference on Computer Vision (ECCV) , 2014.\n",
      "- [7] J. Long, E. Shelhamer, and T. Darrell, 'Fully convolutional networks for semantic segmentation,' in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015.\n",
      "- [8] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan, 'Object detection with discriminatively trained partbased models,' IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) , 2010.\n",
      "- [9] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun, 'Overfeat: Integrated recognition, localization and detection using convolutional networks,' in International Conference on Learning Representations (ICLR) , 2014.\n",
      "- [10] S. Ren, K. He, R. Girshick, and J. Sun, 'Faster R-CNN: Towards\n",
      "\n",
      "real-time object detection with region proposal networks,' in Neural Information Processing Systems (NIPS) , 2015.\n",
      "\n",
      "- [11] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, 'The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results,' 2007.\n",
      "- [12] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll´ ar, and C. L. Zitnick, 'Microsoft COCO: Common Objects in Context,' in European Conference on Computer Vision (ECCV) , 2014.\n",
      "- [13] S. Song and J. Xiao, 'Deep sliding shapes for amodal 3d object detection in rgb-d images,' arXiv:1511.02300 , 2015.\n",
      "- [14] J. Zhu, X. Chen, and A. L. Yuille, 'DeePM: A deep part-based model for object detection and semantic part localization,' arXiv:1511.07131 , 2015.\n",
      "- [15] J. Dai, K. He, and J. Sun, 'Instance-aware semantic segmentation via multi-task network cascades,' arXiv:1512.04412 , 2015.\n",
      "- [16] J. Johnson, A. Karpathy, and L. Fei-Fei, 'Densecap: Fully convolutional localization networks for dense captioning,' arXiv:1511.07571 , 2015.\n",
      "- [17] D. Kislyuk, Y. Liu, D. Liu, E. Tzeng, and Y. Jing, 'Human curation and convnets: Powering item-to-item recommendations on pinterest,' arXiv:1511.04003 , 2015.\n",
      "- [18] K. He, X. Zhang, S. Ren, and J. Sun, 'Deep residual learning for image recognition,' arXiv:1512.03385 , 2015.\n",
      "- [19] J. Hosang, R. Benenson, and B. Schiele, 'How good are detection proposals, really?' in British Machine Vision Conference (BMVC) , 2014.\n",
      "- [20] J. Hosang, R. Benenson, P. Doll´ ar, and B. Schiele, 'What makes for effective detection proposals?' IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) , 2015.\n",
      "- [21] N. Chavali, H. Agrawal, A. Mahendru, and D. Batra, 'Object-Proposal Evaluation Protocol is 'Gameable',' arXiv: 1505.05836 , 2015.\n",
      "- [22] J. Carreira and C. Sminchisescu, 'CPMC: Automatic object segmentation using constrained parametric min-cuts,' IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) , 2012.\n",
      "- [23] P. Arbel´ aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik, 'Multiscale combinatorial grouping,' in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2014.\n",
      "- [24] B. Alexe, T. Deselaers, and V. Ferrari, 'Measuring the objectness of image windows,' IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) , 2012.\n",
      "- [25] C. Szegedy, A. Toshev, and D. Erhan, 'Deep neural networks for object detection,' in Neural Information Processing Systems (NIPS) , 2013.\n",
      "- [26] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, 'Scalable object detection using deep neural networks,' in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2014.\n",
      "- [27] C. Szegedy, S. Reed, D. Erhan, and D. Anguelov, 'Scalable, high-quality object detection,' arXiv:1412.1441 (v1) , 2015.\n",
      "- [28] P. O. Pinheiro, R. Collobert, and P. Dollar, 'Learning to segment object candidates,' in Neural Information Processing Systems (NIPS) , 2015.\n",
      "- [29] J. Dai, K. He, and J. Sun, 'Convolutional feature masking for joint object and stuff segmentation,' in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015.\n",
      "- [30] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, 'Object detection networks on convolutional feature maps,' arXiv:1504.06066 , 2015.\n",
      "- [31] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, 'Attention-based models for speech recognition,' in Neural Information Processing Systems (NIPS) , 2015.\n",
      "- [32] M. D. Zeiler and R. Fergus, 'Visualizing and understanding convolutional neural networks,' in European Conference on Computer Vision (ECCV) , 2014.\n",
      "- [33] V. Nair and G. E. Hinton, 'Rectified linear units improve restricted boltzmann machines,' in International Conference on Machine Learning (ICML) , 2010.\n",
      "- [34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, and A. Rabinovich, 'Going deeper with convolutions,' in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015.\n",
      "- [35] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel, 'Backpropagation applied to handwritten zip code recognition,' Neural computation , 1989.\n",
      "- [36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, 'ImageNet Large Scale Visual Recognition Challenge,' in International Journal of Computer Vision (IJCV) , 2015.\n",
      "- [37] A. Krizhevsky, I. Sutskever, and G. Hinton, 'Imagenet classification with deep convolutional neural networks,' in Neural Information Processing Systems (NIPS) , 2012.\n",
      "- [38] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell, 'Caffe: Convolutional architecture for fast feature embedding,' arXiv:1408.5093 , 2014.\n",
      "- [39] K. Lenc and A. Vedaldi, 'R-CNN minus R,' in British Machine Vision Conference (BMVC) , 2015.\n",
      "Document 10:\n",
      "c © Rinton Press\n",
      "\n",
      "## CONSTRUCTIONS OF q -ARY ENTANGLEMENT-ASSISTED QUANTUM MDS CODES WITH MINIMUM DISTANCE GREATER THAN q +1\n",
      "\n",
      "## JIHAO FAN\n",
      "\n",
      "Department of Computer Science and Engineering, Southeast University Nanjing, Jiangsu 211189, China fanjh12@seu.edu.cn\n",
      "\n",
      "HANWU CHEN\n",
      "\n",
      "Department of Computer Science and Engineering, Southeast University Nanjing, Jiangsu 211189, China hw\\_chen@seu.edu.cn\n",
      "\n",
      "JUAN XU\n",
      "\n",
      "College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics Nanjing, Jiangsu 210016, China juanxu@nuaa.edu.cn\n",
      "\n",
      "Received (received date) Revised (revised date)\n",
      "\n",
      "The entanglement-assisted stabilizer formalism provides a useful framework for constructing quantum error-correcting codes (QECC), which can transform arbitrary classical linear codes into entanglement-assisted quantum error correcting codes (EAQECCs) by using pre-shared entanglement between the sender and the receiver. In this paper, we construct five classes of entanglement-assisted quantum MDS (EAQMDS) codes based on classical MDS codes by exploiting one or more pre-shared maximally entangled states. We show that these EAQMDS codes have much larger minimum distance than the standard quantum MDS (QMDS) codes of the same length, and three classes of these EAQMDS codes consume only one pair of maximally entangled states.\n",
      "\n",
      "Keywords : Entanglement-assisted quantum error-correcting codes, Quantum errorcorrecting codes, Maximal-distance-separable (MDS) codes, Maximally entangled state Communicated by : to be filled by the Editorial\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Quantum error-correcting codes (QECC) play a key role in protecting quantum information from decoherence and quantum noise. The theory of quantum stabilizer codes allows one to import classical additive codes that satisfy certain dual-containing relationship for use as a QECC [1, 2, 3]. Recently, a more general framework called entanglement-assisted stabilizer formalism was developed to construct QECCs with the help of pre-shared entanglement between the sender and the receiver [4]. This framework has the advantage that it allows to construct QECCs from arbitrary classical linear codes, without the dual-containing constraint. Currently, many works have focused on the construction of binary EAQECCs based on classical binary or quaternary linear codes, see [5, 6, 7, 8, 9, 10], since binary QECCs might be the\n",
      "\n",
      "most useful ones in the future quantum computers and quantum communications. However, nonbinary cases have received less attention. Nonbinary EAQECCs would be useful in some quantum communication protocols [11, 12]. Just as in the classical error-correcting codes (ECC) and standard QECCs, EAQECCs over higher alphabets can be used for constructing easily decodable binary EAQECCs by using concatenation technology [13, 14]. Furthermore, nonbinary QECCs and EAQECCs, especially nonbinary quantum MDS (QMDS) codes and entanglement-assisted quantum MDS (EAQMDS) codes, are of significantly theoretical interest, since QMDS codes and EAQMDS codes can achieve the quantum Singleton bound [3] and the entanglement-assisted quantum Singleton bound [4], respectively.\n",
      "\n",
      "Let q be a prime power. We use Q = [[ n, k, d ]] q to denote a standard q -ary QECC of length n with size q k and minimum distance d . Then Q is a q k -dimensional subspace of the q n -dimensional Hilbert space ( C q ) ⊗ n , which can detect up to d -1 and correct up to /floorleft ( d -1) / 2 /floorright quantum errors. The parameters of Q have to satisfy the quantum Singleton bound: k ≤ n -2 d + 2 in [3]. If Q attains the quantum Singleton bound, then it is called a quantum maximum-distance-separable (MDS) code. According to the MDS conjecture in [3], the maximal length of a QMDS code cannot exceed q 2 + 1, i.e., n ≤ q 2 + 1, except for the trivial and some special cases in [15], and except for the existence of QMDS codes with parameters [[ q 2 +2 , q 2 -4 , 4]] q for q = 2 m shown in [16]. As mentioned in [17], QMDS codes of length up to q +1 have been constructed for all possible dimensions, see [18, 19]. However, the problem of constructing QMDS codes with length n greater than q +1 is much more difficult. Many QMDS codes with certain lengths between q + 1 and q 2 + 1 have been obtained, see [20, 21, 22, 23, 24, 25, 26, 27]. Up to now, the minimum distance of all known nontrivial q -ary QMDS codes is less than or equal to q +1, except for a few sporadic QMDS codes with large minimum distance in [16]. It seems very difficult to improve this limit by using the standard Euclidean or Hermitian construction.\n",
      "\n",
      "Inspired by these works, in this paper, we propose several constructions of EAQMDS codes based on classical MDS codes, and we get new q -ary EAQMDS codes with minimum distance greater than q +1 for some certain code lengths, while consuming a few pre-shared maximally entangled states. If we denote a q -ary EAQECC by [[ n, k, d ; c ]] q , where c is the number of maximally entangled states required, we get five classes of EAQMDS codes with parameters:\n",
      "\n",
      "- (i) [[ q 2 +1 , q 2 -2 d +4 , d ; 1]] q , where q is a prime power, 2 ≤ d ≤ 2 q is an even integer.\n",
      "- (ii) [[ q 2 , q 2 -2 d +3 , d ; 1]] q , where q is a prime power, q +1 ≤ d ≤ 2 q -1.\n",
      "- (iii) [[ q 2 -1 , q 2 -2 d +2 , d ; 1]] q , where q is a prime power, 2 ≤ d ≤ 2 q -2.\n",
      "- (iv) [[ q 2 -1 2 , q 2 -1 2 -2 d +4 , d ; 2]] q , where q is an odd prime power, q +1 2 +2 ≤ d ≤ 3 2 q -1 2 .\n",
      "- (v) [[ q 2 -1 t , q 2 -1 t -2 d + t +2 , d ; t ]] q , where q is an odd prime power with t | ( q +1), t ≥ 3 is an odd integer, and ( t -1)( q +1) t +2 ≤ d ≤ ( t +1)( q +1) t -2.\n",
      "\n",
      "EAQMDS codes in (i)-(v) have minimum distance upper limit greater than q + 1 by consuming a few pre-shared maximally entangled states. In particular, each code in (i)-(iii) has nearly double minimum distance upper limit of the standard QMDS code of the same length constructed so far, and consumes only one pair of maximally entangled states. This means that these codes have much better error-correction abilities than the standard QMDS codes of the same length and consume little entanglement.\n",
      "\n",
      "This paper is organized as follows. In Section 2, we introduce some basic notations and definitions of classical ECCs and EAQECCs. We propose several constructions of EAQMDS codes in Section 3. The conclusion is given in Section 4.\n",
      "\n",
      "## 2 Preliminaries\n",
      "\n",
      "Firstly, we review some basic results of classical RS codes, constacyclic codes and several formulas for EAQECCs. For details on classical ECCs and EAQECCs, see the literature [13, 28] and [4, 11, 29], respectively.\n",
      "\n",
      "Let p be a prime number and q a power of p , i.e., q = p r for some r &gt; 0. F q 2 denotes the finite field with q 2 elements. For any a ∈ F q 2 , we denote by a = a q the conjugation of a . For two vectors x = ( x 1 , x 2 , . . . , x n ) and y = ( y 1 , y 2 , . . . , y n ) ∈ F n q 2 , their Hermitian inner product is defined as\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Let C = [ n, k ] be a q 2 -ary linear code of length n and dimension k . The Hermitian dual code of C is defined as\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "If C ⊆ C ⊥ h , then C is called a Hermitian self-orthogonal code. On the contrary, if C ⊥ h ⊆ C , then C is called a Hermitian dual-containing code. Let H = ( a ij ) ( n -k ) × n be the parity check matrix of C over F q 2 with indices 1 ≤ i ≤ n -k and 1 ≤ j ≤ n , then the Hermitian conjugate of H is defined as\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where the dagger ( † ) denotes the conjugate transpose operation over matrices in F q 2 .\n",
      "\n",
      "A Reed-Solomon code (denoted by RS ( n, r )) over F q m is a cyclic code of length n = q m -1 with roots α , α 2 , . . . , α r -1 , where r is an integer with 1 ≤ r ≤ n -2, α is a primitive element of F q m . Its generator polynomial is g ( x ) = ( x -α )( x -α 2 ) · · · ( x -α r -1 ). The parameters of RS ( n, r ) are [ n, k, d ] q m , where k = n -r +1, d = r . The parity check matrix of RS ( n, r ) is given by\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Let λ be a nonzero element of F q 2 , then a linear code C of length n over F q 2 is said to be λ -constacyclic if ( λc n , c 1 , . . . , c n -1 ) ∈ C for every ( c 1 , c 2 , . . . , c n ) ∈ C . If λ = 1, C is a cyclic code. If λ = -1, C is called a negacyclic code. We assume that gcd( n, q 2 ) = 1. A codeword ( c 1 , c 2 , . . . , c n ) ∈ C is identified with its polynomial representation c ( x ) = c 0 + c 1 x + · · · + c n -1 x n -1 . It is easy to find that a λ -constacyclic code C of length n over F q 2 is an ideal of the quotient ring F q 2 [ x ] / 〈 x n -λ 〉 . It is known that C is generated by a monic divisor g ( x ) of x n -λ . The polynomial g ( x ) is called the generator polynomial of the code C . Let λ ∈ F q 2 be a primitive r th root of unity. Let η denote a primitive rn th root of unity (exists in some extension field) such that η n = λ . Let ζ = η r be a primitive n th root of\n",
      "\n",
      "unity. It follows from [30] that the roots of x n -λ are { ηζ i = η 1+ ri | 0 ≤ i ≤ n -1 } . Denote Ω = { 1 + ri | 0 ≤ i ≤ n -1 } . Then the defining set of a λ -constacyclic code C with generator polynomial g ( x ) is Z = { i ∈ Ω | g ( η i ) = 0 } . It is easy to see that the defining set Z is a union of some q 2 -cyclotomic cosets modulo rn . There exist the following BCH bound for cyclic codes and the generalized BCH bound for λ -constacyclic codes.\n",
      "\n",
      "Lemma 1 ( [13], Ch.7) Let C be a cyclic code of length n over F q 2 . Let α ∈ F q 2 be a primitive n -th root of unity. Suppose that C has generator polynomial g ( x ) such that for some integers b ≥ 0 and δ ≥ 1 , g ( α b ) = g ( α b +1 ) = · · · = g ( α b + δ -2 ) = 0 , that is, the code has a string of δ -1 consecutive powers of α as zeros. Then the minimum distance of C is at least δ .\n",
      "\n",
      "Lemma 2 ( [30], Lemma 4) Let C be a λ -constacyclic code of length n over F q 2 , where λ ∈ F q 2 is a primitive r th root of unity. Suppose that the generator polynomial g ( x ) of C has the elements { η 1+ ri | i 0 ≤ i ≤ i 0 + d -2 } as roots, where η is a primitive rn th root of unity, i 0 is an integer. Then the minimum distance of C is at least d .\n",
      "\n",
      "The following lemma gives a sufficient and necessary condition for a q 2 -ary λ -constacyclic code to be Hermitian dual-containing.\n",
      "\n",
      "Lemma 3 ( [24], Lemma 2.2) Let C be a λ -constacyclic code of length n over F q 2 with defining set Z and let λ ∈ F q 2 be a primitive r th root of unity. Then C is a Hermitian dual-containing code if and only if Z ∩ Z -q = ∅ where Z -q = {-qz (mod rn ) | z ∈ Z } .\n",
      "\n",
      "An [[ n, k, d ; c ]] q EAQECC encodes k information qudits into n channel qudits with the help of c pairs of maximally entangled states. The minimum distance is d . One of the focuses of the construction of EAQECCs is to determine the number of maximally entangled pairs required for the encoding. For example, the optimal number of entangled pairs required by an arbitrary binary EAQECC is given in [29].\n",
      "\n",
      "Theorem 1 ( [29], Theorem 1) Suppose that an EAQECC is constructed from generators corresponding to the rows in a quantum check matrix\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where H is an [( n -k ) × 2 n ] -dimensional binary matrix representing the quantum code (see [2, 31]), and both H Z and H X are [( n -k ) × n ] -dimensional binary matrices. Then the resulting code is an [ n, k + c ; c ] entanglement-assisted code and requires c ebits, where\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "and addition is binary.\n",
      "\n",
      "Several formulas for different EAQECCs are given as corollaries in [29]. Similar results are also available for nonbinary EAQECCs. According to [29], a formula similar to (2) holds for q -ary EAQECCs by using q -dimensional entangled pairs. The number of the corresponding entangled pairs is given by\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "and subtraction is in the finite field F q . There are the following corollaries for general EAQECCs.\n",
      "\n",
      "Corollary 1 ( [29]) Let H be the parity check matrix of an [ n, k, d ] q 2 classical linear code over F q 2 . Then an [[ n, 2 k -n + c, d ; c ]] q EAQECC can be obtained, where c = rank ( HH † ) is the number of maximally entangled states required.\n",
      "\n",
      "Corollary 2 ( EA-Singleton Bound, [4]) An [[ n, k, d ; c ]] q EAQECC satisfies\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where 0 ≤ c ≤ n -1 .\n",
      "\n",
      "## 3 Constructions of q -ary EAQMDS codes\n",
      "\n",
      "A classical linear MDS code can lead to an EAQECC that meets the corresponding EASingleton bound [4]. The main task is to determine the number of maximally entangled pairs that required. For the q -ary QMDS code of length n , the construction problem has been completely solved when length n ≤ q + 1, see [18, 19]. Therefore, we do not need to consume extra entanglement resources for the construction when length n ≤ q +1. However, the introduction of a certain amount of pre-shared entanglement is useful for the case when length n &gt; q +1, since we may have more variety for the parameters of EAQMDS codes than those of standard QMDS codes.\n",
      "\n",
      "## 3.1 EAQMDS codes based on cyclic MDS codes\n",
      "\n",
      "We take C as a q 2 -ary cyclic code over F q 2 of length n , where n | q 2 + 1. Then the q 2 -cyclotomic coset modulo n containing i is denoted by C i = { i, iq 2 , iq 4 , . . . , iq 2( m i -1) } , where m i is the smallest positive integer such that q m i i = i (mod n ). The following result gives the q 2 -cyclotomic cosets modulo n .\n",
      "\n",
      "Lemma 4 ( [22], Lemma 4.1) Let n | q 2 +1 and let s = /floorleft n 2 /floorright . If n is odd, then the q 2 -cyclotomic cosets modulo n containing integers from 0 to n are: C 0 = { 0 } , C i = { i, -i } = { i, n -i } , where 1 ≤ i ≤ s . If n is even, then the q 2 -cyclotomic cosets modulo n containing integers from 0 to n are: C 0 = { 0 } , C s = { s } and C i = { i, -i } = { i, n -i } , where 1 ≤ i ≤ s -1 .\n",
      "\n",
      "Lemma 5 Let n | q 2 +1 and s = /floorleft n 2 /floorright . Let C be a q 2 -ary cyclic code of length n with defining set Z = ∪ δ i =0 C i , where 1 ≤ δ ≤ δ max = /floorleft n q +1 /floorright , and let H be the parity check matrix of C over F q 2 , then rank ( HH † ) = 1 .\n",
      "\n",
      "/negationslash\n",
      "\n",
      "Proof. We divide the defining set Z of C into two mutually disjoint subsets, i.e., Z = C 0 ∪ Z 1 , where Z 1 = ∪ δ i =1 C i . Let C 1 be a q 2 -ary cyclic code of length n with defining set Z 1 . We show C ⊥ h 1 ⊆ C 1 . Suppose that C 1 is not a Hermitian dual-containing code, then Z 1 ∩ Z -q 1 = ∅ by Lemma 3. There exist i and j , where 1 ≤ i, j ≤ δ max , such that i = -qj (mod n ) or i = qj (mod n ). If the first case holds, it follows that q +1 ≤ i + qj &lt; n , which is a contradiction. If the second case holds, it follows that 1 ≤ i ≤ δ max &lt; q ≤ qj ≤ qδ max &lt; n , which is also a contradiction. Therefore, we have C ⊥ h 1 ⊆ C 1 . Let the parity check matrix of C 1 over F q 2 be H 1 , then H 1 H † 1 = 0. It is easy to see that the parity check matrix of C over F q 2 is given by H = ( h 0 H 1 ) , where h 0 = (1 , 1 , . . . , 1). Since n | q 2 +1, then we have h 0 h † 0 = 0. It is obvious that C 0 ∩ Z -q 1 = ∅ , and it follows that h 0 H † 1 = 0. Therefore, the rank of HH † is equal to 1. /intersectionsq /unionsq\n",
      "\n",
      "/negationslash\n",
      "\n",
      "Theorem 2 Let n | q 2 +1 . There exists an EAQMDS code with parameters\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where 2 ≤ d ≤ 2 /floorleft n q +1 /floorright +2 is an even integer.\n",
      "\n",
      "Proof. Let C be a cyclic code of length n with defining set Z = ∪ δ i =0 C i , where 0 ≤ δ ≤ δ max = /floorleft n q +1 /floorright . From Lemma 4, we know that the defining set Z consists of 2 δ +1 consecutive integers {-δ, -δ +1 , . . . , -1 , 0 , 1 , . . . , δ -1 , δ } . Then the dimension of C is dim C = n -2 δ -1. From the BCH bound for cyclic codes in Lemma 1, we know that the minimum distance of C is at least 2 δ +2. Then C has parameters [ n, n -2 δ -1 , ≥ 2 δ +2] q 2 . Combining Corollary 1, Lemma 5 and the EA-Singleton bound, we can obtain an EAQMDS code with parameters [[ n, n -4 δ -1 , 2 δ +2;1]] q . Let d = 2 δ +2, then we have 2 ≤ d ≤ 2 δ max +2 = 2 /floorleft n q +1 /floorright +2. /intersectionsq /unionsq Let n = q 2 + 1, then we can get the following EAQMDS code with minimum distance greater than q +1.\n",
      "\n",
      "Corollary 3 There exists an EAQMDS code with parameters\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where q is a prime power, 2 ≤ d ≤ 2 q is an even integer.\n",
      "\n",
      "Example 1 Let q = 4 , then n = q 2 +1 = 17 . Applying Corollary 3, we get two EAQMDS codes with minimum distance greater than q + 1 = 5 whose parameters are [[17 , 8 , 6; 1]] 4 , [[17 , 4 , 8; 1]] 4 .\n",
      "\n",
      "If we consider cyclic codes whose lengths satisfy n | q 2 -1, then the corresponding q 2 -cyclotomic coset modulo n containing i is C i = { i } , 0 ≤ i ≤ n -1.\n",
      "\n",
      "Lemma 6 Let n | q 2 -1 . Let C be a q 2 -ary cyclic code of length n with defining set Z = ∪ δ i = -δ C i , where 1 ≤ δ ≤ δ max = /floorleft n q +1 /floorright1 , and let H be the parity check matrix of C over F q 2 , then rank ( HH † ) = 1 .\n",
      "\n",
      "Proof. We divide the defining set Z of C into three mutually disjoint subsets, i.e., Z = Z 1 ∪ C 0 ∪ Z 2 , where Z 1 = ∪ -1 i = -δ C i and Z 2 = ∪ δ i =1 C i . Let C 1 and C 2 be two q 2 -ary cyclic codes of length n with defining sets Z 1 and Z 2 , respectively. It is easy to verify that there are C ⊥ h 1 ⊆ C 1 , C ⊥ h 2 ⊆ C 2 and C ⊥ h 1 ⊆ C 2 . Let the parity check matrices of C 1 and C 2 over F q 2 be H 1 and H 2 , respectively, then we have H 1 H † 1 = 0, H 2 H † 2 = 0 and H 1 H † 2 = 0. Then the\n",
      "\n",
      "/negationslash parity check matrix of C over F q 2 is given by H =   H 1 h 0 H 2   , where h 0 = (1 , 1 , . . . , 1). Since\n",
      "\n",
      "n | q 2 -1, then we have h 0 h † 0 = 0. It is obvious that C 0 ∩ Z -q 1 = ∅ and C 0 ∩ Z -q 2 = ∅ , and it follows that h 0 H † 1 = 0 and h 0 H † 2 = 0. Therefore, the rank of HH † is equal to 1. /intersectionsq /unionsq\n",
      "\n",
      "Theorem 3 Let n | q 2 -1 . There exists an EAQMDS code with parameters\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where 2 ≤ d ≤ 2 /floorleft n q +1 /floorright .\n",
      "\n",
      "Proof. Let C be a cyclic code of length n with defining set Z = ∪ δ i = -δ C i , where 0 ≤ δ ≤ δ max = /floorleft n q +1 /floorright1. Then the defining set Z which consists of 2 δ +1 consecutive integers is given by {-δ, -δ +1 , . . . , -1 , 0 , 1 , . . . , δ -1 , δ } . Therefore, dim C = n -2 δ -1, and the minimum distance of C is at least 2 δ +2 by Lemma 1. Then C has parameters [ n, n -2 δ -1 , ≥ 2 δ +2] q 2 . Combining Corollary 1, Lemma 6 and the EA-Singleton bound, we can obtain an EAQMDS code with parameters [[ n, n -4 δ -1 , 2 δ + 2; 1]] q , where 2 ≤ 2 δ + 2 ≤ 2 δ max + 2 = 2 /floorleft n q +1 /floorright . In order to get EAQMDS codes with odd minimum distance, we take the defining set of C as Z = ∪ δ ′ -1 i = -δ ′ C i , where 1 ≤ δ ′ ≤ δ max = /floorleft n q +1 /floorright1. Then we can obtain an EAQMDS code with parameters [[ n, n -4 δ ′ +1 , 2 δ ′ +1;1]] q , where 3 ≤ 2 δ ′ +1 ≤ 2 δ max +1 = 2 /floorleft n q +1 /floorright1. /intersectionsq /unionsq\n",
      "\n",
      "Corollary 4 There exists an EAQMDS code with parameters\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where q is a prime power, 2 ≤ d ≤ 2 q -2 is an integer.\n",
      "\n",
      "Example 2 Let q = 5 , then n = q 2 -1 = 24 . Applying Corollary 4, we get four EAQMDS codes with minimum distance greater than q -1 = 4 whose parameters are [[24 , 17 , 5; 1]] 5 , [[24 , 15 , 6; 1]] 5 , [[24 , 13 , 7; 1]] 5 , [[24 , 11 , 8; 1]] 5 .\n",
      "\n",
      "## 3.2 Length n = q 2\n",
      "\n",
      "Let RS ( n -1 , r ) denote a RS code of length n -1 over F q 2 with parameters [ n -1 , n -r, r ]. We extend RS ( n -1 , r ) by adding an overall parity check, and denote the extended code by ̂ RS ( n -1 , r ). Then ̂ RS ( n -1 , r ) has parameters [ n, n -r, r +1]. Let α be a primitive element of F q 2 and let ( α 1 , α 2 , . . . , α n ) = (0 , 1 , . . . , α n -2 ). Then the parity check matrix of ̂ RS ( n -1 , r ) is given by\n",
      "\n",
      "Lemma 7 If q ≤ r ≤ 2 q -2 , then the rank of H ̂ RS ( n -1 ,r ) H † ̂ RS ( n -1 ,r ) is equal to 1.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Proof. It is easy to find that 1 ≤ r ≤ q -1 ⇔ ̂ RS ( n -1 , r ) ⊥ h ⊆ ̂ RS ( n -1 , r ) by [19, Lemma 8]. If q ≤ r ≤ 2 q -2, then we have\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where the '-1' in the q th row and q th column of matrix (7) is given by\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "The zero elements of matrix (7) are given by\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where 1 ≤ r 1 , r 2 ≤ r -1, and then r 1 and r 2 are not equal to q -1 simultaneously. Therefore, the rank of H ̂ RS ( n -1 ,r ) H † ̂ RS ( n -1 ,r ) is equal to 1. /intersectionsq /unionsq\n",
      "\n",
      "Combining Corollary 1 and Lemma 7, we can obtain the following EAQMDS code with length q 2 .\n",
      "\n",
      "Theorem 4 There exists an EAQMDS code with parameters [[ q 2 , q 2 -2 d +3 , d ; 1]] q , where q is a prime power, q +1 ≤ d ≤ 2 q -1 is an integer.\n",
      "\n",
      "Example 3 Let q = 5 , then n = q 2 = 25 . Applying Thoerem 4, we get four EAQMDS codes with minimum distance greater than q = 5 whose parameters are [[25 , 16 , 6; 1]] 5 , [[25 , 14 , 7; 1]] 5 , [[25 , 12 , 8; 1]] 5 , [[25 , 10 , 9; 1]] 5 .\n",
      "\n",
      "## 3.3 EAQMDS codes that consume more than one maximally entangled states\n",
      "\n",
      "In [23, 24, 26, 27], many QMDS codes have been constructed based on negacyclic codes and constacyclic codes. If we introduce a certain amount of extra pre-shared entanglement in some special cases, we can get EAQMDS codes with larger minimum distance.\n",
      "\n",
      "Let q be an odd prime power and n = q 2 -1 2 . For 1 ≤ j ≤ n , it is easy to see that the q 2 -ary cyclotomic coset containing 2 j -1 modulo 2 n has only one element 2 j -1, i.e., C 2 j -1 = { 2 j -1 } .\n",
      "\n",
      "Lemma 8 Let q be an odd prime power and n = q 2 -1 2 . Let C be a q 2 -ary negacyclic code of length n with defining set Z = ∪ δ 2 j = -δ 1 C 2 j -1 , where 1 ≤ δ 1 ≤ q -1 2 -1 and q +1 2 ≤ δ 2 ≤ q -1 , and let H be the parity check matrix of C over F q 2 , then rank( HH † ) = 2 .\n",
      "\n",
      "/negationslash\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Proof. We divide the defining set Z of C into three mutually disjoint subsets, i.e., Z = Z 1 ∪ C -1 ∪ Z 2 , where Z 1 = ∪ δ 1 j =1 C -2 j -1 and Z 2 = ∪ δ 2 j =1 C 2 j -1 . Let C 1 and C 2 be two q 2 -ary negacyclic codes of length n with defining sets Z 1 and Z 2 , respectively. We know that C ⊥ h 1 ⊆ C 1 and C ⊥ h 2 ⊆ C 2 by [24, Lemma 3.1]. We show that C ⊥ h 1 ⊆ C 2 . Seeking a contradiction, we assume that Z 1 ∩ Z -q 2 = ∅ by Lemma 3. Then there exist k and l , where 1 ≤ k ≤ q -1 2 -1 and 1 ≤ l ≤ q -1, such that -2 k -1 = -q (2 l -1) (mod 2 n ), which means that q (2 l -1) -(2 k +1) = 0 (mod 2 n ). It follows that q (2 l -1) -(2 k +1) = q 2 -1 since 2 ≤ q (2 l -1) -(2 k +1) ≤ 2 q 2 -3 q -3. However, there is 0 ≤ 2 k = q (2 l -q -1) ≤ q 2 -3 q . Then we have k = 0 or 2 k ≥ 2 q , which are both contradictions. Therefore, we have C ⊥ h 1 ⊆ C 2 . Let the parity check matrices of C 1 and C 2 over F q 2 be H 1 and H 2 , respectively, then we have H 1 H † 1 = 0, H 2 H † 2 = 0 and H 1 H † 2 = 0. It is easy to see that C -1 ∩ C -q -1 = ∅ , C -1 ∩ Z -q 1 = Z 1 ∩ C -q -1 = ∅ , C -1 ∩ Z -q 2 = {-1 } and Z 2 ∩ C -q -1 = { q } , hence, h -1 h † -1 = 0, h -1 H † 1 = 0, H 1 h † -1 = 0, h -1 H † 2 is a nonzero row vector and H 2 h † -1 is a nonzero column vector. Then the parity check matrix of C over F q 2 is given\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "It follows that the rank of HH † is equal to 2. /intersectionsq /unionsq\n",
      "\n",
      "Theorem 5 There exists an EAQMDS code with parameters [[ q 2 -1 2 , q 2 -1 2 -2 d + 4 , d ; 2]] q , where q is an odd prime power, q +1 2 +2 ≤ d ≤ 3 2 q -1 2 .\n",
      "\n",
      "Proof. Consider the negacyclic code C over F q 2 of length q 2 -1 2 with defining set Z = ∪ δ 2 j = -δ 1 C 2 j -1 , where 0 ≤ δ 1 ≤ q -1 2 -1 and q +1 2 ≤ δ 2 ≤ q -1. Then the defining set Z which consists of δ 1 + δ 2 + 1 consecutive odd integers is given by {-2 δ 1 -1 , -2 δ 1 + 1 , . . . , -1 , 1 , . . . , 2 δ 2 -3 , 2 δ 2 -1 } . Therefore, we have dim C = q 2 -1 2 -δ 1 -δ 2 -1. From the BCH bound for negacyclic codes in Lemma 2, the minimum distance of C is at least δ 1 + δ 2 + 2. Then C has parameters [ q 2 -1 2 , q 2 -1 2 -δ 1 -δ 2 -1 , ≥ δ 1 + δ 2 + 2] q 2 . Combining Corollary 1, Lemma 8 and the EA-Singleton bound, we can obtain an EAQMDS code with parameters [[ q 2 -1 2 , q 2 -1 2 -2 δ 1 -2 δ 2 -2 , δ 1 + δ 2 +2;2]] q . Let d = δ 1 + δ 2 +2, we have q +1 2 +2 ≤ d ≤ 3 2 q -1 2 . /intersectionsq /unionsq\n",
      "\n",
      "Example 4 Let q = 5 , then n = q 2 -1 2 = 12 . Applying Theorem 5, we get three EAQMDS codes with parameters [[12 , 6 , 5; 2]] 5 , [[12 , 4 , 6; 2]] 5 , [[12 , 2 , 7; 2]] 5 .\n",
      "\n",
      "Let t ≥ 3 be an odd integer and let q be an odd prime power with t | ( q + 1). Denote n = q 2 -1 t . Let λ ∈ F q 2 be a primitive t -th root of unity. It is easy to see that every q 2 -cyclotomic coset modulo tn contains only one element. In [26, 27], q -ary QMDS codes of length n = q 2 -1 t have been constructed from Hermitian dual-containing λ -constacyclic MDS codes. Based on the λ -constacyclic MDS codes, and if we introduce a certain amount of extra pre-shared entanglement, we can get EAQMDS codes with larger minimum distance compared with QMDS codes in [26, 27] of length n = q 2 -1 t . Let C be a λ -constacyclic code of length n over F q 2 with defining set\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where C 1+ t ( ( t -1)( q -1) -2 2 t + i ) = { 1 + t ( ( t -1)( q -1) -2 2 t + i ) } for -δ 1 ≤ i ≤ δ 2 , ( t -1)( q +1) 2 t ≤ δ 1 ≤ ( t +1)( q +1) 2 t -2 and ( t -1)( q +1) 2 t ≤ δ 2 ≤ ( t +1)( q +1) 2 t -2.\n",
      "\n",
      "Lemma 9 Let t ≥ 3 be an odd integer and let q be an odd prime power with t | ( q + 1) . Denote n = q 2 -1 t . Let C be a q 2 -ary λ -constacyclic code of length n with defining set Z = ∪ δ 2 i = -δ 1 C 1+ t ( ( t -1)( q -1) -2 2 t + i ) , where ( t -1)( q +1) 2 t ≤ δ 1 ≤ ( t +1)( q +1) 2 t -2 and ( t -1)( q +1) 2 t ≤ δ 2 ≤ ( t +1)( q +1) -2 , and let H be the parity check matrix of C over F 2 , then rank( HH † ) = t .\n",
      "\n",
      "be H 1 and H 2 , respectively. Then the parity check matrix of C is given by H =   H 1 h q -1 H 2   ,\n",
      "\n",
      "2 t q Proof. Denote s = ( t -1) / 2. We can divide the defining set Z of C into three mutually disjoint subsets, i.e., Z = Z 1 ∪ C s ( q -1) ∪ Z 2 , where Z 1 = ∪ δ 1 j =1 C 1+ t ( ( t -1)( q -1) -2 2 t -j ) and Z 2 = ∪ δ 2 k =1 C 1+ t ( ( t -1)( q -1) -2 2 t + k ) . Let C 1 and C 2 be two q 2 -ary λ -constacyclic codes of length n with defining sets Z 1 and Z 2 , respectively. Let the parity check matrices of C 1 and C 2 over F q 2\n",
      "\n",
      "/negationslash where h q -1 = (1 , η q -1 , . . . , η ( n -1)( q -1) ). From [26, Lemma 3.6] and [27, Lemma 4.1], there are C ⊥ h 1 ⊆ C 1 and C ⊥ h 2 ⊆ C 2 , hence H 1 H † 1 = 0 and H 2 H † 2 = 0. It is easy to see that C s ( q -1) ∩ C -q s ( q -1) = { s ( q -1) } , Z 1 ∩ C -q s ( q -1) = C s ( q -1) ∩ Z -q 1 = ∅ and Z 2 ∩ C -q s ( q -1) = C s ( q -1) ∩ Z -q 2 = ∅ , then there are h -1 h † -1 = 1 + 1 + · · · +1 = 0, H 1 h † -1 = 0, h -1 H † 1 = 0 and h -1 H † 2 = 0, H 2 h † -1 = 0. Then we have\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "It follows that rank ( HH † ) = 2 rank ( H 1 H † 2 ) + 1. Next, we have to compute the rank of H 1 H † 2 . We determine the intersection of Z 1 and Z -q 2 . We assume that there exist j and k , where 1 ≤ j ≤ ( t +1)( q +1) 2 t -2 and 1 ≤ k ≤ ( t +1)( q +1) 2 t -2, such that 1 + t ( ( t -1)( q -1) -2 2 t -j ) = -q (1 + t ( ( t -1)( q -1) -2 2 t + k )) (mod q 2 -1), which means that tqk -tj = 0 (mod q 2 -1). Since t -1 2 q + 3 t -1 2 ≤ tqk -tj ≤ t +1 2 q 2 -3 t -1 2 q -t , it follows that tqk -tj ∈ { q 2 -1 , . . . , t -1 2 ( q 2 -1) } . Denote tqk -tj = x t ( q 2 -1), where x t ∈ { 1 , . . . , s } , then we have x t ( q 2 -1)+ t tq ≤ k ≤ 2 x t ( q 2 -1)+( t +1) q -3 t +1 2 tq . Note that x t ( q +1) t -1 &lt; x t ( q 2 -1)+ t tq ≤ k ≤ 2 x t ( q 2 -1)+( t +1) q -3 t +1 2 tq &lt; x t ( q +1) t + 1. It follows that k = x t ( q +1) t and j = x t ( q +1) t for x t ∈ { 1 , . . . , s } . Therefore, we have Z 1 ∩ Z -q 2 = { ( t -2 x t -1) q -2 x t -t -1 2 t | x t = 1 , . . . , s } and | Z 1 ∩ Z -q 2 | = s . We can redivide Z 1 and Z 2 into mutually disjoint subsets, respectively, then the rank of H 1 H † 2 is equal to s . Therefore, rank ( HH † ) = 2 · s +1 = t . /intersectionsq /unionsq\n",
      "\n",
      "Theorem 6 Let t ≥ 3 be an odd integer and let q be an odd prime power with t | ( q + 1) . Then, there exists an EAQMDS code with parameters [[ q 2 -1 t , q 2 -1 t -2 d + t +2 , d ; t ]] q , where ( t -1)( q +1) t +2 ≤ d ≤ ( t +1)( q +1) t -2 .\n",
      "\n",
      "Proof. Let C be a λ -constacyclic code over F q 2 of length q 2 -1 t with defining set Z = ∪ δ 2 i = -δ 1 C 1+ t ( ( t -1)( q -1) -2 2 t + i ) , where ( t -1)( q +1) 2 t ≤ δ 1 ≤ ( t +1)( q +1) 2 t -2 and ( t -1)( q +1) 2 t ≤ δ 2 ≤ ( t +1)( q +1) 2 t -2. Note that dim C = q 2 -1 t -δ 1 -δ 2 -1, and the minimum distance of C is at least δ 1 + δ 2 + 2 by the BCH bound for constacyclic codes in Lemma 2. Then C has parameters [ q 2 -1 t , q 2 -1 t -δ 1 -δ 2 -1 , ≥ d ] q 2 , where d = δ 1 + δ 2 +2. Combining Corollary 1, Lemma 9 and the EA-Singleton bound, we can obtain an EAQMDS code with parameters [[ q 2 -1 t , q 2 -1 t -2 δ 1 -2 δ 2 +1 , d ; t ]] q , where ( t -1)( q +1) t +2 ≤ d ≤ ( t +1)( q +1) t -2. /intersectionsq /unionsq\n",
      "\n",
      ". Example 6 Let t = 5 and q = 19 , then n = q 2 -1 5 = 72 . We get five EAQMDS codes with parameters [[72 , 43 , 18; 5]] 19 , [[72 , 41 , 19; 5]] 19 , [[72 , 39 , 20; 5]] 19 , [[72 , 37 , 21; 5]] 19 , [[72 , 35 , 22; 5]] 19 . q 2 -1\n",
      "\n",
      "Example 5 Let t = 3 and q = 11 , then n = q 2 -1 3 = 40 . We get five EAQMDS codes with parameters [[40 , 25 , 10; 3]] 11 , [[40 , 23 , 11; 3]] 11 , [[40 , 21 , 12; 3]] 11 , [[40 , 19 , 13; 3]] 11 , [[40 , 17 , 14; 3]] 11\n",
      "\n",
      "Example 7 Let t = 7 , q = 27 , then n = 7 = 104 . We get five EAQMDS codes with parameters [[104 , 61 , 26; 7]] 27 , [[104 , 59 , 27; 7]] 27 , [[104 , 57 , 28; 7]] 27 , [[104 , 55 , 29; 7]] 27 , [[104 , 53 , 30; 7]] 27 .\n",
      "\n",
      "## 4 Conclusion\n",
      "\n",
      "We have constructed several classes of entanglement-assisted quantum MDS (EAQMDS) codes based on classical MDS codes for some certain code lengths. We list a comparison in Table 1 between EAQMDS codes constructed in this paper and the standard QMDS codes. Compared with the known QMDS codes of the same length, these EAQMDS codes have much larger minimum distance upper limit by exploiting one or more pre-shared maximally entangled states. In the future work, we look forward to getting more q -ary EAQMDS codes with minimum distance greater than q +1.\n",
      "\n",
      "## Acknowledgements\n",
      "\n",
      "The authors are grateful to the Editor and the anonymous referee for their constructive comments and valuable suggestions. The first author J. Fan thanks the financial support\n",
      "\n",
      "Table 1. Comparison between EAQMDS codes and standard QMDS codes\n",
      "\n",
      "| Length                                    | q -ary EAQMDS codes                                                                              | q -ary QMDS codes                                                       | Reference              |\n",
      "|-------------------------------------------|--------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|------------------------|\n",
      "| q 2 +1                                    | [[ q 2 +1 , q 2 - 2 d +4 , d ; 1]], 2 ≤ d ≤ 2 q , d even                                         | [[ q 2 +1 , q 2 - 2 d +3 , d ]], 2 ≤ d ≤ q +1                           | [17], [21], [22], [23] |\n",
      "| q 2                                       | [[ q 2 , q 2 - 2 d +3 , d ; 1]], q +1 ≤ d ≤ 2 q - 1                                              | [[ q 2 , q 2 - 2 d +2 , d ]], 2 ≤ d ≤ q                                 | [15], [19]             |\n",
      "| q 2 - 1                                   | [[ q 2 - 1 , q 2 - 2 d +2 , d ; 1]], 2 ≤ d ≤ 2 q - 2                                             | [[ q 2 - 1 , q 2 - 2 d +1 , d ]], 2 ≤ d ≤ q - 1                         | [15], [19]             |\n",
      "| q 2 - 1 2 , q odd                         | [[ q 2 - 1 2 , q 2 - 1 2 - 2 d +4 ,d ; 2]], ( q +1) / 2+2 ≤ d ≤ 3 2 q - 1 2                      | [[ q 2 - 1 2 , q 2 - 1 2 - 2 d +2 ,d ]], 2 ≤ d ≤ q                      | [24], [26]             |\n",
      "| q 2 - 1 t , q odd, t | ( q +1), t ≥ 3 odd | [[ q 2 - 1 t , q 2 - 1 t - 2 d + t +2 ,d ; t ]], ( t - 1)( q +1) t +2 ≤ d ≤ ( t +1)( q +1) t - 2 | [[ q 2 - 1 t , q 2 - 1 t - 2 d +2 ,d ]], 2 ≤ d ≤ ( t +1)( q +1) 2 t - 1 | [26], [27]             |\n",
      "\n",
      "from China Scholarship Council (CSC, No. 201406090079). J. Fan thanks Dr. Bocong Chen for the helpful communication. This work was supported by the National Natural Science Foundation of China (Grant No. 61170321), the Specialized Research Fund for the Doctoral Program of Higher Education (Grant No. 20110092110024), the Natural Science Foundation of Jiangsu Province (Grant No. BK20140823), China Postdoctoral Science Foundation (Grant No. 2013M531353) and the Scientific Research Innovation Plan for College Graduates of Jiangsu Province (Grant No. CXZZ13 0105). This work was partially carried out when the first author was visiting the School of Electrical and Information Engineering, University of Sydney. He thanks the school for its hospitality.\n",
      "\n",
      "## References\n",
      "\n",
      "1. A. R. Calderbank, E. M. Rains, P. Shor, and N. J. Sloane, 'Quantum error correction via codes over GF(4),' IEEE Trans. Inform. Theory , vol. 44, no. 4, pp. 1369-1387, 1998.\n",
      "2. D. Gottesman, 'Stabilizer codes and quantum error correction,' Ph.D. dissertation, California Institute of Technology, 1997.\n",
      "3. A. Ketkar, A. Klappenecker, S. Kumar, and P. K. Sarvepalli, 'Nonbinary stabilizer codes over finite fields,' IEEE Trans. Inform. Theory , vol. 52, no. 11, pp. 4892-4914, 2006.\n",
      "4. T. Brun, I. Devetak, and M.-H. Hsieh, 'Correcting quantum errors with entanglement,' Science , vol. 314, no. 5798, pp. 436-439, 2006.\n",
      "5. M.-H. Hsieh, T. A. Brun, and I. Devetak, 'Entanglement-assisted quantum quasicyclic low-density parity-check codes,' Phys. Rev. A , vol. 79, no. 3, p. 032340, 2009.\n",
      "6. M.-H. Hsieh, W.-T. Yen, and L.-Y. Hsu, 'High performance entanglement-assisted quantum LDPC codes need little entanglement,' IEEE Trans. Inform. Theory , vol. 57, no. 3, pp. 1761-1769, 2011.\n",
      "7. Y. Fujiwara, D. Clark, P. Vandendriessche, M. De Boeck, and V. D. Tonchev, 'Entanglementassisted quantum low-density parity-check codes,' Phys. Rev. A , vol. 82, no. 4, p. 042338, 2010.\n",
      "8. Y. Fujiwara and V. D. Tonchev, 'A characterization of entanglement-assisted quantum low-density parity-check codes,' IEEE Trans. Inform. Theory , vol. 59, no. 6, pp. 3347-353, 2013.\n",
      "9. M. M. Wilde, M.-H. Hsieh, and Z. Babar, 'Entanglement-assisted quantum turbo codes,' IEEE Trans. Inform. Theory , vol. 60, no. 2, pp. 1203-1222, 2014.\n",
      "10. L.-D. L¨ u and R. Li, 'Entanglement-assisted quantum codes constructed from primitive quaternary BCH codes,' Int. J. Quantum Inf. , vol. 12, no. 03, p. 1450015, 2014.\n",
      "11. D. A. Lidar and T. A. Brun, Quantum error correction . Cambridge: Cambridge University Press,\n",
      "\n",
      "2013.\n",
      "\n",
      "12. M. M. Wilde, Quantum Information Theory . Cambridge: Cambridge University Press, 2013.\n",
      "13. F. J. MacWilliams and N. J. A. Sloane, The Theory of Error-Correcting Codes . Amsterdam: The Netherlands: North-Holland, 1981.\n",
      "14. M. Grassl, W. Geiselmann, and T. Beth, 'Quantum Reed-Solomon codes,' in Applied Algebra, Algebraic Algorithms and Error-correcting Codes . Springer, 1999, pp. 231-244.\n",
      "15. Z. Li, L.-J. Xing, and X.-M. Wang, 'Quantum generalized Reed-Solomon codes: Unified framework for quantum maximum-distance-separable codes,' Phys. Rev. A , vol. 77, no. 1, p. 012308, 2008.\n",
      "16. M. Grassl and M. Roetteler, 'Quantum MDS codes over small fields,' in Proc. IEEE Int. Symp. Inf. Theory , Hong Kong, June 2015, pp. 1104-1108.\n",
      "17. L. Jin and C. Xing, 'A construction of new quantum MDS codes,' IEEE Trans. Inform. Theory , vol. 60, no. 5, pp. 2921-2925, 2014.\n",
      "18. M. R¨ otteler, M. Grassl, and T. Beth, 'On quantum MDS codes,' in Proc. IEEE Int. Symp. Inf. Theory , Chicago, IL, USA, June 2004, pp. 356-356.\n",
      "19. M. Grassl, T. Beth, and M. Roetteler, 'On optimal quantum codes,' Int. J. Quantum Inf. , vol. 2, no. 01, pp. 55-64, 2004.\n",
      "20. R. Li and Z. Xu, 'Construction of [[ n, n -4 , 3]] q quantum codes for odd prime power q ,' Phys. Rev. A , vol. 82, no. 5, p. 052316, 2010.\n",
      "21. L. Jin, S. Ling, J. Luo, and C. Xing, 'Application of classical hermitian self-orthogonal MDS codes to quantum MDS codes,' IEEE Trans. Inform. Theory , vol. 56, no. 9, pp. 4735-4740, 2010.\n",
      "22. G. G. La Guardia, 'New quantum MDS codes,' IEEE Trans. Inform. Theory , vol. 57, no. 8, pp. 5551-5554, 2011.\n",
      "23. X. Kai and S. Zhu, 'New quantum MDS codes from negacyclic codes,' IEEE Trans. Inform. Theory , vol. 59, no. 2, pp. 1193-1197, 2013.\n",
      "24. X. Kai, S. Zhu, and P. Li, 'Constacyclic codes and some new quantum MDS codes,' IEEE Trans. Inform. Theory , vol. 60, no. 4, pp. 2080-2086, 2014.\n",
      "25. G. Zhang and B. Chen, 'New quantum MDS codes,' Int. J. Quantum Inf. , vol. 12, no. 04, 2014.\n",
      "26. L. Wang and S. Zhu, 'New quantum MDS codes derived from constacyclic codes,' Quantum Inf. Process. , vol. 14, no. 3, pp. 881-889, 2015.\n",
      "27. B. Chen, S. Ling, and G. Zhang, 'Application of constacyclic codes to quantum MDS codes,' IEEE Trans. Inform. Theory , vol. 61, no. 3, pp. 1474-1484, 2015.\n",
      "28. E. Berlekamp, Algebraic Coding Theory . New York, McGraw-Hill, 1968.\n",
      "29. M. M. Wilde and T. A. Brun, 'Optimal entanglement formulas for entanglement-assisted quantum coding,' Phys. Rev. A , vol. 77, no. 6, p. 064302, 2008.\n",
      "30. A. Krishna and D. V. Sarwate, 'Pseudocyclic maximum-distance-separable codes,' IEEE Trans. Inform. Theory , vol. 36, no. 4, pp. 880-884, 1990.\n",
      "31. M. A. Nielsen and I. L. Chuang, Quantum Computation and Quantum Information . Cambridge: Cambridge University Press, 2000.\n",
      "Document 11:\n",
      "## INTEGER QUANTIZATION FOR DEEP LEARNING INFERENCE: PRINCIPLES AND EMPIRICAL EVALUATION\n",
      "\n",
      "Hao Wu 1\n",
      "\n",
      "Patrick Judd 1\n",
      "\n",
      "Xiaojie Zhang 1\n",
      "\n",
      "Mikhail Isaev 2 ∗\n",
      "\n",
      "Paulius Micikevicius 1\n",
      "\n",
      "1\n",
      "\n",
      "NVIDIA 2\n",
      "\n",
      "Georgia Institute of Technology\n",
      "\n",
      "{skyw, pjudd, viczhang, pauliusm}@nvidia.com\n",
      "\n",
      "michael.v.isaev@gatech.edu\n",
      "\n",
      "## ABSTRACT\n",
      "\n",
      "Quantization techniques can reduce the size of Deep Neural Networks and improve inference latency and throughput by taking advantage of high throughput integer instructions. In this paper we review the mathematical aspects of quantization parameters and evaluate their choices on a wide range of neural network models for different application domains, including vision, speech, and language. We focus on quantization techniques that are amenable to acceleration by processors with high-throughput integer math pipelines. We also present a workflow for 8-bit quantization that is able to maintain accuracy within 1% of the floating-point baseline on all networks studied, including models that are more difficult to quantize, such as MobileNets and BERT-large.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "While 32-bit single-precision floating-point was the dominant numerical format for Deep Learning (DL) applications, more recently a variety of alternative formats have been proposed to increase the computational performance of deep learning applications. It is becoming commonplace to train neural networks in 16-bit floating-point formats, either IEEE fp16 [35] or bfloat16 [57], supported by most DL accelerators. Once trained, neural networks can be deployed for inference using even lower-precision formats, including floating-point, fixed-point, and integer. Low-precision formats offer several performance benefits. First, many processors provide higher throughput math pipelines the low-bit formats, which can speed up math-intensive operations, such as convolutions and matrix multiplications. Second, smaller word sizes reduce memory bandwidth pressure, improving performance for bandwidth-limited computations. Third, smaller word sizes lead to lower memory size requirements, which can improve cache utilization as well as other aspects of memory-system operation.\n",
      "\n",
      "In this paper we focus on integer quantization for neural network inference, where trained networks are modified to use integer weights and activations so that integer math pipelines can be used for many operations. Table 1 lists the relative tensor operation throughputs of various data types on the NVIDIA Turing Graphics Processing Unit (GPU) architecture [40]. Math-intensive tensor operations executed on 8-bit integer types can see up to a 16x speed-up compared to the same operations in fp32. Memory-limited operations could see up to a 4x speed-up compared to the fp32 version, due to the smaller word size. Other processors, such as TPUv1 [23], Intel CPUs with VNNI\n",
      "\n",
      "| Input Data type   | Accumulation Data type   | Math Throughput   | Bandwidth Reduction   |\n",
      "|-------------------|--------------------------|-------------------|-----------------------|\n",
      "| FP32              | FP32                     | 1x                | 1x                    |\n",
      "| FP16              | FP16                     | 8x                | 2x                    |\n",
      "| INT8              | INT32                    | 16x               | 4x                    |\n",
      "| INT4              | INT32                    | 32x               | 8x                    |\n",
      "| INT1              | INT32                    | 128x              | 32x                   |\n",
      "\n",
      "Table 1: Benefits of lower precision data types for tensor operations on the NVIDIA Turing GPU architecture\n",
      "\n",
      "∗ Work done during an internship at NVIDIA\n",
      "\n",
      "instructions [28], and a number of emerging accelerator designs also provide significant acceleration for int8 operations. The process of neural network quantization can be automated by software tools [36, 61] or controlled manually. In either case, care must be taken to minimize any impact quantization has on the model accuracy.\n",
      "\n",
      "In this paper we review the mathematical fundamentals underlying various integer quantization choices (Section 3) as well as techniques for recovering accuracy lost due to quantization (Section 5). Section 6 combines this information into a recommended workflow. In Section 4 and the Appendices we present empirical evaluation of various quantization choices on a wide range of network models from different application domains - image processing, language modeling, language translation, and speech recognition. These models include the major network topologies - convolutional networks, recurrent networks, as well as attention-based networks. With the presented workflow for int8 quantization we are able to maintain model accuracy within 1% of each baseline floating-point network, even for the networks that are known to be challenging to quantize, such as MobileNets and BERT-large.\n",
      "\n",
      "## 2 Related Work\n",
      "\n",
      "Vanhoucke et al. [52] showed that earlier neural networks could be quantized after training to use int8 instructions on Intel CPUs while maintaining the accuracy of the floating-point model. More recently it has been shown that some modern networks require training to maintain accuracy when quantized for int8. Jacob et al. [20] described models optimized for inference where all inference operations were performed with integer data types. Here batch normalization layers were folded into the preceding convolution layer before quantization, reducing the number of layers that needed to be executed during inference. Krishnamoorthi [26] evaluated various quantization methods and bit-widths on a variety of Convolutional Neural Networks (CNNs). He showed that even with per-channel quantization, networks like MobileNet do not reach baseline accuracy with int8 Post Training Quantization (PTQ) and require Quantization Aware Training (QAT). McKinstry et al. [33] demonstrated that many ImageNet CNNs can be finetuned for just one epoch after quantizing to int8 and reach baseline accuracy. They emphasized the importance of using an annealing learning rate schedule and a very small final learning rate. They also set the quantization range based on a percentile of activations sampled from the training set. Instead of using fixed ranges, Choi et al. [6] proposed PACT which learns the activation ranges during training.\n",
      "\n",
      "Much of the earlier research in this area focused on very low bit quantization [7, 13, 59], all the way down to ternary (2-bit) [60, 34] and binary weights [8] and activations [45, 18]. These works showed that for lower bit-widths, training with quantization was required to achieve high accuracy, though accuracy was still lower than the floating-point network on harder tasks such as ImageNet image classification [47]. They also demonstrated the importance of techniques such as using higher precision for weight updates and the Straight-through Estimator (STE) for gradient backpropagation [3]. Also, in many cases the first and last layer were not quantized, or quantized with a higher bit-width, as they are more sensitive to quantization [59, 45, 18]. Multi-bit quantization schemes use either uniform [7, 59], or non-uniform quantization [13, 60, 34, 2]. Uniform quantization enables the use of integer or fixed-point math pipelines, allowing computation to be performed in the quantized domain. Non-uniform quantization requires dequantization, e.g. a codebook lookup, before doing computation in higher precision, limiting its benefits to model compression and bandwidth reduction. This paper focuses on leveraging quantization to accelerate computation, so we will restrict our focus to uniform quantization schemes.\n",
      "\n",
      "While much of the aforementioned work has focused on CNNs for image classification, there are also many examples of applying quantization to other types of network architectures. Wu et al. [55] described how Google's Neural Machine Translation (GNMT), which employs a Long Short Term Memory (LSTM) Recurrent Neural Network (RNN), was trained with hard range constraints on multiple tensors to be more amenable to PTQ. A similar strategy was taken on MobileNet v2 [48], which restricts activations to be in the range [0, 6] (ReLU6). Bhandare et al. [4] quantized the smaller base Transformer [53] model targeting the int8 VNNI instructions on Intel CPUs. They use KL-Divergence [36] to calibrate the quantization ranges and apply PTQ. Zafrir et al. [58] quantized BERT [10] to int8 using both PTQ and QAT. In this paper, we present an evaluation of int8 quantization on all of the major network architectures with both PTQ and QAT.\n",
      "\n",
      "More complex methods have also been proposed for training quantized models. Distillation has been used to train a quantized 'student' model with a high precision, and often larger, 'teacher' model. It has been applied to training quantized CNNs [37, 43], LSTMs [43] and Transformers [24]. Leng et al. [31] used the Alternating Direction Method of Multipliers (ADMM) as an alternative to STE when training quantized model. These methods generally target lower bit-width quantization, as QAT has been shown to be sufficient for int8 quantization. We have also found QAT to be sufficient for int8 quantization on the models we evaluated, and as such we chose not to included these methods in our evaluation of int8 quantization.\n",
      "\n",
      "Figure 1: Quantization mapping of real values to int8\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## 3 Quantization Fundamentals\n",
      "\n",
      "We focus on uniform integer quantization as it enables computing matrix multiplications and convolutions in the integer domain, allowing the use of high throughput integer math pipelines. Uniform quantization can be divided in to two steps. First, choose the range of the real numbers to be quantized, clamping the values outside this range. Second, map the real values to integers representable by the bit-width of the quantized representation (round each mapped real value to the closest integer value).\n",
      "\n",
      "In this Section we will consider higher precision floating-point formats like fp16 and fp32 to be real numbers for the purpose of discussion. Enabling integer operations in a pre-trained floating-point neural network requires two fundamental operations:\n",
      "\n",
      "Quantize : convert a real number to a quantized integer representation (e.g. from fp32 to int8).\n",
      "\n",
      "Dequantize : convert a number from quantized integer representation to a real number (e.g. from int32 to fp16).\n",
      "\n",
      "We will first define the quantize and dequantize operations in Section 3.1 and discuss their implications in neural network quantization in Sections 3.2 and 3.3. Then we will discuss how the real ranges are chosen in Section 3.4.\n",
      "\n",
      "## 3.1 Range Mapping\n",
      "\n",
      "Let [ β, α ] be the range of representable real values chosen for quantization and b be the bit-width of the signed integer representation. Uniform quantization transforms the input value x ∈ [ β, α ] to lie within [ -2 b -1 , 2 b -1 -1] , where inputs outside the range are clipped to the nearest bound. Since we are considering only uniform transformations, there are only two choices for the transformation function: f ( x ) = s · x + z and its special case f ( x ) = s · x , where x, s, z ∈ R . In this paper we refer to these two choices as affine and scale , respectively.\n",
      "\n",
      "## 3.1.1 Affine Quantization\n",
      "\n",
      "Affine quantization maps a real value x ∈ R to a b -bit signed integer x q ∈ {-2 b -1 , -2 b -1 + 1 , . . . , 2 b -1 -1 } . Equations 1 and 2 define affine transformation function, f ( x ) = s · x + z :\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where s is the scale factor and z is the zero-point - the integer value to which the real value zero is mapped. In the 8-bit case, s = 255 α -β and z = -round ( β · s ) -128 . Note that z is rounded to an integer value so that the real value of zero is exactly representable. This will result in a slight adjustment to the real representable range [ β, α ] [20].\n",
      "\n",
      "The quantize operation is defined by Equation 3 and 4:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where round() rounds to the nearest integer. Figure 1a shows the mapping of real values to int8 representation with affine quantization. Note that s is the ratio of the integer-representable and chosen real ranges.\n",
      "\n",
      "Equation 5 shows the corresponding dequantize function, which computes an approximation of the original real valued input, ˆ x ≈ x .\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "## 3.1.2 Scale Quantization\n",
      "\n",
      "Scale quantization performs range mapping with only a scale transformation. For simplicity we describe the symmetric variant of scale quantization (often called symmetric quantization [26]), where the input range and integer range are symmetric around zero. This means that for int8 we use the integer range [ -127 , 127] , opting not to use the value -128 in favor of symmetry. For 8-bit quantization, losing one out of 256 representable values is insignificant, but for lower bit quantization the trade-off between representable values and symmetry should be re-evaluated.\n",
      "\n",
      "Figure 1b illustrates the mapping of real values to int8 with scale quantization. Equation 6 and 7 define scale quantization of a real value x , with a chosen representable range [ -α, α ] , producing a b -bit integer value, x q .\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Equation 8 shows the corresponding dequantize operation for scale quantization.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "## 3.2 Tensor Quantization Granularity\n",
      "\n",
      "There are several choices for sharing quantization parameters among tensor elements. We refer to this choice as quantization granularity. At the coarsest, per-tensor granularity, the same quantization parameters are shared by all elements in the tensor. The finest granularity would have individual quantization parameters per element. Intermediate granularities reuse parameters over various dimensions of the tensor - per row or per column for 2D matrices, per channel for 3D (image-like) tensors, etc.\n",
      "\n",
      "We will consider two factors when choosing granularity: impact on model accuracy and computational cost. To understand the computational cost, we will examine matrix multiplication (note that this results in no loss of generality for math-intensive operations since convolutions can be expressed as matrix multiplications [5, 54]).\n",
      "\n",
      "Consider a linear (fully-connected) layer that performs a matrix multiplication Y = XW , where X = ( x ik ) ∈ R m × p is the input activation tensor, W = ( w kj ) ∈ R p × n is the weight tensor, and Y = ( y ij ) ∈ R m × n is the output tensor. The result of the real-valued matrix multiplication Y = XW can be approximated with quantized tensors X q = ( x q,ik ) ∈ Z m × p and W q = ( w q,kj ) ∈ Z p × n , by first dequantizing them, and then performing the matrix multiplication. First, consider tensors quantized at the finest granularity, per-element, with scale quantization:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "In order to use integer matrix multiplication the scales must be factored out of the summation on the right-hand side of Equation 9, for which the scales must be independent of k :\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Thus, integer matrix multiplication is possible as long as the quantization granularity is per-row or per-tensor for activations and per-column or per-tensor for weights. For activations, only per-tensor quantization is practical for performance reasons. In the above formulation different rows belong to either different batch instances or items in a sequence and thus row count can vary at inference time. This prevents the per-row scaling factor from being computation offline (which would not be meaningful for different instances in a mini-batch), whereas determining them online imposes a compute overhead and in some cases results in poor accuracy (Dynamic Quantization discussion in [58]).\n",
      "\n",
      "For maximum performance, activations should use per-tensor quantization granularity. Weights should be quantized at either per-tensor or per-column granularity for linear layers of the form Y = XW (per-row for linear layers of the form Y = XW T ). The corresponding granularity to per-column in convolutions is per-kernel, or equivalently per-output-channel since each kernel produces a separate output channel [27, 29]. This is commonly referred to as 'per-channel' weight quantization in literature and we follow that convention [21, 25, 26, 38, 46]. We examine the granularity impact on accuracy in Section 4.1.\n",
      "\n",
      "## 3.3 Computational Cost of Affine Quantization\n",
      "\n",
      "While both affine and scale quantization enable the use of integer arithmetic, affine quantization leads to more computationally expensive inference. As shown in equation 10, scale quantization results in an integer matrix multiply, followed by a point-wise floating-point multiplication. Given that a typical dot-product in a DNN comprises 100s to 1000s of multiply-add operations, a single floating-point operation at the end is a negligible cost. Furthermore, if per-tensor quantization is used for both arguments, a single floating-point multiplier is needed and is part of the GEMM API (often referred to as alpha) in BLAS libraries [11].\n",
      "\n",
      "Affine quantization yields a more complex expression:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Computation can be broken down into three terms, as annotated in Equation 11. The first term is the integer dot product, just as in scale quantization (Equation 10). The second term consists of only integer weights and zero-points. As a result, this term can be computed offline, only adding an element-wise addition at inference time. If the layer has a bias then this term can be folded in without increasing inference cost. The third term, however, involves the quantized input matrix X q , and thus cannot be computed offline. This extra computation, depending on implementation, can introduce considerable overhead, reducing or even eliminating the throughput advantage that integer math pipelines have over reduced precision floating-point. Note that this extra computation is incurred only if affine quantization is used for the weight matrix. Thus, to maximize inference performance we recommend using scale quantization for weights. While affine quantization could be used for activations without a performance penalty, we show in later sections that scale quantization is sufficient for int8 quantization of all the networks we studied.\n",
      "\n",
      "Figure 2: Histogram of input activations to layer 3 in ResNet50 and calibrated ranges\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## 3.4 Calibration\n",
      "\n",
      "Calibration is the process of choosing α and β for model weights and activations. For simplicity we describe calibration of a symmetric range, as needed for scale quantization. In this paper we consider three calibration methods:\n",
      "\n",
      "Max : Use the maximum absolute value seen during calibration [52].\n",
      "\n",
      "Entropy : Use KL divergence to minimize information loss between the original floating-point values and values that could be represented by the quantized format. This is the default method used by TensorRT [36].\n",
      "\n",
      "Percentile : Set the range to a percentile of the distribution of absolute values seen during calibration [33]. For example, 99% calibration would clip 1% of the largest magnitude values.\n",
      "\n",
      "Figure 2 shows a log scaled histogram of activations feeding into layer1.0.conv2 of ResNet50. Ranges derived from max, entropy, and 99.99% percentile calibration are shown with dashed lines. Note that the activations are strictly non-negative because this layer directly follows a ReLU activation function [39]. Max calibration represents the largest value in the distribution, maintaining the full range while having low precision. Clipping the distribution trades off a large clipping error on a few outlier values for smaller rounding errors on a majority of the values. Both entropy and percentile calibration clip some outlier values in order to increase the resolution of inlier values.\n",
      "\n",
      "## 4 Post Training Quantization\n",
      "\n",
      "In this section we evaluate various Post Training Quantization (PTQ) parameter choices, as described in Section 3. Quantization parameters are calibrated offline by processing the trained model weights and activations generated by running inference on a sample dataset, no further training is involved. These quantization parameters are evaluated on a variety of neural network tasks and models, summarized in Table 2. More details on these networks can be found in Appendix A. The selected models comprise multiple types of network architectures: convolutional feed forward networks, recurrent networks, and attention-based networks. We report accuracy metrics computed on the evaluation set of the corresponding dataset. Metrics for all tasks are reported as percentages, where higher is better and 100% is a perfect score. For metric consistency, we report word accuracy (WAcc) for speech recognition instead of the more commonly used Word Error Rate (WER), where WAcc = 100% -WER . Note that accuracy metrics for different tasks are computed in very different ways, thus it is not meaningful to compare absolute changes in accuracy when quantizing different models. Therefore, when discussing accuracy impact we will refer to the relative accuracy change, computed by (acc int 8 -acc fp 32 ) / acc fp 32 .\n",
      "\n",
      "Our experiments are conducted using PyTorch [42], with custom quantization operations. We focus on quantizing the computationally intensive operations, including convolutions, linear (fully-connected) layers, LSTM cells, projection layers and other matrix multiplications. Most of the other layers, such as softmax and batch normalization, are not quantized unless stated otherwise. An operation is quantized by quantizing all of its inputs (e.g. weights and activations). The output of a quantizated operation is not quantized to int8 because the operation that follows it may require higher precision, e.g. nonlinear operations. Furthermore, consecutive operations can be executed with a fused implementation, avoiding memory reads and writes for the intermediate values. Therefore we leave quantization of the output activations to the input of the next operation. Appendix C discusses how batch normalization can be eliminated by folding it into the preceding layer for inference.\n",
      "\n",
      "Table 2: Summary of networks and pre-trained model accuracy\n",
      "\n",
      "| Task               | Model                                                                                                                                 | Accuracy                                                    | Metric     | Dataset (evaluation set)   |\n",
      "|--------------------|---------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|------------|----------------------------|\n",
      "| Classification     | MobileNet v1 MobileNet v2 ResNet50 v1.5 ResNet152 v1.5 Inception v3 Inception v4 ResNeXt50 ResNeXt101 EfficientNet b0 EfficientNet b3 | 71.88 71.88 76.16 78.32 77.34 79.71 77.61 79.30 76.85 81.61 | Top1       | ImageNet 2012 (val)        |\n",
      "| Detection          | Faster R-CNN Mask R-CNN Retinanet                                                                                                     | 36.95 37.89 39.30                                           | mAP        | COCO 2017 (val)            |\n",
      "| Segmentation       | FCN DeepLabV3                                                                                                                         | 63.70 67.40                                                 | mIoU       | COCO 2017 (val)            |\n",
      "| Translation        | GNMT Transformer                                                                                                                      | 24.27 28.27                                                 | BLEU       | WMT16 en-de (newtest2014)  |\n",
      "| Speech Recognition | Jasper                                                                                                                                | 96.09 (3.91)                                                | WAcc (WER) | LibriSpeech (test-clean)   |\n",
      "| Language model     | BERT Large                                                                                                                            | 91.01                                                       | F1         | Squad v1.1 (dev)           |\n",
      "\n",
      "Table 3: Accuracy with int8 quantization of weights only: per-tensor vs per-channel granularity. Fold BN indicates batch norms were folded into the preceding convolution before quantization\n",
      "\n",
      "| Model           |   fp32 |   Per-channel |   Per-channel fold BN |   Per-tensor |   Per-tensor fold BN |\n",
      "|-----------------|--------|---------------|-----------------------|--------------|----------------------|\n",
      "| MobileNet v1    |  71.88 |         71.59 |                 71.59 |        69.58 |                66.88 |\n",
      "| MobileNet v2    |  71.88 |         71.61 |                 71.61 |        71.12 |                70.21 |\n",
      "| ResNet50 v1.5   |  76.16 |         76.14 |                 76.14 |        75.83 |                75.84 |\n",
      "| ResNeXt50       |  77.61 |         77.62 |                 77.62 |        77.48 |                77.45 |\n",
      "| EfficientNet b0 |  76.85 |         76.72 |                 76.72 |        76.68 |                12.93 |\n",
      "\n",
      "## 4.1 Weight Quantization\n",
      "\n",
      "We first evaluate weight quantization in isolation, since their values do not depend on network inputs, and demonstrate that max calibration is sufficient to maintain accuracy for int8 weights. Table 3 compares the accuracy impact of the per-tensor and per-channel quantization granularities, which in Section 3.2 were shown to require minimal compute overheads. While per-tensor quantization results in substantial accuracy losses for some networks, accuracy loss is more pronounced and even catastrophic for EfficientNet once batch-normalization (BN) parameters are folded into convolution layers. BN folding (Appendix C) is a common technique to speed up inference as it completely eliminates this memory-limited operation without changing the underlying mathematics. However, as BN parameters are learned per channel, their folding can result in significantly different weight value distributions across channels. Fortunately, as Table 3 shows, per-channel quantization granularity maintains model accuracy even with BN folding. Table 4 reports per-channel (per-column for linear layers) granularity and indicates that max calibration is sufficient to maintain accuracy when quantizing weights to int8. The rest of the experiments in this paper use per-channel max calibration for weights.\n",
      "\n",
      "## 4.2 Activation Quantization\n",
      "\n",
      "Table 5 shows activation quantization results for different calibration methods: max, entropy and percentiles from 99.9% to 99.9999%. Details on activation calibration can be found in Appendix A. In all cases, weights were quantized per-channel with max calibration as described in Section 4.1.\n",
      "\n",
      "Table 4: Accuracy with int8 quantization of weights only: per-channel granularity, max calibration\n",
      "\n",
      "| Model           |   fp32 |   Accuracy | Relative   | Model        | fp32   | Accuracy   | Relative   |\n",
      "|-----------------|--------|------------|------------|--------------|--------|------------|------------|\n",
      "| MobileNet v1    |  71.88 |      71.59 | -0.40%     | Faster R-CNN | 36.95  | 36.86      | -0.24%     |\n",
      "| MobileNet v2    |  71.88 |      71.61 | -0.38%     | Mask R-CNN   | 37.89  | 37.84      | -0.13%     |\n",
      "| ResNet50 v1.5   |  76.16 |      76.14 | -0.03%     | Retinanet    | 39.30  | 39.20      | -0.25%     |\n",
      "| ResNet152 v1.5  |  78.32 |      78.28 | -0.05%     | FCN          | 63.70  | 63.70      | 0.00%      |\n",
      "| Inception v3    |  77.34 |      77.44 | 0.13%      | DeepLabV3    | 67.40  | 67.40      | 0.00%      |\n",
      "| Inception v4    |  79.71 |      79.64 | -0.09%     | GNMT         | 24.27  | 24.41      | 0.58%      |\n",
      "| ResNeXt50       |  77.61 |      77.62 | 0.01%      | Transformer  | 28.27  | 28.58      | 1.10%      |\n",
      "| ResNeXt101      |  79.3  |      79.29 | -0.01%     | Jasper       | 96.09  | 96.10      | 0.01%      |\n",
      "| EfficientNet b0 |  76.85 |      76.72 | -0.17%     | Bert Large   | 91.01  | 90.94      | -0.08%     |\n",
      "| EfficientNet b3 |  81.61 |      81.55 | -0.07%     |              |        |            |            |\n",
      "\n",
      "Table 5: Post training quantization accuracy. Weights use per-channel or per-column max calibration. Activations use the calibration listed. Best quantized accuracy per network is in bold.\n",
      "\n",
      "| Models          |   fp32 |   Max |   Entropy |   99.9% |   99.99% |   99.999% |   99.9999% |\n",
      "|-----------------|--------|-------|-----------|---------|----------|-----------|------------|\n",
      "| MobileNet v1    |  71.88 | 69.51 |     70.19 |   70.39 |    70.29 |     69.97 |      69.57 |\n",
      "| MobileNet v2    |  71.88 | 69.41 |     70.28 |   70.68 |    71.14 |     70.72 |      70.23 |\n",
      "| ResNet50 v1.5   |  76.16 | 75.82 |     76.05 |   75.68 |    75.98 |     75.97 |      76    |\n",
      "| ResNet152 v1.5  |  78.32 | 77.93 |     78.21 |   77.62 |    78.17 |     78.17 |      78.19 |\n",
      "| Inception v3    |  77.34 | 72.53 |     77.54 |   76.21 |    77.52 |     77.43 |      77.37 |\n",
      "| Inception v4    |  79.71 |  0.12 |     79.6  |   78.16 |    79.63 |     79.12 |      71.19 |\n",
      "| ResNeXt50       |  77.61 | 77.31 |     77.46 |   77.04 |    77.39 |     77.45 |      77.39 |\n",
      "| ResNeXt101      |  79.3  | 78.74 |     79.09 |   78.77 |    79.15 |     79.17 |      79.05 |\n",
      "| EfficientNet b0 |  76.85 | 22.3  |     72.06 |   70.87 |    68.33 |     51.88 |      42.49 |\n",
      "| EfficientNet b3 |  81.61 | 54.27 |     76.96 |   77.8  |    80.28 |     80.06 |      77.13 |\n",
      "| Faster R-CNN    |  36.95 | 36.38 |     36.82 |   35.22 |    36.69 |     36.76 |      36.78 |\n",
      "| Mask R-CNN      |  37.89 | 37.51 |     37.75 |   36.17 |    37.55 |     37.72 |      37.8  |\n",
      "| Retinanet       |  39.3  | 38.9  |     38.97 |   35.34 |    38.55 |     39.19 |      39.19 |\n",
      "| FCN             |  63.7  | 63.4  |     64    |   62.2  |    64    |     63.9  |      63.6  |\n",
      "| DeepLabV3       |  67.4  | 67.2  |     67.4  |   66.4  |    67.4  |     67.5  |      67.4  |\n",
      "| GNMT            |  24.27 | 24.31 |     24.53 |   24.34 |    24.36 |     24.38 |      24.33 |\n",
      "| Transformer     |  28.27 | 21.23 |     21.88 |   24.49 |    27.71 |     20.22 |      20.44 |\n",
      "| Jasper          |  96.09 | 95.99 |     96.11 |   95.77 |    96.09 |     96.09 |      96.03 |\n",
      "| BERT Large      |  91.01 | 85.92 |     37.4  |   26.18 |    89.59 |     90.2  |      90.1  |\n",
      "\n",
      "For most of the networks, there is at least one activation calibration method that achieves acceptable accuracy, except for MobileNets, EfficientNets, Transformer and BERT where the accuracy drop is larger than 1%. Max calibration leads to inconsistent quality across various networks, leading to particularly large accuracy drops for Inception v4, EfficientNets and Transformer, presumably due to their outlier values. 99.9% percentile calibration clips the large magnitude values too aggressive and leads to significant accuracy drops on most networks. The best post training quantization results are achieved with entropy, 99.99%, or 99.999% percentile calibrations, though no single calibration is best for all networks.\n",
      "\n",
      "## 5 Techniques to Recover Accuracy\n",
      "\n",
      "While many networks maintain accuracy after post training quantization, there are cases where accuracy loss is substantial. A number of techniques are available to recover accuracy. The simplest one is partial quantization, described in Section 5.1, which leaves the most sensitive layers unquantized. One also has an option to train networks with quantization, as described in Section 5.2. Finally, there are also approaches that jointly learn the model weights and quantization parameters.\n",
      "\n",
      "Figure 3: Partial quantization of EfficientNet b0, showing the 10 most sensitive layers in order of increasing accuracy. Sensitivity shows the accuracy from the sensitivity analysis when only the corresponding layer inputs are quantized. Partial quantization shows the accuracy when the corresponding layer, and all layers to the left, are not quantized.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Table 6: Partial post training quantization\n",
      "\n",
      "|                 | fp32     |             | Full int8              | Full int8   | Partial int8   | Partial int8   |\n",
      "|-----------------|----------|-------------|------------------------|-------------|----------------|----------------|\n",
      "| Model           | Accuracy | Calibration | Total quantized layers | Accuracy    | Skipped layers | Accuracy       |\n",
      "| MobileNet v1    | 71.88    | max         | 28                     | 69.51       | 2              | 71.50          |\n",
      "| EfficientNet b0 | 76.85    | entropy     | 82                     | 72.06       | 10             | 76.35          |\n",
      "| EfficientNet b3 | 81.61    | 99.99%      | 131                    | 76.96       | 3              | 81.27          |\n",
      "| Transformer     | 28.27    | max         | 121                    | 21.23       | 5              | 28.20          |\n",
      "| BERT large      | 91.01    | max         | 244                    | 85.92       | 141            | 90.41          |\n",
      "\n",
      "## 5.1 Partial Quantization\n",
      "\n",
      "Often just a few quantized layers contribute to most of the accuracy loss of a quantized model. We can trade off some performance to increase accuracy by leaving these sensitive layers unquantized (i.e. leaving their inputs and computation in floating-point). Since quantization of one layer affects the inputs of others, finding the optimal set of layers to quantize can require evaluating an exponential number of configurations. Instead, we propose using a one-at-a-time sensitivity analysis as a more tractable approach to infer which layers contribute most to the accuracy drop.\n",
      "\n",
      "During sensitivity analysis a single layer is quantized at a time, and model accuracy is evaluated. We refer to layers that result in lower accuracy when quantized as being more 'sensitive' to quantization. We sort the layers in descending order of sensitivity, and skip quantization of the most sensitive layers until the desired accuracy is achieved.\n",
      "\n",
      "Figure 3 shows an example of sensitivity analysis and partial quantization of EfficientNet b0. Starting from entropy calibration, we quantize one layer at a time and evaluate accuracy. For clarity we are only showing the 10 most sensitive layers. In this example, skipping the 10 most sensitive layers reduces the relative top-1 accuracy drop to 0.65%. Since there are 82 convolution layers, keeping 10 in floating-point while quantizing the remaining 72 maintains most of the performance benefit.\n",
      "\n",
      "As reported in Table 5, MobileNet v1, EfficientNets, Transformer, and BERT all incurred a substantial loss in accuracy when quantized with various calibrations. We list the results of partial quantization for these networks in Table 6. With the exception of BERT, these networks need to skip quantization of only a few of the most-sensitive layers to recover accuracy to within 1% of the fp32 accuracy. For BERT, sensitivity analysis does not reveal any particular layer that contributes more to the accuracy drop. As a result we cannot identify a small subset of layers to leave in floating-point. To address this we need to consider different approaches. Section 5.2, incorporates quantization with training to recover accuracy. Additionally, Appendix D examines the GELU activation function in BERT and presents a simple augmentation to significantly improve post training quantization accuracy.\n",
      "\n",
      "̂\n",
      "\n",
      "̂\n",
      "\n",
      "Figure 4: 3-bit fake quantization forward and backward pass with STE derivative approximation.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## 5.2 Quantization-Aware Training\n",
      "\n",
      "Quantization Aware Training (QAT) describes the technique of inserting quantization operations in to the neural network before training or fine-tuning, to allow the network to adapt to the quantized weights and activations. Appendix B illustrates how this can lead to a better result. We apply QAT to fine-tuning as it has been shown that starting from a pre-trained network and fine-tuning leads to better accuracy [37, 26] and requires significantly fewer iterations [33]. This also allows us to leverage the calibrated pre-trained models from Section 4. Note that we keep the quantization ranges fixed throughout fine-tuning. Another approach is to learn the ranges, which we evaluate in Section 5.3.\n",
      "\n",
      "A common approach to implementing QAT is to insert fake quantization, also called simulated quantization [26], operations into a floating-point network. Equation 12 defines fake quantization as a quantize and dequantize operation that produces an approximate version of the input, ˆ x ≈ x , where x and ˆ x are both floating-point values.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "We add fake quantization operations at the inputs of the operation we wish to quantize to simulate the effects of quantization. Recall the matrix multiplication example in Section 3.2. Equation 9 is effectively a fake quantized matrix multiplication. After training, we transform the network to enable a quantized integer matrix multiply as shown in Equation 10.\n",
      "\n",
      "One challenge to training in floating-point with quantization is that the quantization operation's derivative is undefined at the step boundaries and zero everywhere else. The derivative is required to compute loss gradients on the backward pass of each training iteration. QAT addresses this by using the Straight-through Estimator (STE) [3] as shown in Figure 4. As defined in Equation 13, STE approximates the derivative of the fake quantization function to be 1 for inputs in the representable range [ β, α ] as defined in Section 3.1.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Table 7 summarizes the best results of both post training quantization and fine-tuned quantization. PTQ best reports the best result for each quantized network in Table 5 and the corresponding calibration. QAT reports the accuracy after fine-tuning using the best calibration as determined by PTQ. Details of the finetuning methodology and the complete set of QAT results can be found in Appendix A.2.\n",
      "\n",
      "As Table 7 shows, quantization-aware fine-tuning improves accuracy in most cases, the only exceptions being ResNeXt101, Mask R-CNN, and GNMT where post training quantization achieves a marginally better result. It is worth noting that for all 3 of these cases the differences in accuracy are essentially at the noise level (differences in accuracy one would observe when training from different random initializations). We do not interpret these cases as evidence that fine-tuning reduces accuracy, they are more likely to indicate that fine-tuning does not appreciably change accuracy beyond run-to-run variation. Likewise, we do not interpret cases where accuracy is higher than fp32 as quantization acting as a regularizer, it is more likely to be noise or the result of the additional fine-tuning. EfficientNet b3 is another case worth examining - as our code did not have auto augmentation [9], used to train the original model, fine-tuning even in fp32 causes a slight accuracy drop to 81.3. Nevertheless, with fine-tuning all networks were able to maintain their accuracy well within 1% of the original pre-trained fp32 model.\n",
      "\n",
      "Table 7: Summary of Post Training Quantization and Quantization Aware Training. PTQ best reports the best accuracy and corresponding calibration for each model. QAT reports accuracy after fine-tuning starting from the best PTQ model.\n",
      "\n",
      "|                 | fp32     | PTQ best    | PTQ best   | PTQ best   | QAT      | QAT      |\n",
      "|-----------------|----------|-------------|------------|------------|----------|----------|\n",
      "| Model           | Accuracy | Calibration | Accuracy   | Relative   | Accuracy | Relative |\n",
      "| MobileNet v1    | 71.88    | 99.9%       | 70.39      | -2.07%     | 72.07    | 0.26%    |\n",
      "| MobileNet v2    | 71.88    | 99.99%      | 71.14      | -1.03%     | 71.56    | -0.45%   |\n",
      "| ResNet50 v1.5   | 76.16    | Entropy     | 76.05      | -0.14%     | 76.85    | 0.91%    |\n",
      "| ResNet152 v1.5  | 78.32    | Entropy     | 78.21      | -0.14%     | 78.61    | 0.37%    |\n",
      "| Inception v3    | 77.34    | Entropy     | 77.54      | 0.26%      | 78.43    | 1.41%    |\n",
      "| Inception v4    | 79.71    | 99.99%      | 79.63      | -0.10%     | 80.14    | 0.54%    |\n",
      "| ResNeXt50       | 77.61    | Entropy     | 77.46      | -0.19%     | 77.67    | 0.08%    |\n",
      "| ResNeXt101      | 79.30    | 99.999%     | 79.17      | -0.16%     | 79.01    | -0.37%   |\n",
      "| EfficientNet b0 | 76.85    | Entropy     | 72.06      | -6.23%     | 76.95    | 0.13%    |\n",
      "| EfficientNet b3 | 81.61    | 99.99%      | 80.28      | -1.63%     | 81.07    | -0.66%   |\n",
      "| Faster R-CNN    | 36.95    | Entropy     | 36.82      | -0.35%     | 36.76    | -0.51%   |\n",
      "| Mask R-CNN      | 37.89    | 99.9999%    | 37.80      | -0.24%     | 37.75    | -0.37%   |\n",
      "| Retinanet       | 39.30    | 99.999%     | 39.19      | -0.28%     | 39.25    | -0.13%   |\n",
      "| FCN             | 63.70    | Entropy     | 64.00      | 0.47%      | 64.10    | 0.63%    |\n",
      "| DeepLabV3       | 67.40    | 99.999%     | 67.50      | 0.15%      | 67.50    | 0.15%    |\n",
      "| GNMT            | 24.27    | Entropy     | 24.53      | 1.07%      | 24.38    | 0.45%    |\n",
      "| Transformer     | 28.27    | 99.99%      | 27.71      | -1.98%     | 28.21    | -0.21%   |\n",
      "| Jasper          | 96.09    | Entropy     | 96.11      | 0.02%      | 96.10    | 0.01%    |\n",
      "| BERT Large      | 91.01    | 99.999%     | 90.20      | -0.89%     | 90.67    | -0.37%   |\n",
      "\n",
      "Table 8: Learned and fixed range fine-tuning accuracy. Activation ranges initialized to max and best PTQ accuracy\n",
      "\n",
      "| Models       |   fp32 |   Fixed max |   Learned from max |   Fixed best |   Learned from best |\n",
      "|--------------|--------|-------------|--------------------|--------------|---------------------|\n",
      "| Inception v3 |  77.34 |       76.43 |              78.33 |        78.43 |               78.5  |\n",
      "| Inception v4 |  79.71 |       68.38 |              73.88 |        80.14 |               80    |\n",
      "| Faster R-CNN |  36.95 |       36.62 |              36.68 |        36.76 |               36.81 |\n",
      "| FCN          |  63.7  |       63.4  |              63.5  |        64.1  |               64    |\n",
      "| Transformer  |  28.27 |       28.42 |              28.08 |        28.21 |               28.39 |\n",
      "| Jasper       |  96.09 |       96.11 |              96.05 |        96.1  |               96.06 |\n",
      "| BERT Large   |  91.01 |       90.29 |              90.55 |        90.67 |               90.61 |\n",
      "\n",
      "## 5.3 Learning Quantization Parameters\n",
      "\n",
      "While the techniques described in the previous sections relied on quantization parameters calibrated on the pre-trained network, it is also possible to jointly learn the quantization parameters along with the model weights. PACT [6] proposed learning the ranges for activation quantization during training. In this section we adopt PACT as an enhancement to our quantization aware fine-tuning procedure. We follow the same fine-tuning schedule as before, described in Appendix A, but allow the ranges of each quantized activation tensor to be learned along with the weights, as opposed to keeping them fixed throughout fine-tuning.\n",
      "\n",
      "Table 8 shows a selection of networks fine-tuned with fixed and learned activation ranges for different initial calibrations. The 'best' calibration refers to the calibration that produced the best accuracy with PTQ, as shown in Table 5. When the activation quantization is initialized with max calibration, learning the range results in higher accuracy than keeping it fixed for most networks. In particular it results in substantial accuracy improvements where fixed max ranges resulted in a significant accuracy drop. However, when activation ranges are initialized the to the best calibration for each network, learning the ranges yield very similar results to fixed ranges. This suggests that learning the ranges does not offer additional benefit for int8 over QAT if activation ranges are already carefully calibrated. However, this may not be the optimal application of PACT. Comparing the learned range results on Inception v4 suggest that when starting from max, the network was not able to learn a good activation ranges in the given fine-tuning schedule. We expect that PACT would be able to learn a better range with longer fine-tuning, or a separate optimization schedule and hyperparameters for the range parameters, such and learning rate and weight decay.\n",
      "\n",
      "Figure 5: Flow chart of our recommended quantization workflow\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## 6 Recommended Workflow\n",
      "\n",
      "Based on the results in Sections 4 and 5, we recommend the following for int8 quantization:\n",
      "\n",
      "- Weights:\n",
      "- -Use scale quantization with per-column/per-channel granularity\n",
      "- -Use a symmetric integer range for quantization [-127, 127]) and max calibration\n",
      "- Activations:\n",
      "- -Use scale quantization with with per-tensor granularity\n",
      "\n",
      "We recommend the following procedure to quantize a pre-trained neural network.\n",
      "\n",
      "- PTQ : Quantize all the computationally intensive layers (convolution, linear, matrix multiplication, etc.) and run activation calibration including max, entropy and 99.99%, 99.999% percentile. If none of the calibrations yield the desired accuracy continue to partial quantization or QAT.\n",
      "- Partial Quantization : Perform sensitivity analysis to identify the most sensitive layers and leave them in floating-point. If the impact on computational performance is not acceptable or an acceptable accuracy cannot be reached, continue to QAT.\n",
      "- QAT : Start from the best calibrated quantized model. Use QAT to fine-tune for around 10% of the original training schedule with an annealing learning rate schedule starting at 1% of the initial training learning rate. Refer to Appendix A.2 for specific hyperparameter choices.\n",
      "\n",
      "Figure 5 summarizes the above workflow in a flowchart.\n",
      "\n",
      "## 7 Conclusions\n",
      "\n",
      "This paper reviewed the mathematical background for integer quantization of neural networks, as well as some performance-related reasons for choosing quantization parameters. We empirically evaluated various choices for int8 quantization of a variety of models, leading to a quantization workflow proposal. Following this workflow we demonstrated that all models we studied can be quantized to int8 with accuracy that either matches or is within 1% of the floating-point model accuracy. This included networks that are challenging for quantization, such as MobileNets and BERT. The workflow involves only post-training quantization, partial quantization, and quantization-aware fine-tuning techniques. Some more complex techniques, such as ADMM and distillation, were not required for int8 quantization of these models. However, these techniques should be evaluated when quantizing to even lower-bit integer representations, which we leave to future work.\n",
      "\n",
      "## References\n",
      "\n",
      "- [1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system for large-scale machine learning. In 12th { USENIX } Symposium on Operating Systems Design and Implementation ( { OSDI } 16) , pages 265-283, 2016.\n",
      "- [2] Chaim Baskin, Eli Schwartz, Evgenii Zheltonozhskii, Natan Liss, Raja Giryes, Alex M Bronstein, and Avi Mendelson. Uniq: Uniform noise injection for non-uniform quantization of neural networks. arXiv preprint arXiv:1804.10969 , 2018.\n",
      "- [3] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 , 2013.\n",
      "- [4] Aishwarya Bhandare, Vamsi Sripathi, Deepthi Karkada, Vivek Menon, Sun Choi, Kushal Datta, and Vikram Saletore. Efficient 8-bit quantization of transformer neural machine language translation model. arXiv preprint arXiv:1906.00532 , 2019.\n",
      "- [5] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, and Evan Shelhamer. cudnn: Efficient primitives for deep learning. arXiv preprint arXiv:1410.0759 , 2014.\n",
      "- [6] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085 , 2018.\n",
      "- [7] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Training deep neural networks with low precision multiplications. arXiv preprint arXiv:1412.7024 , 2014.\n",
      "- [8] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. BinaryConnect: Training Deep Neural Networks with binary weights during propagations. NIPS , 28:3123-3131, 2015.\n",
      "- [9] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 113-123, 2019.\n",
      "- [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.\n",
      "- [11] Jack J Dongarra, Jeremy Du Croz, Sven Hammarling, and Iain S Duff. A set of level 3 basic linear algebra subprograms. ACM Transactions on Mathematical Software (TOMS) , 16(1):1-17, 1990.\n",
      "- [12] Boris Ginsburg, Patrice Castonguay, Oleksii Hrinchuk, Oleksii Kuchaiev, Vitaly Lavrukhin, Ryan Leary, Jason Li, Huyen Nguyen, and Jonathan M Cohen. Stochastic gradient methods with layer-wise adaptive moments for training of deep networks. arXiv preprint arXiv:1905.11286 , 2019.\n",
      "- [13] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149 , 2015.\n",
      "- [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770-778, 2016.\n",
      "\n",
      "- [15] Dan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs). arXiv preprint arXiv:1606.08415 , 2016.\n",
      "- [16] Sepp Hochreiter and Jürgen Schmidhuber. Simplifying neural nets by discovering flat minima. In Advances in neural information processing systems , pages 529-536, 1995.\n",
      "- [17] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861 , 2017.\n",
      "- [18] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. In Advances in neural information processing systems , pages 4107-4115, 2016.\n",
      "- [19] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167 , 2015.\n",
      "- [20] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 2704-2713, 2018.\n",
      "- [21] Sambhav R Jain, Albert Gural, Michael Wu, and Chris H Dick. Trained quantization thresholds for accurate and efficient fixed-point inference of deep neural networks. arXiv preprint arXiv:1903.08066 , 2(3):7, 2019.\n",
      "- [22] Stanisław Jastrz˛ ebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623 , 2017.\n",
      "- [23] Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. In-datacenter performance analysis of a tensor processing unit. SIGARCH Comput. Archit. News , 45(2):1-12, June 2017.\n",
      "- [24] Marcin Junczys-Dowmunt, Kenneth Heafield, Hieu Hoang, Roman Grundkiewicz, and Anthony Aue. Marian: Cost-effective high-quality neural machine translation in c++. arXiv preprint arXiv:1805.12096 , 2018.\n",
      "- [25] Alexander Kozlov, Ivan Lazarevich, Vasily Shamporov, Nikolay Lyalyushkin, and Yury Gorbachev. Neural network compression framework for fast model inference. arXiv preprint arXiv:2002.08679 , 2020.\n",
      "- [26] Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv preprint arXiv:1806.08342 , 2018.\n",
      "- [27] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems , pages 1097-1105, 2012.\n",
      "- [28] Akhilesh Kumar, Sailesh Kottapalli, Ian Steiner, Bob Valentine, Israel Hirsh, Geetha Vearaman, Lily Looi, Mohamed Arafa, Andy Rudoff, Sreenivas Mandava, Bahaa Fahim, and Sujal Vora. Future Intel Xeon Scalable Processor (Codename: Cascade Lake-SP). In Hotchips 2018 , 2018.\n",
      "- [29] Yann LeCun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne E Hubbard, and Lawrence D Jackel. Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems , pages 396-404, 1990.\n",
      "- [30] Dongsoo Lee and Byeongwook Kim. Retraining-based iterative weight quantization for deep neural networks. arXiv preprint arXiv:1805.11233 , 2018.\n",
      "- [31] Cong Leng, Zesheng Dou, Hao Li, Shenghuo Zhu, and Rong Jin. Extremely low bit neural network: Squeeze the last bit out with admm. In Thirty-Second AAAI Conference on Artificial Intelligence , 2018.\n",
      "- [32] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision , pages 2980-2988, 2017.\n",
      "- [33] Jeffrey L McKinstry, Steven K Esser, Rathinakumar Appuswamy, Deepika Bablani, John V Arthur, Izzet B Yildiz, and Dharmendra S Modha. Discovering low-precision networks close to full-precision networks for efficient embedded inference. arXiv preprint arXiv:1809.04191 , 2018.\n",
      "\n",
      "- [34] Naveen Mellempudi, Abhisek Kundu, Dheevatsa Mudigere, Dipankar Das, Bharat Kaul, and Pradeep Dubey. Ternary neural networks with fine-grained quantization. arXiv preprint arXiv:1705.01462 , 2017.\n",
      "- [35] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. arXiv preprint arXiv:1710.03740 , 2017.\n",
      "- [36] Szymon Migacz. Nvidia 8-bit inference width tensorrt. In GPU Technology Conference , 2017.\n",
      "- [37] Asit Mishra and Debbie Marr. Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy. arXiv preprint arXiv:1711.05852 , 2017.\n",
      "- [38] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE International Conference on Computer Vision , pages 1325-1334, 2019.\n",
      "- [39] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10) , pages 807-814, 2010.\n",
      "- [40] NVIDIA. NVIDIA Turing GPU architecture: Graphics reinvented. https://www.nvidia.com/ content/dam/en-zz/Solutions/designvisualization/technologies/turing-architecture/ NVIDIATuring-Architecture-Whitepaper.pdf , 2018.\n",
      "- [41] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 5206-5210. IEEE, 2015.\n",
      "- [42] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32 , pages 8024-8035. Curran Associates, Inc., 2019.\n",
      "- [43] Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and quantization. arXiv preprint arXiv:1802.05668 , 2018.\n",
      "- [44] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint arXiv:1710.05941 , 2017.\n",
      "- [45] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In European conference on computer vision , pages 525-542. Springer, 2016.\n",
      "- [46] Manuele Rusci, Alessandro Capotondi, and Luca Benini. Memory-driven mixed low precision quantization for enabling deep network inference on microcontrollers. arXiv preprint arXiv:1905.13082 , 2019.\n",
      "- [47] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) , 115(3):211-252, 2015.\n",
      "- [48] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4510-4520, 2018.\n",
      "- [49] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In Thirty-first AAAI conference on artificial intelligence , 2017.\n",
      "- [50] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 2818-2826, 2016.\n",
      "- [51] Mingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946 , 2019.\n",
      "- [52] Vincent Vanhoucke, Andrew Senior, and Mark Z. Mao. Improving the speed of neural networks on cpus. In Deep Learning and Unsupervised Feature Learning Workshop, NIPS , 2011.\n",
      "- [53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems , pages 5998-6008, 2017.\n",
      "\n",
      "- [54] Pete Warden. Why gemm is at the heart of deep learning. https://petewarden.com/2016/05/03/ how-to-quantize-neural-networks-with-tensorflow , 2015.\n",
      "- [55] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144 , 2016.\n",
      "- [56] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 1492-1500, 2017.\n",
      "- [57] Chris Ying, Sameer Kumar, Dehao Chen, Tao Wang, and Youlong Cheng. Image classification at supercomputer scale. arXiv preprint arXiv:1811.06992 , 2018.\n",
      "- [58] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8bert: Quantized 8bit bert. arXiv preprint arXiv:1910.06188 , 2019.\n",
      "- [59] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160 , 2016.\n",
      "- [60] Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. arXiv preprint arXiv:1612.01064 , 2016.\n",
      "- [61] Neta Zmora, Guy Jacob, Lev Zlotnik, Bar Elharar, and Gal Novik. Neural network distiller: A python package for dnn compression research. arXiv preprint arXiv:1910.12232 , 2019.\n",
      "\n",
      "## Appendices\n",
      "\n",
      "## A Evaluation Details\n",
      "\n",
      "## A.1 Model Definitions\n",
      "\n",
      "Table 9: Network details\n",
      "\n",
      "| Model                                                               | Configuration                                 | Calibration samples      | Source                                                            |\n",
      "|---------------------------------------------------------------------|-----------------------------------------------|--------------------------|-------------------------------------------------------------------|\n",
      "| MobileNet v1                                                        | width_mult=1.0                                | 1024                     | github.com/marvis/pytorch-mobilenet                               |\n",
      "| MobileNet v2 ResNet50 v1.5 ResNet152 v1.5 Inception v3 Inception v4 | width_mult=1.0                                | 1024 1024 1024 1024 1024 | github.com/pytorch/vision                                         |\n",
      "| EfficientNet b0 EfficientNet b3                                     |                                               | 1024 1024                | github.com/lukemelas/efficientnet-pytorch                         |\n",
      "| Faster R-CNN Mask R-CNN FCN DeepLabV3                               | resnet50-fpn resnet50-fpn resnet101 resnet101 | 512 512 512 512          | github.com/pytorch/vision                                         |\n",
      "| Retinanet                                                           | resnext101-32x4d-fpn                          | 512                      | github.com/open-mmlab/mmdetection                                 |\n",
      "| Transformer GNMT                                                    | vaswani_en_de_big 4 layers                    | 14336 tokens 128         | github.com/pytorch/fairseq github.com/nvidia/deeplearningexamples |\n",
      "| Jasper Bert Large                                                   | fine-tuned for QA                             | full dev-clean 128       | github.com/huggingface/pytorch-transformers                       |\n",
      "\n",
      "Table 9 lists additional details of the models listed in Table 2. We evaluated a large variety of CNNs for image classification. MobileNets are small networks that target inference on mobile devices [17, 48]. They are parameterized to scale to various channel widths and image resolutions. In this paper we evaluate the base configurations with width multiplier 1 and resolution 224x224. We also evaluated a number of larger CNNs [14, 56, 50, 49], including EfficientNets [51], which achieve state-of-the-art accuracy on ImageNet. All CNNs use 224x224 inputs except for Inception v3 and v4, which use 299x299. We evaluated two detection and two segmentation networks from Torchvision, and an additional segmentation network, RetinaNet [32]. We evaluated two translation models, the 4 layers GNMT model [55] and the large configuration of Transformer [53]. For speech recognition we evaluated Jasper which achieves state-of-the-art WER on public speech datasets [41]. For language modeling we use BERT large uncased and fine-tuned for question answering.\n",
      "\n",
      "Models were calibrated with the number of samples listed from the training set of the respective dataset listed in Table 2, except for Jasper, which was calibrated on the dev set and evaluated on the test set. PyTorch implementations of all the models along were provided by the listed source repositories. We used the pre-trained weights provided by each repository, except for MobileNet v1 and EfficientNets where pre-trained weights were not available. MobileNet v1 was trained using the reference training script and hyperparameters for MobileNet v2 from Torchvision. Pre-trained weights for EfficientNets were converted to PyTorch from weights provided by TensorFlow [1] 1 .\n",
      "\n",
      "1 https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet\n",
      "\n",
      "Table 10: Fine-tuning schedule and configuration\n",
      "\n",
      "| Task/Model     | Dataset       | Optimizer    |   Epochs |   Initial learning rate | Batch size                 |\n",
      "|----------------|---------------|--------------|----------|-------------------------|----------------------------|\n",
      "| Classification | ImageNet 2012 | SGD          |       15 |                 0.001   | 256                        |\n",
      "| Detection      | COCO 2017     | SGD          |        3 |                 2.5e-05 | 16                         |\n",
      "| Segmentation   | COCO 2017     | SGD          |        3 |                 2.5e-05 | 32                         |\n",
      "| Transformer    | WMT16 en-de   | ADAM         |        3 |                 0.0005  | max tokens = 3584          |\n",
      "| GNMT           | WMT16 en-de   | SGD          |        1 |                 0.002   | 1024, max seq. length = 50 |\n",
      "| Jasper         | LibriSpeech   | NovoGrad[12] |       80 |                 0.015   | 256                        |\n",
      "| BERT           | SQuAD v1.1    | ADAM         |        2 |                 3e-05   | 12, max seq. length = 384  |\n",
      "\n",
      "Table 11: Fine-tuned quantization. Best accuracy in bold. Accuracy from best PTQ calibration per network underlined.\n",
      "\n",
      "| Models          |   fp32 |   Max |   Entropy |   99.9% |   99.99% |   99.999% |   99.9999% |\n",
      "|-----------------|--------|-------|-----------|---------|----------|-----------|------------|\n",
      "| MobileNet v1    |  71.88 | 71.8  |     72.11 |   72.07 |    72.14 |     71.89 |      71.91 |\n",
      "| MobileNet v2    |  71.88 | 71.11 |     71.5  |   71.48 |    71.56 |     71.28 |      71.34 |\n",
      "| ResNet50 v1.5   |  76.16 | 76.68 |     76.85 |   76.59 |    76.67 |     76.77 |      76.81 |\n",
      "| ResNet152 v1.5  |  78.32 | 78.64 |     78.61 |   78.61 |    78.69 |     78.65 |      78.65 |\n",
      "| Inception v3    |  77.34 | 76.43 |     78.43 |   78.33 |    78.49 |     78.36 |      78.38 |\n",
      "| Inception v4    |  79.71 | 68.38 |     80.07 |   80.01 |    80.14 |     79.94 |      78.82 |\n",
      "| ResNeXt50       |  77.61 | 77.38 |     77.67 |   77.48 |    77.56 |     77.51 |      77.51 |\n",
      "| ResNeXt101      |  79.3  | 78.98 |     78.99 |   79    |    78.99 |     79.01 |      79.04 |\n",
      "| EfficientNet b0 |  76.85 | 76.16 |     76.95 |   76.85 |    77.09 |     76.98 |      76.63 |\n",
      "| EfficientNet b3 |  81.61 | 80.51 |     80.63 |   80.62 |    81.07 |     81.09 |      80.92 |\n",
      "| Faster R-CNN    |  36.95 | 36.62 |     36.76 |   36.31 |    36.76 |     36.83 |      36.76 |\n",
      "| Mask R-CNN      |  37.89 | 37.63 |     37.74 |   37.26 |    37.67 |     37.76 |      37.75 |\n",
      "| Retinanet       |  39.3  | 39.03 |     39.11 |   37.76 |    38.97 |     39.25 |      39.2  |\n",
      "| FCN             |  63.7  | 63.4  |     64.1  |   63.9  |    64.2  |     63.9  |      63.4  |\n",
      "| DeepLabV3       |  67.4  | 67.1  |     67.3  |   66.9  |    67.2  |     67.5  |      67.2  |\n",
      "| GNMT            |  24.27 | 24.49 |     24.38 |   24.35 |    24.41 |     24.48 |      24.35 |\n",
      "| Transformer     |  28.27 | 28.42 |     28.46 |   28.23 |    28.21 |     28.04 |      28.1  |\n",
      "| Jasper          |  96.09 | 96.11 |     96.1  |   95.23 |    95.94 |     96.01 |      96.08 |\n",
      "| BERT Large      |  91.01 | 90.29 |     90.14 |   89.97 |    90.5  |     90.67 |      90.6  |\n",
      "\n",
      "## A.2 Quantization Aware Training\n",
      "\n",
      "Table 10 shows the fine-tuning hyperparameters used in the quantization aware fine-tuning experiments. For networks that are trained on multiple datasets (detection/segmentation networks and BERT) we only fine-tuned on the final dataset (COCO and SQuAD). In general, only the initial learning rate value and learning rate schedule are changed from the original training session. We fine-tune for around 1/10th of the original training steps. The fine-tuning learning rate starts at 1/100th of the initial value used in the original training session and is decayed down to 1/100th of the initial fine-tuning learning rate. BERT is an exception. Since it pre-trains a language model and only fine-tunes on SQuAD for 2 epochs, we instead repeat the full fine-tuning schedule for QAT. We used a cosine annealing learning rate schedule which follows the monotonically decreasing half period of the cosine function.\n",
      "\n",
      "Table 11 shows fine-tuned quantization accuracy for all networks and activation range calibration settings. Note that we always use the full per-column/per-channel range for weights (max calibration). It shows that with fine-tuning, accuracy improves for almost all the cases, especially those that suffer large accuracy drops after PTQ, for example max calibration. For many of the models, the best PTQ calibration is also the best calibration for QAT, indicated by results that are both bold and underlined. Even when QAT achieves higher accuracy with a different calibration, the difference in results is marginal. This result suggests that evaluating multiple activation calibrations during PTQ is a good heuristic to choose a calibration for QAT.\n",
      "\n",
      "Figure 6: Example 1D loss function. The model, w , is scale quantized with scale factor 1. a) PTQ: model converges to a narrow minimum. b) QAT: model finds a wide minimum with lower loss quantization points.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## B Intuition for QAT\n",
      "\n",
      "To gain some intuition for why quantization-aware training may improve accuracy of the quantized model, consider the simple example in Figure 6. Neural networks are trained by minimizing a loss function with stochastic gradient descent. Loss gradients with respect to the network weights, δL δw , are computed and weights are iteratively updated in the direction of the negative gradient until the model converges to some minimum. Figure 6a shows a one-dimensional loss function for a model with a single parameter, w , that has converged to a local minimum, w ≈ -0 . 5 . When post training quantization is applied, with a scale factor of 1 for sake of example, the weight is quantized to the nearest integer, w q = -1 , causing a significant increase in the loss. In such a case we say that the model has converged to a 'narrow' minimum, since a small change in the weight leads to a large change in loss.\n",
      "\n",
      "By training with quantization, we may potentially avoid these narrow minima by computing gradients with respect to the quantized weights, as shown in Figure 6b. In doing so, narrow minima will result in larger gradients, potentially allowing the model to explore the loss landscape for 'wide' [22] or 'flat' [16, 30] minima, where quantization points have lower loss, and thus higher accuracy.\n",
      "\n",
      "## C Batch normalization folding\n",
      "\n",
      "Batch normalization folding is a common inference optimization applied to neural networks [20]. At inference, batch normalization layers performs the affine operation shown in Equation 14:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where β , γ , E [ y ] , and V ar [ y ] are determined during training and fixed during inference, and glyph[epsilon1] is a constant [19]. Typically, following a fully connected layer the batch normalization is computed per activation. Consider a fully connected layer that performs the matrix multiplication and bias add shown in Equation 15:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "When the fully connected layer is followed by a batch normalization layer, z = BN( x W + b ) , the batch normalization can be folded into the weights and biases of the fully connected layer, as show in Equation 16:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "resulting in a fully connected layer performing the operation z = x W ′ + b ′ . Since convolutions can be mapped to fully connected layers, and batch normalization in CNNs is per channel, we can apply the same optimization.\n",
      "\n",
      "## D Novel activation functions\n",
      "\n",
      "Two more recently developed activation functions are Swish (Equation 17) [44] and GELU (Equation 18) [15], used in EfficientNets and BERT, respectively.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "These activation functions, shown in Figure 7a, are both smooth and ReLU-like but with small, bounded negative output ranges. Specifically, Swish has an output range of [ -0 . 2785 , ∞ ] and GELU has an output range of [ -0 . 1700 , ∞ ] . This poses a challenge for uniform quantization as it should represent both small negative values and large positive values.\n",
      "\n",
      "Figure 7b shows the composition of GELU and fake quantization with different symmetric ranges. If the output of GELU is quantized to [-50, 50], then all negative values will round to zero. However, if we restrict the range to [-10, 10] then two negative values can be represented. Table 12 shows the accuracy of post training quantization with GELU outputs clipped to 10 (GELU10), and then calibrated with max calibration. Just by clipping the output of GELU we can achieve the best post training quantization accuracy with a simple max calibration, exceeding the previous best activation calibration by 0.46 F1. Furthermore, this result almost matches the best QAT result of 90.67 F1.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Figure 7: Quantization mapping of real values to int8\n",
      "\n",
      "| Model      |   fp32 |   Max |   Entropy |   99.9% |   99.99% |   99.999% |   99.9999% |   Max with GELU10 |\n",
      "|------------|--------|-------|-----------|---------|----------|-----------|------------|-------------------|\n",
      "| BERT Large |  91.01 | 85.92 |      37.4 |   26.18 |    89.59 |      90.2 |       90.1 |             90.66 |\n",
      "\n",
      "Table 12: BERT int8 post training quantization. Comparing previous calibration results to max with GELU10.\n",
      "Document 12:\n",
      "## DEFECTIVE COLOURING OF GRAPHS EXCLUDING A SUBGRAPH OR MINOR\n",
      "\n",
      "PATRICE OSSONA DE MENDEZ, SANG-IL OUM, AND DAVID R. WOOD\n",
      "\n",
      "Abstract. Archdeacon (1987) proved that graphs embeddable on a fixed surface can be 3-coloured so that each colour class induces a subgraph of bounded maximum degree. Edwards, Kang, Kim, Oum and Seymour (2015) proved that graphs with no K t +1 -minor can be t -coloured so that each colour class induces a subgraph of bounded maximum degree. We prove a common generalisation of these theorems with a weaker assumption about excluded subgraphs. This result leads to new defective colouring results for several graph classes, including graphs with linear crossing number, graphs with given thickness (with relevance to the earth-moon problem), graphs with given stack- or queue-number, linklessly or knotlessly embeddable graphs, graphs with given Colin de Verdi` ere parameter, and graphs excluding a complete bipartite graph as a topological minor.\n",
      "\n",
      "## 1. Introduction\n",
      "\n",
      "A graph G is k -colourable with defect d , or d -improper k -colourable , or simply ( k, d ) -colourable , if each vertex v of G can be assigned one of k colours so that at most d neighbours of v are assigned the same colour as v . That is, each monochromatic subgraph has maximum degree at most d . Obviously the case d = 0 corresponds to the usual notion of graph colouring. Cowen et al. [25] introduced the notion of defective graph colouring, and now many results for various graph classes are known. This paper presents ( k, d )-colourability results for graph classes defined by an excluded subgraph, subdivision or minor. Our primary focus is on minimising the number of colours k rather than the degree bound d . This viewpoint motivates the following definition. The defective chromatic number of a graph class C is the minimum integer k (if such a k exists) for which there exists an integer d such that every graph in C is ( k, d )-colourable.\n",
      "\n",
      "Consider the following two examples: Archdeacon [3] proved that for every surface Σ, the defective chromatic number of graphs embeddable in Σ equals 3. And Edwards, Kang, Kim, Oum, and Seymour [31] proved that the class of graphs containing no K t +1 minor has defective chromatic number t (which is a weakening of Hadwiger's conjecture). This paper proves a general theorem that implies both these\n",
      "\n",
      "Date : April 5, 2017.\n",
      "\n",
      "Ossona de Mendez is supported by grant ERCCZ LL-1201 and by the European Associated Laboratory 'Structures in Combinatorics' (LEA STRUCO), and partially supported by ANR project Stint under reference ANR-13-BS02-0007. Research of Wood is supported by the Australian Research Council.\n",
      "\n",
      "Figure 1. The graph K ∗ 7 , 13 .\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "results as special cases. Indeed, our theorem only assumes an excluded subgraph, which enables it to be applied in more general settings.\n",
      "\n",
      "For integers s, t glyph[greaterorequalslant] 1, let K ∗ s,t be the bipartite graph obtained from K s,t by adding ( s 2 ) new vertices, each adjacent to a distinct pair of vertices in the colour class of s vertices in K s,t (see Figure 1). Our main result shows that every graph excluding K ∗ s,t as a subgraph is ( s, d )-colourable, where d depends on s , t and certain density parameters, which we now introduce.\n",
      "\n",
      "For a graph G , the most natural density parameter to consider is the maximum average degree , denoted mad( G ), which is the maximum of the average degrees of all subgraphs of G ; that is,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Closely related to maximum average degree is degeneracy. A graph G is k -degenerate if every subgraph of G has a vertex of degree at most k . Every graph is glyph[floorleft] mad( G ) glyph[floorright] -degenerate. It follows that the chromatic number (and even the choice number) of a graph G is at most glyph[floorleft] mad( G ) + 1 glyph[floorright] . Bounds on defective colourings have also been obtained in terms of maximum average degree. In particular, Havet and Sereni [37] proved that every graph G with mad( G ) &lt; k + kd k + d is ( k, d )-colourable, and that there exist non-( k, d )-colourable graphs whose maximum average degree tends to 2 k when d goes to infinity, which shows the limit of the maximum average degree approach for defective colouring (see also [9, 12-14, 19, 23, 28, 51]).\n",
      "\n",
      "In addition to maximum average degree we consider the density of shallow topological minors (see [65] for more on this topic). A graph H is a minor of a graph G if a graph isomorphic to H can be obtained from a subgraph of G by contracting edges. A graph H is a topological minor of a graph G if a subdivision of H is a subgraph of G . A ( glyph[lessorequalslant] k ) -subdivision of a graph G is a graph obtained from G by subdividing each edge at most k times, or equivalently replacing each edge by a path of length at most k + 1. The exact 1 -subdivision of G is the graph obtained from G by subdividing each edge exactly once. For a half integer r (that is, a number r\n",
      "\n",
      "such that 2 r is an integer), a graph H is a depthr topological minor of a graph G if a ( glyph[lessorequalslant] 2 r )-subdivision of H is a subgraph of G . For a graph G , let G ˜ glyph[triangleinv] r be the set of all depthr topological minors of G . The topological greatest reduced average density (or top-grad ) with rank r of a graph G is defined as\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Note that ˜ ∇ 1 ( G ) glyph[greaterorequalslant] ˜ ∇ 0 ( G ) = 1 2 mad( G ).\n",
      "\n",
      "The following is our main result (see Theorem 2.3 for a more precise version).\n",
      "\n",
      "Theorem 1.1. Every graph G with no K ∗ s,t subgraph has an ( s, d ) -colouring, where d depends on s , t , mad( G ) and ˜ ∇ 1 / 2 ( G )\n",
      "\n",
      "We actually prove this result in the setting of defective list colourings, introduced by Eaton and Hull [30] and since studied by several authors [16-18, 37, 60, 80, 86, 88, 91-93]. A k -list assignment of a graph G is a function L that assigns a set L ( v ) of exactly k colours to each vertex v ∈ V ( G ). Then an L -colouring of G is a function that assigns a colour in L ( v ) to each vertex v ∈ V ( G ). If an L -colouring has the property that every vertex v has at most d neighbours having the same colour as v , then we call it an L -colouring with defect d , or d -defective . A graph G is k -choosable with defect d , or d -improper k -choosable , or simply ( k, d ) -choosable if for every k -list assignment L of G , there exists a d -defective L -colouring of G . For example, the result of Havet and Sereni [37] mentioned above applies in the setting of ( k, d )-choosability. The defective choice number of a graph class C is the minimum integer k (if such a k exists) for which there exists an integer d such that every graph in C is ( k, d )-choosable.\n",
      "\n",
      "The paper is organised as follows. Section 2 presents the proof of our main result (a choosability version of Theorem 1.1). The subsequent sections present several applications of this main result. In particular, Section 3 gives results for graphs with no 4-cycle, and other graph classes defined by an excluded subgraph. Section 4 presents defective 3-colourability results for graphs drawn on surfaces, even allowing for a linear number of crossings, thus generalising the result of Archdeacon mentioned above. Section 5 gives bounds on the defective chromatic number and defective choice number of graphs with given thickness, and of graphs with given stack- or queue-number. One result here is relevant to the earth-moon problem, which asks for the chromatic number of graphs with thickness 2. While it is open whether such graphs are 11-colourable, we prove they are 11-colourable with defect 2. Section 6 studies the defective chromatic number of minor-closed classes. We determine the defective chromatic number and defective choice number of linklessly embeddable graphs, knotlessly embeddable graphs, and graphs with given Colin de Verdi` ere parameter. We then prove a strengthening of the result of Edwards et al. [31] mentioned above. Finally, we formulate a conjecture about the defective chromatic number of H -minor-free graphs, and prove several special cases of it.\n",
      "\n",
      "## 2. Main Proof\n",
      "\n",
      "An edge e in a graph is glyph[lscript] -light if both endpoints of e have degree at most glyph[lscript] . There is a large literature on light edges in graphs; see [8, 10, 15, 46, 48, 49] for example. Many of our results rely on the following sufficient condition for ( k, d )-choosability. Its proof is essentially identical to the proof of a lemma by Lih et al. [60, Lemma 1]. ˇ Skrekovski [80] proved a similar result with k = 1.\n",
      "\n",
      "Lemma 2.1. For integers glyph[lscript] glyph[greaterorequalslant] k glyph[greaterorequalslant] 1 , if every subgraph H of a graph G has a vertex of degree at most k or an glyph[lscript] -light edge, then G is ( k +1 , glyph[lscript] -k ) -choosable.\n",
      "\n",
      "glyph[negationslash]\n",
      "\n",
      "Proof. Let L be a ( k +1)-list assignment for G . We prove by induction on | V ( H ) | + | E ( H ) | that every subgraph H of G is L -colourable with defect glyph[lscript] -k . The base case with | V ( H ) | + | E ( H ) | = 0 is trivial. Consider a subgraph H of G . If H has a vertex v of degree at most k , then by induction H -v is L -colourable with defect glyph[lscript] -k , and there is a colour in L ( v ) used by no neighbour of v which can be assigned to v . Now assume that H has minimum degree at least k +1. By assumption, H contains an glyph[lscript] -light edge xy . By induction, H -xy has an L -colouring c with defect glyph[lscript] -k . If c ( x ) = c ( y ), then c is also an L -colouring of H with defect glyph[lscript] -k . Now assume that c ( x ) = c ( y ). We may further assume that c is not an L -colouring of H with defect glyph[lscript] -k . Without loss of generality, x has exactly glyph[lscript] -k + 1 neighbours (including y ) coloured by c ( x ). Since deg H ( x ) glyph[lessorequalslant] glyph[lscript] , there are at most k -1 neighbours not coloured by c ( x ). Since L ( v ) contains k colours different from c ( x ), there is a colour used by no neighbour of x which can be assigned to x . glyph[square]\n",
      "\n",
      "To state our main result, we use the following auxiliary function. For positive integers s , t and positive reals δ and δ 1 , let\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Lemma 2.2. For positive integers s , t , and positive reals δ , δ 1 , let glyph[lscript] = glyph[floorleft] N 1 ( s, t, δ, δ 1 ) glyph[floorright] . If every subgraph of a graph G has average degree at most δ and every graph whose exact 1 -subdivision is a subgraph of G has average degree at most δ 1 , then at least one of the following holds:\n",
      "\n",
      "- (i) G contains a K ∗ s,t subgraph,\n",
      "- (ii) G has a vertex of degree at most s -1 ,\n",
      "- (iii) G has an glyph[lscript] -light edge.\n",
      "\n",
      "Proof. The case s = 1 is simple: If (i) does not hold, then ∆( G ) glyph[lessorequalslant] t -1, in which case either G has no edges and (ii) holds, or G has an edge and (iii) holds since glyph[lscript] = t -1. Now assume that s &gt; 1.\n",
      "\n",
      "Assume for contradiction that G has no K ∗ s,t subgraph, that every vertex of G has degree at least s (thus s glyph[lessorequalslant] δ ), and that G contains no glyph[lscript] -light edge.\n",
      "\n",
      "Let A be the set of vertices in G of degree at most glyph[lscript] . Let B := V ( G ) \\ A . Let a := | A | and b := | B | . Since G has a vertex of degree at most δ and δ glyph[lessorequalslant] glyph[lscript] , we deduce that a &gt; 0. Note that no two vertices in A are adjacent.\n",
      "\n",
      "Since the average degree of G is at most δ ,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "That is,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Let G ′ be a minor of G obtained from G -E ( G [ B ]) by greedily finding a vertex w ∈ A having a pair of non-adjacent neighbours x , y in B and replacing w by an edge joining x and y (by deleting all edges incident with w except xw , yw and contracting xw ), until no such vertex w exists.\n",
      "\n",
      "Let A ′ := V ( G ′ ) \\ B and a ′ := | A ′ | . Clearly the exact 1-subdivision of G ′ [ B ] is a subgraph of G . So every subgraph of G ′ [ B ] has average degree at most δ 1 . Since G ′ [ B ] contains at least a -a ′ edges,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "A clique in a graph is a set of pairwise adjacent vertices. Let M be the number of cliques of size s in G ′ [ B ]. Since G ′ [ B ] is glyph[floorleft] δ 1 glyph[floorright] -degenerate,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "(see [65, p. 25] or [87]). If s = 2, then the following better inequality holds:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "For each vertex v ∈ A ′ , since v was not contracted in the creation of G ′ , the set of neighbours of v in B is a clique of size at least s . Thus if a ′ &gt; M ( t -1), then there are at least t vertices in A ′ sharing at least s common neighbours in B . These t vertices and their s common neighbours in B with the vertices in A -A ′ form a K ∗ s,t subgraph of G , contradicting our assumption. Thus,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "By (1), (2) and (3),\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "contradicting the definition of glyph[lscript]\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      ". glyph[square]\n",
      "\n",
      "Lemmas 2.1 and 2.2 imply our main result:\n",
      "\n",
      "Theorem 2.3. For integers s, t glyph[greaterorequalslant] 1 , every graph G with no K ∗ s,t subgraph is ( s, d ) -choosable, where d := glyph[floorleft] N 1 ( s, t, mad( G ) , 2 ˜ ∇ 1 / 2 ( G )) glyph[floorright]s +1 .\n",
      "\n",
      "Proof. By definition, every subgraph of G has average degree at most mad( G ) and every graph whose exact 1-subdivision is a subgraph of G has average degree at most 2 ˜ ∇ 1 / 2 ( G ). By Lemma 2.2, every subgraph of G has a vertex of degree at most s -1 or has an glyph[lscript] -light edge, where glyph[lscript] := glyph[floorleft] N 1 ( s, t, mad( G ) , 2 ˜ ∇ 1 / 2 ( G )) glyph[floorright] . By Lemma 2.1 with k = s -1, we have that G is ( s, glyph[lscript] -s +1)-choosable. glyph[square]\n",
      "\n",
      "The following recursive construction was used by Edwards et al. [31] to show that their theorem mentioned above is tight. We use this example repeatedly, so include the proof for completeness. If s = 2, then let G ( s, N ) := K 1 ,N +1 . If s &gt; 2, then let G ( s, N ) be obtained from the disjoint union of N +1 copies of G ( s -1 , N ) by adding one new vertex v adjacent to all other vertices. Note that Havet and Sereni [37] used a similar construction to prove their lower bound mentioned above.\n",
      "\n",
      "Lemma 2.4 (Edwards et al. [31]) . For integers s glyph[greaterorequalslant] 2 and N glyph[greaterorequalslant] 1 , the graph G = G ( s, N ) has no K s,s minor and no ( s -1 , N ) -colouring.\n",
      "\n",
      "Proof. We proceed by induction on s . In the base case, G = K 1 ,N +1 , and every 1colouring has a colour class (the whole graph) inducing a subgraph with maximum degree larger than N . Thus G is not (1 , N )-colourable. Now assume that s glyph[greaterorequalslant] 3 and the claim holds for s -1. Let v be the dominant vertex in G . Let C 1 , . . . , C N +1 be the components of G -v , where each C i is isomorphic to G ( s -1 , N ).\n",
      "\n",
      "If G contains a K s,s minor, then some component of G -v contains a K s -1 ,s -1 minor, which contradicts our inductive assumption. Thus G contains no K s,s minor.\n",
      "\n",
      "Suppose that c is an ( s -1)-colouring of G ( s, N ). We may assume that c ( v ) = 1. If C i has a vertex of colour 1 for each i ∈ { 1 , 2 , . . . , N +1 } , then v has more than N neighbours of colour 1, which is not possible. Thus some component C i has no vertex coloured 1, and at most s -2 colours are used on C i . This contradicts the assumption that C i has no ( s -2 , N )-colouring. glyph[square]\n",
      "\n",
      "Of course, for integers t glyph[greaterorequalslant] s glyph[greaterorequalslant] 2, the graph G ( s, N ) has no K s,t minor, no K s,t topological minor, and no K ∗ s,t -minor. Thus Lemma 2.4 shows that the number of colours in Theorem 2.3 is best possible. In other words, Theorem 2.3 states that defective chromatic number and defective choice number of every class of graphs of bounded ˜ ∇ 1 / 2 with no K ∗ s,t subgraph are at most s , and Lemma 2.4 shows that the number s of colours cannot be decreased.\n",
      "\n",
      "## 3. Excluded Subgraphs\n",
      "\n",
      "This section presents several applications of our main result, in the setting of graph classes defined by an excluded subgraph. Since K ∗ s,t contains K s,t and a ( glyph[lessorequalslant] 1)-subdivision of K s +1 , Theorem 2.3 immediately implies:\n",
      "\n",
      "- Every graph G with no K s,t subgraph is ( s, d )-choosable, where d depends on s , t , mad( G ) and ˜ ∇ 1 / 2 ( G ).\n",
      "- Every graph G with no subgraph isomorphic to a ( glyph[lessorequalslant] 1)-subdivision of K s +1 is ( s, d )-choosable, where d depends on s , mad( G ) and ˜ ∇ 1 / 2 ( G ).\n",
      "\n",
      "- 3.1. No 4 -Cycles. Since K ∗ 2 , 1 is the 4-cycle, Theorem 2.3 with s = 2 and t = 1 implies the following.\n",
      "- Corollary 3.1. Every graph G with no 4 -cycle is (2 , d ) -choosable, where d := glyph[floorleft] 2(( ˜ ∇ 0 ( G ) -1) ˜ ∇ 1 / 2 ( G ) + ˜ ∇ 0 ( G )) -1 glyph[floorright] .\n",
      "- Corollary 3.2. Every graph with no K t minor nor 4 -cycle subgraph is (2 , O ( t 2 log t )) -choosable.\n",
      "\n",
      "For planar graphs, ˜ ∇ 0 glyph[lessorequalslant] ˜ ∇ 1 / 2 glyph[lessorequalslant] 3. Indeed ˜ ∇ 0 &lt; 3 for planar graphs with at least three vertices. Corollary 3.1 says that every planar graph with no 4-cycle is (2 , 16)choosable. However, better degree bounds are known. Borodin et al. [11] proved that every planar graph with no cycle of length 4 has a vertex of degree at most 1 or a 7-light edge. By Lemma 2.1, planar graphs with no 4-cycle are (2 , 6)-choosable. Note that planar graphs with no 4-cycle are also known to be (3 , 1)-choosable [86].\n",
      "\n",
      "- 3.2. Edge Partitions. He et al. [38] proved the following theorem on partitioning a graph into edge-disjoint subgraphs.\n",
      "- Theorem 3.3 (He et al. [38, Theorem 3.1]) . If every subgraph of a graph G has a vertex of degree at most 1 or an N -light edge, then G has an edge-partition into two subgraphs T and H such that T is a forest and H is a graph with maximum degree at most N -1 .\n",
      "\n",
      "This theorem and Lemma 2.2 imply the following.\n",
      "\n",
      "- Theorem 3.4. For an integer t glyph[greaterorequalslant] 2 , every graph G with no K 2 ,t subgraph has an edge-partition into two subgraphs T and H such that T is a forest and H has maximum degree at most glyph[floorleft] ( ˜ ∇ 0 ( G ) -1) ˜ ∇ 1 / 2 ( G )( t -1) + 2 ˜ ∇ 0 ( G ) glyph[floorright]1 .\n",
      "- 3.3. Nowhere Dense Classes. A class C of graphs is nowhere dense [63] if, for every integer k there is some n such that no ( glyph[lessorequalslant] k )-subdivision of K n is a subgraph of a graph in C . Nowhere dense classes are also characterised [64] by the property that for every integer r there exists a function f r : N → [0 , 1] with lim n →∞ f r ( n ) = 0 such that every graph G of order n in the class has ˜ ∇ r ( G ) glyph[lessorequalslant] n f r ( n ) . In other words, for each integer r every graph G in the class has ˜ ∇ r ( G ) = | V ( G ) | o (1) .\n",
      "\n",
      "For nowhere dense classes, there is no hope to find an improper colouring with a bounded number of colours, since the chromatic number of a nowhere dense class is typically unbounded (as witnessed by the class of graphs G such that ∆( G ) glyph[lessorequalslant] girth( G )). However, by the above characterisation, Theorem 2.3 implies there is a partition of the vertex set into a bounded number of parts, each with 'small' maximum degree.\n",
      "\n",
      "Corollary 3.5. Let C be a nowhere dense class. Then there exist c ∈ N and a function f : N → [0 , 1] with lim n →∞ f ( n ) = 0 such that every n -vertex graph in C is ( c, n f ( n ) ) -choosable.\n",
      "\n",
      "## 4. 3-Colouring Graphs on Surfaces\n",
      "\n",
      "This section considers defective colourings of graphs drawn on a surface, possibly with crossings. First consider the case of no crossings. For example, Cowen et al. [25] proved that every planar graph is (3 , 2)-colourable, improved to (3 , 2)-choosable by Eaton and Hull [30]. Since G (3 , N ) is planar, by Lemma 2.4 the class of planar graphs has defective chromatic-number and defective choice number equal to 3. More generally, Archdeacon [3] proved the conjecture of Cowen et al. [25] that for every fixed surface Σ, the class of graphs embeddable in Σ has defective chromaticnumber 3. Woodall [89] proved that such graphs have defective choice number 3. It follows from Euler's formula that K 3 ,t is not embeddable on Σ for some constant t (see Lemma 4.3), and that graphs embeddable in Σ have bounded average degree and ˜ ∇ 1 / 2 . Thus Theorem 2.3 implies Woodall's result. The lower bound follows from Lemma 2.4 since G (3 , N ) is planar.\n",
      "\n",
      "Theorem 4.1 ([3, 89]) . For every surface Σ , the class of graphs embeddable in Σ has defective chromatic-number 3 and defective choice number 3.\n",
      "\n",
      "While our main goal is to bound the number of colours in a defective colouring, we now estimate the degree bound using our method for a graph embeddable in a surface Σ of Euler genus g . The Euler genus of an orientable surface with h handles is 2 h . The Euler genus of a non-orientable surface with c cross-caps is c . The Euler genus of a graph G is the minimum Euler genus of a surface in which G embeds. For g glyph[greaterorequalslant] 0, define\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "The next two lemmas are well known. We include their proofs for completeness.\n",
      "\n",
      "Lemma 4.2. Every n -vertex graph G embeddable in a surface of Euler genus g has at most d g n edges.\n",
      "\n",
      "Proof. Suppose that | E ( G ) | &gt; dn , where d := d g . We may assume that n glyph[greaterorequalslant] 3. By Euler's Formula, dn &lt; | E ( G ) | glyph[lessorequalslant] 3( n + g -2), implying ( d -3) n &lt; 3 g -6. Since dn &lt; | E ( G ) | glyph[lessorequalslant] ( n 2 ) we have n &gt; 2 d +1. Since d glyph[greaterorequalslant] 3,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Thus 2 d 2 -5 d +(3 -3 g ) &lt; 0. By the quadratic formula, d &lt; 1 4 (5+ √ 1 + 24 g ), which is a contradiction. Hence | E ( G ) | glyph[lessorequalslant] dn . glyph[square]\n",
      "\n",
      "Lemma 4.3 (Ringel [73]) . For every surface Σ of Euler genus g , the complete bipartite graph K 3 , 2 g +3 does not embed in Σ .\n",
      "\n",
      "Proof. By Euler's formula, every triangle-free graph with n glyph[greaterorequalslant] 3 vertices that embeds in Σ has at most 2( n + g -2) edges. The result follows. glyph[square]\n",
      "\n",
      "Lemmas 4.2 and 4.3 and Theorem 2.3 imply that graphs embeddable in Σ are (3 , O ( g 5 / 2 ))-choosable. This degree bound is weaker than the bound of max { 15 , 1 2 (3 g -\n",
      "\n",
      "- 8) } obtained by Archdeacon. However, our bound is easily improved. Results by Jendro ' l and Tuh´ arsky [47] and Ivanˇ co [43] show that every graph with Euler genus g has a (2 g +8)-light edge. Then Lemma 2.1 directly implies that every graph with Euler genus g is (3 , 2 g + 6)-choosable. Still this bound is weaker than the subsequent improvements to Archdeacon's result of (3 , max { 12 , 6 + √ 6 g } )-colourability by Cowen et al. [24] and to (3 , max { 9 , 2 + √ 4 g +6 } )-choosability by Woodall [89]; also see [18].\n",
      "2. 4.1. Linear Crossing Number. We now generalise Theorem 4.1 to the setting of graphs with linear crossing number. For an integer g glyph[greaterorequalslant] 0 and real number k glyph[greaterorequalslant] 0, say a graph G is k -close to Euler genus g (resp. k -close to planar) if every subgraph H of G has a drawing on a surface of Euler genus g (resp. on the plane) with at most k | E ( H ) | crossings. This says that the average number of crossings per edge is at most 2 k (for every subgraph). Of course, a graph is planar if and only if it is 0-close to planar, and a graph has Euler genus at most g if and only if it is 0-close to Euler genus g . Graphs that can be drawn in the plane with at most k crossings per edge, so called k -planar graphs , are examples of graphs ( k 2 )-close to planar. Pach and T´ oth [68] proved that k -planar graphs have average degree O ( √ k ). It follows that k -planar graphs are O ( √ k )-colourable, which is best possible since K n is O ( n 2 )-planar. For defective colourings, three colours suffice even in the more general setting of graphs k -close to Euler genus g .\n",
      "\n",
      "Theorem 4.4. For all integers g, k glyph[greaterorequalslant] 0 the class of graphs k -close to Euler genus g has defective chromatic number and defective choice number equal to 3. In particular, every graph k -close to Euler genus g is (3 , O (( k +1) 5 / 2 ( g +1) 7 / 2 )) -choosable.\n",
      "\n",
      "We prove this theorem by a series of lemmas, starting with a straightforward extension of the standard probabilistic proof of the crossing lemma. Note that Shahrokhi et al. [79] obtained a better bound for a restricted range of values for m relative to n .\n",
      "\n",
      "Lemma 4.5. Every drawing of a graph with n vertices and m glyph[greaterorequalslant] 2 d g n edges on a surface Σ of Euler genus g has at least m 3 / (8( d g n ) 2 ) crossings.\n",
      "\n",
      "Proof. By Lemma 4.2, every n -vertex graph that embeds in Σ has at most d g n edges. Thus every drawing of an n -vertex m -edge graph on Σ has at least m -d g n crossings.\n",
      "\n",
      "Given a graph G with n vertices and m glyph[greaterorequalslant] 2 d g n edges and a crossing-minimal drawing of G on Σ, choose each vertex of G independently and randomly with probability p := 2 d g n/m . Note that p glyph[lessorequalslant] 1. Let G ′ be the induced subgraph obtained. The expected number of vertices in G ′ is pn , the expected number of edges in G ′ is p 2 m , and the expected number of crossings in the induced drawing of G ′ is p 4 c , where c is the number of crossings in the drawing of G . By linearity of expectation and the above naive bound, p 4 c glyph[greaterorequalslant] p 2 m -d g pn . Thus c glyph[greaterorequalslant] ( pm -d g n ) /p 3 = d g n/p 3 = m 3 / (8( d g n ) 2 ). glyph[square]\n",
      "\n",
      "This lemma leads to the following bound on the number of edges.\n",
      "\n",
      "Lemma 4.6. If an n -vertex m -edge graph G has a drawing on a surface of Euler genus g with at most km crossings, then\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Proof. If m&lt; 2 d g n then m&lt; √ 8 k +4 d g n , and we are done. Otherwise, m glyph[greaterorequalslant] 2 d g n , and Lemma 4.5 is applicable. Thus every drawing of G on a surface of Euler genus g has at least m 3 / (8( d g n ) 2 ) crossings. Hence m 3 / (8( d g n ) 2 ) glyph[lessorequalslant] km , implying m glyph[lessorequalslant] √ 8 k d g n . glyph[square]\n",
      "\n",
      "To apply Theorem 2.3 we bound the size of K 3 ,t subgraphs.\n",
      "\n",
      "Lemma 4.7. Every drawing of K 3 ,t in a surface of Euler genus g has at least\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "crossings.\n",
      "\n",
      "Proof. By Lemma 4.3, K 3 , 2 g +3 does not embed (crossing-free) in a surface of Euler genus g . Consider a drawing of K 3 ,t in a surface of Euler genus g . There are ( t 2 g +3 ) copies of K 3 , 2 g +3 in K 3 ,t . Each such copy has a crossing. Each crossing is in at most ( t -2 2 g +1 ) copies of K 3 , 2 g +3 . Thus the number of crossings is at least\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Lemma 4.8. If a graph G is k -close to Euler genus g and contains K 3 ,t as a subgraph, then\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Proof. Suppose that G contains K 3 ,t as a subgraph. Since G is k -close to Euler genus g , so is K 3 ,t . Thus K 3 ,t has a drawing in a surface of Euler genus g where the number of crossings is at most 3 kt . By Lemma 4.7,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "The result follows.\n",
      "\n",
      "We now prove the main result of this section.\n",
      "\n",
      "Proof of Theorem 4.4. Say G is a graph k -close to Euler genus g . By Lemma 4.8, G contains no K 3 ,t with t = 3 k (2 g + 3)(2 g + 2) + 2. By Lemma 4.6, mad( G ) glyph[lessorequalslant] 2 √ 8 k +4 d g . We now bound ˜ ∇ 1 / 2 ( G ). Consider a subgraph H of G that is a ( glyph[lessorequalslant] 1)subdivision of a graph X . Since G is k -close to Euler genus g , so is H . Thus H has a drawing on a surface of Euler genus g with at most k | E ( H ) | crossings. Remove each division vertex and replace its two incident edges by one edge. We obtain a drawing of X with the same number of crossings as the drawing of H . Now | E ( H ) | glyph[lessorequalslant] 2 | E ( X ) | . Thus X has a drawing on a surface of Euler genus g with\n",
      "\n",
      "at most 2 k | E ( X ) | crossings. By Lemma 4.6, | E ( X ) | glyph[lessorequalslant] √ 16 k +4 d g | V ( X ) | . Hence ˜ ∇ 1 / 2 ( G ) glyph[lessorequalslant] √ 16 k +4 d g . By Theorem 2.3, G is (3 , d )-choosable, where\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "## 5. Thickness Parameters\n",
      "\n",
      "This section studies defective colourings of graphs with given thickness or other related parameters. Yancey [90] first proposed studying defective colourings of graphs with given thickness.\n",
      "\n",
      "5.1. Light Edge Lemma. Our starting point is the following sufficient condition for a graph to have a light edge. The proof uses a technique by Bose et al. [15], which we present in a general form.\n",
      "\n",
      "Lemma 5.1. Let G be a graph with n vertices, at most an + b edges, and minimum degree δ , such that every spanning bipartite subgraph has at most a ′ n + b ′ edges, for some a, a ′ ∈ R + and b, b ′ ∈ R and δ ∈ Z + satisfying:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "- ( δ -a ′ ) glyph[lscript] &gt; (2 a -a ′ ) δ, and (5)\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Then G has an ( glyph[lscript] -1) -light edge.\n",
      "\n",
      "Proof. Let X be the set of vertices with degree at most glyph[lscript] -1. Since vertices in X have degree at least δ and vertices not in X have degree at least glyph[lscript] ,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Thus\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Suppose on the contrary that X is an independent set in G . Let G ′ be the spanning bipartite subgraph of G consisting of all edges between X and V ( G ) \\ X . Since each of the at least δ edges incident with each vertex in X are in G ′ ,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Since glyph[lscript] &gt; 2 a -a ′ ′ δ &gt; δ (hence glyph[lscript] -δ &gt; 0) and δ glyph[greaterorequalslant]\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "If n glyph[lessorequalslant] glyph[lscript] then every edge is ( glyph[lscript] -1)-light. Now assume that n glyph[greaterorequalslant] glyph[lscript] + 1. Since ( δ -a ′ ) glyph[lscript] -(2 a -a ′ ) δ &gt; 0,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Thus\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "which is a contradiction. Thus X is not an independent set. Hence G contains an ( glyph[lscript] -1)-light edge. glyph[square]\n",
      "\n",
      "Remark. To verify (6), the following approximation can be useful: If α, β, γ are strictly positive reals, then the larger root of αx 2 -βx -γ = 0 is at most\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Lemma 2.1 with k = δ -1 and Lemma 5.1 imply the following sufficient condition for defective choosability. With δ := glyph[floorleft] a ′ glyph[floorright] +1, which is the minimum possible value for δ , the number of colours only depends on the coefficient of | V ( H ) | in the bound on the number of edges in a bipartite subgraph H .\n",
      "\n",
      "Lemma 5.2. Fix constants a, a ′ ∈ R + and b, b ′ ∈ R and glyph[lscript], δ ∈ Z + satisfying (4) , (5) and (6) . Let G be a graph such that every subgraph H of G with minimum degree at least δ satisfies the following conditions:\n",
      "\n",
      "- (i) H has at most a | V ( H ) | + b edges.\n",
      "- (ii) Every spanning bipartite subgraph of H has at most a ′ | V ( H ) | + b ′ edges.\n",
      "\n",
      "Then G is ( δ, glyph[lscript] -δ ) -choosable. In particular, G is ( glyph[floorleft] a ′ glyph[floorright] +1 , glyph[lscript] -1 -glyph[floorleft] a ′ glyph[floorright] ) -choosable.\n",
      "\n",
      "Lemma 5.1 with a = 3 and b = 3( g -2) and a ′ = 2 and b ′ = 2( g -2) and glyph[lscript] = 2 g +13 implies that every graph G with minimum degree at least 3 and Euler genus g has a (2 g +12)-light edge. Note that this bound is within +10 of being tight since K 3 , 2 g +2 has minimum degree 3, embeds in a surface of Euler genus g , and every edge has an endpoint of degree 2 g + 2. More precise results, which are typically proved by discharging with respect to an embedding, are known [8, 43, 47]. Lemma 5.2 then implies that every graph with Euler genus g is (3 , 2 g +10)-choosable. As mentioned earlier, this result with a better degree bound was proved by Woodall [89]; also see [18]. The utility of Lemma 5.2 is that it is immediately applicable in more general settings, as we now show.\n",
      "\n",
      "5.2. Thickness. The thickness of a graph G is the minimum integer k such that G is the union of k planar subgraphs; see [62] for a survey on thickness. A minimumdegree-greedy algorithm properly 6 k -colours a graph with thickness k , and it is an open problem to improve this bound for k glyph[greaterorequalslant] 2. The result of Havet and Sereni [37] implies that graphs with thickness k , which have maximum average degree less than 6 k , are (3 k +1 , O ( k 2 ))-choosable, but gives no result with at most 3 k colours. We show below that graphs with thickness k are (2 k +1 , O ( k 2 ))-choosable, and that no result with at most 2 k colours is possible. That is, both the defective chromatic number and defective choice number of the class of graphs of thickness at most k equal 2 k + 1. In fact, the proof works in the following more general setting. For an integer g glyph[greaterorequalslant] 0, the g -thickness of a graph G is the minimum integer k such that\n",
      "\n",
      "G is the union of k subgraphs each with Euler genus at most g . This definition was implicitly introduced by Jackson and Ringel [44]. By Euler's Formula, every graph with n glyph[greaterorequalslant] 3 vertices and g -thickness k has at most 3 k ( n + g -2) edges, and every spanning bipartite subgraph has at most 2 k ( n + g -2) edges. Lemma 5.1 with glyph[lscript] = 2 kg +8 k 2 +4 k +1 (using (7) to verify (6)) implies:\n",
      "\n",
      "Lemma 5.3. Every graph with minimum degree at least 2 k +1 and g -thickness at most k has a (2 kg +8 k 2 +4 k ) -light edge.\n",
      "\n",
      "We now determine the defective chromatic number and defective choice number of graphs with given g -thickness.\n",
      "\n",
      "Theorem 5.4. For integers g glyph[greaterorequalslant] 0 and k glyph[greaterorequalslant] 1 , the class of graphs with g -thickness at most k has defective chromatic number and defective choice number equal to 2 k +1 . In particular, every graph with g -thickness at most k is (2 k + 1 , 2 kg + 8 k 2 + 2 k ) -choosable.\n",
      "\n",
      "Proof. Lemmas 5.2 and 5.3 imply the upper bound. As usual, the lower bound is provided by G (2 k +1 , N ). We now prove that G = G (2 k +1 , N ) has g -thickness at most k by induction on k (with g fixed). Note that G (3 , N ) is planar, and thus has g -thickness 1. Let r be the vertex of G such that G -r is the disjoint union of N +1 copies of G (2 k, N ). For i ∈ [ N +1], let v i be the vertex of the i -th component C i of G -r such that C i -v i is the disjoint union of N +1 copies of G (2 k -1 , N ). Let H := G -{ r, v 1 , v 2 , . . . , v N +1 } . Observe that each component of H is isomorphic to G (2 k -1 , N ) and by induction, H has g -thickness at most k -1. Since G -E ( H ) consists of N +1 copies of K 2 ,N ′ pasted on r for some N ′ , G -E ( H ) is planar and thus has g -thickness 1. Hence G has g -thickness at most k . By Lemma 2.4, G (2 k +1 , N ) has no (2 k, N )-colouring. Therefore the class of graphs with g -thickness at most k has defective chromatic number and defective choice number at least 2 k +1. glyph[square]\n",
      "\n",
      "The case g = 0 and k = 2 relates to the famous earth-moon problem [2, 34, 42, 44, 72], which asks for the maximum chromatic number of graphs with thickness 2. The answer is in { 9 , 10 , 11 , 12 } . The result of Havet and Sereni [37] mentioned in Section 1 implies that graphs with thickness 2 are (7 , 18)-choosable, (8 , 9)-choosable, (9 , 5)choosable, (10 , 3)-choosable, and (11 , 2)-choosable because their maximum average degree is less than 12. But their result gives no bound with at most 6 colours. Theorem 5.4 says that the class of graphs with thickness 2 has defective chromatic number and defective choice number equal to 5. In particular, Lemma 5.2 implies that graphs with thickness 2 are (5 , 36)-choosable, (6 , 19)-choosable, (7 , 12)-choosable, (8 , 9)-choosable, (9 , 6)-choosable, (10 , 4)-choosable, and (11 , 2)-choosable. This final result, which is also implied by the result of Havet and Sereni [37], is very close to the conjecture that graphs with thickness 2 are 11-colourable. Improving these degree bounds provides an approach for attacking the earth-moon problem.\n",
      "\n",
      "5.3. Stack Layouts. A k -stack layout of a graph G consists of a linear ordering v 1 , . . . , v n of V ( G ) and a partition E 1 , . . . , E k of E ( G ) such that no two edges in E i cross with respect to v 1 , . . . , v n for each i ∈ [1 , k ]. Here edges v a v b and v c v d cross if a &lt; c &lt; b &lt; d . A graph is a k -stack graph if it has a k -stack layout. The stack-number of a graph G is the minimum integer k for which G is a k -stack graph. Stack layouts are also called book embeddings , and stack-number is also called bookthickness , fixed outer-thickness and page-number . The maximum chromatic number of k -stack graphs is in { 2 k, 2 k +1 , 2 k +2 } ; see [29]. For defective colourings, k +1 colours suffice.\n",
      "\n",
      "Theorem 5.5. The class of k -stack graphs has defective chromatic number and defective choice number equal to k + 1 . In particular, every k -stack graph is ( k + 1 , 2 O ( k log k ) ) -choosable.\n",
      "\n",
      "Proof. The lower bound follows from Lemma 2.4 since an easy inductive argument shows that G ( k + 1 , N ) is a k -stack graph for all N . For the upper bound, K k +1 ,k ( k +1)+1 is not a k -stack graph [5]; see also [26]. Every k -stack graph G has average degree less than 2 k + 2 (see [5, 29, 50]) and ˜ ∇ 1 / 2 ( G ) glyph[lessorequalslant] 20 k 2 (see [66]). The result follows from Theorem 2.3 with s = k + 1 and t = k ( k + 1) + 1, where glyph[floorleft] N 1 ( k +1 , k ( k +1) + 1 , 2 k +2 , 40 k 2 ) glyph[floorright]k glyph[lessorequalslant] 2 O ( k log k ) . glyph[square]\n",
      "\n",
      "5.4. Queue Layouts. A k -queue layout of a graph G consists of a linear ordering v 1 , . . . , v n of V ( G ) and a partition E 1 , . . . , E k of E ( G ) such that no two edges in E i are nested with respect to v 1 , . . . , v n for each i ∈ [1 , k ]. Here edges v a v b and v c v d are nested if a &lt; c &lt; d &lt; b . The queue-number of a graph G is the minimum integer k for which G has a k -queue layout. A graph is a k -queue graph if it has a k -queue layout. Dujmovi´ c and Wood [29] state that determining the maximum chromatic number of k -queue graphs is an open problem, and showed lower and upper bounds of 2 k +1 and 4 k . We provide the following partial answer to this question.\n",
      "\n",
      "Theorem 5.6. Every k -queue graph is (2 k +1 , 2 O ( k log k ) ) -choosable.\n",
      "\n",
      "Proof. Heath and Rosenberg [40] proved that K 2 k +1 , 2 k +1 is not a k -queue graph. Every k -queue graph G has mad( G ) &lt; 4 k (see [29, 40, 69]) and ˜ ∇ 1 / 2 ( G ) &lt; (2 k +2) 2 (see [66]). The result then follows from Theorem 2.3 with s = 2 k +1 and t = 2 k +1, where glyph[floorleft] N 1 (2 k +1 , 2 k +1 , 4 k, 2(2 k +2) 2 ) glyph[floorright]2 k glyph[lessorequalslant] 2 O ( k log k ) . glyph[square]\n",
      "\n",
      "Since G ( k +1 , n ) has a k -queue layout, the defective chromatic number of the class of k -queue graphs is at least k +1 and at most 2 k +1 by Lemma 2.4 and Theorem 5.6. It remains an open problem to determine its defective chromatic number.\n",
      "\n",
      "glyph[negationslash]\n",
      "\n",
      "5.5. Posets. Consider the problem of partitioning the domain X of a given poset P = ( X, glyph[precedesequal] ) into X 1 , . . . , X k so that each ( X i , glyph[precedesequal] ) has small poset dimension. The Hasse diagram H ( P ) of P is the graph whose vertices are the elements of P and whose edges correspond to the cover relation of P . Here x covers y in P if y = x , y glyph[precedesequal] x and there is no element z of P such that z = y , z = x , and y glyph[precedesequal] z glyph[precedesequal] x . A\n",
      "\n",
      "glyph[negationslash]\n",
      "\n",
      "glyph[negationslash]\n",
      "\n",
      "linear extension of P = ( X, glyph[precedesequal] ) is a total order ≤ on X such that x glyph[precedesequal] y implies x ≤ y for every x, y ∈ X . The jump number of P is the minimum number of consecutive elements of a linear extension of P that are not comparable in P , where the minimum is taken over all possible linear extensions of P .\n",
      "\n",
      "Theorem 5.7. For every integer k there is an integer d such that the domain of any poset with jump number at most k can be coloured with 2 k +3 colours, such that each colour induces a poset with dimension at most d .\n",
      "\n",
      "Proof. Heath and Pemmaraju [39] showed that the queue-number of the Hasse diagram of a poset P is at most one more than the jump number of P , and F¨ uredi and Kahn [33] proved that if the Hasse diagram of a poset has maximum degree ∆, then its dimension is at most 50∆(log ∆) 2 . The result then follows from Theorem 5.6. glyph[square]\n",
      "\n",
      "## 6. Minor-Closed Classes\n",
      "\n",
      "This section shows that for many minor-closed classes, Theorem 2.3 determines the defective chromatic number and defective choice number. For example, every outerplanar graph has average degree less than 4 and contains no K 2 , 3 subgraph. Thus Theorem 2.3 implies that every outerplanar graph is (2 , 14)-choosable. A better degree bound was obtained by Cowen et al. [25], who proved that outerplanar graphs are (2 , 2)-colourable. Since G (1 , N ) is outerplanar, by Lemma 2.4 the defective chromatic number and defective choice number of the class of outerplanar graphs equal 2. As shown in Section 4, the defective chromatic number and defective choice number of the class of graphs embeddable in any fixed surface equal 3. We now consider some other minor-closed classes.\n",
      "\n",
      "6.1. Linklessly and Knotlessly Embeddable Graphs. A graph is linklessly embeddable if it has an embedding in R 3 with no two topologically linked cycles [75, 77]. Linklessly embeddable graphs form a minor-closed class whose minimal excluded minors are the so-called Petersen family [76], which includes K 6 , K 4 , 4 minus an edge, and the Petersen graph. Since linklessly embeddable graphs exclude K 6 minors, they are 5-colourable [74] and 8-choosable [4]. It is open whether K 6 -minor-free graphs or linklessly embeddable graphs are 6-choosable. A graph is apex if deleting at most one vertex makes it planar. Every apex graph is linklessly embeddable [75]. Since G (3 , N ) is planar, G (4 , N ) is apex, and thus linklessly embeddable. By Lemma 2.4, the class of linklessly embeddable graphs has defective chromatic number at least 4. Mader's theorem [61] for K 6 -minor-free graphs implies that linklessly embeddable graphs have average degree less than 8 and minimum degree at most 7. Since linklessly embeddable graphs exclude K 4 , 4 minors, Theorem 2.3 implies the following result.\n",
      "\n",
      "Theorem 6.1. The class of linklessly embeddable graphs has defective chromatic number and defective choice number 4 . In particular, every linklessly embeddable graph is (4 , 440) -choosable.\n",
      "\n",
      "A graph is knotlessly embeddable if it has an embedding in R 3 in which every cycle forms a trivial knot; see [70] for a survey. Knotlessly embeddable graphs form a minor-closed class whose minimal excluded minors include K 7 and K 3 , 3 , 1 , 1 [22, 32]. More than 260 minimal excluded minors are known [35], but the full list of minimal excluded minors is unknown. Since knotlessly embeddable graphs exclude K 7 minors, they are 8-colourable [1, 45]. Mader [61] proved that K 7 -minor-free graphs have average degree less than 10, which implies they are 9-degenerate and thus 10-choosable. It is open whether K 7 -minor-free graphs or knotlessly embeddable graphs are 6-colourable or 7-choosable [4]. A graph is 2 -apex if deleting at most two vertices makes it planar. Blain et al. [6] and Ozawa and Tsutsumi [67] proved that every 2-apex graph is knotlessly embeddable. Since every block of G (5 , N ) is 2-apex, G (5 , N ) is knotlessly embeddable. By Lemma 2.4, the class of knotlessly embeddable graphs has defective chromatic number at least 5. Since K 3 , 3 , 1 , 1 is a minor of K ∗ 5 , 3 , knotlessly embeddable graphs do not contain a K ∗ 5 , 3 subgraph. Since knotlessly embeddable graphs have average degree less than 10, Theorem 2.3 implies the following result.\n",
      "\n",
      "Theorem 6.2. The class of knotlessly embeddable graphs has defective chromatic number and defective choice number 5 . In particular, every knotlessly embeddable graph is (5 , 660) -choosable.\n",
      "\n",
      "- 6.2. Excluded Complete and Complete Bipartite Minors. Now consider graphs excluding a given complete graph as a minor. Edwards et al. [31] proved that the class of K s +1 -minor-free graphs has defective chromatic-number s , which is a weakening of Hadwiger's conjecture. They also noted that the same method proves the same result for K s +1 -topological minor-free graphs. We have the following choosability versions of these results.\n",
      "\n",
      "Theorem 6.3. For each integer s glyph[greaterorequalslant] 2 , the class of K s +1 -minor-free graphs has defective chromatic-number s and defective choice number s . In particular, if δ is the maximal density of a K s +1 -minor-free graph, then every K s +1 -minor-free graph is ( s, glyph[floorleft] δ (2 δ -s +1) glyph[floorright]s +1) -choosable. The same result holds replacing 'minor' by 'topological minor'.\n",
      "\n",
      "The lower bound in Theorem 6.3 follows from Lemma 2.4. The upper bound follows from Theorem 2.3 with t = 1 since K ∗ s, 1 has a K s +1 -topological-minor. Indeed, in the t = 1 case, the proof of Theorem 2.3 is the same as the proof of Edwards et al. [31] with essentially the same degree bound. For K s +1 -minorfree graphs, Kostochka [53, 54] and Thomason [82, 83] proved that the maximum density δ = Θ( s √ log s ), and thus every K s +1 -minor-free graph is ( s, O ( s 2 log s ))-choosable. For K s +1 -topological-minor-free graphs, Bollob´ as and Thomason [7] and Koml´ os and Szemer´ edi [52] proved that the maximum density δ = Θ( s 2 ), and thus every K s +1 -topological-minor-free graph is ( s, O ( s 4 ))-choosable. Finally, note that\n",
      "\n",
      "for K s +1 -minor-free graphs, choice number and defective choice number substantially differ, since Bar´ at et al. [4] constructed K s +1 -minor-free graphs that are not 4 3 ( s -1)-choosable (for infinitely many s ).\n",
      "\n",
      "Now we deduce a theorem for the class of graphs with no K s,t topological minor.\n",
      "\n",
      "Theorem 6.4. For integers t glyph[greaterorequalslant] s glyph[greaterorequalslant] 1 , the defective chromatic number and the defective choice number of the class of K s,t topological minor-free graphs are equal to s . In particular, every K s,t topological minor-free graph is ( s, 2 O ( s log t ) ) -choosable.\n",
      "\n",
      "Proof. The lower bound follows from Lemma 2.4, since G ( s, N ) contains no K s,t topological minor. For the upper bound, Reed and Wood [71] noted that a method of Diestel [27], which is based on a result about linkages due to Thomas and Wollan [81], shows that for every graph H with p vertices and q edges, every graph with average degree at least 4 p + 20 q contains H as a topological minor. Thus every K s,t -topological-minor-free graph G has mad( G ) glyph[lessorequalslant] 20 st + 4( s + t ) glyph[lessorequalslant] 4(5 s + 2) t and ˜ ∇ 1 / 2 ( G ) glyph[lessorequalslant] 2(5 s + 2) t . By Theorem 2.3, G is ( s, d )-choosable, where d := glyph[floorleft] N 1 ( s, t, 4(5 s +2) t, 4(5 s +2) t ) -s +1 glyph[floorright] , which is in 2 O ( s log t ) . glyph[square]\n",
      "\n",
      "Note that Theorem 6.4 implies and is more general than Theorem 6.3, since K s,t contains K s +1 as a minor (for t glyph[greaterorequalslant] s ). For K s,t -minor-free graphs, the degree bound in Theorem 6.4 can be improved by using known results on the extremal function for K s,t -minor-free graphs [36, 55-59].\n",
      "\n",
      "- 6.3. Colin de Verdi` ere Parameter. The Colin de Verdi` ere parameter µ ( G ) is an important graph invariant introduced by Colin de Verdi` ere [20, 21]; see [78, 84, 85] for surveys. It is known that µ ( G ) glyph[lessorequalslant] 1 if and only if G is a forest of paths, µ ( G ) glyph[lessorequalslant] 2 if and only if G is outerplanar, µ ( G ) glyph[lessorequalslant] 3 if and only if G is planar, and µ ( G ) glyph[lessorequalslant] 4 if and only if G is linklessly embeddable. A famous conjecture of Colin de Verdi` ere [20] states that χ ( G ) glyph[lessorequalslant] µ ( G )+1 (which implies the 4-colour theorem, and is implied by Hadwiger's Conjecture). For defective colourings one fewer colour suffices.\n",
      "- Theorem 6.5. For k glyph[greaterorequalslant] 1 , the defective chromatic number and the defective choice number of the class of graphs G with µ ( G ) glyph[lessorequalslant] k are equal to k . In particular, every graph G with µ ( G ) glyph[lessorequalslant] k is ( k, 2 O ( k log log k ) ) -choosable.\n",
      "\n",
      "Proof. Graphs with µ ( G ) glyph[lessorequalslant] k form a minor-closed class [20, 21]. van der Holst et al. [85] proved that µ ( K s,t ) = s +1 for t glyph[greaterorequalslant] max { s, 3 } . Thus, if µ ( G ) glyph[lessorequalslant] k then G contains no K k, max( k, 3) minor, and mad( G ) glyph[lessorequalslant] 2 ˜ ∇ 1 / 2 ( G ) glyph[lessorequalslant] O ( k √ log k ). Theorem 2.3 with s = k and t = max { k, 3 } implies that G is ( k, 2 O ( k log log k ) )-choosable. Now we prove the lower bound. van der Holst et al. [85] proved that µ ( G ) equals the maximum of µ ( G ′ ), taken over the components G ′ of G , and if G has a dominant vertex v , then µ ( G ) = µ ( G -v ) + 1. It follows that µ ( G ( k, N )) = k for N glyph[greaterorequalslant] 2. Lemma 2.4 then implies that the class of graphs with µ ( G ) glyph[lessorequalslant] k has defective chromatic number and defective choice number at least k . glyph[square]\n",
      "\n",
      "Theorem 6.5 generalises Theorem 6.1 which corresponds to the case k = 4.\n",
      "\n",
      "6.4. H -Minor-Free Graphs. This section considers, for an arbitrary graph H , the defective chromatic number of the class of H -minor-free graphs, which we denote by f ( H ). That is, f ( H ) is the minimum integer such that there exists an integer d ( H ) such that every H -minor-free graph has a ( f ( H ) , d ( H ))-colouring. Obviously, f is minor-monotone: if H ′ is a minor of H , then every H ′ -minor-free graph is H -minor-free, and thus f ( H ′ ) glyph[lessorequalslant] f ( H ).\n",
      "\n",
      "A set S of vertices in a graph H is a vertex cover if E ( H -S ) = ∅ . Let τ ( H ) be the minimum size of a vertex cover in H , called the vertex cover number of H . The tree-depth of a connected graph H , denoted by td( H ), is the minimum height of a rooted tree T such that H is a subgraph of the closure of T . Here the closure of T is obtained from T by adding an edge between every ancestor and descendent in T . The height of a rooted tree is the maximum number of vertices on a root-to-leaf path. The tree-depth of a disconnected graph H is the maximum tree-depth of the connected components of H .\n",
      "\n",
      "Proposition 6.6. For every graph H ,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Proof. Obviously, H is a minor of K ∗ τ ( H ) , | V ( H ) |-τ ( H ) . Thus every H -minor-free graph is K ∗ τ ( H ) , | V ( H ) |-τ ( H ) -free. By Theorem 2.3, f ( H ) glyph[lessorequalslant] τ ( H ).\n",
      "\n",
      "We now establish the lower bound on f ( H ). Observe that G ( s, N ) is the closure of the complete ( N +1)-ary tree of height s , and G ( s, N ) has tree-depth at most s . Since tree-depth is minor-monotone [65], every minor of G ( s, N ) has tree-depth at most s . Thus a graph H is not a minor of G (td( H ) -1 , N ). By Lemma 2.4, every (td( H ) -2)-colouring of G (td( H ) -1 , N ) has a colour class that induces a subgraph with maximum degree at least N . Thus f ( H ) glyph[greaterorequalslant] td( H ) -1. glyph[square]\n",
      "\n",
      "The lower and upper bounds in Proposition 6.6 match in some important cases, like H = K t or H = K s,t (or H = K ∗ s,t ). The Petersen graph P is an example where they do not match. Proposition 6.6 implies f ( P ) ∈ { 5 , 6 } . On the other hand, every P -minor-free graph is 9-colourable [41] and this is best possible since K 9 is P -minor-free. So our upper bound improves the obvious bound deduced from chromatic number. Paths provide an interesting example where the bounds in Proposition 6.6 are far apart. In particular, for a path of order 2 t -1, Proposition 6.6 gives\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "It is easy to characterise the graphs with f ( H ) = 1, in which case the lower and upper bounds in Proposition 6.6 are equal.\n",
      "\n",
      "Proposition 6.7. f ( H ) = 1 if and only if H is a star plus some isolated vertices.\n",
      "\n",
      "Proof. Say H is a k -leaf star plus glyph[lscript] isolated vertices. Consider a graph G . If G has maximum degree at most k -1, then G is (1 , k -1)-colourable. If G has at most k + glyph[lscript] vertices, then G is (1 , k + glyph[lscript] -1)-colourable. Otherwise, G has maximum degree at\n",
      "\n",
      "least k and has at least k +1+ glyph[lscript] vertices, in which case G contains H as a minor. Thus every H -minor-free graph is (1 , k + glyph[lscript] -1)-colourable, and f ( H ) = 1. Conversely, say H is not a star plus some isolated vertices. Then H has two disjoint edges. For each integer d , the ( d + 1)-leaf star has no H -minor and is not (1 , d )-colourable. Thus f ( H ) glyph[greaterorequalslant] 2. glyph[square]\n",
      "\n",
      "The upper bound in Proposition 6.6 is not tight in general. For example, if H is the k -ary tree of height 3, then τ ( H ) = k but f ( H ) = 2 (as proved in Theorem 6.9 below). These observations lead to the following conjecture:\n",
      "\n",
      "Conjecture 6.8. f ( H ) = td( H ) -1 for every graph H , unless H has distinct connected components H 1 and H 2 with td( H ) = td( H 1 ) = td( H 2 ) , in which case f ( H ) = td( H ) .\n",
      "\n",
      "We now explain the necessity of the exception in Conjecture 6.8. Suppose H has connected components H 1 and H 2 with td( H ) = td( H 1 ) = td( H 2 ) = s . If H is a minor of G ( s, n ), then only one of H 1 and H 2 can use the root vertex of G ( s, n ), implying one of H 1 and H 2 is a minor of G ( s -1 , n ), which contradicts the treedepth assumption. Thus, H is not a minor of G ( s, n ). By Lemma 2.4, the class of H -minor-free graphs has defective chromatic number at least s = td( H ).\n",
      "\n",
      "Proposition 6.7 confirms Conjecture 6.8 when f ( H ) = 1. We now prove the first non-trivial case.\n",
      "\n",
      "Theorem 6.9. For every graph H with tree-depth 3 and with at most one component of tree-depth 3, the defective chromatic number of the class of H -minor-free graphs equals 2.\n",
      "\n",
      "Proof. The lower bound is proved above. For the upper bound, since at most one component of H has tree-depth 3, H is a subgraph of G (2 , k ) for some integer k glyph[lessorequalslant] | V ( H ) | . By Lemma 6.10 below with glyph[lscript] = k , every G (2 , k )-minor-free graph is (2 , d )-colourable, for some increasing function d = d ( k ). Every H -minor-free graph is G (2 , k )-minor-free. Since k glyph[lessorequalslant] | V ( H ) | , every H -minor-free graph is (2 , d )-colourable, where d = d ( k ) glyph[lessorequalslant] d ( | V ( H ) | ). glyph[square]\n",
      "\n",
      "Lemma 6.10. Let H be the graph obtained from glyph[lscript] disjoint copies of K 1 ,k by adding one dominant vertex, for some glyph[lscript] glyph[greaterorequalslant] 2 and k glyph[greaterorequalslant] 1 (as illustrated in Figure 2). Then every H -minor-free graph G is (2 , O ( glyph[lscript] 10 k 3 )) -colourable.\n",
      "\n",
      "Proof. Since H is 2-degenerate, there exists δ &lt; 7( glyph[lscript]k + glyph[lscript] + 1) such that every H -minor-free graph has average degree at most δ by a result of Reed and Wood [71, Lemma 3.3]. Let r := ( glyph[lscript] 2 -1 2 ) ( k +1) + glyph[lscript] 2 + glyph[lscript] . Let X be the set of vertices v ∈ V ( G ) such that | N G ( v ) ∩ N G ( w ) | glyph[greaterorequalslant] r for some vertex w ∈ V ( G ) \\ { v } . Note that w is also in X . Let Q be the graph with vertex set V ( G ) where vw ∈ E ( Q ) whenever | N G ( v ) ∩ N G ( w ) | glyph[greaterorequalslant] r . For each edge e = vw ∈ E ( Q ), let N ( e ) := N G ( v ) ∩ N G ( w ). Thus | N ( e ) | glyph[greaterorequalslant] r . Let Y := V ( G ) \\ X .\n",
      "\n",
      "Claim 1. Q has maximum degree less than glyph[lscript] .\n",
      "\n",
      "/Bullet\n",
      "\n",
      "/Bullet\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "/Bullet\n",
      "\n",
      "Figure 2. The graph H .\n",
      "\n",
      "Proof. Suppose on the contrary that some vertex v in Q is adjacent to distinct vertices v 1 , . . . , v glyph[lscript] in Q . For i = 1 , 2 , . . . , glyph[lscript] , choose k +1 common neighbours of v and v i in G that have not already been chosen and are different from v, v 1 , . . . , v glyph[lscript] . This is possible, since v and v i have r glyph[greaterorequalslant] ( k +1) glyph[lscript] + glyph[lscript] -1 common neighbours in G . For each i ∈ [ glyph[lscript] ], contract the edge between v i and one of the chosen common neighbours of v and v i . The chosen vertices along with v, v 1 , . . . , v glyph[lscript] form H as a minor of G , which is a contradiction. ♦\n",
      "\n",
      "Claim 2. If R is a set of more than glyph[lscript] ( glyph[lscript] -1) vertices in X , then Q contains an glyph[lscript] -edge matching, each edge of which has at least one endpoint in R .\n",
      "\n",
      "Proof. Let Z be the subgraph of Q induced by R ∪ N Q ( R ). Label vertices in R red and vertices in N Q ( R ) \\ R blue . If ∆ is the maximum degree of Z , then ∆ glyph[lessorequalslant] glyph[lscript] -1 by Claim 1. The number of red vertices is | R | &gt; (∆+1)( glyph[lscript] -1). Every vertex in R is in X and thus has a neighbour in Q , which is in N Q ( R ). Hence Z has no red isolated vertex. Let Z ′ be an edge-minimal spanning subgraph of Z with no red isolated vertex. By minimality, each edge of Z ′ has a red endpoint with degree 1. Thus each component of Z ′ is either a blue isolated vertex, a redblue edge, or a star with all its leaves coloured red. Since each component of Z ′ has at most ∆ + 1 red vertices, and there are strictly greater than (∆ + 1)( glyph[lscript] -1) red vertices, Z ′ contains at least glyph[lscript] non-singleton components. Let v 1 w 1 , . . . , v glyph[lscript] w glyph[lscript] be a matching obtained by choosing one edge from each non-singleton component of Z ′ , where v 1 , . . . , v glyph[lscript] are red and thus in R . ♦\n",
      "\n",
      "## Claim 3. Every vertex is adjacent in G to less than glyph[lscript] 2 vertices in X .\n",
      "\n",
      "glyph[negationslash]\n",
      "\n",
      "Proof. Suppose on the contrary that | N G ( v ) ∩ X | glyph[greaterorequalslant] glyph[lscript] 2 for some vertex v ∈ V ( G ). If v ∈ X then let R := ( N G ( v ) ∩ X ) \\ N Q ( v ), otherwise let R := N G ( v ) ∩ X . Then | R | &gt; glyph[lscript] ( glyph[lscript] -1) since | N Q ( v ) | glyph[lessorequalslant] glyph[lscript] -1 by Claim 1. By Claim 2, Q contains a matching v 1 w 1 , . . . , v glyph[lscript] w glyph[lscript] , where v 1 , . . . , v glyph[lscript] are in R . By construction, each w i = v . For i = 1 , 2 , . . . , glyph[lscript] , choose k + 1 common neighbours of v i and w i in G that have not already been chosen and are different from v, v 1 , w 1 , . . . , v glyph[lscript] , w glyph[lscript] . This is possible, since v i and w i have r glyph[greaterorequalslant] glyph[lscript] ( k +1)+2 glyph[lscript] -1 common neighbours. For each i ∈ [ glyph[lscript] ], contract the edge vv i into v , and contract the edge between w i and one\n",
      "\n",
      "of the chosen common neighbours of v i and w i . The chosen vertices along with v, w 1 , . . . , w glyph[lscript] form H as a minor of G , which is a contradiction. ♦\n",
      "\n",
      "Let G ′ be the graph obtained from G as follows. For each component C of Q , identify V ( C ) into one vertex, and delete resulting loops and parallel edges. Each vertex of G ′ corresponds to a component of Q .\n",
      "\n",
      "The final step of this proof applies Theorem 2.3 with s = 2 to obtain a defective 2-colouring of G ′ , from which we obtain a defective 2-colouring of G . To apply Theorem 2.3 we show that G ′ has no large K 2 ,t subgraph, has bounded ˜ ∇ 1 / 2 , and (thus) bounded average degree.\n",
      "\n",
      "Consider a K 2 ,t subgraph in G ′ . There are distinct components C, D, A 1 , . . . , A t of Q , such that for i ∈ [ t ], some vertex in A i is adjacent in G to some vertex in C , and some vertex in A i is adjacent in G to some vertex in D . Note each A i is either a single-vertex component of Q contained in Y or is contained in X with at least two vertices.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Proof. Suppose on the contrary and without loss of generality that A 1 , . . . , A glyph[lscript] 2 ⊆ X . The component C is not a single vertex, as otherwise, this vertex would have at least glyph[lscript] 2 neighbours in X contradicting Claim 3. Thus C is contained in X and has at least two vertices. For each i ∈ [ glyph[lscript] ], let v i be a vertex in A i adjacent to a vertex in C . Since v i is in A i ⊆ X , there is an edge e i = u i v i ∈ E ( Q ) and thus u i is also in A i . Note that u 1 , . . . , u glyph[lscript] are distinct since they belong to different components A i . Let E ( C ) be the set of edges of Q between vertices in C . Construct a bipartite graph B with colour classes\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where the vertex corresponding to each f ∈ E ( C ) is adjacent to each vertex in N ( f ) \\ X , and similarly the vertex e j i is adjacent to each vertex in N ( e i ) \\ X for each i ∈ [ glyph[lscript] ] and j ∈ [ k + 1]. The endpoints of each edge in Q have at least r common neighbours in G , at most glyph[lscript] 2 -1 of which are in X by Claim 3. Thus, in B , every vertex in B 1 has degree at least r -glyph[lscript] 2 + 1, and every vertex in B 2 has degree at most ( glyph[lscript] 2 -1 2 ) ( k + 1) by Claim 3. Consider a subset S ⊆ B 1 . The number of edges between S and N B ( S ) is at least ( r -glyph[lscript] 2 + 1) | S | and at most ( glyph[lscript] 2 -1 2 ) ( k + 1) | N B ( S ) | , implying | N B ( S ) | glyph[greaterorequalslant] | S | since r -glyph[lscript] 2 + 1 glyph[greaterorequalslant] ( glyph[lscript] 2 -1 2 ) ( k + 1). By Hall's Theorem, B contains a matching with every vertex in B 1 matched. For i ∈ [ glyph[lscript] ] and j ∈ [ k + 1], let x j i be the vertex in B 2 matched with e j i . Then x j i is a common neighbour of u i and v i in G . For each edge f ∈ E ( C ), let x f be the vertex in B 2 matched with f . Then x f is a common neighbour of the endpoints of f in G . All these x -vertices are distinct and are contained in Y . Hence V ( C ) ∪ { x f : f = pq ∈ E ( C ) } ∪ { v 1 , v 2 , . . . , v glyph[lscript] } induces a connected subgraph of G -( { u 1 , u 2 , . . . , u glyph[lscript] }∪{ x j i : i ∈ [ glyph[lscript] ] , j ∈ [ k +1] } ); contract this connected subgraph into a vertex z . Now z is adjacent to x j i for each i ∈ [ glyph[lscript] ] and j ∈ [ k +1]. Finally,\n",
      "\n",
      "contract the edge u i x k +1 i into u i , for each i ∈ [ glyph[lscript] ]. Now z is adjacent to u 1 , . . . , u glyph[lscript] , and x 1 i , . . . , x k i are common neighbours of z and u i . Hence H is a minor of G , which is a contradiction. ♦\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Proof. Define Z := ⋃ { A i : i ∈ [ t ] , A i ⊆ Y } . Note that | Z | = |{ i ∈ [ t ] : A i ⊆ Y }| because | A i | = 1 if A i ⊆ Y . Let C ′ be the set of vertices in C with some neighbour in Z . Let D ′ be the set of vertices in D with some neighbour in Z . For v ∈ V ( C ′ ) and w ∈ V ( D ′ ), less than r vertices are common neighbours of v and w (since vw glyph[negationslash]∈ E ( Q )). Thus | Z | ≤ | C ′ | | D ′ | ( r -1), and we are done if | C ′ | ≤ glyph[lscript] ( glyph[lscript] -1) and | D ′ | ≤ glyph[lscript] ( glyph[lscript] -1).\n",
      "\n",
      "Suppose for the sake of contradiction and without loss of generality that | C ′ | &gt; glyph[lscript] ( glyph[lscript] -1). Then V ( C ) ⊆ X since glyph[lscript] ( glyph[lscript] -1) glyph[greaterorequalslant] 2. By Claim 2 with R = C ′ , there is a matching e 1 , . . . , e glyph[lscript] in Q , where e i = v i u i and v i ∈ C ′ for each i ∈ [ glyph[lscript] ]. For i ∈ [ glyph[lscript] ], let a i be a (not necessarily distinct) neighbour of v i in Z . Let E ( D ) be the set of edges in Q between vertices in D . Construct a bipartite graph B with colour classes\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "glyph[negationslash]\n",
      "\n",
      "where the vertex in B 1 corresponding to each edge f ∈ E ( D ) is adjacent to each vertex in N ( f ) \\ ( X ∪ V ( D ) ∪ { a 1 , . . . , a glyph[lscript] } ), and similarly the vertex in B 1 corresponding to each edge e j i is adjacent to each vertex in N ( e i ) \\ ( X ∪ V ( D ) ∪ { a 1 , . . . , a glyph[lscript] } ). Note that | Y ∩ V ( D ) | ≤ 1 and if E ( D ) = ∅ , then V ( D ) ⊆ X . The endpoints of each edge in Q have at least r common neighbours in G , at most ( glyph[lscript] 2 -1) + glyph[lscript] +1 of which are in X ∪ V ( D ) ∪{ a 1 , . . . , a glyph[lscript] } by Claim 3. Thus, in B , every vertex in B 1 has degree at least r -glyph[lscript] 2 -glyph[lscript] , and every vertex in B 2 has degree at most ( glyph[lscript] 2 -1 2 ) ( k + 1) by Claim 3. Consider a subset S ⊆ B 1 . The number of edges between S and N B ( S ) is at least ( r -glyph[lscript] 2 -glyph[lscript] ) | S | and at most ( glyph[lscript] 2 -1 2 ) ( k +1) | N B ( S ) | , implying | N B ( S ) | glyph[greaterorequalslant] | S | since r -glyph[lscript] 2 -glyph[lscript] glyph[greaterorequalslant] ( glyph[lscript] 2 -1 2 ) ( k +1). By Hall's Theorem, B contains a matching with every vertex in B 1 matched.\n",
      "\n",
      "For f ∈ E ( D ), let x f be the vertex in B 2 matched with f and for each i ∈ [ glyph[lscript] ] and j ∈ [ k +1], let x j i be the vertex in B 2 matched with e j i . Then x f is a common neighbour (in G ) of the endpoints of f and x j i is a common neighbour of u i and v i in G . Note that all these x -vertices are distinct and are in V ( G ) \\ ( X ∪ V ( D ) ∪ { a 1 , . . . , a glyph[lscript] } ). Each vertex a i has a neighbour in D . Hence { a 1 , . . . , a glyph[lscript] , v 1 , . . . , v glyph[lscript] }∪ V ( D ) ∪{ x f : f ∈ E ( D ) } induces a connected subgraph of G -( { u 1 , u 2 , . . . , u glyph[lscript] } ∪ { x j i : i ∈ [ glyph[lscript] ] , j ∈ [ k + 1] } ). Contract this connected subgraph into a vertex z . Now z is adjacent to x j i for each i ∈ [ glyph[lscript] ] and j ∈ [ k +1]. Finally, contract the edge u i x k +1 i into u i , for each i ∈ [ glyph[lscript] ]. Now z is adjacent to u 1 , . . . , u glyph[lscript] , and x 1 i , . . . , x k i are common neighbours of z and u i for each i ∈ [ glyph[lscript] ]. Hence H is a minor of G , which is a contradiction. ♦\n",
      "\n",
      "Claims 4 and 5 show that t &lt; glyph[lscript] 2 + glyph[lscript] 2 ( glyph[lscript] -1) 2 ( r -1) ≤ glyph[lscript] 2 ( glyph[lscript] -1) 2 r . That is, G ′ has no K 2 ,glyph[lscript] 2 ( glyph[lscript] -1) 2 r subgraph.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Proof. Suppose that a ( glyph[lessorequalslant] 1)-subdivision of some graph G ′′ is a subgraph of G ′ . Let X ′′ be the set of vertices of G ′′ that arise from components of Q contained in X .\n",
      "\n",
      "Assume for contradiction that some vertex in G ′′ has at least glyph[lscript] neighbours in X ′′ . That is, there are distinct components C, C 1 , . . . , C glyph[lscript] of Q , such that for each i ∈ [ glyph[lscript] ], C i is a non-singleton component of Q contained in X , and there exists an edge joining a vertex of C to a vertex of C i in G , or a component D i of Q having a neighbour of C and a neighbour of C i in G . If C and C i are joined by an edge in G , then let D i := C (for convenience). Note that C might be a singleton component of Q contained in Y or a non-singleton component contained in X , and similarly for D 1 , . . . , D glyph[lscript] , but C 1 , . . . , C glyph[lscript] are non-singleton components of Q . Let Y ′ = Y ∩ ( V ( C ) ∪ V ( D 1 ) ∪··· ∪ V ( D glyph[lscript] )). Then | Y ′ | glyph[lessorequalslant] glyph[lscript] +1. For each i ∈ [ glyph[lscript] ], let v i be a vertex in C i adjacent to some vertex in V ( C ) ∪ V ( D i ). Since v i is in X , there is an edge e i = u i v i ∈ E ( Q ) and thus u i is also in C i . Construct a bipartite graph B with colour classes\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where the vertex corresponding to each edge f ∈ E ( C ) ∪ E ( D 1 ) ∪··· ∪ E ( D glyph[lscript] ) is adjacent to each vertex in N ( f ) \\ ( X ∪ Y ′ ), and similarly the vertex e j i is adjacent to each vertex in N ( e i ) \\ ( X ∪ Y ′ ). The endpoints of each edge in Q have at least r common neighbours in G , at most glyph[lscript] 2 -1+ | Y ′ | of which are in X ∪ Y ′ by Claim 3. Thus, in B , every vertex in B 1 has degree at least r -( glyph[lscript] 2 -1+ | Y ′ | ) glyph[greaterorequalslant] r -glyph[lscript] 2 -glyph[lscript] , and every vertex in B 2 has degree at most ( glyph[lscript] 2 -1 2 ) ( k +1) by Claim 3. Consider a subset S ⊆ B 1 . The number of edges between S and N B ( S ) is at least ( r -glyph[lscript] 2 -glyph[lscript] ) | S | and at most ( glyph[lscript] 2 -1 2 ) ( k +1) | N B ( S ) | , implying | N B ( S ) | glyph[greaterorequalslant] | S | since r -glyph[lscript] 2 -glyph[lscript] glyph[greaterorequalslant] ( glyph[lscript] 2 -1 2 ) ( k +1).\n",
      "\n",
      "By Hall's Theorem, B contains a matching with every vertex in B 1 matched. For i ∈ [ glyph[lscript] ] and j ∈ [ k +1], let x j i be the vertex in B 2 matched with e j i . Then x j i is a common neighbour of u i and v i in G . For each edge f ∈ E ( C ) ∪ E ( D 1 ) ∪ · · · ∪ E ( D glyph[lscript] ), let x f be the vertex in B 2 matched with f . Then x f is a common neighbour (in G ) of the endpoints of f . All these x -vertices are distinct and are contained in V ( G ) \\ ( X ∪ Y ′ ). Hence\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "induces a connected subgraph of G -( { u 1 , u 2 , . . . , u glyph[lscript] }∪{ x j i : i ∈ [ glyph[lscript] ] , j ∈ [ k +1] } ); contract this connected subgraph into a vertex z . Now z is adjacent to x j i for each i ∈ [ glyph[lscript] ] and j ∈ [ k + 1]. Finally, contract the edge u i x k +1 i into u i for each\n",
      "\n",
      "i ∈ [ glyph[lscript] ]. Now z is adjacent to u 1 , . . . , u glyph[lscript] , and x 1 i , . . . , x k i are common neighbours of z and u i for each i ∈ [ glyph[lscript] ]. Hence H is a minor of G . This contradiction proves that each vertex in G ′′ has less than glyph[lscript] neighbours in X ′′ .\n",
      "\n",
      "Thus G ′′ contains at most ( glyph[lscript] -1) | V ( G ′′ ) | edges with at least one endpoint in X ′′ . Since G ′′ -X ′′ is a subgraph of G , the average degree of G ′′ -X ′′ is at most δ , and | E ( G ′′ -X ′′ ) | ≤ δ | Y ′ | / 2. In total, | E ( G ′′ ) | glyph[lessorequalslant] δ | Y ′ | / 2 + ( glyph[lscript] -1) | V ( G ′′ ) | glyph[lessorequalslant] ( δ/ 2 + glyph[lscript] -1) | V ( G ′′ ) | , and ˜ ∇ 1 / 2 ( G ′ ) glyph[lessorequalslant] δ/ 2 + glyph[lscript] -1. ♦\n",
      "\n",
      "Note that Claim 6 implies mad( G ) = 2 ˜ ∇ 0 ( G ) glyph[lessorequalslant] 2 ˜ ∇ 1 / 2 ( G ) glyph[lessorequalslant] δ + 2 glyph[lscript] -2. By Theorem 2.3, G ′ is (2 , d ′ )-colourable, where\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Colour each vertex v of G by the colour assigned to the vertex of G ′ corresponding to the component of Q containing v . Then, for each vertex v of C , the number of neighbours of v having the same colour as v is at most d ′ + glyph[lscript] 2 -1, because v has at most glyph[lscript] 2 -1 neighbours in X by Claim 3 and at most d ′ neighbours of the same colour in Y . Therefore G is (2 , d ′ + glyph[lscript] 2 -1)-colourable. We now estimate the degree bound. We have r glyph[lessorequalslant] O ( glyph[lscript] 4 k ) and δ +2 glyph[lscript] -2 glyph[lessorequalslant] O ( glyph[lscript]k ). Thus d ′ glyph[lessorequalslant] O (( δ +2 glyph[lscript] ) 2 glyph[lscript] 2 ( glyph[lscript] -1) 2 r ) glyph[lessorequalslant] O ( glyph[lscript] 10 k 3 ). Therefore G is (2 , O ( glyph[lscript] 10 k 3 ))-colourable. glyph[square]\n",
      "\n",
      "Our final result provides further evidence for Conjecture 6.8. It concerns graphs that exclude a fixed tree as a subgraph.\n",
      "\n",
      "Proposition 6.11. Let T be a tree with n glyph[greaterorequalslant] 2 vertices and radius r glyph[greaterorequalslant] 1 . Then every graph containing no T subgraph is ( r, n -2) -colourable.\n",
      "\n",
      "Proof. For i = 1 , 2 , . . . , r -1, let V i be the set of vertices v ∈ V ( G ) \\ ( V 1 ∪···∪ V i -1 ) that have at most n -2 neighbours in V ( G ) \\ ( V 1 ∪ · · · ∪ V i -1 ). Let V r := V ( G ) \\ ( V 1 ∪ · · · ∪ V r -1 ). Then V 1 ∪ · · · ∪ V r is a partition of V ( G ). For i ∈ [1 , r -1], by construction, G [ V i ] has maximum degree at most n -2, as desired. Suppose that G [ V r ] has maximum degree at least n -1. We now show that T is a subgraph of G , where each vertex v of T is mapped to a vertex v ′ of G . Let x be the centre of T . Map the vertices of T to vertices in G in order of their distance from x in T , where x is mapped to a vertex x ′ with degree at least n -1 in G [ V r ]. The key invariant is that each vertex v at distance i from x in T is mapped to a vertex v ′ in V r -i +1 ∪ · · · ∪ V r . If i = 0 then v = x and by assumption, v ′ has at least n -1 neighbours in V r . If i ∈ [1 , r -1] then by construction, v ′ has at least n -1 neighbours in V r -i ∪··· ∪ V r (otherwise v ′ would be in V r -i ). Thus there are always unmapped vertices in V r -i ∪ · · · ∪ V r to choose as the children of v . Hence T is a subgraph. This contradiction shows that G [ V r ] has maximum degree at least n -2, and G is ( r, n -2)-colourable. glyph[square]\n",
      "\n",
      "Note that Proposition 6.11 is best possible for the complete binary tree T of radius r , which has tree-depth r +1 (see [65, Exercise 6.1]). Thus G ( r, N ) contains no T\n",
      "\n",
      "subgraph, and Lemma 2.4 and Proposition 6.11 imply that the defective chromatic number of the class of graphs containing no T subgraph equals r .\n",
      "\n",
      "Note that the behaviour shown in Proposition 6.11 is qualitatively different from the chromatic number of graphs excluding a given tree as a subgraph. Say T is a tree with n vertices. A well known greedy embedding procedure shows that every graph with minimum degree at least n -1 contains T as a subgraph. That is, every graph containing no T subgraph is ( n -2)-degenerate, and is thus ( n -1)-colourable. This bound is tight since K n -1 contains no T subgraph and is ( n -1)-chromatic. In short, for the class of graphs containing no T subgraph, the chromatic number equals n -1, whereas Proposition 6.11 says that the defective chromatic number is at most the radius of T .\n",
      "\n",
      "Conjecture 6.8 suggests similar behaviour for H -minor-free graphs. Say H has n vertices. Hadwiger's Conjecture says that the maximum chromatic number of the class of H -minor-free graphs equals n -1. It is at least n -1 since K n -1 is H -minor-free, and at most O ( n √ log n ) in general. Conjecture 6.8 says that if H is connected, then the defective chromatic number of the class of H -minor-free graphs equals the tree-depth of H minus 1.\n",
      "\n",
      "## References\n",
      "\n",
      "- [1] B. Albar and D. Gon¸ calves. On triangles in K r -minor free graphs. 2013. arXiv: 1304.5468 .\n",
      "- [2] M. O. Albertson, D. L. Boutin, and E. Gethner. More results on r -inflated graphs: arboricity, thickness, chromatic number and fractional chromatic number. Ars Math. Contemp. , 4(1):5-24, 2011.\n",
      "- [3] D. Archdeacon. A note on defective colorings of graphs in surfaces. J. Graph Theory , 11(4):517-519, 1987. doi: 10.1002/jgt.3190110408 .\n",
      "- [4] J. Bar´ at, G. Joret, and D. R. Wood. Disproof of the list Hadwiger conjecture. Electron. J. Combin. , 18(1):P232, 2011. http://www.combinatorics. org/ojs/index.php/eljc/article/view/v18i1p232 .\n",
      "- [5] F. Bernhart and P. C. Kainen. The book thickness of a graph. Journal of Combinatorial Theory, Series B , 27(3):320-331, 1979. doi: 10.1016/0095-8956(79)90021-2 .\n",
      "- [6] P. Blain, G. Bowlin, T. Fleming, J. Foisy, J. Hendricks, and J. Lacombe. Some results on intrinsically knotted graphs. J. Knot Theory Ramifications , 16(6): 749-760, 2007. doi: 10.1142/S021821650700552X .\n",
      "- [7] B. Bollob´ as and A. Thomason. Proof of a conjecture of Mader, Erd˝ os and Hajnal on topological complete subgraphs. European J. Combin. , 19(8):883-887, 1998. doi: 10.1006/eujc.1997.0188 .\n",
      "- [8] O. V. Borodin. On the total coloring of planar graphs. J. Reine Angew. Math. , 394:180-185, 1989. doi: 10.1515/crll.1989.394.180 .\n",
      "- [9] O. V. Borodin and A. V. Kostochka. Defective 2-colorings of sparse graphs. J. Combin. Theory Ser. B , 104:72-80, 2014. doi: 10.1016/j.jctb.2013.10.002 .\n",
      "\n",
      "- [10] O. V. Borodin and D. P. Sanders. On light edges and triangles in planar graphs of minimum degree five. Math. Nachr. , 170:19-24, 1994. doi: 10.1002/mana.19941700103 .\n",
      "- [11] O. V. Borodin, A. V. Kostochka, N. N. Sheikh, and G. Yu. M -degrees of quadrangle-free planar graphs. J. Graph Theory , 60(1):80-85, 2009. doi: 10.1002/jgt.20346 .\n",
      "- [12] O. V. Borodin, A. O. Ivanova, M. Montassier, and A. Raspaud. ( k, j )-coloring of sparse graphs. Discrete Appl. Math. , 159(17):1947-1953, 2011. doi: 10.1016/j.dam.2011.06.021 .\n",
      "- [13] O. V. Borodin, A. O. Ivanova, M. Montassier, and A. Raspaud. ( k, 1)coloring of sparse graphs. Discrete Math. , 312(6):1128-1135, 2012. doi: 10.1016/j.disc.2011.11.031 .\n",
      "- [14] O. V. Borodin, A. Kostochka, and M. Yancey. On 1-improper 2coloring of sparse graphs. Discrete Math. , 313(22):2638-2649, 2013. doi: 10.1016/j.disc.2013.07.014 .\n",
      "- [15] P. Bose, M. Smid, and D. R. Wood. Light edges in degree-constrained graphs. Discrete Math. , 282(1-3):35-41, 2004. doi: 10.1016/j.disc.2003.12.003 .\n",
      "- [16] M. Chen and A. Raspaud. On (3 , 1) ∗ -choosability of planar graphs without adjacent short cycles. Discrete Applied Mathematics , 162(C):159-166, 2014. doi: 10.1016/j.dam.2013.09.009 .\n",
      "- [17] M. Chen, A. Raspaud, and W. Wang. A (3 , 1) ∗ -choosable theorem on planar graphs. Journal of Combinatorial Optimization , 32(3):927-940, 2016. doi: 10.1007/s10878-015-9913-7 .\n",
      "- [18] I. Choi and L. Esperet. Improper coloring of graphs on surfaces. arXiv: 1603.02841 , 2016.\n",
      "- [19] I. Choi and A. Raspaud. Planar graphs with girth at least 5 are (3 , 5)-colorable. Discrete Math. , 338(4):661-667, 2015. doi: 10.1016/j.disc.2014.11.012 .\n",
      "- [20] Y. Colin de Verdi` ere. Sur un nouvel invariant des graphes et un crit` ere de planarit´ e. J. Combin. Theory Ser. B , 50(1):11-21, 1990.\n",
      "- [21] Y. Colin de Verdi` ere. On a new graph invariant and a criterion for planarity. In Graph structure theory , volume 147 of Contemp. Math. , pages 137-147. Amer. Math. Soc., 1993. doi: 10.1090/conm/147/01168 .\n",
      "- [22] J. H. Conway and C. M. Gordon. Knots and links in spatial graphs. J. Graph Theory , 7:445-453, 1983. doi: 10.1002/jgt.3190070410 .\n",
      "- [23] R. Corrˆ ea, F. Havet, and J.-S. Sereni. About a Brooks-type theorem for improper colouring. Australas. J. Combin. , 43:219-230, 2009. https://ajc. maths.uq.edu.au/pdf/43/ajc\\_v43\\_p219.pdf .\n",
      "- [24] L. Cowen, W. Goddard, and C. E. Jesurum. Defective coloring revisited. J. Graph Theory , 24(3):205-219, 1997. doi: 10.1002/(SICI)1097-0118(199703)24:3&lt;205::AID-JGT2&gt;3.0.CO;2-T .\n",
      "- [25] L. J. Cowen, R. H. Cowen, and D. R. Woodall. Defective colorings of graphs in surfaces: partitions into subgraphs of bounded valency. J. Graph Theory , 10\n",
      "\n",
      "- (2):187-195, 1986. doi: 10.1002/jgt.3190100207 .\n",
      "- [26] E. de Klerk, D. V. Pasechnik, and G. Salazar. Book drawings of complete bipartite graphs. Discrete Appl. Math. , 167:80-93, 2014. doi: 10.1016/j.dam.2013.11.001 .\n",
      "- [27] R. Diestel. Graph theory , volume 173 of Graduate Texts in Mathematics . Springer, 4th edition, 2010.\n",
      "- [28] P. Dorbec, T. Kaiser, M. Montassier, and A. Raspaud. Limits of near-coloring of sparse graphs. J. Graph Theory , 75(2):191-202, 2014. doi: 10.1002/jgt.21731 .\n",
      "- [29] V. Dujmovi´ c and D. R. Wood. On linear layouts of graphs. Discrete Math. Theor. Comput. Sci. , 6(2):339-358, 2004. http://dmtcs.episciences.org/ 317 .\n",
      "- [30] N. Eaton and T. Hull. Defective list colorings of planar graphs. Bull. Inst. Combin. Appl , 25:79-87, 1999.\n",
      "- [31] K. Edwards, D. Y. Kang, J. Kim, S. Oum, and P. Seymour. A relative of Hadwiger's conjecture. SIAM J. Discrete Math. , 29(4):2385-2388, 2015. doi: 10.1137/141002177 .\n",
      "- [32] J. Foisy. Intrinsically knotted graphs. J. Graph Theory , 39(3):178-187, 2002. doi: 10.1002/jgt.10017 .\n",
      "- [33] Z. F¨ uredi and J. Kahn. On the dimensions of ordered sets of bounded degree. Order , 3(1):15-20, 1986. doi: 10.1007/BF00403406 .\n",
      "- [34] E. Gethner and T. Sulanke. Thickness-two graphs. II. More new ninecritical graphs, independence ratio, cloned planar graphs, and singly and doubly outerplanar graphs. Graphs Combin. , 25(2):197-217, 2009. doi: 10.1007/s00373-008-0833-5 .\n",
      "- [35] N. Goldberg, T. W. Mattman, and R. Naimi. Many, many more intrinsically knotted graphs. Algebr. Geom. Topol. , 14(3):1801-1823, 2014. doi: 10.2140/agt.2014.14.1801 .\n",
      "- [36] D. J. Harvey and D. R. Wood. Average degree conditions forcing a minor. Electron. J. Combin. , 23(1):#P1.42, 2016. http://www.combinatorics.org/ ojs/index.php/eljc/article/view/v23i1p42/ .\n",
      "- [37] F. Havet and J.-S. Sereni. Improper choosability of graphs and maximum average degree. J. Graph Theory , 52(3):181-199, 2006. doi: 10.1002/jgt.20155 .\n",
      "- [38] W. He, X. Hou, K.-W. Lih, J. Shao, W. Wang, and X. Zhu. Edge-partitions of planar graphs and their game coloring numbers. J. Graph Theory , 41(4): 307-317, 2002. doi: 10.1002/jgt.10069 .\n",
      "- [39] L. S. Heath and S. V. Pemmaraju. Stack and queue layouts of posets. SIAM J. Discrete Math. , 10(4):599-625, 1997. doi: 10.1137/S0895480193252380 .\n",
      "- [40] L. S. Heath and A. L. Rosenberg. Laying out graphs using queues. SIAM J. Comput. , 21(5):927-958, 1992. doi: 10.1137/0221055 .\n",
      "- [41] K. Hendrey and D. R. Wood. The extremal function for Petersen minors. arXiv: 1508.04541 , 2016.\n",
      "\n",
      "- [42] J. P. Hutchinson. Coloring ordinary maps, maps of empires and maps of the moon. Math. Mag. , 66(4):211-226, 1993. doi: 10.2307/2690733 .\n",
      "- [43] J. Ivanˇ co. The weight of a graph. Ann. Discrete Math. , 51:113-116, 1992.\n",
      "- [44] B. Jackson and G. Ringel. Variations on Ringel's earth-moon problem. Discrete Math. , 211(1-3):233-242, 2000. doi: 10.1016/S0012-365X(99)00278-2 .\n",
      "- [45] I. T. Jakobsen. Weakenings of the conjecture of Hadwiger for 8- and 9-chromatic graphs. Technical Report 22, Matematisk Institut, Aarhus Universitet, Denmark, 1971.\n",
      "- [46] S. Jendro ' l and T. Madaras. On light subgraphs in plane graphs of minimum degree five. Discuss. Math. Graph Theory , 16(2):207-217, 1996.\n",
      "- [47] S. Jendro ' l and M. Tuh´ arsky. A Kotzig type theorem for non-orientable surfaces. Mathematica Slovaca , 56(3):245-253, 2006. http://dml.cz/dmlcz/130967 .\n",
      "- [48] S. Jendro ' l and H.-J. Voss. Light subgraphs of multigraphs on compact 2dimensional manifolds. Discrete Math. , 233(1-3):329-351, 2001.\n",
      "- [49] S. Jendro ' l and H.-J. Voss. Light subgraphs of graphs embedded in 2-dimensional manifolds of Euler characteristic glyph[lessorequalslant] 0. A survey. In Paul Erd˝ os and his Mathematics, II , volume 11 of Bolyai Soc. Math. Stud. , pages 375-411. J´ anos Bolyai Math. Soc., 2002.\n",
      "- [50] C. D. Keys. Graphs critical for maximal bookthickness. Pi Mu Epsilon J. , 6: 79-84, 1975.\n",
      "- [51] J. Kim, A. Kostochka, and X. Zhu. Improper coloring of sparse graphs with a given girth, II: constructions. J. Graph Theory , 81(4):403-413, 2016. doi: 10.1002/jgt.21886 .\n",
      "- [52] J. Koml´ os and E. Szemer´ edi. Topological cliques in graphs. II. Combin. Probab. Comput. , 5(1):79-90, 1996.\n",
      "- [53] A. V. Kostochka. The minimum Hadwiger number for graphs with a given mean degree of vertices. Metody Diskret. Analiz. , 38:37-58, 1982.\n",
      "- [54] A. V. Kostochka. Lower bound of the Hadwiger number of graphs by their average degree. Combinatorica , 4(4):307-316, 1984. doi: 10.1007/BF02579141 .\n",
      "- [55] A. V. Kostochka and N. Prince. On K s,t -minors in graphs with given average degree. Discrete Math. , 308(19):4435-4445, 2008. doi: 10.1016/j.disc.2007.08.041 .\n",
      "- [56] A. V. Kostochka and N. Prince. Dense graphs have K 3 ,t minors. Discrete Math. , 310(20):2637-2654, 2010. doi: 10.1016/j.disc.2010.03.026 .\n",
      "- [57] A. V. Kostochka and N. Prince. On K s,t -minors in graphs with given average degree, II. Discrete Math. , 312(24):3517-3522, 2012. doi: 10.1016/j.disc.2012.08.004 .\n",
      "- [58] D. K¨ uhn and D. Osthus. Complete minors in K s,s -free graphs. Combinatorica , 25(1):49-64, 2005. doi: 10.1007/s00493-005-0004-8 .\n",
      "- [59] D. K¨ uhn and D. Osthus. Forcing unbalanced complete bipartite minors. European J. Combin. , 26(1):75-81, 2005. doi: 10.1016/j.ejc.2004.02.002 .\n",
      "\n",
      "- [60] K.-W. Lih, Z. Song, W. Wang, and K. Zhang. A note on list improper coloring planar graphs. Appl. Math. Lett. , 14(3):269-273, 2001. doi: 10.1016/S0893-9659(00)00147-6 .\n",
      "- [61] W. Mader. Homomorphies¨ atze f¨ ur Graphen. Math. Ann. , 178:154-168, 1968. doi: 10.1007/BF01350657 .\n",
      "- [62] P. Mutzel, T. Odenthal, and M. Scharbrodt. The thickness of graphs: a survey. Graphs Combin. , 14(1):59-73, 1998. doi: 10.1007/PL00007219 .\n",
      "- [63] J. Neˇ setˇ ril and P. Ossona de Mendez. First order properties on nowhere dense structures. J. Symb. Log. , 75(3):868-887, 2010. doi: 10.2178/jsl/1278682204 .\n",
      "- [64] J. Neˇ setˇ ril and P. Ossona de Mendez. On nowhere dense graphs. European Journal of Combinatorics , 32(4):600-617, 2011. doi: 10.1016/j.ejc.2011.01.006 .\n",
      "- [65] J. Neˇ setˇ ril and P. Ossona de Mendez. Sparsity (Graphs, Structures, and Algorithms) , volume 28 of Algorithms and Combinatorics . Springer, 2012.\n",
      "- [66] J. Neˇ setˇ ril, P. Ossona de Mendez, and D. R. Wood. Characterisations and examples of graph classes with bounded expansion. European J. Combinatorics , 33(3):350-373, 2011. doi: 10.1016/j.ejc.2011.09.008 .\n",
      "- [67] M. Ozawa and Y. Tsutsumi. Primitive spatial graphs and graph minors. Rev. Mat. Complut. , 20(2):391-406, 2007. doi: 10.5209/rev REMA.2007.v20.n2.16496 .\n",
      "- [68] J. Pach and G. T´ oth. Graphs drawn with few crossings per edge. Combinatorica , 17(3):427-439, 1997. doi: 10.1007/BF01215922 .\n",
      "- [69] S. V. Pemmaraju. Exploring the Powers of Stacks and Queues via Graph Layouts . PhD thesis, Virginia Polytechnic Institute and State University, U.S.A., 1992.\n",
      "- [70] J. L. Ram´ ırez Alfons´ ın. Knots and links in spatial graphs: a survey. Discrete Math. , 302(1-3):225-242, 2005. doi: 10.1016/j.disc.2004.07.035 .\n",
      "- [71] B. Reed and D. R. Wood. Forcing a sparse minor. Combin. Probab. Comput. , 25:300-322, 2016. doi: 10.1017/S0963548315000073 .\n",
      "- [72] G. Ringel. F¨ arbungsprobleme auf Fl¨ achen und Graphen , volume 2 of Mathematische Monographien . VEB Deutscher Verlag der Wissenschaften, Berlin, 1959.\n",
      "- [73] G. Ringel. Das Geschlecht des vollst¨ andigen paaren Graphen. Abh. Math. Sem. Univ. Hamburg , 28:139-150, 1965.\n",
      "- [74] N. Robertson, P. D. Seymour, and R. Thomas. Hadwiger's conjecture for K 6 -free graphs. Combinatorica , 13(3):279-361, 1993. doi: 10.1007/BF01202354 .\n",
      "- [75] N. Robertson, P. D. Seymour, and R. Thomas. A survey of linkless embeddings. In N. Robertson and P. D. Seymour, editors, Graph structure theory. Proc. of AMS-IMS-SIAM Joint Summer Research Conf. on Graph Minors , volume 147 of Contempory Mathematics , pages 125-136. American Mathematical Society, 1993.\n",
      "\n",
      "- [76] N. Robertson, P. Seymour, and R. Thomas. Petersen family minors. J. Combin. Theory Ser. B , 64(2):155-184, 1995. doi: 10.1006/jctb.1995.1031 .\n",
      "- [77] H. Sachs. On a spatial analogue of Kuratowski's theorem on planar graphs an open problem. In M. Borowiecki, J. W. Kennedy, and M. M. Syslo, editors, Proc. Conf. on Graph Theory , volume 1018 of Lecture Notes in Mathematics , pages 230-241. Springer, 1983.\n",
      "- [78] A. Schrijver. Minor-monotone graph invariants. In Surveys in combinatorics , volume 241 of London Math. Soc. Lecture Note Ser. , pages 163-196. Cambridge Univ. Press, 1997. doi: 10.1017/CBO9780511662119.007 .\n",
      "- [79] F. Shahrokhi, L. A. Sz´ ekely, O. S´ ykora, and I. Vr ˇ to. Drawings of graphs on surfaces with few crossings. Algorithmica , 16(1):118-131, 1996. doi: 10.1007/BF02086611 .\n",
      "- [80] R. ˇ Skrekovski. List improper colorings of planar graphs with prescribed girth. Discrete Math. , 214(1-3):221-233, 2000. doi: 10.1016/S0012-365X(99)00145-4 .\n",
      "- [81] R. Thomas and P. Wollan. An improved linear edge bound for graph linkages. European J. Combin. , 26(3-4):309-324, 2005. doi: 10.1016/j.ejc.2004.02.013 .\n",
      "- [82] A. Thomason. An extremal function for contractions of graphs. Math. Proc. Cambridge Philos. Soc. , 95(2):261-265, 1984. doi: 10.1017/S0305004100061521 .\n",
      "- [83] A. Thomason. The extremal function for complete minors. J. Combin. Theory Ser. B , 81(2):318-338, 2001. doi: 10.1006/jctb.2000.2013 .\n",
      "- [84] H. van der Holst. On the graph parameters of Colin de Verdi` ere. In Ten years LNMB , pages 37-44. Math. Centrum Centrum Wisk. Inform., Amsterdam, 1997.\n",
      "- [85] H. van der Holst, L. Lov´ asz, and A. Schrijver. The Colin de Verdi` ere graph parameter. In Graph theory and Combinatorial Biology , volume 7 of Bolyai Soc. Math. Stud. , pages 29-85. J´ anos Bolyai Math. Soc., 1999.\n",
      "- [86] Y. Wang and L. Xu. Improper choosability of planar graphs without 4-cycles. SIAM J. Discrete Math. , 27(4):2029-2037, 2013. doi: 10.1137/120885140 .\n",
      "- [87] D. R. Wood. Cliques in graphs excluding a complete graph minor. Electron. J. Combin. , 23(3):#P3.18, 2016. http://www.combinatorics.org/ojs/index. php/eljc/article/view/v23i3p18/ .\n",
      "- [88] R. G. Wood and D. R. Woodall. Defective choosability of graphs without small minors. Electron. J. Combin. , 16(1):#R92, 2009. http://www.combinatorics. org/Volume\\_16/Abstracts/v16i1r92.html .\n",
      "- [89] D. R. Woodall. Defective choosability of graphs in surfaces. Discuss. Math. Graph Theory , 31(3):441-459, 2011. doi: 10.7151/dmgt.1557 .\n",
      "- [90] M. Yancey. Thickness for improper colorings, 2012. http://www.math. illinois.edu/ ~ dwest/regs/impthic.html .\n",
      "\n",
      "- [91] H. Zhang. On (4 , 1) ∗ -choosability of toroidal graphs without chordal 7-cycles and adjacent 4-cycles. Commentationes Mathematicae Universitatis Carolinae , 54(3):339-344, 2013. http://hdl.handle.net/10338.dmlcz/143305 .\n",
      "- [92] H. Zhang. (3, 1)*-choosability of graphs of nonnegative characteristic without intersecting short cycles. Proceedings - Mathematical Sciences , 126(2):159-165, 2016. doi: 10.1007/s12044-016-0272-9 .\n",
      "- [93] L. Zhang. A (3 , 1) ∗ -choosable theorem on toroidal graphs. Discrete Applied Mathematics , 160(3):332-338, 2012. doi: 10.1016/j.dam.2011.10.019 .\n",
      "\n",
      "Patrice Ossona de Mendez\n",
      "\n",
      "Centre d'Analyse et de Math´ ematiques Sociales (CNRS, UMR 8557)\n",
      "\n",
      "190-198 avenue de France, 75013 Paris, France\n",
      "\n",
      "- and -\n",
      "\n",
      "Computer Science Institute of Charles University (IUUK)\n",
      "\n",
      "Malostransk´ e n´ am.25, 11800 Praha 1, Czech Republic\n",
      "\n",
      "E-mail address :\n",
      "\n",
      "pom@ehess.fr\n",
      "\n",
      "Sang-il Oum\n",
      "\n",
      "Department of Mathematical Sciences, KAIST\n",
      "\n",
      "Daejeon, South Korea\n",
      "\n",
      "E-mail address :\n",
      "\n",
      "sangil@kaist.edu\n",
      "\n",
      "David R. Wood\n",
      "\n",
      "School of Mathematical Sciences, Monash University\n",
      "\n",
      "Melbourne, Australia\n",
      "\n",
      "E-mail address :\n",
      "\n",
      "david.wood@monash.edu\n",
      "Document 13:\n",
      "Observations of heating along intermittent structures in the inner heliosphere from PSP data\n",
      "\n",
      "R. A. Qudsi, 1 B. A. Maruca, 2 W. H. Matthaeus, 2 T. N. Parashar, 1 Riddhi Bandyopadhyay, 1 R. Chhiber, 1 A. Chasapis, 3 Melvyn L. Goldstein, 4,5 S. D. Bale, 6,7,8 J. W. Bonnell, 6 T. Dudok de Wit, 9 K. Goetz, 10 P. R. Harvey, 6 R. J. MacDowall, 11 D. Malaspina, 3 M. Pulupa, 6 J. C. Kasper, 12,13 K. E. Korreck, 13 A. W. Case, 13 M. Stevens, 13 P. Whittlesey, 6 D. Larson, 6 R. Livi, 6 M. Velli, 14 and N. Raouafi 15\n",
      "\n",
      "1 Department of Physics and Astronomy, University of Delaware, Newark, DE 19716, USA\n",
      "\n",
      "2 Department of Physics and Astronomy, Bartol Research Institute, University of Delaware, Newark, DE 19716, USA\n",
      "\n",
      "3\n",
      "\n",
      "Laboratory for Atmospheric and Space Physics, University of Colorado Boulder, Boulder, CO 80303, USA 4\n",
      "\n",
      "NASA Goddard Space Flight Center, Greenbelt, MD 20771, USA\n",
      "\n",
      "5 University of Maryland Baltimore County, Baltimore, MD 21250, USA\n",
      "\n",
      "6 Space Sciences Laboratory, University of California, Berkeley, CA 94720-7450, USA\n",
      "\n",
      "7 Physics Department, University of California, Berkeley, CA 94720-7300, USA\n",
      "\n",
      "8 The Blackett Laboratory, Imperial College London, London, SW7 2AZ, UK 9 LPC2E, CNRS and University of Orl´ eans, Orl´ eans, France\n",
      "\n",
      "School of Physics and Astronomy, University of Minnesota, Minneapolis, MN 55455, USA 11 Code 695, NASA Goddard Space Flight Center, Greenbelt, MD 20771, USA\n",
      "\n",
      "10\n",
      "\n",
      "Climate and Space Sciences and Engineering, University of Michigan, Ann Arbor, MI 48109, USA 13 Smithsonian Astrophysical Observatory, Cambridge, MA 02138 USA\n",
      "\n",
      "12\n",
      "\n",
      "14 Department of Earth, Planetary, and Space Sciences, University of California, Los Angeles, CA 90095, USA 15 Johns Hopkins University Applied Physics Laboratory, Laurel, MD, USA\n",
      "\n",
      "(Received December 12, 2019)\n",
      "\n",
      "## ABSTRACT\n",
      "\n",
      "The solar wind proton temperature at 1-au has been found to be correlated with small-scale intermittent magnetic structures, i.e., regions with enhanced temperature are associated with coherent structures such as current sheets. Using Parker Solar Probe data from the first encounter, we study this association using measurements of radial proton temperature, employing the Partial Variance of Increments (PVI) technique to identify intermittent magnetic structures. We observe that the probability density functions of high-PVI events have higher median temperatures than those with lower PVI, The regions in space where PVI peaks were also locations that had enhanced temperatures when compared with similar regions suggesting a heating mechanism in the young solar wind that is associated with intermittency developed by a nonlinear turbulent cascade.n the immediate vicinity.\n",
      "\n",
      "## Keywords: PSP- PVI\n",
      "\n",
      "## 1. INTRODUCTION\n",
      "\n",
      "Solar wind is a stream of charged particles emanating from Sun originating in the corona (Parker 1960, 1963). It is highly magnetized collisionless plasma streaming at supersonic speed and is primarily composed of ionized hydrogen (i.e., protons) (Marsch et al. 1982; Kasper et al. 2012).\n",
      "\n",
      "Despite decades of observation, the exact process that originally heats and accelerates solar-wind plasma remains unknown, but several candidates have been proposed. Turbulence cascade transfers energy from large to small scales, which can ultimate lead to dissipation and heating (Velli et al. 1989; Velli 1993; Matthaeus et al. 1999; Dmitruk et al. 2002; Cranmer &amp; van Ballegooijen 2005; Cranmer et al. 2007; Cranmer &amp; van Ballegooijen 2012; Cranmer 2014; Verdini &amp; Velli 2007; Verdini et al. 2009a,b; Chandran &amp; Hollweg 2009; Perez &amp; Chandran 2013; Lionello et al. 2014). Current sheets, generated by cascading vortices, can also lead to localized heating (Parashar et al. 2009; Osman et al. 2011, 2012a,b; Gingell et al. 2015). Wave particle interactions - including, e.g., microinstabilities, Landau damping, and ion-cyclotron resonance - can likewise result in heating and other dramatic changes to the particles' phasespace distribution (Gary 1993; Sahraoui et al. 2010; Klein &amp; Howes 2015).\n",
      "\n",
      "In this study, we focused on coherent structures: features in the plasma that are persistent through time, concentrated in space, or both (Greco et al. 2018). Such structures can be produced by turbulent cascade (Osman et al. 2012b) and are also associated with current sheets (Yordanova et al. 2016). Osman et al. (2011, 2012b) analyzed in-situ observations and of near-Earth solar wind and found clear indications that coherent structures correlate with local enhancements in temperature.\n",
      "\n",
      "In this study, we revisit the techniques of Osman et al. (2011, 2012b), and, by applying them to observations from Parker Solar Probe (PSP), explore the relationship between plasma structures and heating in nascent solarwind plasma. Section 2 provides the background on such structures and introduces the reader to the physics of technique employed in data analysis which is described in section 3. In section 4 we present the results and discuss its implication. Section 5 summarizes the results along with a conclusion and potential future works.\n",
      "\n",
      "## 2. BACKGROUND\n",
      "\n",
      "The solar wind at 1-au exhibits localized structures that have been studied since the pioneering work of Burlaga (1968); Hudson (1970); Tsurutani &amp; Smith (1979) and others, and more recently by Ness &amp; Burlaga (2001); Neugebauer (2006); Erd˝ oS &amp; Balogh (2008). Several studies have found evidence that plasma turbulence generates these structures dynamically (Matthaeus &amp; Lamkin 1986; Veltri 1999; Osman et al. 2013). The structures are inhomogeneous and highly intermittent (Osman et al. 2011, 2013; Greco et al. 2008) .\n",
      "\n",
      "Some recent studies, both observational and numerical, have shown that these structures are correlated with the regions of enhanced temperature in the plasma (Osman et al. 2012a, 2011; Greco et al. 2012) and understanding the mechanisms by which the turbulence heats the plasma may also help solve the coronal heating problem (Osman et al. 2012b). This is a particularly attractive scenario especially given the ubiquity of the localized structures. Study performed on data from PIC simulation by Wu et al. (2013) shows that the correlation between enhanced temperature and coherent structures exists for sub ion inertial length ( d i ). Further evidence of this is provided by TenBarge &amp; Howes (2013) for Gyrokinetic simulation, Parashar et al. (2009); Wan et al. (2012); Karimabadi et al. (2013); Wan et al. (2015) for PIC and Servidio et al. (2012); Servidio et al. (2015) for Vlasov simulations respectively. Work done by Chasapis et al. (2015) and Yordanova et al. (2016) on from Clus- ter and MMS data show similar results from observation vantage point.\n",
      "\n",
      "In this study, we investigate these discontinuities in the magnetic field and explore their association with local enhancements in ion temperature. One method for identifying a discontinuity in a time series of magneticfield (or any other) data is Partial Variance of Increments (PVI), which is both a powerful and reliable tool for identifying and locating such regions. Following Greco et al. (2008) we define normalized PVI as:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where, ∆ B ( t, ∆ t ) = B ( t +∆ t ) -B ( t ), is the vector increment in magnetic field at any given time t and a time lag of ∆ t . For studying local structures induced by turbulence, ∆ t is typically chosen to be, assuming the validity of Taylor's hypothesis (Taylor 1938), of the order of d i . 〈 ... 〉 is the ensemble average over a period of time, and I is the normalized PVI. Osman et al. (2012b) showed that PVI values greater than 2.4 imply the existence of strong non-Gaussian coherent structures. Although they constitute only a small fraction of total data set their contribution to the total internal energy per unit volume is high. This emphasizes the importance of using the PVI technique for such studies. We also note that an analogous examination of the association of PVI events with energetic particles was carried out at 1-au, (Tessein et al. 2016). A corresponding study of the first PSP orbit using IS glyph[circledot] IS data is reported by Bandyopadhyay et al (this volume).\n",
      "\n",
      "## 3. DATA SELECTION AND METHODOLOGY\n",
      "\n",
      "We analyzed data from PSP's first encounter with the Sun (October 31 to November 11, 2018). The FIELDS fluxgate magnetometers provided measurements of the local magnetic field at a rate of 64 samples/NYseconds, where 1 NYsecond is defined as 0.837 seconds (Bale et al. 2016). Radial proton temperature/thermal speed data was obtained from the Solar Probe Cup (SPC), part of the Solar Wind Electron, Alpha and Proton (SWEAP) suite (Kasper et al. 2016). The average speed of solar wind during the first encounter was around 350 km/s for the most part and crossed 500km/s only on the last day of the encounter. Thus, using Taylors Hypothesis, 1 NYs corresponds to a length scale of 300 km.\n",
      "\n",
      "Since SPC only measures radial temperature, and proton temperature is expected to be significantly anisotropic we needed to ensure that the temperature we were measuring was indeed parallel temperature. Thus, we only considered data points where magnetic field was mostly radial. Any interval where the angle\n",
      "\n",
      "between B r ˆ r and B was more than 30 degrees was not considered. This ensured that the temperature measured by SPC was indeed the parallel temperature. For the calculation of PVI according to Equation 1, we used 64 NYHz data, with a lag of 1 NYs, which is the native cadence of SPC (Kasper et al. 2016). The ensemble averaging was done over 8 hours, which is several times the estimated correlation time. In this study we used the correlation time computed in Parashar et al. (2019). However there are there are few subtleties associated with this calculation, and Smith et al. (2001); Isaacs et al. (2015); Krishna Jagarlamudi et al. (2019); Bandyopadhyay et al. (2019) offer more insights and discussion on this topic along with potential issues in such determination. We also carried out the analysis for various different averaging times (from 1 to 12 hours) and it was observed to have minimal affect on the outcome. PVI time series was then resampled to ion cadence of 1NYHz in the way such that for each interval of 1 NYs, the maximum value of PVI in that interval was chosen.\n",
      "\n",
      "In this study we focused on the second half of the encounter, immediately after PSP was at its perihelion. The second half of the encounter has very different properties compared to the first half. A greater number of energetic particles were observed (McComas &amp; IS glyph[circledot] IS 2019), the solar wind speed was higher (Kasper et al. 2019) , and there were many more switchbacks (Bale &amp; Fields 2019). Bandyopadhyay et al. (2019) observed enhanced local energy transfer, which points towards a more turbulent period in general, and thus a suitable environment for PVI study.\n",
      "\n",
      "## 4. RESULTS AND DISCUSSION\n",
      "\n",
      "Figure 1 shows the joint histogram of radial protontemperature and PVI for the first encounter. Increasing PVI color contours have an upwards trend, as we see temperature distribution showing a positive slope with increase in value of PVI. The positive correlation between temperature and PVI suggest some kind of heating in the regions with high PVI. We then conditionally sampled radial proton temperature. Conditionally sampled means that we arrange the data by increasing value of PVI and then divide all the data points in 6 bins such that each bin has equal number of points. We then calculate the temperature distribution within each bin which is shown in Figure 2.\n",
      "\n",
      "As PVI increases, the probability density increases for the higher temperature and decreases for the lower temperature which is opposite of what we see at the low temperatures where probability density is highest for the lowest PVI. Median temperature, shown by vertical lines in Figure 2, for each of the distribution increases\n",
      "\n",
      "Figure 1. Joint histogram of radial proton-temperature and PVI for the second half of first encounter on a log-log scale. There is an upward trend between PVI and temperature as the blue region in the plot tilts upwards showing an increase of temperature as PVI increases.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Figure 2. PDFs for the radial proton-temperature for the second half of first encounter. Each PDF corresponds to a different PVI range such that each PVI bin has equal number of data points. The probability density increases with increase in temperature for high PVI whereas it decreases for low PVI PDF. Vertical lines show the median temperature for each of the PDF plot.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "implying presence of stronger and stronger heating as we go to higher and more extreme values of PVI. For PVI &lt; 1, median value of the temperature is 5 . 32 × 10 5 K whereas for PVI &gt; 6, the median temperature increases to 1 . 01 × 10 6 K. Osman et al. (2011) observed similar increase in average temperature in their study of solar\n",
      "\n",
      "Figure 3. Figure shows conditional average temperature for different PVI thresholds at the point of a a PVI event. ˜ T p peaks at the instant of PVI event and continues to have elevated temperature in its vicinity within the correlation time scale. Red curve, corresponding to lowest PVI shows a dip suggesting no heating when the magnetic field is very smooth.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "wind at 1-au. This is consistent with heating occurring in the regions with small scale coherent structure in MHD turbulence.\n",
      "\n",
      "In order to further demonstrate this relationship, we looked at the temperature at the point of high PVI event and its immediate surrounding in space using the methodology described by Osman et al. (2012b). We compute the mean value of temperature at the point of the PVI event and for points near the PVI events separated from it by up to one correlation length. Formally, these averages may be expressed as:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where ˜ T p is the conditionally average temperature for all the events, ∆ t is the time difference relative to the position of PVI events, t I is the time of PVI events between the threshold θ 1 and θ 2\n",
      "\n",
      "Figure 3 shows the plot of ˜ T p for various thresholds for the second half of the encounter. Not only do we observe enhanced temperature at the point of high PVI events, suggesting localized heating at those points, we also see that ˜ T p for a higher PVI event is consistently higher than nearby points separated by up to a correlation length. This implies that the points nearby an identified PVI event have an elevated average temperature, continuously approaching the elevated average temperature found at the PVI event itself. Some of this effect may be due to clustering of PVI events (see Chhiber et al. (2019)). Another point worth noting in Figure 3 is the valley in the temperature profile for small PVI. This is the region where background magnetic field is smooth and it appears that in such regions, the temperature is lower than the temperature of plasma in its immediate surrounding , which is concurrent with the fact that in those places there is no turbulence heating. Osman et al. (2012b) found similar result in their study at 1AU. However, in our study we find a significant dip compared to the dip reported in Osman et al. (2012b), ∼ 10 5 K compared to ∼ 2 × 10 3 K.\n",
      "\n",
      "## 5. CONCLUSION\n",
      "\n",
      "In this study, we used in-situ observations from PSP's first encounter with the Sun to explore the association of proton heating with coherent magnetic structures in the young solar wind. We identified enhancements of PVI (Greco et al. 2008) as indicating the presence of such a structure (Osman et al. 2011, 2012b). We observed that the joint histogram of PVI and proton radial temperature shows positive trend as shown in Figure 1. We also observed that the PDF of data, as shown in Figure 2 with higher PVI has higher mean temperature compared to those with lower PVI. This strongly supports the theory that the solar wind in those regions are heated by coherent structures which are generated by plasma turbulence.\n",
      "\n",
      "The present results demonstrate both the shifting of the PDF of temperature towards higher values with increasing PVI condition, (in Figure 2) and the spatial/temporal localization of the temperature enhancement near PVI events (in Figure 3). Both of these are fully consistent with findings in the two papers that examine these effects ( Osman et al. (2011, 2012a), respectively) . It is interesting that these effects are present clearly in the PSP first orbit where turbulence is presumably younger and possibly less well developed than it is at 1 AU. It is possible that the temperature differential between low and high PVI is somewhat less in the PSP data than in the ACE data at 1 AU (Osman 2011), but additional samples by PSP will be needed to draw any firm conclusion of this type.\n",
      "\n",
      "In order to further demonstrate this association we looked at the conditionally average temperatures at the point of a high PVI event and in its immediate surrounding up to 1 correlation length. We observed that not only the point of event has the highest temperature, its vicinity shows enhanced temperature compared to lower PVI events. The local maxima of these temperature profiles are most prominent from higher PVI events suggesting stronger heating. The plateau region of each thresholds are distinct, and for higher threshold they maintain\n",
      "\n",
      "a high value suggesting clustering of PVI events around a large discontinuity. For very smooth magnetic field we see a dip in the average temperature at that point. Osman et al. (2012b) found similar behaviour in their study of solar wind at 1AU, though neither the heating nor the dip in temperature for small PVI that they reported in their study was as high as what we observed in our study. This suggests that either coherent structures are more efficient in heating the plasma near the Sun compared to 1-au or we have a lot more such structures as we move closer to the Sun. Since these coherent structures are generated by plasma turbulence, these observations suggest that non-linear turbulence cascade play a crucial role in heating the nascent solar wind. Given the ubiquitous nature of such structures, this process can help explain the coronal heating.\n",
      "\n",
      "magnetic field (see Section 3). Once ion temperature anisotropy data are available, we will revisit this work to explore both scalar and anisotropic heating. Theoretical studies have found that turbulent cascade can generate strong temperature anisotropy near coherent structures (Parashar &amp; Matthaeus 2016).\n",
      "\n",
      "We also hope to further explore Figure 3. Careful inspection of this plot reveals a very slight asymmetry in the shape of the temperature profile before and after the PVI event. The phenomenon was also noted in 1au solar wind by (Osman et al. 2012b). The cause and significance of this asymmetry remain unclear, but it may it suggests a connection between local heating and large-scale processes such as heat flux.\n",
      "\n",
      "A significant limitation of this study was unavailability of temperature-anisotropy data. The temperature measures we used were not the scalar temperature but rather the radial temperature, for which reason we limited our observations to period of nearly-radial\n",
      "\n",
      "## ACKNOWLEDGMENT\n",
      "\n",
      "This work was supported as a part of the PSP mission under contract NNN06AA01C. This research was partially supported by the Parker Solar Probe Plus project through Princeton IS glyph[circledot] IS subcontract SUB0000165.\n",
      "\n",
      "## REFERENCES\n",
      "\n",
      "- Bale, S., &amp; Fields. 2019, submitted.\n",
      "- Bale, S. D., Goetz, K., Harvey, P. R., et al. 2016, Space Science Reviews, 204, 49\n",
      "- Gingell, P. W., Burgess, D., &amp; Matteini, L. 2015, The Astrophysical Journal, 802, 4\n",
      "- Bandyopadhyay, R., Matthaeus, W. H., Parashar, T. N., et al. 2019, This Volume\n",
      "- Burlaga, L. F. 1968, Solar Physics, 4, 67\n",
      "- Chandran, B. D. G., &amp; Hollweg, J. V. 2009, The Astrophysical Journal, 707, 1659\n",
      "- Chasapis, A., Retin` o, A., Sahraoui, F., et al. 2015, ApJ, 804, L1\n",
      "- Chhiber, R., Goldstein, M. L., Maruca, B. A., et al. 2019, This Volume\n",
      "- Cranmer, S. R. 2014, The Astrophysical Journal Supplement Series, 213, 16\n",
      "- Cranmer, S. R. 2012, The Astrophysical Journal, 754, 92\n",
      "- Cranmer, S. R., van Ballegooijen, A. A., &amp; Edgar, R. J. 2007, The Astrophysical Journal Supplement Series, 171, 520\n",
      "- Cranmer, S. R., &amp; van Ballegooijen, A. A. 2005, The Astrophysical Journal Supplement Series, 156, 265\n",
      "- Dmitruk, P., Matthaeus, W. H., Milano, L. J., et al. 2002, The Astrophysical Journal, 575, 571\n",
      "- Erd˝ oS, G., &amp; Balogh, A. 2008, Advances in Space Research, 41, 287\n",
      "- Gary, S. P. 1993, Theory of Space Plasma Microinstabilities (Cambridge, UK: Cambridge University Press)\n",
      "- Greco, A., Matthaeus, W. H., Perri, S., et al. 2018, SSRv, 214, 1\n",
      "- Greco, A., Valentini, F., Servidio, S., &amp; Matthaeus, W. H. 2012, PhRvE, 86, 066405\n",
      "- Greco, A., Chuychai, P., Matthaeus, W. H., Servidio, S., &amp; Dmitruk, P. 2008, Geophysical Research Letters, 35, L19111\n",
      "- Hudson, P. 1970, Planetary and Space Science, 18, 1611 Isaacs, J. J., Tessein, J. A., &amp; Matthaeus, W. H. 2015, Journal of Geophysical Research (Space Physics), 120, 868\n",
      "- Karimabadi, H., Roytershteyn, V., Wan, M., et al. 2013, Physics of Plasmas, 20, 012303\n",
      "- Kasper, J. C., SWEAP, &amp; FIELDS. 2019, submitted.\n",
      "- Kasper, J. C., Abiad, R., Austin, G., et al. 2016, Space Science Reviews, 204, 131\n",
      "- Kasper, J. C., Stevens, M. L., Korreck, K. E., et al. 2012, The Astrophysical Journal, 745, 162\n",
      "- Klein, K. G., &amp; Howes, G. G. 2015, Phys. Plasmas, 22, 032903\n",
      "- Krishna Jagarlamudi, V., Dudok de Wit, T., Krasnoselskikh, V., &amp; Maksimovic, M. 2019, ApJ, 871, 68\n",
      "- Lionello, R., Velli, M., Downs, C., et al. 2014, The Astrophysical Journal, 784, 120\n",
      "\n",
      "- Marsch, E., Mhlhuser, K.-H., Rosenbauer, H., Schwenn, R., &amp; Neubauer, F. M. 1982, J. Geophys. Res., 87, 35\n",
      "- Matthaeus, W. H., Zank, G. P., Oughton, S., Mullan, D. J., &amp; Dmitruk, P. 1999, The Astrophysical Journal, 523, L93\n",
      "- Matthaeus, W. H., &amp; Lamkin, S. L. 1986, The Physics of Fluids, 29, 2513\n",
      "- McComas, D. J., &amp; IS glyph[circledot] IS. 2019, submitted.\n",
      "- Ness, N. F., &amp; Burlaga, L. F. 2001, Journal of Geophysical Research: Space Physics, 106, 15803\n",
      "- Neugebauer, M. 2006, Journal of Geophysical Research: Space Physics, 111, doi:10.1029/2005JA011497\n",
      "- Osman, K. T., Matthaeus, W. H., Kiyani, K. H., Hnat, B., &amp; Chapman, S. C. 2013, PhRvL, 111, 201101\n",
      "- Osman, K. T., Matthaeus, W. H., Hnat, B., &amp; Chapman, S. C. 2012a, PhRvL, 108, 261103\n",
      "- Osman, K. T., Matthaeus, W. H., Wan, M., &amp; Rappazzo, A. F. 2012b, PhRvL, 108, 261102\n",
      "- Osman, K. T., Matthaeus, W. H., Greco, A., &amp; Servidio, S. 2011, ApJ, 727, L11\n",
      "- Parashar, T. N., Goldstein, M. L., Maruca, B. A., et al. 2019, This Volume\n",
      "- Parashar, T. N., &amp; Matthaeus, W. H. 2016, ApJ, 832, 57\n",
      "- Parashar, T. N., Shay, M. A., Cassak, P. A., &amp; Matthaeus, W. H. 2009, Physics of Plasmas, 16, 032310\n",
      "- Parker, E. N. 1960, ApJ, 132, 821\n",
      "- -. 1963, Interplanetary dynamical processes.\n",
      "- Perez, J. C., &amp; Chandran, B. D. G. 2013, The Astrophysical Journal, 776, 124\n",
      "- Sahraoui, F., Goldstein, M. L., Belmont, G., Canu, P., &amp; Rezeau, L. 2010, Phys. Rev. Lett., 105, 131101\n",
      "- Servidio, S., Valentini, F., Perrone, D., et al. 2015, Journal of Plasma Physics, 81, 325810107\n",
      "- Servidio, S., Valentini, F., Califano, F., &amp; Veltri, P. 2012, Physical Review Letters,\n",
      "\n",
      "doi:10.1103/PhysRevLett.108.045001\n",
      "\n",
      "- Smith, C. W., Matthaeus, W. H., Zank, G. P., et al. 2001, J. Geophys. Res., 106, 8253\n",
      "- Taylor, G. I. 1938, Proceedings of the Royal Society of London Series A, 164, 476\n",
      "- TenBarge, J. M., &amp; Howes, G. G. 2013, The Astrophysical Journal, 771, L27\n",
      "- Tessein, J. A., Ruffolo, D., Matthaeus, W. H., &amp; Wan, M. 2016, Geophysical Research Letters, 43, 3620\n",
      "- Tsurutani, B. T., &amp; Smith, E. J. 1979, Journal of Geophysical Research: Space Physics, 84, 2773\n",
      "- Velli, M. 1993, A&amp;A, 270, 304\n",
      "- Velli, M., Grappin, R., &amp; Mangeney, A. 1989, PhRvL, 63, 1807\n",
      "- Veltri, P. 1999, Plasma Physics and Controlled Fusion, 41, A787\n",
      "- Verdini, A., Velli, M., &amp; Buchlin, E. 2009a, The Astrophysical Journal, 700, L39\n",
      "- Verdini, A., Velli, M., Matthaeus, W. H., Oughton, S., &amp; Dmitruk, P. 2009b, The Astrophysical Journal, 708, L116\n",
      "- Verdini, A., &amp; Velli, M. 2007, The Astrophysical Journal, 662, 669\n",
      "- Wan, M., Matthaeus, W. H., Roytershteyn, V., et al. 2015, Phys. Rev. Lett., 114, 175002\n",
      "- Wan, M., Matthaeus, W. H., Karimabadi, H., et al. 2012, Phys. Rev. Lett., 109, 195001\n",
      "- Wu, P., Perri, S., Osman, K., et al. 2013, The Astrophysical Journal, 763, L30\n",
      "- Yordanova, E., Vrs, Z., Varsani, A., et al. 2016, Geophysical Research Letters, 43, 5969\n",
      "Document 14:\n",
      "## Similar complex kinematics within two massive, filamentary infrared dark clouds glyph[star]\n",
      "\n",
      "A . T. Barnes 1 , 2 , 3 † , J. D. Henshaw 4 , P. Caselli 3 , I. Jim´ enez-Serra 5 , J. C. Tan 6 ,\n",
      "\n",
      "1 Astrophysics Research Institute, Liverpool John Moores University, 146 Brownlow Hill, Liverpool L3 5RF, UK\n",
      "\n",
      "## F. Fontani 7 , A. Pon 8 , and S. Ragan 9\n",
      "\n",
      "- 2 School of Physics and Astronomy, University of Leeds, LS2 9JT, Leeds, UK\n",
      "- 3 Max Plank Institute for Extraterrestrial Physics (MPE), Giessenbachstrasse 1, 85748 Garching, Germany\n",
      "- 4 Max Plank Institute for Astronomy (MPIA), Konigstuhl 17, 69117 Heidelberg, Germany\n",
      "- 5 Queen Mary University of London, Astronomy Unit, Mile End Road, London E1 4NS, UK\n",
      "- 6 Department of Astronomy, University of Florida, Gainesville, FL 32611, USA\n",
      "- 7 INAF - Osservatorio Astrofisico di Arcetri, L.go E. Fermi 5, I-50125, Firenze, Italy\n",
      "- 8 Department of Physics and Astronomy, The University of Western Ontario, 1151 Richmond Street, London, N6A 3K7, Canada\n",
      "- 9 School of Physics &amp; Astronomy, Cardiff University, Queen's building, The parade, Cardiff, CF24 3AA, UK\n",
      "\n",
      "Accepted 2018 January 17. Received 2018 January 11; in original form 2017 November 27.\n",
      "\n",
      "## ABSTRACT\n",
      "\n",
      "Infrared dark clouds (IRDCs) are thought to be potential hosts of the elusive early phases of high-mass star formation. Here we conduct an in-depth kinematic analysis of one such IRDC, G034.43+00.24 (Cloud F), using high sensitivity and high spectral resolution IRAM-30m N 2 H + (1 -0) and C 18 O(1 -0) observations. To disentangle the complex velocity structure within this cloud we use Gaussian decomposition and hierarchical clustering algorithms. We find that four distinct coherent velocity components are present within Cloud F. The properties of these components are compared to those found in a similar IRDC, G035.39-00.33 (Cloud H). We find that the components in both clouds have: high densities (inferred by their identification in N 2 H + ), trans-to-supersonic non-thermal velocity dispersions with Mach numbers of ∼ 1 . 5 -4, a separation in velocity of ∼ 3 kms -1 , and a mean red-shift of ∼ 0.3 km s -1 between the N 2 H + (dense gas) and C 18 O emission (envelope gas). The latter of these could suggest that these clouds share a common formation scenario. We investigate the kinematics of the larger-scale Cloud F structures, using lower-density-tracing 13 CO(1 -0) observations. A good correspondence is found between the components identified in the IRAM-30m observations and the most prominent component in the 13 CO data. We find that the IRDC Cloud F is only a small part of a much larger structure, which appears to be an inter-arm filament of the Milky Way.\n",
      "\n",
      "Key words: stars: formation - stars: massive - ISM: clouds - ISM: individual (G034.43+00.24) - ISM: molecules.\n",
      "\n",
      "ing, one needs to study the initial conditions under which they form, before protostellar feedback removes information (e.g. kinematic and chemical) of the environment in which the earliest stages of star formation occur. Therefore, observations of quiescent star-forming regions have to be made in order to study the initial conditions of high-mass star, and stellar cluster, formation. This necessitates the identification of molecular clouds with sufficient mass and density, which currently exhibit a low star formation activity.\n",
      "\n",
      "Infrared dark clouds (IRDCs) are a group of molecular clouds that were first identified in the mid-nineties as promising astro-laboratories to study the initial conditions of high-mass star formation. The Infrared Space Ob-\n",
      "\n",
      "## 1 INTRODUCTION\n",
      "\n",
      "Young stars, particularly the most massive, are of great astrophysical importance. The huge amounts of energy and momentum that they inject into the interstellar medium have a significant effect on the evolution of their host galaxy. Yet, despite ongoing efforts, the formation process of massive stars is not fully understood. To gain such an understand-\n",
      "\n",
      "glyph[star] Based on observations carried out with the IRAM 30m Telescope. IRAM is supported by INSU/CNRS (France), MPG (Germany) and IGN (Spain).\n",
      "\n",
      "† E-mail: a.t.barnes@2014.ljmu.ac.uk\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "servatory ( ISO ; 15 µ m; Perault et al. 1996) and the Midcourse Space Experiment ( MSX ; 7 to 25 µ m; Egan et al. 1998) were used to initially discover IRDCs, and identified them as regions of strong mid-infrared extinction against the background Galactic emission. More recent works have shown that IRDCs are cold ( &lt; 20 K; Pillai et al. 2006; Ragan et al. 2011), are massive ( ∼ 10 3 -5 M glyph[circledot] ; Rathborne et al. 2006; Longmore et al. 2012; Kainulainen &amp; Tan 2013), have large column densities ( N (H 2 ) ∼ 10 22 -25 cm -2 ; Egan et al. 1998; Carey et al. 1998; Simon et al. 2006a; Vasyunina et al. 2009), and have high volume densities ( n (H 2 ) ∼ 10 3 -5 cm -3 ; e.g. Peretto et al. 2010; Hernandez et al. 2011; Butler &amp; Tan 2012). Of particular importance, IRDCs have been shown to have large reservoirs of relatively pristine gas, which has not been influenced by star formation, as inferred from their chemical composition (e.g. Miettinen et al. 2011; Gerner et al. 2015; Barnes et al. 2016; Kong et al. 2016).\n",
      "\n",
      "Although the physical and chemical properties of IRDCs have been well studied (the extinction or continuum dust morphology, dust and gas masses, dust and gas temperatures, and levels of molecular depletion) only recently have dedicated studies of their complex kinematic structure been attempted. Observations of molecular line transitions have shown that molecular clouds, even with relatively simple extinction morphologies, can contain a complex network of velocity components (Henshaw et al. 2014; Hacar et al. 2013, 2017). However, reliably disentangling such structures is difficult (e.g. Jim´ enez-Serra et al. 2010, 2014; Devine et al. 2011; Henshaw et al. 2013, 2014; Tan et al. 2013; Pon et al. 2016b; Kong et al. 2017; Zamora-Avil´ es et al. 2017), yet doing so is key to understanding the role of molecular cloud structure and evolution in star formation (e.g. Ragan et al. 2006; Devine et al. 2011; Rygl et al. 2013; Kirk et al. 2013; Tackenberg et al. 2014).\n",
      "\n",
      "This work will focus on the IRDC G034.43+00.24 (henceforth Cloud F), which was first identified by Miralles et al. (1994) as an unresolved elongated structure in NH 3 emission (a tracer of cold, dense gas) to the north of the bright IRAS source 1807+0121 (see Figure 1). Further investigation of this region was, however, delayed until the advent of higher resolution infrared telescopes, such as the MSX , which Simon et al. (2006a) used to identify Cloud F, along with 10,930 other candidate IRDCs, as having an extended structure silhouetted against diffuse background emission. Simon et al. (2006b) then investigated the global properties of the clouds from the Simon et al. (2006a) sample which resided within the Galactic Ring Survey's coverage (a survey of 13 CO(1 -0) molecular line emission), and were especially extended (major axis &gt; 1. ′ 53) and had a strong average extinction contrast against the background (with [background-image]/background &gt; 0.25). Using 1.2 mm continuum observations, Rathborne et al. (2006) then investigated the core properties within 38 of these clouds (the positions of these cores within Cloud F are shown in Figure 1), selecting those which had kinematic distance estimates (Simon et al. 2006b). Butler &amp; Tan (2009, 2012) and Kainulainen &amp; Tan (2013) studied the core properties within 10 of the Rathborne et al. (2006) sample IRDCs, which were relatively nearby, massive, dark, and showed relatively simple surrounding diffuse emission (positions shown in Figure 1). These maps highlighted Cloud F in particular (along with G035.39-00.33; see section 5.1), as having a complex fila-\n",
      "\n",
      "3\n",
      "\n",
      "Figure 1. Shown in greyscale is the high-resolution, highdynamic-range mass surface density map of the IRDC G034.43+00.24, produced by combining the dust extinction at the near- and far-infrared wavelengths (Kainulainen &amp; Tan 2013). The black rectangles show the coverage of the Galactic Ring Survey (Jackson et al. 2006, see AppendixE) and IRAM-30m observations. Shown with + and × symbols and labeled are the positions of the 'core' regions identified by Butler &amp; Tan (2012, with F prefix) and those from Rathborne et al. (2006, with MM prefix), respectively. Shown as coloured circles are the young stellar objects candidates, identified by their spectral energy distribution in the Spitzer bands (Shepherd et al. 2007): 'good' and 'poor' (i.e. those with a poor or no spectral energy distribution fit) detections are shown in blue and red, respectively. Sources with extended, enhanced 4.5 µ m emission, or 'green fuzzies', are plotted as green triangles (Chambers et al. 2009). Shown in the lower right of the map is the approximate shell of the H ii region G34.325+0.211, see Xu et al. 2016 for a discussion of its influence on the IRDC.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "mentary morphology containing several massive cores, and a large amount of dense, quiescent gas (also see Fontani et al. 2011 and Kong et al. 2017 for chemical studies towards the quiescent gas within this cloud). The stringent selection process, through several datasets, summarised here, has singled out Cloud F as an ideal candidate in which to investigate the initial conditions of massive star formation. Table 1 presents the physical properties of interest for Cloud F (determined within the area mapped by the IRAM-30m observations), and Figure 1 shows the mass surface density map across the cloud region (Kainulainen &amp; Tan 2013).\n",
      "\n",
      "To investigate the kinematic structures on various scales within Cloud F, we use emission from the C 18 O(1 -0) and N 2 H + (1 -0) molecular line transitions. Assuming that the C 18 O(1 -0) line is thermalised and optically thin, this should trace the more extended gas, as its critical density is comparable to the average volume density expected within IRDCs ( ∼ 10 3 -4 cm -3 when observed at scales of ∼ 0.5 pc; e.g. Henshaw et al. 2013). The N 2 H + (1 -0) transition has a significantly higher critical density ( ∼ 10 4 -5 cm -3 ), and therefore is expected to trace the higher density regions. In nearby low-mass star-forming regions N 2 H + (1 -0) typically traces dense cores (e.g. Caselli et al. 2002b; Andr´ e et al. 2007; Friesen et al. 2010), however, given the significantly higher volume densities seen within some IRDCs, this line is found to be extended (e.g Tackenberg et al. 2014; Henshaw et al. 2013, 2014).\n",
      "\n",
      "We note, however, that the transitions from the N 2 H + molecule contain hyperfine structure, which can complicate the analysis of kinematically complex regions, where the hyperfine components can be merged to form one broad component (e.g in the case where the line width is larger than the separation of the components). Unlike its higher J-transitions, however, N 2 H + (1 -0) has a hyperfine component (the F 1 , F = 0,1 → 1,2 transition) which is 'isolated' by &gt; 7 km s -1 from the main group (i.e. those with a separation of ∼ 1 km s -1 ; Caselli et al. 1995), and is, therefore, unlikely to merge given the typical line properties observed within IRDCs (e.g. with line-widths of ∼ 1 km s -1 ; Henshaw et al. 2013). As all the analysis presented in this work will be conducted on the isolated hyperfine component of N 2 H + (1 -0) henceforth, unless otherwise stated, when mentioning the N 2 H + (1 -0) transition we are referring to this hyperfine component. To do so, we will centre on the frequency of the isolated hyperfine component from Pagani et al. (2009). We note, however, that slightly different frequencies for the isolated hyperfine component are available in the literature (93176 . 2637 -93176 . 2650 MHz; Caselli et al. 1995; Cazzoli et al. 2012), 1 yet changing to these will only shift the centroid velocity by (3 . 7 -4 . 1) × 10 -2 kms -1 . As this variation is below the spectral resolution of ∼ 6 × 10 -2 kms -1 of the N 2 H + (1 -0) observations used throughout this work (see Table 2), we do not expect this to significantly affect the results presented throughout this work (particular importance for section 4.3).\n",
      "\n",
      "This paper is structured in the following manner. The details of the IRAM-30m observations towards Cloud F can be found in Section 2. The results are presented in Section 3. The analysis of the kinematic structure is then given in Sec-\n",
      "\n",
      "1 See https://www.astro.uni-koeln.de/cdms\n",
      "\n",
      "Table 1. Cloud properties within the IRAM-30m mapped region, shown in Figure 1. See section 5.1 for comparison to the IRDC G035.39-00.33 (or Cloud H; Butler &amp; Tan 2009).\n",
      "\n",
      "| Cloud property within IRAM-30 map   | Cloud F (G034.43+00.24)   | Cloud H (G035.39-00.33)   |\n",
      "|-------------------------------------|---------------------------|---------------------------|\n",
      "| Distance, d (kpc) a                 | 3.7 ± 0.6                 | 2.9 ± 0.4                 |\n",
      "| Map size, R (pc) b                  | 3.4 ± 0.5                 | 2.3 ± 0.3                 |\n",
      "| Aspect ratio, A 0                   | 2.4                       | 2.6                       |\n",
      "| Σ (gcm - 2 ) c                      | 0.10 ± 0.03               | 0.09 ± 0.03               |\n",
      "| f D d                               | 1.1 ± 0.6                 | 2.8 ± 1.4                 |\n",
      "| Mass, M (M glyph[circledot] ) e     | 4700 ± 1400               | 1700 ± 500                |\n",
      "| T (K) f                             | ∼ 17                      | ∼ 13                      |\n",
      "| m (M pc - 1 ) g                     | 1400 400                  | 740 200                   |\n",
      "\n",
      "glyph[circledot]\n",
      "\n",
      "a : Near kinematic distance to the sources (Simon et al. 2006b; Roman-Duval et al. 2009). See section 5.2 for further discussion of the source distance.\n",
      "\n",
      "±\n",
      "\n",
      "±\n",
      "\n",
      "b : Calculated from the mean value of the Ra and Dec range at the assumed source distance.\n",
      "\n",
      "c : Average mass surface density (Kainulainen &amp; Tan 2013).\n",
      "\n",
      "d : CO depletion factor presented in Appendix E compared to the value measured by Hernandez et al. (2012).\n",
      "\n",
      "e : Masses calculated for the region covered by the IRAM-30m observations. Total cloud masses calculated by (Butler &amp; Tan 2012) are 4,460 and 13,340 M glyph[circledot] for Clouds F and H, respectively. f : Dirienzo et al. (2015); Pon et al. (2016a); Sokolov et al. (2017) g : The mass per unit length can be given as m = M/R .\n",
      "\n",
      "tion 4 and is discussed in Section 5. Here we compare the structures identified to those identified in a similar IRDC (G035.39-00.33), and the larger scale structures identified from 13 CO(1 -0) observations. The conclusions of this work are given in Section 6. A discussion of kinematics with reference to previous analyses of Cloud F, and calculation of the CO depletion within the cloud are given in Appendices A and D. In AppendixB we briefly discuss the physical interpretation of the structures identified from the molecular lines observations presented in this work. The analysis of the IRAM-30m Cloud H observations and of the Galactic Ring Survey (GRS; Jackson et al. 2006) observations are presented in Appendices C and E.\n",
      "\n",
      "## 2 OBSERVATIONS\n",
      "\n",
      "The C 18 O(1 -0) and N 2 H + (1 -0) observations towards Cloud F were obtained using the Institute for Radio Astronomy in the Millimeter Range 30-m telescope (IRAM30m) on Pico Veleta, Spain, over the 27 th - 28 th July 2012. 2 The data cubes were produced from On-The-Fly (OTF) mapping, covering an area of ∼ 104 ′′ × 240 ′′ (corresponding to 2pc × 4.8pc, at the source distance of 3.7 kpc; Simon et al. 2006b), using central reference coordinates of RA(J2000)=18 h 53 m 19 s , Dec(J2000)=01 ◦ 27 ′ 21 ′′ , 3 which is shown on Figure 1. These observations were carried out using the EMIR receivers. The VErsatile Spectrometer Assembly (VESPA) provided spectral resolutions of ∼ 20 - 80 kHz.\n",
      "\n",
      "The gildas 4 packages class and mapping were used to\n",
      "\n",
      "2 Project code: 025-12\n",
      "\n",
      "3 In Galatic coordinates l = 34.441 ◦ , b = 0.247 ◦ .\n",
      "\n",
      "4 see https://www.iram.fr/IRAMFR/GILDAS/\n",
      "\n",
      "Table 2. Observational parameters.\n",
      "\n",
      "| Observational parameter         | N 2 H + (1 - 0)   | C 18 O(1 - 0)    |\n",
      "|---------------------------------|-------------------|------------------|\n",
      "| Frequency (MHz) HPBW ( ′′ ) c   | 93176.7637 a 26   | 109782.1780 b 23 |\n",
      "| Velocity Resolution (km s - 1 ) | 6.28 × 10 - 2     | 5.33 × 10 - 2    |\n",
      "| Beam Efficiency                 | 0.81              | 0.78             |\n",
      "| Forward Efficiency              | 0.95              | 0.94             |\n",
      "| rms (K)                         | 0.13              | 0.15             |\n",
      "\n",
      "a : Frequency of main hyperfine component, the isolated component N 2 H + (J, F 1 , F = 1,0,1 → 0,1,2) has a frequency of 93176.2522 MHz (Pagani et al. 2009).\n",
      "\n",
      "b : Cazzoli et al. (2003)\n",
      "\n",
      "- c : Calculated as θ HPBW = 1 . 16 λ/D , where λ and D are the wavelength and telescope diameter, respectively (see http://www.iram.es/IRAMES/mainWiki/Iram30mEfficiencies ).\n",
      "\n",
      "reduce and post-process the data. This included subtracting a single-order polynomial function to produce a flat baseline and convolving the OTF-data with a Gaussian kernel, thereby increasing the signal-to-noise ratio and allowing us to resample the data onto a regularly spaced grid. All the intensities were converted from units of antenna temperature, T ∗ A , to main-beam brightness temperature, T MB , using the beam and forward efficiencies shown in Table 2. The native angular resolution of the IRAM-30m antenna at the frequency of the C 18 O(1 -0) and N 2 H + (1 -0) transitions are ∼ 23 ′′ and 26 ′′ , respectively. Both data sets are smoothed to achieve an effective angular resolution of ∼ 28 ′′ , with a pixel spacing of 14 ′′ , to allow comparison (corresponding to a spatial resolution of ∼ 0.5 pc at the source distance of ∼ 3 . 7 kpc; Simon et al. 2006b).\n",
      "\n",
      "## 3 RESULTS\n",
      "\n",
      "## 3.1 Moment analysis\n",
      "\n",
      "To gain an initial insight into the intensity distribution and kinematics of the molecular line emission we conduct a moment analysis, using the spectral cube package for python . 5 This analysis has been carried out for a velocity range of 55 -61 kms -1 for both lines, which was chosen to best incorporate all the significant emission from Cloud F identified in the spectrum averaged across the whole mapped area, shown in Figure 2. The average uncertainty on the integrated intensity towards each position for N 2 H + (1 -0) and C 18 O(1 -0) are σ ∼ 0.08 K km s -1 and σ ∼ 0.09 K km s -1 , respectively (the rms is shown in Table 2, and the uncertainty has been calculated following Caselli et al. 2002a). The pixels below a 3 σ threshold have been masked after the moment analysis procedure.\n",
      "\n",
      "The results of the moment analysis towards Cloud F are presented in Figure 3. Shown in greyscale in the first column is the mass surface density map determined from extinction in the near infrared (Kainulainen &amp; Tan 2013). Shown in the second, third and fourth columns are the integrated intensities (0 th order moment), intensity weighted velocity field\n",
      "\n",
      "5 https://spectral-cube.readthedocs.io/en/latest/ .\n",
      "\n",
      "Figure 2. Shown are the average spectrum of the N 2 H + (1 -0) transition (upper) and the C 18 O(1 -0) transition (lower) across the mapped region of Cloud F. The horizontal dotted line represents the rms level on the average spectrum of ∼ 0.02 K and ∼ 0.04 K for N 2 H + (1 -0) and C 18 O(1 -0), respectively. Note, these values are different to the average of the rms within individual positions, which is given in Table 2. The shaded region shows the velocity range used for the moment map analysis (see Figure 3).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "(1 st order moment), and intensity weighted line width (2 nd order moment) maps, respectively. For reference, contours of the integrated intensity are overlaid on each panel, and the positions of the Rathborne et al. (2006) and Butler &amp; Tan (2012) core regions are plotted on the mass surface density map.\n",
      "\n",
      "This analysis shows that both the N 2 H + (1 -0) and C 18 O(1 -0) emission is extended across the length of the IRDC, where only a few ( &lt; 10 per cent) of the pixels do not meet the 3 σ integrated intensity threshold. The N 2 H + (1 -0) emission traces the mass surface density map morphology relatively well, with peaks towards the MM3 and MM1 core regions. The C 18 O(1 -0) emission also traces the mass surface density morphology, albeit to a lesser extent than the N 2 H + (1 -0) emission, peaking at the position of the MM7 core, to the west of the F1 and MM8 regions, to the south-east of the MM3 region, and towards the MM1 region. A likely cause of the different spatial distributions of the C 18 O(1 -0) and N 2 H + (1 -0) emission is that C 18 O traces the extended envelope material, whereas N 2 H + is expected to trace the dense gas, which follows the continuum cores and mass surface density distribution. Furthermore, towards these densest regions, unlike N 2 H + , C 18 O can suffer from freeze-out (see Appendix D).\n",
      "\n",
      "The intensity weighted velocity field maps for both transitions show an increasing velocity from the west to east (right to left on Figure 3). The total difference of velocity across the mapped region is ∼ 2 -3 kms -1 , which corresponds to a gradient of the order ∼ 0 . 5 -0 . 7 kms -1 pc -1 for the approximate distance diagonally across the mapped region of 3 -4 pc, at the assumed source distance (see Table 1).\n",
      "\n",
      "The intensity-weighted line width maps show differ-\n",
      "\n",
      "Figure 3. Moment map analysis of the Cloud F N 2 H + (1 -0) (upper row) and C 18 O(1 -0) (lower row) observations. Shown in greyscale in the first column is the mass surface density map of Cloud F, determined from near and mid- infrared extinction (Kainulainen &amp; Tan 2013). Shown with + and × symbols are the positions of the 'core' regions identified by Butler &amp; Tan (2012) and those from Rathborne et al. (2006), respectively, which are labeled in both the upper left and upper centre-left panels. Shown in the second, third and fourth columns are the integrated intensities (0 th order moment), intensity weighted velocity field (1 st order moment), and intensity weighted line width (2 nd order moment) maps. Overlaid as red (first column) and black (second, third and fourth columns) contours in the upper row is the integrated intensity of N 2 H + (1 -0), in steps of { 5 , 10 , 15 , 20 , 25 , 45 , 55 } σ ; where σ ∼ 0.08 K km s -1 . Overlaid as blue (first column) and black (second, third and fourth columns) contours in the lower row is the integrated intensity of C 18 O(1 -0), in steps of { 40 , 50 , 60 , 70 , 80 } σ ; where σ ∼ 0.09 K km s -1 . The moment analysis has been performed above 3 σ for all transitions. Shown in the lower left corner of the second upper panel is the smoothed angular beam size.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "ent morphologies. The C 18 O(1 -0) shows the largest values of the intensity weighted line width towards the MM1 region and the south-east corner of the mapped region ( ∼ 3.5 km s -1 ), with peaks towards the MM3 region ( ∼ 2.7 km s -1 ), and towards the peak in integrated intensity towards the west of the F1 and MM8 regions ( ∼ 2.9 km s -1 ). The N 2 H + (1 -0) emission shows narrower line widths towards the centre of the cloud, with values of ∼ 2.5 km s -1 towards the MM1 region, and ∼ 2 kms -1 towards the MM3 region.\n",
      "\n",
      "## 3.2 Channel map analysis\n",
      "\n",
      "To investigate the velocity gradients identified in the N 2 H + and C 18 O moment map analysis, the emission from these transitions has been integrated across subsets of the total velocity range used to create the moment maps (referred to as channel maps). We integrate the N 2 H + (1 -0) and C 18 O(1 -0) transitions from 55 -61 kms -1 in steps of 0.5 km s -1 (which corresponds to approximately 10 channels for both lines). Figure 4 shows contours of the integrated intensity in these steps for N 2 H + (1 -0) (in red) and C 18 O(1 -0) (in blue), overlaid on the mass surface density map (Kainulainen &amp; Tan 2013).\n",
      "\n",
      "Figure 4. Cloud F channel maps of N 2 H + (1 -0) and C 18 O(1 -0) shown in red and blue filled contours, which begin at 5 σ , and increase in steps of 5 σ , where σ ∼ 0.023 K km s -1 and σ ∼ 0.024 K km s -1 , respectively. The intensities are integrated from 55 -61 kms -1 in steps of 0.5 km s -1 , as shown below or above each map. Each map is overlaid on the mass surface density map of Kainulainen &amp; Tan (2013). Shown with + and × symbols are the positions of the 'core' regions identified by Butler &amp; Tan (2012) and those from Rathborne et al. (2006), respectively.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "The channel maps show a complex morphology, where both lines appear to peak towards the south for the majority of the velocity range, with several local maxima appearing at different velocities towards the north of the cloud. These suggest that the velocity gradients identified in the moment map analysis are not continuous, but rather that they are due to distinct peaks in velocity across the map, which, when averaged, mimic a smoothly varying centroid velocity. Identifying velocity structures by arbitrarily separating these maxima can be, however, dependent on the applied spatial and/or velocity boundaries.\n",
      "\n",
      "## 4 ANALYSIS\n",
      "\n",
      "To determine if multiple velocity components are present across Cloud F, as the various intensity peaks in the channel map analysis would seem to suggest, we check the individual N 2 H + (1 -0) and C 18 O(1 -0) spectra. The two panels of Figure 5 show the spectra at each position across the cloud. Multiple distinct velocity components can indeed be clearly identified, predominately in the C 18 O(1 -0) emission, at several positions across the cloud. A result which is not evident from the average spectra, shown in Figure 2. A more reliable method to separate these components than is possible with moment or channel maps is, therefore, required to accurately analyse the kinematics within this complex IRDC. In this section, we use use a semi-automated gaussian fitting algorithm and automated hierarchical clustering algorithm. These have been chosen such that the identified coherent velocity structures can be tested for robustness against a range of input parameters, within both the fitting and clustering algorithms. This method ensures that the structures are both reliable and reproducible. Importantly, in section 5.1 we investigate an apparently similar IRDC to Cloud H, for which we use this same method to identify the coherent velocity structures, allowing for a systematic comparison of their kinematic properties.\n",
      "\n",
      "## 4.1 Spectral line fitting and velocity coherent features\n",
      "\n",
      "To separate the velocity components, we fit Gaussian profiles to the spectra across the cloud using the Semi-automated\n",
      "\n",
      "Figure 5. The shown are the spectra of the N 2 H + (1 -0) transition (left) and the C 18 O(1 -0) transition (right) across the mapped region of Cloud F. The velocity ranges are 54 to 62 km s -1 , and the intensity ranges are -0.5 to 2.5 K for N 2 H + and -0.5 to 3.5 K for C 18 O. Overlaid on each spectrum are the results of the line fitting ( scouse ) and clustering ( acorns ) routines, which are discussed in section 4.1. The colours of these profiles represent the various velocity component associations given in Table 3. The background greyscale is the mass surface density map, determined from near and mid-infrared extinction (Kainulainen &amp; Tan 2013).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "multi-COmponent Universal Spectral-line fitting Engine ( scouse ; Henshaw et al. 2016). 6 scouse has been chosen over manually fitting each individual spectrum (a total of ∼ 300 for both Cloud F maps), as this algorithm was specially produced to efficiently and systematically fit a large number of spectra. To do so, scouse works in several steps. Firstly, the map is split into regions (referred to as 'spectral averaging areas', SAA), within which the data are spatially averaged. For each SAA spectra, the user is instructed to\n",
      "\n",
      "6 Written in the idl programming language. See https:// github.com/jdhenshaw/SCOUSE for more details.\n",
      "\n",
      "fit the appropriate number of Gaussian components. The individual spectra contained within the SAAs are then automatically fitted using the parameters from the Gaussian fits of their SAA within given tolerance limits on the peak intensity, line centroid velocity, line width and separation between components. As a final step, the results are checked for anomalies, which can be re-fitted if required.\n",
      "\n",
      "As discussed in Henshaw et al. 2016, the size of the SAA selected will be somewhat data dependent. A size of 30 ′′ represented the maximum size for which the spatially averaged spectrum was a good representation of the line profiles of its composite spectra. Changing the SAA size will not signifi-\n",
      "\n",
      "## 8 Barnes, Henshaw, Caselli, Jim´ enez-Serra, Tan, Fontani, Pon, Ragan\n",
      "\n",
      "Figure 6. Displayed in each panel is the position-position-velocity diagram of Cloud F, shown at three viewing angles for comparison. The left and right panels show N 2 H + (1 -0) and C 18 O(1 -0) results, respectively. The colour of each point represents its association to one of the coherent velocity components, F PPV1 in blue, F PPV2 in green, F PPV3 in purple, and F PPV4 in red. The size of each point represents its relative peak intensity. The mass surface density map of Kainulainen &amp; Tan (2013) is shown on the base of each plot. Note that the coordinate offsets of these plots are relative to the centre of the mapped region: RA (J2000) = 18 h 53 m 19 s , Dec(J2000)=01 ◦ 27 ′ 21 ′′ ( l = 34.441 ◦ , b = 0.247 ◦ ).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Table 3. Parameters of the velocity components identified in the IRAM-30m observations towards Cloud F (F PPV , upper rows) and Cloud H (H PPV , lower rows). Shown are the molecules used to identify the components, and for each component: the name with the colour used for each Figure in parentheses, the total number of points, the average centroid velocity, the average line width, the velocity gradient and the angle of this gradient with respect to East of North. When the uncertainty on the velocity gradient is larger than or equal to the calculated velocity gradient, the velocity gradient angle is unconstrained, and therefore not shown.\n",
      "\n",
      "| Line            | Component (colour)   | # points   | Centroid velocity ( V 0 ) kms - 1   | Centroid velocity ( V 0 ) kms - 1   | Centroid velocity ( V 0 ) kms - 1   | Line width (∆ υ ) kms   | Line width (∆ υ ) kms   | - 1    | Velocity gradient ( ∇ v ) kms - 1 pc - 1   | Velocity gradient ( ∇ v ) kms - 1 pc - 1   | Velocity gradient ( ∇ v ) kms - 1 pc - 1   | Gradient angle ( θ ∇ v ) degrees   |\n",
      "|-----------------|----------------------|------------|-------------------------------------|-------------------------------------|-------------------------------------|-------------------------|-------------------------|--------|--------------------------------------------|--------------------------------------------|--------------------------------------------|------------------------------------|\n",
      "| C 18 O(1 - 0)   |                      |            |                                     |                                     |                                     |                         |                         |        |                                            |                                            |                                            |                                    |\n",
      "|                 | F PPV1 (blue)        | 54         |                                     | 59.56                               | 0.24                                |                         | 0.96                    | ± 0.31 |                                            | 0.12 ±                                     | 0.03                                       | -38.89 ± 14.81                     |\n",
      "|                 | F PPV2 (green)       | 53         |                                     | 56.68                               | ± ± 0.40                            |                         | 1.48                    | ± 0.31 |                                            | 0.25 ±                                     | 0.08                                       | -31.07 ± 23.86                     |\n",
      "|                 | F PPV3 (purple)      | 22         |                                     | 58.39                               | ± 0.12                              |                         | 1.04                    | ± 0.27 |                                            | 0.16 ±                                     | 0.05                                       | -83.77 ± 4.32                      |\n",
      "|                 | F PPV4 (red)         | 128        |                                     | 58.26                               | ± 0.43                              |                         | 1.75                    | ± 0.60 |                                            | 0.28 ±                                     | 0.07                                       | -85.83 ± 5.93                      |\n",
      "| N 2 H + (1 - 0) |                      |            |                                     |                                     |                                     |                         |                         |        |                                            |                                            |                                            |                                    |\n",
      "|                 | F PPV4 (red)         | 41         |                                     | 58.44                               | ± 0.51                              |                         | 1.75                    | ± 0.50 |                                            | 0.75                                       | ± 0.15                                     | 70.20 ± 3.20                       |\n",
      "| C 18 O(1 - 0)   |                      |            |                                     |                                     |                                     |                         |                         |        |                                            |                                            |                                            |                                    |\n",
      "|                 | H PPV1 (orange)      | 20         |                                     | 46.12                               | ± 0.11                              |                         | 0.47                    | ± 0.15 |                                            | 0.12 ±                                     | 0.06                                       | 76.08 ± 12.41                      |\n",
      "|                 | H PPV2 (purple)      | 27         |                                     | 46.61                               | ± 0.20                              |                         | 1.39                    | ± 0.41 |                                            | 0.38                                       | ± 0.11                                     | 61.15 ± 9.66                       |\n",
      "|                 | H PPV3 (green)       | 26         |                                     | 43.67                               | ± 0.16                              |                         | 1.33                    | ± 0.38 |                                            | 0.29 ±                                     | 0.09                                       | -87.80 ± 7.77                      |\n",
      "|                 | H PPV4a (red)        | 21         |                                     | 45.07                               | ± 0.06                              |                         | 1.48                    | ± 0.34 |                                            | 0.01 ±                                     | 0.02 . .                                   | .                                  |\n",
      "|                 | H PPV4b (blue)       | 32         |                                     | 45.52                               | ± 0.25                              |                         | 1.35                    | ± 0.47 |                                            | 0.33 ±                                     | 0.10                                       | -56.27 ± 11.72                     |\n",
      "| N 2 H + (1 - 0) |                      |            |                                     |                                     |                                     |                         |                         |        |                                            |                                            |                                            |                                    |\n",
      "|                 | H PPV2 (purple)      | 14         |                                     | 46.88                               | ± 0.10                              |                         | 0.74                    | ± 0.20 |                                            | 0.15 ±                                     | 0.08 -29.98                                | ± 29.89                            |\n",
      "|                 | H PPV4a (red)        | 38         |                                     | 45.42                               | ± 0.10                              |                         | 1.26                    | ± 0.16 |                                            | 0.02 ±                                     | 0.02 .                                     | . .                                |\n",
      "|                 | H PPV4b (blue)       | 27         |                                     | 45.99                               | 0.09                                |                         | 1.02                    | 0.35   |                                            | 0.10                                       | 0.05                                       | -77.40 10.68                       |\n",
      "\n",
      "±\n",
      "\n",
      "cantly affect the final best-fitting solutions across the cloud. Rather, decreasing the size of the SAA will result in an increase in the number of spectral that require manual fitting during the SAA fitting stage (and a reduction of fits that need to be corrected during the later stages). Alternatively increasing the size of the SAA will have the opposite effect. An SAA radius of 30 ′′ for the Cloud F data, such that each SAA contained four to six spectra, therefore, represented the most efficient choice. The tolerance limits were set such that each fit had to have a peak intensity of at least three times the rms , a centroid velocity similar to an SAA fit to within three times the value of the velocity dispersion and a line width to within a factor of two. To be considered a multi-component fit, the components had to be separated by a factor of two times the line width. These parameters gave reasonable fits across the cloud for both N 2 H + (1 -0) and C 18 O(1 -0), where the mean residual across all positions after fitting was &lt; 3 rms and only ∼ 10 per cent of the spectra required manual checking. The results of scouse are over-plotted on the spectrum at each position in Figure 5.\n",
      "\n",
      "To identify coherent velocity features within our decomposed data, we use Agglomerative Clustering for ORganising Nested Structures ( acorns ; Henshaw et al. in prep). 7 A complete description of the algorithm and the process will be presented in Henshaw et al. (in prep), however, the key details are included below.\n",
      "\n",
      "acorns is specifically designed to work on decomposed spectroscopic data, i.e. the output of scouse (or an equivalent algorithm). The algorithm follows the principles of hierarchical agglomerative clustering and searches for a hierarchical system of clusters within the decomposed dataset. In agglomerative clustering, each data point begins its life as a 'cluster'. Clusters grow by merging with other clusters pro-\n",
      "\n",
      "7 Written in the python programming language, soon available at https://github.com/jdhenshaw/acorns .\n",
      "\n",
      "±\n",
      "\n",
      "±\n",
      "\n",
      "±\n",
      "\n",
      "vided they satisfy a number of conditions which are supplied by the user (see below). As the algorithm progresses, hierarchies develop. These hierarchical clusters, as in many areas of research, can be visualised graphically as a dendrogram.\n",
      "\n",
      "The merging process is governed by a series of conditions controlled by the user. These conditions are known as linkage criteria, and can refer to, for example, a euclidean distance between two clusters or an absolute difference in a variable of the users choice (e.g. velocity or velocity dispersion). If two adjacent clusters satisfy the linkage criteria, they will be merged. In this work, for two data points to be linked, we require several that criteria are satisfied. Namely, adjacent clusters must: i) have a Euclidean separation which less than a beam size; ii) have an absolute velocity difference less than twice the spectral resolution of the data; iii) have an absolute difference in velocity dispersion which is less than the thermal velocity dispersion of the gas (estimated to be cs=0.23 km/s at 17K, from Table 1, given a mean molecular weight of 2.33 a.m.u). 8 We consider these criteria to be fairly strict and representative of the limitations of our data (i.e. our spatial and spectral resolution). Once this initial robust hierarchy has been established (i.e. all possible links satisfying these criteria have been exhausted), acorns then allows the user to relax these conditions in order to further develop the hierarchy. This can be performed in several ways, in incremental stages, both interactively and noninteractively, or in a single step. In this study we relaxed the\n",
      "\n",
      "8 The observed velocity dispersion is given as σ obs =∆ υ (8 ln (2)) -0 . 5 , where ∆ υ is the observed full width half maximum (or line width). Here and throughout this work, we use the classical value of the abundance to be consistent with the previous IRDC analyses (e.g. Henshaw et al. 2013; Jim´ enez-Serra et al. 2014), and not the value obtained when accounting for heavier elements (2.37; see Kauffmann et al. 2008). Taking the latter would not significantly affect the results of this work.\n",
      "\n",
      "conditions in a single step, however, we conducted a parameter space study in order to establish the set of relaxation parameters which produced the most robust hierarchy. These optimal parameters were chosen when a hierarchy appeared most persistent across an area of the parameter space (i.e. when the hierarchy did not significantly change for a range of relaxation parameters), whilst being comparable for both the C 18 O and N 2 H + data (assuming that N 2 H + traces similar, or a least the densest, components traced by C 18 O). This was achieved with relaxation factors of 2.5, 1.75, and 0.75, for the spatial separation, centroid velocity and line width, respectively, for both transitions.\n",
      "\n",
      "Figure 6 shows the position-position-velocity diagram for the above analysis, where the centroid velocities of the Gaussian profiles at each position have been plotted, and the colours correspond to the identified components (shown in the legend). Four coherent velocity components have been identified in the C 18 O(1 -0) emission from Cloud F: F PPV1 , F PPV2 , F PPV3 and F PPV4 , which are shown in blue, green, purple, and red, respectively, on Figures 5 and 6. We find that the components F PPV1 and F PPV4 are extended across the length of the cloud, whereas F PPV2 and F PPV3 are limited to the southern and northern portions of the cloud, respectively. The component F PPV4 is also identified in the N 2 H + (1 -0) transition emission. The basic properties of these components are given in Table 3, and they are analysed in the following sections.\n",
      "\n",
      "## 4.2 Velocity gradients\n",
      "\n",
      "As previously shown in the moment map analysis, Cloud F appears to have a smooth velocity gradient increasing in velocity from for west to east (see Figure 3). However, in the kinematic structure identified from the Gaussian decomposition, we do not see such a smooth gradient, instead, we observe the velocity components at distinct velocities across the cloud. We find that the F PPV1 is at a high velocity ( ∼ 60 kms -1 ) on the east of the mapped region, and the F PPV2 is at a low velocity ( ∼ 57 kms -1 ) on the west of the mapped region (see Figure 6). When averaged with F PPV4 , as in the case of the moment map analysis, these would mimic a smooth velocity gradient across the cloud. Rather than a primarily west to east velocity gradient, the identified components show large-scale velocity gradients running along the south-north axis of the cloud. Here we determine the magnitude and angle of the larger scale gradients across the cloud following the analysis of Goodman et al. (1993), who assume that the line centroid velocities can be represented by the linear function,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where ∆RA and ∆Dec are the offset right ascension and declination in radians, V 0 is the average velocity of the velocity component, V LSR is the centroid velocity of the Gaussian profile fit at each position, and A and B are solved for using the non-linear least squares optimisation routine scipy.optimize.curve fit in python . The velocity gradient, ∇ v , can be calculated with,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where d is the source distance. The angle of the gradient, θ ∇ v , can be determined from,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "The magnitudes and angles, with respect to East of North, of the velocity gradients for each velocity component, are given in Table 3. We find velocity gradients across the cloud in the range of 0.12-0.75 km s -1 pc -1 , with an average over all components of ∼ 0.3 km s -1 pc -1 , which is lower than the range determined from the moment map analysis ∼ 0 . 5 -0 . 7 kms -1 pc -1 . This is due to the fact that here we are analysing the gradients of the individual components, rather than the gradient produced by the separation of the components when they are averaged. These gradients are discussed further in appendix A.\n",
      "\n",
      "## 4.3 Different N 2 H + (1 -0) and C 18 O(1 -0) centroid velocities\n",
      "\n",
      "Comparing the distribution of different molecular species, both spatially and in velocity, within star-forming regions, can provide clues of their formation scenarios (e.g Henshaw et al. 2013). To do so within Cloud F, we compare the centroid velocity at each position of the component identified in the N 2 H + (1 -0) and C 18 O(1 -0), F PPV4 . The centroid velocity difference, V LSR (N 2 H + - C 18 O), map and histogram are presented in Figure 7. We find that the average difference in velocity is +0 . 32 ± 0 . 06 kms -1 . 9\n",
      "\n",
      "A velocity shift between two tracers may, however, be produced when comparing the velocity from emission which is not tracing the same gas; or in other words, here we choose to compare the emission from the F PPV4 component, as this is seen in both C 18 O and N 2 H + , however, if some of this emission is actually from different component which has been wrongly assigned, then an artificial velocity shift would be produced. Such an effect could be plausible towards the north of the cloud, where the N 2 H + (1 -0) emission appears to originate at a velocity in-between the F PPV1 and F PPV4 components (see Figure 6). To investigate this, we temporarily attribute the emission above Dec (J2000) = 01 ◦ 27 ′ 36 ′′ to the F PPV1 component, and re-determine the velocity shift between the N 2 H + (1 -0) and C 18 O(1 -0). Doing so, we find an average value towards this northern region is -0 . 67 ± 0.10 km s -1 ; a negative shift which is a factor of two larger in magnitude than that found when assigning this region to the F PPV4 component. Demonstrating that there is a significant shift in velocity between the N 2 H + (1 -0) and C 18 O(1 -0) emission regardless of which component the N 2 H + (1 -0) emission within the northern region is assigned, and this result is persistent, regardless of which N 2 H + and C 18 O components are compared. We continue with the assumption that components closer in velocity are the most likely to be physically associated, hence keep the original assignment of all the N 2 H + (1 -0) emission to the F PPV4 component, which provides a smaller, yet still significant, average velocity shift (+0 . 32 ± 0 . 06 kms -1 ).\n",
      "\n",
      "9 Uncertainty given is the standard error on the mean, where the standard deviation is ± 0 . 40 kms -1 .\n",
      "\n",
      "Figure 7. The left and centre panels are maps of the difference in the centroid velocity, V LSR (N 2 H + - C 18 O), of Clouds F and H (see section 5.1), respectively. Over-plotted as + and × symbols are the positions of the 'core' regions identified by Butler &amp; Tan (2012) and those from Rathborne et al. (2006), respectively. Shown in the right panels are histograms of this difference, where the upper is for the Cloud F map, and the lower for the Cloud H map. For reference, the clouds are labeled at the top of each plot.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## 4.4 Velocity dispersions\n",
      "\n",
      "In order to study the non-thermal motions of the gas from the observed line widths we use the expression from Myers (1983),\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where σ NT , σ obs , and σ T , are the non-thermal, the observed, and the thermal velocity dispersion, respectively. ∆ υ is the observed full width half maximum (or line width), k B is the Boltzmann constant, T kin is the kinetic temperature of the gas and m obs refers to the mass of the observed molecule (30 a.m.u for C 18 O; 29 a.m.u for N 2 H + ). We assume a constant kinetic gas temperature of 17 K (e.g. Dirienzo et al. 2015; Pon et al. 2016a), which gives thermal dispersion contributions of 0.064 km s -1 and 0.065 km s -1 , for C 18 O and N 2 H + , respectively. 10\n",
      "\n",
      "Figure 8 shows the non-thermal component of the velocity dispersion compared to both the gas sound speed (lower axis) and thermal component of the dispersion (upper axis) at each position within Cloud F. Comparison to the gas sound speed has been made assuming a temperature of 17 K and a mean molecular weight of 2.33 a.m.u (c s = 0.23 kms -1 ). We find that the velocity dispersions (or Mach numbers M = σ NT /c s ) averaged over all velocity components are 0.75 ± 0.03 km s -1 ( M =3.2 ± 0.14) and 0.63 ± 0.02 km s -1 ( M =2.70 ± 0.07) for N 2 H + (1 -0) and C 18 O(1 -0), respectively. To link these observed nonthermal motions to the turbulent motions (i.e. with corresponding Mach number) within the cloud, we make the\n",
      "\n",
      "10 Given the weak dependence on the temperature, varying between the expected limits within IRDCs does not affect the results presented here.\n",
      "\n",
      "assumption that no velocity gradients or substructure are present with size scales less than the beam size. This may, however, not be the case for these IRAM-30m observations presented here given the large physical beam size ( ∼ 0.5 pc; cf. Henshaw et al. 2013 and Henshaw et al. 2014), hence the values of the turbulent velocity dispersion calculated here most likely represent upper-limits on the true turbulent motions within the cloud. We find that all the velocity components have non-thermal velocity dispersions factors of several larger than the gas sound speed, which suggests that their internal turbulent motions are (at most) only moderately supersonic over scales of ∼ 0.5 pc (traced by these observations).\n",
      "\n",
      "We note that the calculated average N 2 H + velocity dispersion is larger than the average C 18 O velocity dispersion, which is not typically expected if the N 2 H + is tracing the denser, more compact regions within the cloud, unless the dense gas is associated with embedded young stellar objects. However, when comparing the velocity dispersions for the F PPV4 component only, we find comparable values (see Table 3). This suggests that both molecular lines are tracing similar gas within this component.\n",
      "\n",
      "## 5 DISCUSSION\n",
      "\n",
      "The kinematic analysis of Cloud F has unveiled a complex structure, consisting of several extended, coherent velocity components. Previous studies of this cloud have shown that it contains a distribution of both quiescent and active starforming regions, which are discussed in relation to the kinematic structure in appendix A, with a particular focus on the Rathborne et al. (2006) and Butler &amp; Tan (2012) shown on Figure 1. In the following sections, the kinematic structure\n",
      "\n",
      "Figure 8. Cloud F (left two panels) and Cloud H (right two panels) histograms of the non-thermal contribution to the velocity dispersion over the gas sound speed (lower axis on each plot; c s = 0.23 kms -1 at 17 K, given a mean molecular weight of 2.33 a.m.u). The upper axis labels on each plot show the non-thermal contribution to the velocity dispersion over the thermal contribution to the velocity dispersion, where σ T = 0.065 kms -1 and 0.066 kms -1 , for C 18 O (30 a.m.u) and N 2 H + (29 a.m.u), respectively. Colours represent the various velocity components (see Figures 6 and C3).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "of Cloud F is compared to that within similar IRDC, and to the larger scale gas kinematics surrounding the cloud.\n",
      "\n",
      "## 5.1 Cloud F in the context of other massive star-forming regions: comparison to Cloud H\n",
      "\n",
      "Galactic Plane surveys, undertaken with infrared spacebased telescopes (most recently Spitzer and Herschel ), have shown that filamentary structures appear to be ubiquitous throughout the interstellar medium (e.g. Molinari et al. 2016). Recently, kinematic analysis of molecular line emission has shown that coherent structures, believed to be velocity space representation of these filamentary structures, are equally common, appearing in both low- and high-mass star-forming regions (e.g. Hacar et al. 2013; Henshaw et al. 2014; Hacar et al. 2016; Henshaw et al. 2017). However, despite these structures being morphologically and qualitatively similar, their physical properties may be very different. Currently, direct comparisons of the structure and its properties within massive star-forming regions are lacking. To address this, in this section, we discuss how the properties determined for Cloud F compare to a similar IRDC, Cloud H (G035.39-00.33; Butler &amp; Tan 2009; see Table 1), with the aim of highlighting which kinematic properties are shared between these, and potentially other, massive starforming regions.\n",
      "\n",
      "The analysis of Cloud H's kinematic structure from the IRAM-30m observations has been carried out by Henshaw et al. (2013). Here we smooth these data to the same spatial resolution of the Cloud F observations ( ∼ 0.5 pc), such that structure identification is not biased to a spatial scale, given the hierarchical nature of the interstellar medium. Furthermore, we use the same analysis tools to determine the kinematic structure ( scouse and acorns ), such that the systematic comparison is possible (see Appendix C), as it has been recently suggested that the results from different structure finding algorithms can vary (e.g. Chira et al. 2017). We find that Cloud H contains a complex structure, harbouring several coherent velocity components, seen in both the N 2 H + (1 -0) and C 18 O(1 -0) emission, which are in agreement with the results previously found by Henshaw et al. (2013).\n",
      "\n",
      "Similar to Cloud F, from the measured line width, we have determined the non-thermal velocity dispersions within Cloud H. We find Mach numbers of M =2.0 ± 0.07 and M =2.28 ± 0.08 using N 2 H + (1 -0) and C 18 O(1 -0) (averaged over all velocity components), respectively; histograms of these results are shown next to those for Cloud F in Figure 8. Therefore, we find that the non-thermal contributions are factors of 2 -3 larger than the sound speed of the gas within both clouds, which sets an upper limit on the turbulent motions being mildly supersonic over the examined physical scale (i.e. the smoothed spatial resolution of ∼ 0.5 pc).\n",
      "\n",
      "We investigate the velocity distribution of the components by comparing the separation of the lowest and highest velocity component within both clouds. We use the components seen in C 18 O(1 -0) emission, and find a line-of-slight difference for Cloud F (F PPV1 and F PPV2 ) of 2.9 ± 0.5 km s -1 , and for Cloud H (H PPV2 and H PPV3 ) of 2.9 ± 0.3 km s -1 . Assuming a simple three-dimensional morphology, this result could indicate that both clouds have kinematic structures which are interacting at a speed of up to 5 kms -1 ; accounting for a factor of √ 3, assuming the velocity in the plan of the sky is equivalent in all directions.\n",
      "\n",
      "A filament merging scenario is in agreement with the wide-spread narrow SiO emission observed within Cloud H, generated by the sputtering of dust grains within large-scale C-shocks (Jim´ enez-Serra et al. 2010, requiring a shock velocity of ∼ 12 km s -1 ). It is possible that a similar scenario of filament merging is causing the velocity difference within Cloud F, and indeed Cosentino et al. (accepted) also found similarly wide-spread SiO emission throughout this cloud. However, it is difficult to determine if this emission is due to a merging scenario or higher level star-formation within Cloud F, which would have affected the chemistry of the molecular gas as a result of stellar feedback (e.g. towards the F4, or MM3, region). The elevated level of star formation within Cloud F suggest that it is at a later evolutionary stage than Cloud H, which has previously been suggested to be ∼ 3 Myr old (Henshaw et al. 2013; Jim´ enez-Serra et al. 2014; Barnes et al. 2016).\n",
      "\n",
      "We also compare the velocity separation between the coherent velocity components identified in the N 2 H + (1 -0)\n",
      "\n",
      "and C 18 O(1 -0) emission. When doing so within Cloud F we found a significant positive systematic offset of V LSR (N 2 H + - C 18 O) = +0 . 32 ± 0 . 03 kms -1 . To conduct a similar analysis within Cloud H, we compare the H PPV4a and H PPV4b structures. These components have been identified simultaneously at three positions within the N 2 H + (1 -0) map (see Figure C2). At these positions, we average the centroid velocity of the components in N 2 H + (1 -0) and compare this velocity to the component seen in the C 18 O(1 -0) emission (H PPV4 ). 11 We note that omitting these positions, which make up only ∼ 5 per cent of the total positions used for this comparison, would not significantly affect the result presented here. The centroid velocity difference map and histogram for Cloud H is presented with the Cloud F results in Figure 7. We find an average velocity shift of +0 . 26 ± 0 . 02 kms -1 , 12 which is in agreement with the value found by Henshaw et al. (2013). It is intriguing that both clouds share such a similar positive velocity shift between N 2 H + (1 -0) and C 18 O(1 -0), given below are several possible scenarios its formation.\n",
      "\n",
      "It has been previously proposed that the velocity difference within Cloud H is a result of a filament merging, whereby higher velocity filaments (H PPV1 and H PPV2 , from this work) are merging with a lower velocity, less massive filament (H PPV3 ), increasing the density of an intermediate velocity filaments (H PPV4a and H PPV4b ). Given the majority of the mass within Cloud H is situated at a higher velocity, the densest gas forming within the intermediate velocity filament, as traced by N 2 H + , is pushed to a higher velocity with respect to its envelope material traced by C 18 O, also formed by the merging process (Henshaw et al. 2013; Jim´ enez-Serra et al. 2014; Henshaw et al. 2014). Indeed, simulations have shown that certain lines-of-sight through density fluctuations and varying velocity fields within collapsing clouds can cause significant velocity difference between molecular tracers (Smith et al. 2013; Bailey et al. 2015). As previously discussed, such a scenario is plausible for Cloud F, and would also be in agreement with the observed velocity difference between the components. We note, however, a common physical mechanism driving this interaction is not determinable from the data presented here (e.g. cloud-cloud merging or global gravitational collapse).\n",
      "\n",
      "A second explanation, proposed by Zhang et al. (2017), is that velocity shifts between low and high-density tracers could be a signature of gas which is both expanding and contracting within the core regions of the cloud. This is based on the assumption that the higher critical density molecules, in their case HCO + emission, trace the inner dynamics of a core, while lower critical density molecules, C 18 O, trace the outer, envelope dynamics. These authors find blue-shifted and red-shifted profiles of the high and low-density tracers, respectively, towards a sample of cores, which they suggest shows the different core layers are moving in opposing directions; a scenario of 'envelope expansion with core collapse' (e.g. Lou &amp; Gao 2011). Indeed, these authors proposed such a scenario for a core region within Clouds H\n",
      "\n",
      "11 These positions are towards the complex H6 regions (Henshaw et al. 2014, 2017).\n",
      "\n",
      "12 Uncertainty given is the standard error on the mean, where the standard deviation is ± 0 . 14 kms -1 .\n",
      "\n",
      "(H6/MM7; Rathborne et al. 2006; Butler &amp; Tan 2012). We suggest that this could, in theory, be applied to explain the velocity shifts observed across both clouds, yet this would require the effect being extrapolated over larger scales.\n",
      "\n",
      "In summary, here we have shown that two massive, morphologically and qualitatively similar IRDCs share several kinematic properties, hinting at similar internal gas conditions. An intriguing result given that they are drawn from a different of Galactic environments and their various internal physical processes (e.g. level of star formation). It would be interesting to examine a larger sample of clouds to determine if these properties are inherent to the wider IRDC population.\n",
      "\n",
      "## 5.2 Connection between IRDC scales and GMC scales for Cloud F\n",
      "\n",
      "As previously mentioned, the interstellar medium is hierarchically structured, with massive star-forming regions hosting a complex sub-structure through various scales (e.g. filaments to cores). However, these regions are by no means at the top of this hierarchy, rather they are believed to be only a small part of larger, Galactic scale structures, which typically have masses and spatial extents one to two orders of magnitude larger than IRDCs (e.g. Ragan et al. 2014; Hernandez &amp; Tan 2015; Zucker et al. 2015). Indeed, several works have already studied the larger scale environment surrounding both Clouds F and H (as defined here by the IRAM-30m coverage). Hernandez &amp; Tan (2015) find that the larger scale structures which host Clouds F and H share many similar kinematic properties, such as velocity dispersions ( ∼ 3 -5 kms -1 ), velocity gradients ( ∼ few 0.1 km s -1 pc -1 ) and virial parameters ( ∼ unity), and Ragan et al. (2014) showed how these IRDCs could be part of 'Giant Molecular Filaments' (henceforth, GMFs) structures, which stretch over hundreds of parsecs and have masses of ∼ 10 5 -6 M glyph[circledot] .\n",
      "\n",
      "Using the same analysis tools used on the IRAM-30m observations, we determine the kinematic structure of the region surrounding Cloud F (see appendix E). The results of this analysis are presented in the upper panel of Figure 9, which shows the position-position-velocity diagram with each point coloured to the identified velocity components given in Table E1 (features of interest are shown in the figure legend). The most extended and prominent of these structures, F GRS2 , is coherent over nearly the entire mapped region, a projected distance of &gt; 20 pc at the assumed source distance of 3.7 kpc (Simon et al. 2006b).\n",
      "\n",
      "The lower panel of Figure 9 shows the structures identified from the IRAM-30m C 18 O(1 -0) observations overlaid on those from the 13 CO(1 -0) GRS observations. We find that the F PPV4 structure appears to coincide spatially and in velocity with the F GRS2 towards the north of the IRAM-30m mapped region. However, towards the south of the IRAM30m map, towards the MM1 region, the F PPV4 component appears at a velocity in-between the F GRS1 and F GRS2 components, which suggests that the splitting of these two GRS components is an optical depth effect in the 13 CO observations, which would make sense as this is one the densest regions within the cloud. Furthermore, as discussed in appendix A, this optically thick emission profile can be linked\n",
      "\n",
      "Figure 9. Shown are position-position-velocity diagrams covering the large-scale region surrounding Cloud F. The upper panel displays the Gaussian decomposition results for the 13 CO(1 -0) GRS observations, where the colour of each point represents its association with a coherent velocity component (see Appendix E). The three most extended components are shown in the legend in the upper right of the panel. The lower panel displays the same position-position-velocity diagram with the GRS observations shown in grey, overlaid with the structures determined from the IRAM-30m C 18 O(1 -0) observations shown in colours identical to Figure 6 (see legend in upper right of panel). The size of each point represents its relative peak intensity. The mass surface density map of Kainulainen &amp; Tan (2013) is shown on the base of each plot. Note, the coordinate offsets of these plots are relative to the centre of the mapped region: RA(J2000)=18 h 53 m 19 s , Dec(J2000)=01 ◦ 27 ′ 21 ′′ ( l = 34.441 ◦ , b = 0.247 ◦ ).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "to the infall motions previously identified towards this region (Ramesh et al. 1997; Sanhueza et al. 2010).\n",
      "\n",
      "We find that F PPV4 and F PPV2 appear to trace F GRS1 and F GRS2 on the west side of the mapped region, towards the MM8/F1 region. However, given the spatial resolution of GRS observations ( ∼ 0.8 pc), it is difficult to distinguish the transition to the optically thin regime when inspecting the spectra from the MM1 region to the F1/MM8 regions, hence the 13 CO observations towards this region may also be optically thick. Towards the F4/MM3 region, the F PPV1 component does not appear to have any associated component in the GRS observations. It is possible that this component has blended in the GRS data, which seems feasible given the broad line width of the F GRS2 component within this region ( ∼ 2 km s -1 ), and the close proximity in velocity to the F PPV4 component ( ∼ 1 kms -1 ). Despite the caveats discussed here, the brightest and most extended structures in the GRS observations appear to correspond to the structures in the IRAM-30m, indicating that Cloud F could be the central, densest part of this larger scale structure.\n",
      "\n",
      "## 5.2.1 Cloud F as part of a massive inter-arm filament\n",
      "\n",
      "Ragan et al. (2014) identified a structure within the spatial coverage and velocity range (50-60 kms -1 ) of the F GRS1 /F GRS2 components as the Giant Molecular Filament 38.1-32.4a, which has the largest mass ( ∼ 10 6 M glyph[circledot] ) and projected length ( ∼ 200 pc) in their sample. It was suggested that this structure resides between the near and far Sagittarius arm, hence could be classified as an 'inter-arm cloud' (e.g. Zucker et al. 2015). However, the recent Bayesian distance estimator from Reid et al. (2016), which takes into account the kinematic distance, displacement from the plane, and proximity to individual parallax and the probability of residing within a spiral arm (i.e. as priors), places this source in the far Sagittarius arm, at a distance of 10.6 ± 0.3 kpc. This is in disagreement with the kinematic distance analysis from Roman-Duval et al. (2009), which places the cloud at the near distance of 3.7 ± 0.8 kpc based on the absorption of the background HI emission towards this region, the nearinfrared extinction distance of ∼ 3 kpc (Foster et al. 2012), and the parallax distance of 1.56 +0 . 12 -11 kpc (Kurayama et al. 2011; see Foster et al. 2012 and Foster et al. 2014 for discussion of potential issues with this measurement).\n",
      "\n",
      "We adjust the weighting on near/far kinematic distance within the Reid et al. (2016) estimator (the only aforementioned prior easily varied). We find that this has to be set to a 1 per cent probability of the source being at the far distance in order to recover a value consistent with the near kinematic distance (3.6 ± 0.7 kpc). 13 Taking this distance estimate for GMF38.1-32.4a (and Cloud F) places it in-between the near and far Sagittarius arms, as previously suggested by Ragan et al. (2014). This would make this region of particular interest for further study, as one of the most massive and extended inter-arm star-forming complexes in the Milky Way.\n",
      "\n",
      "13 See http://bessel.vlbi-astrometry.org/bayesian , where the default value that the source is at the far distance is 50 per cent. Adjusting the weightings of the other three priors is possible, yet beyond the scope of this work.\n",
      "\n",
      "## 5.2.2 Could Cloud F be interacting with the supernova remnant W44?\n",
      "\n",
      "We note that the complication with the source distance may be caused by the higher than average uncertainty in the spiral arm models towards the 33 ◦ &gt;l&gt; 36 ◦ longitude region, due to the W44 supernova remnant, which has spread the gas over a large velocity range (Dame et al. 1986; Cardillo et al. 2014). A speculatory scenario could then be that the supernova remnant is directly influencing the gas within Cloud F, forcing it to a higher velocity than this predicted for the Sagittarius arm near-arm ( ∼ 30 km s -1 ; Reid et al. 2014). Assuming that the cloud had an original velocity of ∼ 30 km s -1 , the Reid et al. (2016) estimator places the source at a distance of 2.12 ± 0.17 kpc, which is in better agreement with the parallax distance from Kurayama et al. (2011). This is, however, then significantly closer than the distance to W44 of ∼ 3.2 kpc, determined from pulsar timing (Wolszczan et al. 1991). Furthermore, the structure of the W44, observed in the infrared, radio and x-ray, doesn't appear to extend high enough in galactic latitude to be interacting with the Cloud F (or GMF 38.1-32.4a; Castelletti et al. 2007, 2011; Cardillo et al. 2014). It is then not clear if it is possible that these two sources are interacting, nevertheless, such a scenario would further this cloud as an interesting source for future studies.\n",
      "\n",
      "## 6 CONCLUSIONS\n",
      "\n",
      "In this work, we have identified the kinematic structures within a relatively quiescent massive IRDC: G034.43+00.24 (or Cloud F; e.g. Butler &amp; Tan 2009). To do so, we have acquired high-sensitivity, high spectral resolution maps of the C 18 O(1 -0) and N 2 H + (1 -0) molecular line transitions taken with the IRAM-30m telescope. These lines were chosen as they are thought to trace the moderate to high-density gas within quiescent star-forming regions ( ∼ 10 3 -5 cm -3 ). Multiple velocity components are seen in the C 18 O(1 -0) spectra at the majority of positions throughout the cloud. To separate and link these into coherent structures, we use semi-automated Gaussian line fitting and hierarchical clustering ( scouse and acorns ; Henshaw et al. 2016, in prep). Compared to moment and channel map analysis, which are typically used for kinematic studies, the use of these algorithms removes much of the subjectivity in identifying velocity structures, allowing for a reliable investigation into their properties.\n",
      "\n",
      "We find four distinct coherent velocity components within Cloud F in C 18 O(1 -0) emission, some of which are extended along the majority of the cloud ( ∼ 4 pc). We compare the properties of these to the velocity components identified within a similar IRDC, G035.39-00.33 (Cloud H; e.g. Butler &amp; Tan 2009). We find that these share many similar properties, such as the components appear to be very dense (approximately &gt; 10 4 cm -3 , as inferred from the extended N 2 H + emitting area), the components have mildly supersonic velocity dispersions, the components have a similar separation in velocity, and there is a significant (positive) velocity difference between similar components identified in C 18 O and N 2 H + emission. The latter two of these could hint at a common scenario of gentle filament merging, although this requires further investigation.\n",
      "\n",
      "We investigate the large-scale kinematic structure surrounding Cloud F, by using the lower density tracer 13 CO(1 -0) from the Galactic Ring Survey. Several very extended ( &gt; 10 pc) structures are identified throughout the GRS region, some of which may, in fact, be larger if not artificially split in velocity by optical depth effects. We find that the structures identified from the IRAM-30m observations are coincident with the central, brightest and most extended component in the GRS, suggesting that the IRDCs are the densest central parts of less dense, larger scale structures. We find that the structure identified here could be the Giant Molecular Filament 38.1-32.4a found by Ragan et al. (2014), which when taking the kinematic source distance places it as an 'inter-arm cloud' ('spur' or 'feather') residing inbetween the near and far Sagittarius arm.\n",
      "\n",
      "## ACKNOWLEDGEMENTS\n",
      "\n",
      "A.T.B would like to acknowledge the funding provided by Liverpool John Moores University, the Max-Plank -Institute for Extraterrestrial Physics and the University of Leeds. P.C. acknowledges financial support from the European Research Council (ERC Advanced Grant; PALs 320620). Partial salary support for A.P. was provided by a Canadian Institute for Theoretical Astrophysics (CITA) National Fellowship. I.J.-S. acknowledges the financial support received from the STFC through an Ernest Rutherford Fellowship (proposal number ST/L004801/2). The research leading to these results has also received funding from the European Commission (FP/2007-2013) under grant agreement No 283393 (RadioNet3). This analysis has made use of the spectral cube Version 0.4.0 package, and the acorns package.\n",
      "\n",
      "## REFERENCES\n",
      "\n",
      "- Andr´ e P., Belloche A., Motte F., Peretto N., 2007, A&amp;A, 472, 519 Bailey N. D., Basu S., Caselli P., 2015, ApJ, 798, 75\n",
      "- Ballesteros-Paredes J., Mac Low M.-M., 2002, ApJ, 570, 734\n",
      "- Barnes A. T., Kong S., Tan J. C., Henshaw J. D., Caselli P., Jim´ enez-Serra I., Fontani F., 2016, MNRAS, 458, 1990\n",
      "- Barnes A. T., Longmore S. N., Battersby C., Bally J., Kruijssen J. M. D., Henshaw J. D., Walker D. L., 2017, MNRAS, 469, 2263\n",
      "- Butler M. J., Tan J. C., 2009, ApJ, 696, 484\n",
      "- Butler M. J., Tan J. C., 2012, ApJ, 754, 5\n",
      "- Cardillo M., et al., 2014, A&amp;A, 565, A74\n",
      "- Carey S. J., Clark F. O., Egan M. P., Price S. D., Shipman R. F., Kuchar T. A., 1998, ApJ, 508, 721\n",
      "- Caselli P., Myers P. C., Thaddeus P., 1995, ApJL, 455, L77+\n",
      "- Caselli P., Walmsley C. M., Zucconi A., Tafalla M., Dore L., Myers P. C., 2002a, ApJ, 565, 344\n",
      "- Caselli P., Benson P. J., Myers P. C., Tafalla M., 2002b, ApJ, 572, 238\n",
      "- Castelletti G., Dubner G., Brogan C., Kassim N. E., 2007, A&amp;A, 471, 537\n",
      "- Castelletti G., Dubner G., Clarke T., Kassim N. E., 2011, A&amp;A, 534, A21\n",
      "- Cazzoli G., Puzzarini C., Lapinov A. V., 2003, ApJL, 592, L95\n",
      "- Cazzoli G., Cludi L., Buffa G., Puzzarini C., 2012, ApJS, 203, 11\n",
      "- Chambers E. T., Jackson J. M., Rathborne J. M., Simon R., 2009, ApJS, 181, 360\n",
      "- Chira R.-A., Kainulainen J., Ib` a˜ nez-Mej´ ıa J. C., Henning T., Mac Low M.-M., 2017, preprint, ( arXiv:1711.01417 )\n",
      "- Dame T. M., Elmegreen B. G., Cohen R. S., Thaddeus P., 1986, ApJ, 305, 892\n",
      "- Devine K. E., Chandler C. J., Brogan C., Churchwell E., Indebetouw R., Shirley Y., Borg K. J., 2011, ApJ, 733, 44\n",
      "- Dirienzo W. J., Brogan C., Indebetouw R., Chandler C. J., Friesen R. K., Devine K. E., 2015, AJ, 150, 159\n",
      "- Egan M. P., Shipman R. F., Price S. D., Carey S. J., Clark F. O., Cohen M., 1998, ApJL, 494, L199+\n",
      "- Evans II N. J., 1999, ARA&amp;A, 37, 311\n",
      "- Fontani F., Caselli P., Crapsi A., Cesaroni R., Molinari S., Testi L., Brand J., 2006, A&amp;A, 460, 709\n",
      "- Fontani F., et al., 2011, A&amp;A, 529, L7+\n",
      "- Foster J. B., Stead J. J., Benjamin R. A., Hoare M. G., Jackson J. M., 2012, ApJ, 751, 157\n",
      "- Foster J. B., et al., 2014, ApJ, 791, 108\n",
      "- Frerking M. A., Langer W. D., Wilson R. W., 1982, ApJ, 262, 590\n",
      "- Friesen R. K., Di Francesco J., Myers P. C., Belloche A., Shirley Y. L., Bourke T. L., Andr´ e P., 2010, ApJ, 718, 666\n",
      "- Garay G., Fa´ undez S., Mardones D., Bronfman L., Chini R., Nyman L.- ˚ A., 2004, ApJ, 610, 313\n",
      "- Gerner T., Shirley Y., Beuther H., Semenov D., Linz H., Abertsson T., Henning T., 2015, preprint, ( arXiv:1503.06594 )\n",
      "- Goodman A. A., Benson P. J., Fuller G. A., Myers P. C., 1993, ApJ, 406, 528\n",
      "- Hacar A., Tafalla M., Kauffmann J., Kov´ acs A., 2013, A&amp;A, 554, A55\n",
      "- Hacar A., Kainulainen J., Tafalla M., Beuther H., Alves J., 2016, A&amp;A, 587, A97\n",
      "- Hacar A., Tafalla M., Alves J., 2017, preprint, ( arXiv:1703.07029 )\n",
      "- Henshaw J. D., Caselli P., Fontani F., Jim´ enez-Serra I., Tan J. C., Hernandez A. K., 2013, MNRAS, 428, 3425\n",
      "- Henshaw J. D., Caselli P., Fontani F., Jim´ enez-Serra I., Tan J. C., 2014, MNRAS, 440, 2860\n",
      "- Henshaw J. D., et al., 2016, MNRAS, 457, 2675\n",
      "- Henshaw J. D., et al., 2017, MNRAS, 464, L31\n",
      "- Hernandez A. K., Tan J. C., 2011, ApJ, 730, 44\n",
      "- Hernandez A. K., Tan J. C., 2015, ApJ, 809, 154\n",
      "- Hernandez A. K., Tan J. C., Caselli P., Butler M. J., Jim´ enezSerra I., Fontani F., Barnes P., 2011, ApJ, 738, 11\n",
      "- Hernandez A. K., Tan J. C., Kainulainen J., Caselli P., Butler M. J., Jim´ enez-Serra I., Fontani F., 2012, ApJL, 756, L13\n",
      "- Jackson J. M., et al., 2006, ApJS, 163, 145\n",
      "- Jim´ enez-Serra I., Caselli P., Tan J. C., Hernandez A. K., Fontani F., Butler M. J., van Loo S., 2010, MNRAS, 406, 187\n",
      "- Jim´ enez-Serra I., Caselli P., Fontani F., Tan J. C., Henshaw J. D., Kainulainen J., Hernandez A. K., 2014, MNRAS, 439, 1996 Kainulainen J., Tan J. C., 2013, A&amp;A, 549, A53\n",
      "- Kauffmann J., Bertoldi F., Bourke T. L., Evans II N. J., Lee C. W., 2008, A&amp;A, 487, 993\n",
      "- Kirk H., Myers P. C., Bourke T. L., Gutermuth R. A., Hedden A., Wilson G. W., 2013, ApJ, 766, 115\n",
      "- Kong S., et al., 2016, ApJ, 821, 94\n",
      "- Kong S., Tan J. C., Caselli P., Fontani F., Liu M., Butler M. J., 2017, ApJ, 834, 193\n",
      "- Kurayama T., Nakagawa A., Sawada-Satoh S., Sato K., Honma M., Sunada K., Hirota T., Imai H., 2011, PASJ, 63, 513\n",
      "- Longmore S. N., et al., 2012, ApJ, 746, 117\n",
      "- Lou Y.-Q., Gao Y., 2011, MNRAS, 412, 1755\n",
      "- Miettinen O., Hennemann M., Linz H., 2011, A&amp;A, 534, A134\n",
      "- Miralles M. P., Rodriguez L. F., Scalise E., 1994, ApJS, 92, 173\n",
      "- Molinari S., Brand J., Cesaroni R., Palla F., 1996, A&amp;A, 308, 573\n",
      "- Molinari S., et al., 2016, A&amp;A, 591, A149\n",
      "- Myers P. C., 1983, ApJ, 270, 105\n",
      "- Pagani L., Daniel F., Dubernet M., 2009, A&amp;A, 494, 719\n",
      "\n",
      "Perault M., et al., 1996, A&amp;A, 315, L165 Peretto N., et al., 2010, A&amp;A, 518, L98\n",
      "\n",
      "- Pillai T., Wyrowski F., Carey S. J., Menten K. M., 2006, A&amp;A, 450, 569\n",
      "\n",
      "Pon A., Caselli P., Johnstone D., Kaufman M., Butler M. J., Fontani F., Jim´ enez-Serra I., Tan J. C., 2015, A&amp;A, 577, A75\n",
      "\n",
      "- Pon A., et al., 2016a, A&amp;A, 587, A96\n",
      "- Pon A., et al., 2016b, ApJ, 827, 107\n",
      "- Ragan S. E., Bergin E. A., Plume R., Gibson D. L., Wilner D. J., O'Brien S., Hails E., 2006, ApJS, 166, 567\n",
      "- Ragan S. E., Bergin E. A., Wilner D., 2011, ApJ, 736, 163\n",
      "- Ragan S. E., Henning T., Tackenberg J., Beuther H., Johnston K. G., Kainulainen J., Linz H., 2014, A&amp;A, 568, A73\n",
      "- Ramesh B., Bronfman L., Deguchi S., 1997, PASJ, 49, 307\n",
      "- Rathborne J. M., Jackson J. M., Chambers E. T., Simon R., Shipman R., Frieswijk W., 2005, ApJL, 630, L181\n",
      "- Rathborne J. M., Jackson J. M., Simon R., 2006, ApJ, 641, 389 Rathborne J. M., Jackson J. M., Zhang Q., Simon R., 2008, ApJ, 689, 1141\n",
      "- Reid M. J., et al., 2014, ApJ, 783, 130\n",
      "- Reid M. J., Dame T. M., Menten K. M., Brunthaler A., 2016, ApJ, 823, 77\n",
      "- Roman-Duval J., Jackson J. M., Heyer M., Johnson A., Rathborne J., Shah R., Simon R., 2009, ApJ, 699, 1153\n",
      "- Rygl K. L. J., Wyrowski F., Schuller F., Menten K. M., 2013, A&amp;A, 549, A5\n",
      "\n",
      "Sakai T., et al., 2013, ApJL, 775, L31\n",
      "\n",
      "- Sanhueza P., Garay G., Bronfman L., Mardones D., May J., Saito M., 2010, ApJ, 715, 18\n",
      "- Shepherd D. S., N¨ urnberger D. E. A., Bronfman L., 2004, ApJ, 602, 850\n",
      "\n",
      "Shepherd D. S., et al., 2007, ApJ, 669, 464\n",
      "\n",
      "- Simon R., Jackson J. M., Rathborne J. M., Chambers E. T., 2006a, ApJ, 639, 227\n",
      "- Simon R., Rathborne J. M., Shah R. Y., Jackson J. M., Chambers E. T., 2006b, ApJ, 653, 1325\n",
      "- Smith R. J., Shetty R., Beuther H., Klessen R. S., Bonnell I. A., 2013, ApJ, 771, 24\n",
      "- Smith R. J., Glover S. C. O., Klessen R. S., Fuller G. A., 2016, MNRAS, 455, 3640\n",
      "\n",
      "Sokolov V., et al., 2017, A&amp;A, 606, A133\n",
      "\n",
      "Tackenberg J., et al., 2014, A&amp;A, 565, A101\n",
      "\n",
      "- Tan J. C., Kong S., Butler M. J., Caselli P., Fontani F., 2013, ApJ, 779, 96\n",
      "- Vasyunina T., Linz H., Henning T., Stecklum B., Klose S., Nyman L.- ˚ A., 2009, A&amp;A, 499, 149\n",
      "- Walsh A. J., Bertoldi F., Burton M. G., Nikola T., 2001, MNRAS, 326, 36\n",
      "- Wang Y., Zhang Q., Rathborne J. M., Jackson J., Wu Y., 2006, ApJL, 651, L125\n",
      "- Wilson T. L., Matteucci F., 1992, A&amp;AR, 4, 1\n",
      "- Wilson T. L., Rood R., 1994, ARA&amp;A, 32, 191\n",
      "- Wolszczan A., Cordes J. M., Dewey R. J., 1991, ApJL, 372, L99\n",
      "- Xu J.-L., Li D., Zhang C.-P., Liu X.-L., Wang J.-J., Ning C.-C., Ju B.-G., 2016, ApJ, 819, 117\n",
      "\n",
      "Yanagida T., et al., 2014, ApJL, 794, L10\n",
      "\n",
      "- Zamora-Avil´ es M., Ballesteros-Paredes J., Hartmann L. W., 2017, preprint, ( arXiv:1708.01669 )\n",
      "- Zhang C.-P., Yuan J.-H., Li G.-X., Zhou J.-J., Wang J.-J., 2017, A&amp;A, 598, A76\n",
      "- Zucker C., Battersby C., Goodman A., 2015, ApJ, 815, 23\n",
      "\n",
      "## APPENDIX A: THE KINEMATIC STRUCTURE OF CLOUD F\n",
      "\n",
      "The kinematic structure of Cloud F is discussed in this section with reference to previous work on this source. For reference, the Rathborne et al. (2006) and Butler &amp; Tan (2012) core regions mentioned in the follow section are labeled on Figure 1. We also give a note on the physical interpretation of structures identified in molecular line observations.\n",
      "\n",
      "## A1 The F1/MM8 region\n",
      "\n",
      "Towards the south-west of the mapped region, the F1/MM8 core region, we find two distinct velocity components: F PPV2 and F PPV4 , both seen in the C 18 O(1 -0) emission. As shown in Figure 5 (right panel), the higher velocity of these components shows a factor of two narrower line width, with respect to the mean value of this component (line width towards this region and mean width of the F PPV4 component are ∼ 1 kms -1 and ∼ 1.8 km s -1 , respectively).\n",
      "\n",
      "A similar double-peaked line profile may, however, be produced as a result of optically depth. If the emission were optically thick, it would be self-absorbed at the mean centroid velocity of the region (as traced by optically thin emission). Unfortunately, significant N 2 H + (1 -0) emission is not observed towards this region, however, another high-density tracing, optically thin molecular line transition, N 2 D + (3 -2) has been shown to have emission at velocities coincident with only the lower velocity component (Pon et al. 2016a; Tan et al. 2013). This is not expected if these components were produced by optical depth, as instead, the optically thin emission would have a centroid velocity at the centre of these components (see section 5.2 for a discussion).\n",
      "\n",
      "The depletion of CO-bearing molecules, such as C 18 O, onto the dust grains within dense, cold environments, may also artificially produce multiple velocity components. However, a dip in the emission profile of the CO-bearing molecule emission (e.g. C 18 O) would typically be seen at the centroid velocity of the emission from a non-CO bearing molecules (such as N 2 D + ), which we again do not see (Tan et al. 2013). Furthermore, we calculate the level of CO depletion throughout Cloud F and find an average value towards this region of 1.3 ± 0.1 (no depletion of CO would be represented by a value of unity), which we believe is not significant enough to cause this effect (see Appendix D). We, therefore, find that two distinct velocity components with different line profiles are indeed present along the line of sight towards this region, pointing to different internal conditions within these components.\n",
      "\n",
      "Possible formation scenarios for the interesting structure observed towards this region have been discussed in a series of papers by Pon et al. (2015, 2016a,b). These authors identify both narrow and broad velocity components in JCMT mid-J transition 13 CO observations (with a spatial resolution of ∼ 11 ′′ ) towards this region, showing similar centroid velocities to the F PPV2 and F PPV4 components identified here. Pon et al. (2016a) suggest that protostars associated with the 24 µ m source just to the north of the F1 core have created a wind-blown bubble, where the broader, lower velocity component ( ∼ 56 km s -1 , F PPV2 here) traces the compressed gas within the bubble wall. This lower velocity component is also seen in high-J CO transitions (e.g.\n",
      "\n",
      "J = 8 → 7 upwards; Pon et al. 2015). Comparison between the PDR models and the high-J transition emission shows that there may be a hot gas component (of around ∼ 100 K) within this region, which could have been formed by the dissipation of turbulence as this component is compressed within the shell (Pon et al. 2015, 2016b). The origin of the narrow velocity component F PPV4 is, however, still unknown.\n",
      "\n",
      "## A2 The F4/MM3 region\n",
      "\n",
      "Initial molecular line studies towards the F4/MM3 region showed it to be cold, dense and quiescent, hence an ideal region to study the initial stages of massive star formation (Garay et al. 2004). Several more recent studies have, however, found signs that protostars could be already present within this region (e.g. Foster et al. 2014). For example, there is a clear point source towards this region in the Spitzer and Herschel images, which can be seen as a negative mass surface density values in Figure 3. Indeed, Chambers et al. (2009) found two sources within the MM3 region which could be classified as a 'green fuzzie' from their excess of 4.5 µ memission, a signpost of heated dust by embedded protostars, and Cosentino et al. (accepted) have found bright and broad SiO emission toward this core, indicative of an outflow (also see Sakai et al. 2013; Yanagida et al. 2014). These authors also detect water and methanol maser emission towards both of these sources, suggesting that massive protostars may be present within this region (e.g. Walsh et al. 2001). Wang et al. (2006) identified that the water maser emission towards this core has a single component, which is red-shifted by ∼ 20 km s -1 with respect to the molecular gas at ∼ 55 km s -1 , suggesting that the embedded protostar(s) within this region have already begun to influence the kinematics of the surrounding gas. Sanhueza et al. (2010) found that the optically thick emission from CO (3 -2) towards these regions have blue- and red-shifted lobes of ± 15 km s -1 around the mean centroid velocity of optically thin lines, such as C 18 O and CS (similarly broad profiles were found by Rathborne et al. 2005). These authors suggest that such profiles are the result of a molecular outflow with a total mass of ∼ 40M glyph[circledot] . Using the lower mass limit of the embedded sources within this region from Shepherd et al. (2007), 14 and extrapolated with a Kroupa IMF, we estimate that the total mass of protostars within this region is comparable to the mass of the molecular outflow.\n",
      "\n",
      "Towards the F4/MM3 region, we find a relatively simple velocity structure of only a single velocity component, F PPV4 . However, in light of the above discussion, it is possible that this kinematic structure has been influenced by protostellar feedback and/or is causing the star formation within this region. Indeed, we find a systematic offset between the N 2 H + emission towards higher velocities, with respect to the C 18 Oemission (of ∼ 0.2 km s -1 ; see section 4.3).\n",
      "\n",
      "14 The total mass of the sources with SED fits towards this region - IDs '25', '26', '27', '28', '29' and '30' - is ∼ 10M glyph[circledot] . We note that the uncertainty on this value, and on the total embedded stellar mass estimate, could be larger than a factor of three (see Barnes et al. 2017 for a discussion of the uncertainties present when determining embedded stellar masses).\n",
      "\n",
      "This red-shift is not, however, as large as the red-shift lobe of the optically thick CO emission from Sanhueza et al. (2010) or the water maser emission from Wang et al. (2006). Interestingly, recent high-resolution N 2 D + (3 -2) ALMA observations towards the F4 region show emission at a velocity of 57.10 ± 0.05 km s -1 (Kong et al. 2017), which is offset from the N 2 H + (1 -0) and C 18 O emission by &gt; 0.8 km s -1 for the same position (58.67 ± 0.07 kms and 57.90 ± 0.02 kms, respectively). Moreover, despite the evidence for an outflow within the region, observations of optically think molecular lines (HCO + , HCN; Zhang et al. 2017) towards this region show asymmetric line profile characteristic of infall motions (Evans 1999). This suggests that the active star-forming region within this northern portion of Cloud F, F2/MM3, is currently accreting material from the gas reservoir of the cloud.\n",
      "\n",
      "## A3 The MM1 and MM2 regions\n",
      "\n",
      "We observe the most complex spectra towards the south of the cloud, which at some positions show three velocity components along the line of sight: F PPV1 , F PPV2 and F PPV4 . This region is referred to as the MM1 region (Rathborne et al. 2005, 2006), and is located approximately 40 ′′ (or ∼ 0.75 pc, assuming a source distance of 3.7 kpc; Simon et al. 2006b), from the MM2 region, also known as the IRAS 18507+0121 (refer to Figure 1). The MM1 region is thought to host a young, embedded protostar, with a spectral type B2 (Shepherd et al. 2004; Rathborne et al. 2008; Shepherd et al. 2007), whereas the MM2 region is thought to be more massive and evolved, harbouring a B0.5 class star surrounded by an ultracompact HII region (Miralles et al. 1994; Molinari et al. 1996). These sources are thought to have a combined mass of ∼ 50M glyph[circledot] , and are driving a molecular outflow of ∼ 100M glyph[circledot] (Shepherd et al. 2007). When extrapolated using a Kroupa IMF the total embedded stellar mass within these regions is ∼ 200M glyph[circledot] . As with the MM3 region, despite the on-going star formation within MM1 and MM2, there is evidence to show large-scale infall motions towards these regions (Ramesh et al. 1997; Sanhueza et al. 2010; Zhang et al. 2017). Indeed, the spectral profiles of the optically thick 13 CO emission towards these regions show asymmetric profiles with enhanced blue-shifted peaks (see section 5.2). Again, this is suggesting that this active star-forming region is accreting material over scales of up to 2 pc, given the approximate extent of the double-peaked, blue-shifted profile seen in the 13 CO emission (see Figure E1).\n",
      "\n",
      "## APPENDIX B: A NOTE ON THE PHYSICAL INTERPRETATION OF VELOCITY COMPONENTS\n",
      "\n",
      "In this section, we would like to briefly mention the relation between the observed position-position-velocity space and the physical position-position-position space. As throughout this work, we make the assumption that the identified velocity components correspond directly to physical structures within the cloud, however, this has recently been suggested to have several caveats within low-mass star-forming regions (e.g. Ballesteros-Paredes &amp; Mac Low 2002; Smith et al. 2016). For example, Zamora-Avil´ es et al. (2017), used\n",
      "\n",
      "Figure C1. Shown are the average spectrum of the N 2 H + (1 -0) transition (upper) and the C 18 O(1 -0) transition (lower) across the mapped region of Cloud H. The horizontal dotted line represents the rms level of ∼ 0.02 K and ∼ 0.04 K for N 2 H + (1 -0) and C 18 O(1 -0), respectively.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "observational techniques to analyse a molecular cloud within a three dimensional, magnetohydrodynamic simulation and found that unassociated density enhancements can artificially appear as single coherent structures when superposed along the line-of-sight, particularly towards low-density regions. It is not clear, however, how these simulations apply to the massive star-forming regions, which typically have higher densities than their lower mass counterparts. In this work, we have identified velocity component(s) which are coherent across several parsecs within both moderate and higher density molecular line tracers. It would seem unlikely that these extended structures could be produced by the superposition of low-density random fluctuations.\n",
      "\n",
      "## APPENDIX C: REDUCTION AND ANALYSIS OF THE IRAM-30M OBSERVATIONS TOWARDS CLOUD H (G035.39-00.33)\n",
      "\n",
      "One of the aims of this work is a comparison of the kinematic structure of Cloud F (G034.43+00.24) and Cloud H (G035.39-00.33; Butler &amp; Tan 2009). The kinematic analysis of the C 18 O(1 -0) and N 2 H + (1 -0) IRAM-30m observation has already been conducted by Henshaw et al. (2013), yet we choose to analyse these data using the same techniques as Cloud F, such that no artificial differences are introduced into the comparison.\n",
      "\n",
      "## C1 Observations\n",
      "\n",
      "Details of the C 18 O(1 -0) and N 2 H + (1 -0) observations towards Cloud H are presented in Table 1 of Henshaw et al. (2013). Here, we smooth these observations to an angular resolution of 36 ′′ , with a pixel spacing of 18 ′′ , such that they have a spatial resolution of ∼ 0.5 pc at the source distance of 2.9 kpc (Simon et al. 2006b). This was done to match the spatial resolution of the Cloud F observations, the results of which can be found in section 5.1.\n",
      "\n",
      "## C2 Gaussian fitting and hierarchical clustering\n",
      "\n",
      "Figure C1 shows the average spectrum for the N 2 H + (1 -0) hyperfine component and C 18 O(1 -0) transitions across Cloud H. We find that the majority of the emission above the rms levels is between approximately 43 -57 kms -1 . Figure C2 shows the spectra at each pixel position across the cloud, plotted using the same velocity range as the average spectra. As with Cloud F, several spectra across the mapped region, particularly in C 18 O(1 -0), appear to have more than one peak, and show that multiple velocity components are present along the line of sight.\n",
      "\n",
      "We use the Gaussian profile fitting algorithm scouse to separate the velocity components within the spectra, then the hierarchical clustering routine acorns to identify the coherent velocity structures across the cloud. The same spectral averaging area (SAA) radius and threshold values in scouse that were used for Cloud F were used for Cloud H. These gave reasonable fits, and &lt; 10 per cent of the data had to be checked and re-fitted. The same input parameters as used in Cloud F were in acorns for the identification of the initial hierarchy. The parameter space survey of the relaxation factors, however, showed that for values of 0.5, 1.25, 1.0, for the peak intensity, centroid velocity and line width, respectively, were required to identify the most robust structures. 15 The results of the Gaussian fitting and structure finding routines are presented in Figures C2 and C3.\n",
      "\n",
      "We identify five structures in the C 18 O(1 -0) emission, defined as H PPV1 , H PPV2 , H PPV3 , and H PPV4a and H PPV4b (shown in orange, purple, green, red and blue), three of which are also identified in the N 2 H + (1 -0) emission. We choose to define H PPV4a and H PPV4b in this way, as, although they have been defined as separate structures (see Henshaw et al. 2014), previous single dish studies have defined them as one (Henshaw et al. 2013; Jim´ enez-Serra et al. 2014). The basic properties are given in Table 3.\n",
      "\n",
      "## APPENDIX D: CO DEPLETION WITHIN CLOUD F\n",
      "\n",
      "The depletion of CO onto dust-grain surfaces occurs in the coolest, densest regions of IRDCs, and therefore it is a signpost for material at the earliest phases of star formation, away from the effects of stellar feedback. This could artificially give the appearance of multiple velocity components, as the emission at the centroid velocity of the source could be reduced, mimicking optical depth effects. Here we calculate the column density and abundance of C 18 O within Cloud F, which are used to estimate the average CO depletion.\n",
      "\n",
      "The column density, N (C 18 O), is calculated following\n",
      "\n",
      "15 We note that, choosing the same relaxation factors as Cloud F did not significantly change the main structures in Cloud H, rather these cause acorns to identify additional structures using the lower peak intensity positions throughout the cloud.\n",
      "\n",
      "the procedure outlined by Caselli et al. (2002a), assuming the C 18 O(1 -0) emission is optically thin. The column density of hydrogen is given as N (H 2 ) = Σ / µ p m H , where µ p =2.33 a.m.u is the mean mass per particle, m H is the mass of hydrogen, and Σ is the mass surface density taken from Kainulainen &amp; Tan (2013). The abundance of C 18 O with respect to H 2 is calculated as X (C 18 O)= N (C 18 O)/ N (H 2 ). To determine\n",
      "\n",
      "5\n",
      "\n",
      "Figure C2. The upper two panels show the average spectrum of the N 2 H + (1 -0) transition (left) and the C 18 O(1 -0) transition (right) across the mapped region of Cloud H. The horizontal dotted line represents the rms level of ∼ 0.03 K and ∼ 0.07 K for N 2 H + (1 -0) and C 18 O(1 -0), respectively. The lower two panels show the spectra at each pixel position across the cloud (shown in black). The velocity ranges are 40 to 50 km s -1 , and the intensity ranges are -0.5 to 2.5 K for N 2 H + and -0.5 to 3.5 K for C 18 O. Overlaid on each spectrum are the results of the line fitting ( scouse ) and clustering ( acorns ) routines, which are discussed in section C2. The colours of these profiles represent the various velocity component associations given in Table 3. The background greyscale is the mass surface density map (Kainulainen &amp; Tan 2013).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      ".\n",
      "\n",
      "the abundance of CO we use the oxygen isotope ratio 16 O/ 18 O=58.5 d GC +37.1 ∼ 372 (Wilson &amp; Rood 1994), given the Galactocentric distance of Cloud F, d GC ∼ 5.7 kpc (assuming source distance of 3.7 kpc and a distance to the galactic centre of ∼ 8.3 kpc; Simon et al. 2006b; Reid et al. 2014). We find an average CO abundance across the cloud of X(CO)=1.3 × 10 -4 . Comparing this measured CO abundance to the reference (or 'expected') value, X ref (CO),\n",
      "\n",
      "Figure C3. Displayed in each panel is the position-position-velocity diagram of Cloud H, shown at various viewing angles. The left and right panels show N 2 H + (1 -0) and C 18 O(1 -0) results, respectively. The colour of each point represents the association to one of the distinct coherent velocity components identified using the clustering algorithm acorns (Henshaw et al. in prep), H PPV1 in orange, H PPV2 in purple, H PPV3 in green, and H PPV4a in red and H PPV4b in blue. The size of each point represents its relative peak intensity. The mass surface density map of Kainulainen &amp; Tan (2013) is shown on the base of each plot. Note, the coordinate offsets of these plots are relative to the centre of the mapped region: RA (J2000) = 18 h 57 m 08 s , Dec(J2000)=02 ◦ 10 ′ 12 ′′ ( l = 35.512 ◦ , b = -0.277 ◦ ).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "gives the CO depletion factor, f D = X ref (CO)/X(CO). Using the abundance gradients in the Galactic Disk from Wilson &amp; Matteucci 1992 and the Solar neighbourhood abundance of CO from Frerking et al. (1982), Fontani et al. (2006) find that the reference abundance of CO is given by X ref (CO)=9.5 × 10 -5 exp(1.105 - 0.13 d GC ), which for the Galactocentric distance of Cloud F is ∼ 1.4 × 10 -4 . We find that the average CO depletion factor across Cloud F is ∼ 1.2, which peaks with a value of ∼ 2.1 towards the MM3 core region (see Figure 1). These values are in the range previously calculated by Hernandez &amp; Tan (2011) using 13 CO emission towards Cloud F, albeit these authors used a slightly higher value of the reference abundance of ∼ 2 × 10 -4 . We note Pon et al. (2016b) find higher CO depletion values than observed in this work ( f D =5 -9), using higher resolution, higher CO J-transition observations with the James Clerk Maxwell Telescope. These observations are, however, more sensitive to the higher density gas, where CO is expected to be more depleted.\n",
      "\n",
      "In summary, here we have shown that Cloud F contains only a moderate level of CO depletion. Therefore, artificially split line profiles are not expected to contaminate the velocity component analysis from the C 18 O(1 -0) emission.\n",
      "\n",
      "## APPENDIX E: ANALYSIS OF THE GRS OBSERVATIONS TOWARDS CLOUD F (G0.34.43+00.24)\n",
      "\n",
      "## E1 Observations\n",
      "\n",
      "Observations covering a large scale of Cloud F have been taken as part of the Galactic Ring Survey (GRS Jackson et al. 2006). These 13 CO(1 -0) observations have an angular resolution of ∼ 44 ′′ and a spectral resolution of ∼ 0.2 km s -1 -factors of ∼ 1.5 and ∼ 3 larger than the (smoothed) IRAM30m observations. The data are publicly available from https://www.bu.edu/galacticring/new\\_data.html , from which we take the data cube over the region 34 &lt; l &lt; 36 ◦ , | b | &lt; 1 ◦ , 0 &lt; υ &lt; 100 kms -1 . This 2 ◦ × 2 ◦ image is significantly larger than required, hence we trim the image to a ∼ 1300 ′′ × 300 ′′ region covering the filamentary structure identified in the mass surface density map shown in Figure 1.\n",
      "\n",
      "## E2 Gaussian fitting and hierarchical clustering\n",
      "\n",
      "Shown in Figure E1 are the average spectrum and the spectrum at each position across the map. As with the IRAM30m observations of this cloud, these spectra are complex, showing multiple velocity components for the majority of positions. We use the scouse and acorns algorithms to separate and identify the coherent velocity structures across the cloud. We used the same threshold value in scouse as for the IRAM-30m observations. Given the larger number of pixels present in this dataset compared to the IRAM-30m observations, we choose a larger SAA in scouse of ∼ 145 ′′ (i.e. each SAA contained 40 positions, given the pixel spacing of ∼ 22 ′′ ). Nevertheless, this only resulted in a still manageable ∼ 20 per cent of the positions requiring manual inspection. The same input parameters used for the IRAM-30m observations were used in acorns for the identification of the initial hierarchy. The parameter space survey of the relaxation values showed that for values of 2.5, 1.75, 0.75, for the peak intensity, centroid velocity and line width, respectively, were required to identify the most robust structures. The results of these analyses are shown in Figures E1 and 9. Twenty distinct velocity components are identified here, for which the basic properties are presented in Table E1. Given that the observation used to identify these components are different in spatial resolution, angular resolution and extent of the IRAM-30m observations, we choose to differentiate these by using a different, 'F GRS ', nomenclature.\n",
      "\n",
      "Figure E1. The left panel shows the average spectrum of the 13 CO(1 -0) transition from the GRS. The horizontal dotted line represents the rms detection thresholds of ∼ 0.006 K. The right panel shows the spectra at each pixel position across Cloud F, overlaid with coloured profiles of the various velocity components. The velocity ranges are 45 to 70 km s -1 , and the intensity ranges are -0.5 to 5.0 K. The background greyscale is the mass surface density map (Kainulainen &amp; Tan 2013).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## 24 Barnes, Henshaw, Caselli, Jim´ enez-Serra, Tan, Fontani, Pon, Ragan\n",
      "\n",
      "Table E1. Parameters of the velocity components identified in the GRS observations towards Clouds F (F GRS ). Shown is the molecule used to identify the components (for consistency with Table 3), and for each component: the name, the total number of points, the average centroid velocity, the average line width, the velocity gradient and the angle of this gradient with respect to East of North. When the uncertainty on the velocity gradient is larger than or equal to the calculated velocity gradient, the velocity gradient angle is unconstrained, and therefore not shown.\n",
      "\n",
      "| Line         | Component   | # points   | Centroid velocity ( V 0 ) kms - 1   | Line (∆ υ   | width ) kms - 1    | Velocity gradient Gradient angle ( ∇ v ) kms - 1 pc - 1 ( θ ) degrees   |\n",
      "|--------------|-------------|------------|-------------------------------------|-------------|--------------------|-------------------------------------------------------------------------|\n",
      "| 13 CO(1 - 0) |             |            |                                     |             |                    | ∇ v                                                                     |\n",
      "|              | F GRS1      | 297        | 56.80 ± 0.39                        | 3.69 ± 1.17 | 0.02 ± 0.01        | -80.85 ± 9.57                                                           |\n",
      "|              | F GRS2      | 498        | 58.91 ± 0.71                        | 2.11 ± 0.56 | 0.14 ± 0.03        | 82.09 ± 2.55                                                            |\n",
      "|              | F GRS3      | 357        | 53.58 ± 1.05                        | 3.89 ± 1.27 | 0.42 ± 0.07        | -86.67 ± 1.02                                                           |\n",
      "|              | F GRS4      | 60         | 67.44 ± 0.38                        | 2.11 ± 0.35 | 0.20 ± 0.07        | 75.62 ± 6.64                                                            |\n",
      "|              | F GRS5      | 94         | 64.31 ± 0.80                        | 3.03 ± 0.75 | 0.38 ± 0.09        | 82.56 ± 1.91                                                            |\n",
      "|              | F GRS6      | 9          | 46.69 ± 0.21                        | 1.86 ± 0.52 | 0.51 ± 0.11        | -58.46 ± 7.23                                                           |\n",
      "|              | F GRS7      | 22         | 44.32 ± 0.18                        | 2.63 ± 0.81 | 0.11 ± 0.05        | 83.59 ± 4.60                                                            |\n",
      "|              | F GRS8      | 92         | 50.08 ± 0.28                        | 2.28 ± 0.72 | 0.0 ± 0.0          | . . .                                                                   |\n",
      "|              | F GRS9      | 21         | 54.73 ± 0.17                        | 0.92 ± 0.19 | 0.38 ± 0.07        | -70.60 ± 2.56                                                           |\n",
      "|              | F GRS10     | 23         | 64.11 ± 0.12                        | 1.62 ± 0.26 | 0.03 ± 0.06        | . . .                                                                   |\n",
      "|              | F GRS11     | 9          | 55.10 ± 0.57                        | 7.24 ± 0.63 | 0.29 ± 0.30        | 83.89 ± 9.12                                                            |\n",
      "|              | F GRS12     | 9          | 54.82 ± 0.10                        | 2.60 ± 0.19 | 0.18 ± 0.15        | 78.89 ± 13.21                                                           |\n",
      "|              | F GRS13     | 9          | 57.71 ± 0.12                        | 1.54 ± 0.27 | 0.26 ± 0.07        | -82.11 ± 2.39                                                           |\n",
      "|              | F GRS14     | 13         | 58.83 ± 0.23                        | 1.38 ± 0.88 | 0.29 ± 0.13        | 86.05 ± 2.99                                                            |\n",
      "|              | F GRS15     | 16         | 45.79 ± 0.23                        | 2.16 ± 0.23 | 0.41 ± 0.08        | -76.43 ± 2.62                                                           |\n",
      "|              | F GRS16     | 45         | 62.78 ± 0.31                        | 3.30 ± 0.50 | 0.18 ± 0.03 -63.14 | ± 5.15                                                                  |\n",
      "|              |             | 51         |                                     |             | 0.03 ±             |                                                                         |\n",
      "|              | F GRS17     | 40         | 41.01 ± 0.18                        | 1.30 ± 0.34 | 0.02               | 85.75 ± 9.67                                                            |\n",
      "|              | F GRS18     |            | 57.30 ± 0.17                        | 2.21 ± 0.63 | 0.05 ± 0.04        | -82.96 ± 10.16                                                          |\n",
      "\n",
      "±\n",
      "\n",
      "±\n",
      "\n",
      "±\n",
      "\n",
      "±\n",
      "Document 15:\n",
      "## Long Text Generation via Adversarial Training with Leaked Information\n",
      "\n",
      "Jiaxian Guo † , Sidi Lu † , Han Cai † , Weinan Zhang †∗ , Yong Yu † , Jun Wang ‡\n",
      "\n",
      "† Shanghai Jiao Tong University, ‡ University College London { jiaxian,steve lu,hcai,wnzhang,yyu } @apex.sjtu.edu.cn, j.wang@cs.ucl.ac.uk\n",
      "\n",
      "## Abstract\n",
      "\n",
      "Automatically generating coherent and semantically meaningful text has many applications in machine translation, dialogue systems, image captioning, etc. Recently, by combining with policy gradient, Generative Adversarial Nets (GAN) that use a discriminative model to guide the training of the generative model as a reinforcement learning policy has shown promising results in text generation. However, the scalar guiding signal is only available after the entire text has been generated and lacks intermediate information about text structure during the generative process. As such, it limits its success when the length of the generated text samples is long (more than 20 words). In this paper, we propose a new framework, called LeakGAN, to address the problem for long text generation. We allow the discriminative net to leak its own high-level extracted features to the generative net to further help the guidance. The generator incorporates such informative signals into all generation steps through an additional MANAGER module, which takes the extracted features of current generated words and outputs a latent vector to guide the WORKER module for next-word generation. Our extensive experiments on synthetic data and various realworld tasks with Turing test demonstrate that LeakGAN is highly effective in long text generation and also improves the performance in short text generation scenarios. More importantly, without any supervision, LeakGAN would be able to implicitly learn sentence structures only through the interaction between MANAGER and WORKER.\n",
      "\n",
      "## Introduction\n",
      "\n",
      "The ability to generate coherent and semantically meaningful text plays a key role in many natural language processing applications such as machine translation (Yang et al. 2017), dialogue generation (Li et al. 2017), and image captioning (Fang et al. 2015). While most previous work focuses on task-specific applications in supervised settings (Bahdanau, Cho, and Bengio 2014; Vinyals et al. 2015), the generic unsupervised text generation, which aims to mimic the distribution over real text from a corpus, has recently drawn much attention (Graves 2013; Yu et al. 2017; Zhang et al. 2017;\n",
      "\n",
      "∗ Correspondence to Weinan Zhang. This work is financially supported by NSFC (61702327) and Shanghai Sailing Program (17YF1428200).\n",
      "\n",
      "Copyright c © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n",
      "\n",
      "Hu et al. 2017). A typical approach is to train a recurrent neural network (RNN) to maximize the log-likelihood of each ground-truth word given prior observed words (Graves 2013), which, however, suffers from so-called exposure bias due to the discrepancy between training and inference stage: the model sequentially generates the next word based on previously generated words during inference but itself is trained to generate words given ground-truth words (Husz´ ar 2015). A scheduled sampling approach (Bengio et al. 2015) is proposed to addressed this problem, but is proved to be fundamentally inconsistent (Husz´ ar 2015). Generative Adversarial Nets (GAN) (Goodfellow et al. 2014), which is firstly proposed for continous data (image generation etc.), is then extended to discrete, sequential data to alleviate the above problem and has shown promising results (Yu et al. 2017). Due to the discrete nature of text samples, text generation is modeled as a sequential decision making process, where the state is previously generated words, the action is the next word to be generated, and the generative net G is a stochastic policy that maps current state to a distribution over the action space. After the whole text generation is done, the generated text samples are then fed to the discriminative net D , a classifier that is trained to distinguish real and generated text samples, to get reward signals for updating G .\n",
      "\n",
      "Since then, various methods have been proposed in text generation via GAN (Lin et al. 2017; Rajeswar et al. 2017; Che et al. 2017). Nonetheless, the reported results are limited to the cases that the generated text samples are short (say, fewer than 20 words) while more challenging long text generation is hardly studied, which is necessary for practical tasks such as auto-generation of news articles or product descriptions. A main drawback of existing methods to long text generation is that the binary guiding signal from D is sparse as it is only available when the whole text sample is generated. Also, the scalar guiding signal for a whole text is non-informative as it does not necessarily preserve the picture about the intermediate syntactic structure and semantics of the text that is being generated for G to sufficiently learn.\n",
      "\n",
      "On one hand, to make the guiding signals more informative, discriminator D could potentially provide more guidance beside the final reward value, since D is a trained model, e.g. a convolutional neural network (CNN) (Zhang and LeCun 2015), rather than an unknown black box. With that idea, (Zhang et al. 2017) proposed to train generator G\n",
      "\n",
      "via forcing learned feature representations of real and generated text by D to be matched, instead of directly training G to maximize the reward from D (Yu et al. 2017). Such a method can be effective in short text generation, but the guiding signals are still absent until the end of the text (Zhang et al. 2017).\n",
      "\n",
      "On the other hand, to alleviate the sparsity problem of the guiding signal, the idea of hierarchy naturally arises in text generation, since the real text samples are generated following some kinds of hierarchy such as the semantic structure and the part-of-speech (Mauldin 1984). By decomposing the whole generation task into various sub-tasks according to the hierarchical structure, it becomes much easier for the model to learn. Early efforts have been made to incorporate the hierarchy idea in text generation (Dethlefs and Cuay´ ahuitl 2010; Peng et al. 2017) but all use a predefined sub-task set from domain knowledge, which makes them unable to adapt to arbitrary sequence generation tasks.\n",
      "\n",
      "In this paper, we propose a new algorithmic framework called LeakGAN to address both the non-informativeness and the sparsity issues. LeakGAN is a new way of providing richer information from the discriminator to the generator by borrowing the recent advances in hierarchical reinforcement learning (Vezhnevets et al. 2017). As illustrated in Figure 1, we specifically introduce a hierarchical generator G , which consists of a high-level MANAGER module and a low-level WORKER module. The MANAGER is a long shortterm memory network (LSTM) (Hochreiter and Schmidhuber 1997) and serves as a mediator. In each step, it receives generator D 's high-level feature representation, e.g., the feature map of the CNN, and uses it to form the guiding goal for the WORKER module in that timestep. As the information from D is internally-maintained and in an adversarial game it is not supposed to provide G with such information. We thus call it a leakage of information from D .\n",
      "\n",
      "Next, given the goal embedding produced by the MANAGER, the WORKER first encodes current generated words with another LSTM, then combines the output of the LSTM and the goal embedding to take a final action at current state. As such, the guiding signals from D are not only available to G at the end in terms of the scalar reward signals, but also available in terms of a goal embedding vector during the generation process to guide G how to get improved.\n",
      "\n",
      "We conduct extensive experiments based on synthetic and real data. For synthetic data, LeakGAN obtains much lower negative log-likelihood than previous models with sequence length set to 20 and 40. For real data, we use the text in EMNLP2017 WMT News, COCO Image Caption and Chinese Poems as the long, mid-length and short text corpus, respectively. In all those cases, LeakGAN shows significant improvements compared to previous models in terms of BLEU statistics and human Turing test. We further provide a deep investigation on the interaction between MANAGER and WORKER, which indicates LeakGAN implicitly learns sentence structures, such as punctuation, clause structure and long suffix without any supervision.\n",
      "\n",
      "Figure 1: An overview of our LeakGAN text generation framework. While the generator is responsible to generate the next word, the discriminator adversarially judges the generated sentence once it is complete. The chief novelty lies in that, unlike conventional adversarial training, during the process, the discriminator reveals its internal state (feature f t ) in order to guide the generator more informatively and frequently. (See Methodology Section for more details.)\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Related Work\n",
      "\n",
      "Generating text that mimics human's expression has been studied for poem generation (Zhang and Lapata 2014), image captioning (Vinyals et al. 2015), dialogue system (Li et al. 2017) machine translation (Yang et al. 2017). (Graves 2013) proposed a recurent neural network (RNN) based generative model to use the human-generated text where at each step the model tries to predict the next word given previous real word sequence and is trained in a supervised fashion.\n",
      "\n",
      "A common difficulty of all supervised generative models is that it is hard to design an appropriate, differentiable, lowbias metric to evaluate the output of the generator, which inspires the adversarial training mechanisms. (Goodfellow et al. 2014) proposed generative adversarial nets (GANs) to generate continuous data like images. GAN introduces a minimax game between a generative model and a discriminative model, where the discriminator can be viewed as the dynamically-updated evaluation metric to guide the tuning of the generated data. To apply GANs to text generation, (Yu et al. 2017) proposed SeqGAN that models the text generation as a sequential decision making process and trains the generative model with policy gradient methods (Sutton et al. 1999). MaliGAN (Che et al. 2017) modifies the orginal GAN objective and proposes a set of training techniques to reduce the potential variance. To deal with the gradient vanishing problem of GAN, RankGAN (Lin et al. 2017) proposes an alternative solution to this problem by replacing the original binary classifier discriminator with a ranking model by taking a softmax over the expected cosine distances from the generated sequences to the real data. Another problem for the adversarial sequence generation models is that the binary feedback from the discriminator is not sufficiently informative, which requires a huge number of training and generated samples to improve the generator and could result in mode collapse problems. Feature Matching (Zhang et al. 2017) provides a mechanism that matches the latent feature distributions of real and generated sequences via a kernelized discepancy metric to alleviate the weak guidance and mode collapse problems. However, such enhancement only happens when the whole text sample is generated and thus the guiding signal is still sparse during the training.\n",
      "\n",
      "In this work, we model the text generation procedure via adversarial training and policy gradient (Yu et al. 2017). To address the sparse reward issue in long text generation, we follow (Vezhnevets et al. 2017) and propose a hierarchy design, i.e. MANAGER and WORKER, for the generator. As the reward function in our case is a discriminative model rather than a black box in (Vezhnevets et al. 2017), the high-level feature extracted by the discriminator given the current generated word sequence is sent to the MANAGER module. As such, the MANAGER module can be also viewed as a spy that leaks information from the discriminator to better guide the generator. To our knowledge, this is the first work that considers the information leaking in GAN framework for better training generators and combines hierarchical RL to address long text generation problems.\n",
      "\n",
      "Reinforcement learning (RL) on the other hand also faces a similar difficulty when reward signals are sparse (Kulkarni et al. 2016). Hierarchical RL is one of the promising techniques for handling the sparse reward issue (Sutton, Precup, and Singh 1999). A typical approach in hierarchical RL is to manually identify the hierarchical structure for the agent by defining several low-level sub-tasks and learning micropolicies for each sub-task while learning a macro-policy for choosing which sub-task to solve. Such methods can be very effective when the hierarchical structure is known a priori using domain knowledge in a given specific task, but fail to flexibly adapt to other tasks. Recently, (Vezhnevets et al. 2017) proposed an end-to-end framework for hierarchical RL where the sub-tasks are not identified manually but implicitly learned by a MANAGER module which takes current state as input and output a goal embedding vector to guide the low-level WORKER module.\n",
      "\n",
      "## Methodology\n",
      "\n",
      "As we discussed previously, although the above adversarial training is principled, the scalar guiding signal becomes relatively less informative when the sentence length T goes larger. To address this, the proposed LeakGAN framework\n",
      "\n",
      "We formalize the text generation problem as a sequential decision making process (Bachman and Precup 2015). Specifically, at each timestep t , the agent takes the previously generated words as its current state, denoted as s t = ( x 1 , . . . , x i , . . . , x t ) , where x i represents a word token in the given vocabulary V . A θ -parameterized generative net G θ , which corresponds to a stochastic policy, maps s t to a distribution over the whole vocabulary, i.e. G θ ( ·| s t ) , from which the action x t +1 , i.e. the next word to select is sampled. We also train a φ -parameterized discriminative model D φ that provides a scalar guiding signal D φ ( s T ) for G θ to adjust its parameters when the whole sentence s T has been generated.\n",
      "\n",
      "allows discriminator D φ to provide additional information, denoted as features f t , of the current sentence s t (it is internally used for D φ itself for discrimination) to generator G θ ( ·| s t ) . In LeakGAN, a hierarchical RL architecture is used as a promising mechanism to effectively incorporate such leaked information f t into the generation procedure of G θ (also see Figure 1).\n",
      "\n",
      "## Leaked Features from D as Guiding Signals\n",
      "\n",
      "Different from typical model-free RL settings where the reward function is a black box, our adversarial text generation uses D φ as a learned reward function. Typically, D φ is a neural network and can be decomposed into a feature extractor F ( · ; φ f ) and a final sigmoid classification layer with weight vector φ l . Mathematically, given input s , we have\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where φ = ( φ f , φ l ) and sigmoid ( z ) = 1 / (1 + e -z ) . f = F ( s ; φ f ) is the feature vector of s in the last layer of D φ , which is to be leaked to generator G θ . As is shown in Eq. (1), for a given D φ , the reward value for each state s mainly depends on the extracted features f . As such, the objective of getting a higher reward from D φ is equivalent to finding a higher reward region in this extracted feature space F ( S ; φ f ) = {F ( s ; φ f ) } s ∈ S . Specifically, our feature extractor F ( · ; φ f ) in D φ is implemented by a CNN (Zhang and LeCun 2015); thus F ( s ; φ f ) outputs the CNN feature map vector as f after its convolution-pooling-activation layer. Other neural network models such as LSTM (Hochreiter and Schmidhuber 1997) can also be used to implement D φ .\n",
      "\n",
      "Compared to the scalar signal D φ ( s ) , the feature vector f is a much more informative guiding signal for G θ , since it tells what the position of currently-generated words is in the extracted feature space.\n",
      "\n",
      "## A Hierarchical Structure of G\n",
      "\n",
      "In each step t during the generation procedure, to utilize the leaked information f t from D φ , we follow hierarchical RL (Vezhnevets et al. 2017) to have a hierarchical architecture of G θ . Specifically, we introduce a MANAGER module, an LSTM that takes the extracted feature vector f t as its input at each step t and outputs a goal vector g t , which is then fed into the WORKER module to guide the generation of the next word in order to approach the higher reward region in F ( S ; φ f ) . Next we will first describe the detailed generator model in LeakGAN and then show how the MANAGER and WORKER are trained with the guiding signals from D φ .\n",
      "\n",
      "Generation Process. The MANAGER and WORKER modules both start from an all-zero hidden state, denoted as h M 0 and h W 0 respectively. At each step, the MANAGER receives the leaked feature vector f t from the discriminator D φ , which is further combined with current hidden state of the MANAGER to produce the goal vector g t as\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where M ( · ; θ m ) denotes the MANAGER module implemented by an LSTM with parameters θ m and h t M is the recurrent hidden vector of the LSTM.\n",
      "\n",
      "To incorporate goals produced by MANAGER, a linear transformation ψ with weight matrix W ψ is performed on a summation over recent c goals to produce a k -dimensional goal embedding vector w t as\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Given the goal embedding vector w t , the WORKER module takes the current word x t as input and outputs a matrix O t , which is further combined with w t by matrix product to determine the final action space distribution under current state s t through a softmax\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where W ( · ; θ w ) denotes the WORKER module, i.e. an LSTMwith h t W as its recurrent hidden vector, O t is a | V |× k matrix that represents the current vector for all words, thus O t · w t yields the calculated logits for all words, and α is the temperature parameter to control the generation entropy.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "## Training of G\n",
      "\n",
      "Notice that the above procedure is fully differentiable. One can train G θ in an end-to-end manner using a policy gradient algorithm such as REINFORCE (Williams 1992). In LeakGAN, we would hope the MANAGER module to capture some meaningful patterns. Thus, we follow (Vezhnevets et al. 2017) and train the MANAGER and WORKER modules separately, where the MANAGER is trained to predict advantageous directions in the discriminative feature space and the WORKER is intrinsically rewarded to follow such directions. Similar to (Vezhnevets et al. 2017), the gradient of the MANAGER module is defined as where Q F ( s t , g t ) = Q ( F ( s t ) , g t ) = Q ( f t , g t ) = E [ r t ] is the expected reward under the current policy which can be approximately estimated via Monte Carlo search (Sutton et al. 2000; Yu et al. 2017). d cos represents the cosine similarity between the change of feature representation after c -step transitions, i.e. f t + c -f t , and the goal vector g t ( θ m ) 1 produced by MANAGER as in Eq. (2). Intuitively, the loss function is to force the goal vector to match the transition in the feature space while achieving high reward. At the same time, the WORKER is trained to maximize the reward using the REINFORCE algorithm (Williams 1992) as is done in (Yu et al. 2017),\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "1 We use g t ( θ m ) to explicitly show g t is parameterized by θ m .\n",
      "\n",
      "which can be approximated by sampling the state s t -1 and the action x t taken by WORKER. As the WORKER is encouraged to follow the directions produced by the MANAGER, following (Vezhnevets et al. 2017), the intrinsic reward for the WORKER is defined as\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "In practice, before the adversarial training, we need to pretrain G θ . To be consistent, in the pre-train stage, we also use the separate training scheme, where the gradient of MANAGER is where ˆ f t = F (ˆ s t ) , ˆ s t and ˆ s t + c are states of real text, and the state-action value Q F ( s t , g t ) in Eq. (7) is set as 1 here since the data instances used in pre-training are all real sentences. As such, the MANAGER is trained to mimic the transition of real text samples in the feature space. While the WORKER is trained via maximum likelihood estimation (MLE).\n",
      "\n",
      "In the training process, the generator G θ and discriminator D φ are alternatively trained. In the generator, the MANAGER M ( · ; θ m ) and WORKER W ( · ; θ w ) (including ψ and softmax) are alternatively trained while fixing the other. The details of the training procedure are attached in the supplementary material 2 .\n",
      "\n",
      "## Training Techniques\n",
      "\n",
      "Bootstrapped Rescaled Activation. During the adversarial training of SeqGAN (Yu et al. 2017), severe gradient vanishing occurs when D is much stronger than G , i.e. the reward is too small value to update the parameters and thus need be rescaled before being fed into G . Inspired by ranking idea from RankGAN (Lin et al. 2017), we propose a simple, time-efficient, rank-based method to rescale the rewards, named as bootstrapped rescaled activation . For a mini-batch with B sequences, after the rollout of the generative model, the reward matrix is denoted as R B × T . For each timestep t , we rescale the t -th column vector R t via\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "There are two main advantages of the bootstrapped rescaled activation. First, after this transformation, the expectation and variance of the reward in each mini-batch are constant. In this case, the rescale activation serves as a value stabilizer that is helpful for algorithms that are sensitive in numerical variance. Second, as all ranking methods do, it prevents the gradient vanishing problem, which accelerates the model convergence.\n",
      "\n",
      "where rank ( i ) denotes the i -th element's high-to-low ranking in this column vector. δ is a hyperparameter that controls the smoothness of the rescale activation. σ ( · ) is an activation function that re-projects the equidifferent scoring based on ranking to a more effective distribution. In our experiment, for example, the model adopts hyperparameter δ = 12 . 0 and the sigmoid function as σ ( · ) .\n",
      "\n",
      "2 https://arxiv.org/abs/1709.08624\n",
      "\n",
      "Interleaved Training. In traditional generative adversarial models, mode collapse is a common problem. Here we propose a training scheme called interleaved training to alleviate such a problem. As its name is, we adopt an interleaving of supervised training (i.e. MLE) and adversarial training (i.e. GAN) instead of full GAN after the pre-training. For example, we perform one epoch of supervised learning for G after 15 epochs of adversarial training. An explanation of why this scheme works is that blending these two trainings would help GAN get rid of some bad local minimums and alleviate mode collapse. Another justification is that the inserted supervised learning performs an implicit regularization on the generative model to prevent it from going too far away from the MLE solution.\n",
      "\n",
      "Temperature Control. The Boltzmann temperature α in Eq. (6) is a factor that could be used to balance the exploration and exploitation for reinforcement learning problems. Here we select a higher temperature when we are training the model and a lower temperature when we adopt the model to generate samples.\n",
      "\n",
      "## Experiment\n",
      "\n",
      "The experiment consists of three parts: synthetic data experiments, experiments in real-world scenarios and some explanation study. The repeatable experiment code is published for further research 3 .\n",
      "\n",
      "## Training Settings\n",
      "\n",
      "Synthetic Oracle. For the synthetic data experiments, simlar to (Yu et al. 2017), we first initialize the parameters of an LSTM following the normal distribution N (0 , 1) as the oracle describing the real data distribution G oracle ( x t | x 1 , . . . , x t -1 ) . We use it to generate 10,000 sequences of length 20 and 40 respectively as the training set S for the generative models.\n",
      "\n",
      "GAN Setting. For the discriminator, we choose the CNN architecture (Zhang and LeCun 2015) as the feature extractor and the binary classifier. Note that one could design specific structure for different tasks to refine the CNN performance. For the synthetic data experiment, the CNN kernel size ranges from 1 to T . The number of each kernel is between 100 and 200. In this case, the feature of text is a 1,720 dimensional vector. Dropout (Srivastava et al. 2014) with the keep rate 0.75 and L2 regularization are performed to avoid overfitting. For the generator, we adopt LSTM (Hochreiter and Schmidhuber 1997) as the architectures of MANAGER and WORKER to capture the sequence context information. The MANAGER produces the 16-dimensional goal embedding feature vector w t using the feature map extracted by CNN. The goal duration time c is a hyperparameter set as 4 after some preliminary experiments.\n",
      "\n",
      "Compared Models. For most parts of our experiment, three baseline models are mainly compared with LeakGAN, namely an MLE trained LSTM, SeqGAN (Yu et al. 2017) and RankGAN (Zhang et al. 2017). We also compare model\n",
      "\n",
      "3 https://github.com/CR-Gjx/LeakGAN.\n",
      "\n",
      "Figure 2: The illustration of training curve.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Table 1: The over NLL performance on synthetic data.\n",
      "\n",
      "|   Length |    MLE |   SeqGAN |   RankGAN |   LeakGAN |   Real | p -value   |\n",
      "|----------|--------|----------|-----------|-----------|--------|------------|\n",
      "|       20 |  9.038 |    8.736 |     8.247 |     7.038 |  5.75  | < 10 - 6   |\n",
      "|       40 | 10.411 |   10.31  |     9.958 |     7.191 |  4.071 | < 10 - 6   |\n",
      "\n",
      "Table 2: BLEU scores performance on EMNLP2017 WMT.\n",
      "\n",
      "| Method   |   SeqGAN |   RankGAN |   LeakGAN | p -value   |\n",
      "|----------|----------|-----------|-----------|------------|\n",
      "| BLEU-2   |   0.859  |     0.778 |     0.956 | < 10 - 6   |\n",
      "| BLEU-3   |   0.6015 |     0.478 |     0.819 | < 10 - 6   |\n",
      "| BLEU-4   |   0.4541 |     0.411 |     0.627 | < 10 - 6   |\n",
      "| BLEU-5   |   0.4498 |     0.463 |     0.498 | < 10 - 6   |\n",
      "\n",
      "variants, such as SeqGAN with bootstrapped rescaled activation, and include the real data to be referred as the performance upperbound.\n",
      "\n",
      "Evaluation Metrics. Negative log-likehood (NLL) is used for synthetic data experiment since there is the oracle data distribution available for evaluation. For real-world data experiments, BLEU statistics (Papineni et al. 2002) and human rating scores in the Turing test are reported. We further perform a t-test for the improvement of LeakGAN over the second highest performance and report the p -value.\n",
      "\n",
      "## Synthetic Data Experiments\n",
      "\n",
      "Werun the synthetic data experiment with the text-length set as 20 and 40 respectively.\n",
      "\n",
      "The training curves are depicted in Figure 2 and the overall NLL performance is presented in Table 1. One could have two observations from the results. (i) In the pre-training stage, LeakGAN has already shown observable performance superiority compared to other models, which indicates that the proposed hierarchical architecture itself brings improvement over the previous ones. (ii) In the adversarial training stage, LeakGAN shows a better speed of convergence, and the local minimum it explores is significantly better than previous results. The results demonstrate the effectiveness of the information leakage framework and the hierarchical RL architecture for generating both short and long texts.\n",
      "\n",
      "## Long Text Generation: EMNLP2017 WMT News\n",
      "\n",
      "We choose the EMNLP2017 WMT 4 Dataset as the long text corpus. Specifically, we pick the News section from the original dataset. The news dataset consists of 646,459 words\n",
      "\n",
      "4 http://statmt.org/wmt17/translation-task.html\n",
      "\n",
      "Table 3: BLEU scores on COCO Image Captions.\n",
      "\n",
      "| Method   |   SeqGAN |   RankGAN |   LeakGAN | p -value   |\n",
      "|----------|----------|-----------|-----------|------------|\n",
      "| BLEU-2   |    0.831 |     0.85  |     0.95  | < 10 - 6   |\n",
      "| BLEU-3   |    0.642 |     0.672 |     0.88  | < 10 - 6   |\n",
      "| BLEU-4   |    0.521 |     0.557 |     0.778 | < 10 - 6   |\n",
      "| BLEU-5   |    0.427 |     0.544 |     0.686 | < 10 - 6   |\n",
      "\n",
      "Table 4: The BLEU performance on Chinese Poems.\n",
      "\n",
      "| Method   | SeqGAN   | RankGAN   | LeakGAN   |\n",
      "|----------|----------|-----------|-----------|\n",
      "| BLEU-2   | 0.738    | 0.812     | 0.881     |\n",
      "| p -value | < 10 - 6 | < 10 - 6  | -         |\n",
      "\n",
      "and 397,726 sentences. We preprocess the data by eliminating the words with frequency lower than 4,050 as well as the sentence containing these low frequency words. Besides, to focus on long sentences, we remove the sentences with length less than 20. After the preprocessing, the news dataset has 5,742 words and 397,726 sentences. Then we randomly sample 200,000 sentences as the training set and another 10,000 sentences as the test set. We use the BLEU-(2 to 5) scores (Papineni et al. 2002) as the evaluation metrics.\n",
      "\n",
      "The results are provided in Table 2. In all measured metrics, LeakGAN shows significant performance gain compared to baseline models. The consistently higher BLEU scores indicate that the generated sentences of LeakGAN are of high quality in local features to mimic the real text.\n",
      "\n",
      "## Middle Text Generation: COCO Image Captions\n",
      "\n",
      "Another real dataset we use is the COCO Image Captions Dataset (Chen et al. 2015), a dataset which contains groups of image-description pairs. We take the image captions as the text to generate. Note that the COCO Dataset is not a long text dataset, in which most sentences are of about 10 words. Thus we apply some preprocessing on the dataset. The COCO Image Captions training dataset consists of 20,734 words and 417,126 sentences. We remove the words with frequency lower than 10 as well as the sentence containing them. After the preprocessing, the dataset includes 4,980 words. We randomly sample 80,000 sentences for the training set, and another 5,000 for the test set.\n",
      "\n",
      "The results BLEU scores are provided in Table 3. The results of the BLEU scores on the COCO dataset indicate that LeakGAN performs significantly better than baseline models in mid-length text generation task.\n",
      "\n",
      "## Short Text Generation: Chinese Poems\n",
      "\n",
      "To evaluate the performance of LeakGAN in short text generation, we pick the dataset of Chinese poems which is proposed by (Zhang and Lapata 2014) and most related work such as (Yu et al. 2017; Rajeswar et al. 2017; Lin et al. 2017). The dataset consists of 4-line 5-character poems. Following the above work, we use the BLEU-2 scores as the evaluating metrics.\n",
      "\n",
      "The experimental results are provided in Table 4. The results on Chinese Poems indicate that LeakGAN successfully handles the short text generation tasks.\n",
      "\n",
      "Figure 3: The illustration of BLEU improvement change along with the generated text length on WMT News.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Table 5: Turing test results for in real-world experiments.\n",
      "\n",
      "| Dataset   |   SeqGAN |   LeakGAN |   Ground Truth | p -value   |\n",
      "|-----------|----------|-----------|----------------|------------|\n",
      "| WMTNews   |    0.236 |     0.554 |          0.651 | < 10 - 6   |\n",
      "| COCO      |    0.405 |     0.574 |          0.675 | < 10 - 6   |\n",
      "\n",
      "## Performance Robustness in Long Text Generation\n",
      "\n",
      "Long text generation has always been difficult among all text generation problems. The difficulty of the problem is due to many factors, such as LSTM-RNN's failure to capture longterm dependency, discriminator's failure to give those 'good but tiny' sequences appropriate penalty. To explicitly evaluate the superiority of LeakGAN in long text generation, here we use the relative performance gain of LeakGAN over SeqGAN (Yu et al. 2017) and RankGAN (Lin et al. 2017).\n",
      "\n",
      "The results over EMNLP2017 WMT News data are shown in Figure 3. The curves clearly show that LeakGAN yields larger performance gain over the baselines when the generated sentences are longer. This fact supports our claim that LeakGAN is a robust framework for long text.\n",
      "\n",
      "## Turing Test and Generated Samples\n",
      "\n",
      "Since BLEU score is a metric focusing on the local text statistics, which may not be sufficient for evaluating text generation quality, we also conduct a Turing test based on questionnaires on the Internet. In the questionnaire, each (machine generated or real) sentence gets +1 score when it is regarded as a real one, and 0 score otherwise. We conduct the test with text generated by the models trained on WMTNews and COCO Image Captions. The average score for each algorithm is calculated. In practice, we sample 20 sentences from every method and invite 62 people to participate the test, where everyone should judge the quality of 30 sentences from the compared three methods and thus each sentence is judged by 31 people. For the comparison fairness, the sentences used in the questionnaires are randomly sampled. Table 5 gives the results. The performance on two datasets indicates that the generated sentences of LeakGAN are of higher global consistency and better readability than those of SeqGAN.\n",
      "\n",
      "A few samples generated by LeakGAN are illustrated in Table 6. More samples and their comparison with those from\n",
      "\n",
      "Table 6: Samples from different methods on COCO Image Captions and EMNLP2017 WMT News.\n",
      "\n",
      "| Datasets            | LeakGAN                                                                                                                                                                                                                                           | SeqGAN                                                                                                                                                                                                                             |\n",
      "|---------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| COCO Image Captions | (1) A man sitting in front of a microphone with his dog sitting on his shoulder. (2) A young man is holding a bottle of wine in his hand.                                                                                                         | (1) A bathroom with tiled walls and a shower on it. (2) A couple of kids in front of a bathroom that is in a bathroom.                                                                                                             |\n",
      "| EMNLP2017WMT        | (1) The American Medical Association said that the militants had been arrested in connection with the murder of the same incident. (2) This is the first time that the Fed has been able to launch a probe into the country' s nu- clear program. | (1) 'I think you should really really leave for because we hadn't been busy, where it goes to one,' he wrote. (2) What you have to stop, if we do that, as late, law enforcement and where schools use a list of aid, it can rise. |\n",
      "\n",
      "Figure 4: Feature traces during the generation (SeqGAN, RankGAN and LeakGAN) and features of completed real data (all compressed to 2-dim by PCA) on WMT News.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "the baseline models are provided in the supplementary material. These samples are collected for the Turing test questionnaires.\n",
      "\n",
      "## Model Explanation\n",
      "\n",
      "Feature Trace. To verify that LeakGAN successfully exploits of the leaked message, we visualize the feature vector f T extracted from the real data by discriminator. Besides, we visualize the feature trace, i.e. the features f t of prefix s t during the generation, for LeakGAN, SeqGAN and RankGAN via a 2-D principal component analysis (PCA).\n",
      "\n",
      "The visualized traces are plotted in Figure 4 and more cases are presented in the supplementary material. As we can see, during the generation process, in LeakGAN, the feature vector gradually approaches the real data feature vector region. However, previous models, i.e. SeqGAN and RankGAN, fail to match the features even when the generation is completed. This indicates that the proposed LeakGAN does finish its design purpose of exploiting the leaked information from D φ to better match the feature vector distributions of real data.\n",
      "\n",
      "Behaviors of Worker and Manager. To give more details of how WORKER and MANAGER interact with each other and make use of the leaked information in the generative model, we visualize the interaction vector of the WORKER and MANAGER, i.e., the dimension-wise product of their output ( O t · w t as in Eq. (6)). Note that to simplify the explanation, here we reduce the signal dimension from 16 to\n",
      "\n",
      "Figure 5: Illustration of WORKER and MANAGER's behaviors during a generation. (Dimension-wise Product of Worker and Manager)\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "8. Figure 5 presents an example sentence and more cases are provided in the supplementary material.\n",
      "\n",
      "From Figure 5, we find some intuitive interpretations of the implicit rules learned by the interaction of WORKER and MANAGER. (i) The 5th dimension stands for current token's divergence from an entity token. If the 5th value is high, the token would most possibly be a structural token, such as a modal verb, an article or a preposition. (ii) The 6th dimension suggests how long the suffix from current step will be. If a peak occurs in the curve, there must be some token that triggers a long suffix. A frequently occurring example is the formal subject. (iii) Although hard to observe, we do find connections of the 7th dimension and the substructure of a sentence. For example, when the start or the end of a subsentence occurs, there is an observable fluctuation in the 7th dimension. This indicates that the token is most likely to be a punctuation or a conjuction.\n",
      "\n",
      "## Conclusion and Future work\n",
      "\n",
      "In this paper, we proposed a new algorithmic framework called LeakGAN for generating long text via adversarial training. By leaking the feature extracted by the discriminator as the step-by-step guiding signal to guide the generator better generating long text, LeakGAN addresses the non-informativeness and sparsity problems of the scalar reward signal in previous GAN solutions. In the extensive experiments with synthetic data and real world data including long, mid-length and short text, LeakGAN achieved significant performance improvement over previous solutions, on both BLEU scores and human ratings. Moreover, the analysis of the results shows that LeakGAN yields larger performance gain when the longer sentences are generated. Finally, we also visualize and explain the efficacy of the guiding signals that LeakGAN learns without any supervision.\n",
      "\n",
      "For future work, we plan to apply LeakGAN in more natural language process applications like dialogue systems and image captioning by providing more task-specific guiding information. Also, enhancing the capacity of the discriminator to check the global consistency of the whole sentence is a promising direction.\n",
      "\n",
      "## References\n",
      "\n",
      "Bachman, P., and Precup, D. 2015. Data generation as sequential decision making. In NIPS .\n",
      "\n",
      "Bahdanau, D.; Cho, K.; and Bengio, Y. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 .\n",
      "\n",
      "Bengio, S.; Vinyals, O.; Jaitly, N.; and Shazeer, N. 2015. Scheduled sampling for sequence prediction with recurrent neural networks. In NIPS .\n",
      "\n",
      "Che, T.; Li, Y.; Zhang, R.; Hjelm, R. D.; Li, W.; Song, Y.; and Bengio, Y. 2017. Maximum-likelihood augmented discrete generative adversarial networks. arXiv preprint arXiv:1702.07983 .\n",
      "\n",
      "Chen, X.; Fang, H.; Lin, T.-Y.; Vedantam, R.; Gupta, S.; Doll´ ar, P.; and Zitnick, C. L. 2015. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 .\n",
      "\n",
      "Dethlefs, N., and Cuay´ ahuitl, H. 2010. Hierarchical reinforcement learning for adaptive text generation. In Proceedings of the 6th International Natural Language Generation Conference .\n",
      "\n",
      "Fang, H.; Gupta, S.; Iandola, F.; Srivastava, R. K.; Deng, L.; Doll´ ar, P.; Gao, J.; He, X.; Mitchell, M.; Platt, J. C.; et al. 2015. From captions to visual concepts and back. In CVPR .\n",
      "\n",
      "Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y. 2014. Generative adversarial nets. In NIPS .\n",
      "\n",
      "Graves, A. 2013. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 .\n",
      "\n",
      "Hochreiter, S., and Schmidhuber, J. 1997. Long short-term memory. Neural computation .\n",
      "\n",
      "Hu, Z.; Yang, Z.; Liang, X.; Salakhutdinov, R.; and Xing, E. P. 2017. Controllable text generation. arXiv preprint arXiv:1703.00955 .\n",
      "\n",
      "Husz´ ar, F. 2015. How (not) to train your generative model: Scheduled sampling, likelihood, adversary? arXiv preprint arXiv:1511.05101 .\n",
      "\n",
      "Kulkarni, T. D.; Narasimhan, K.; Saeedi, A.; and Tenenbaum, J. 2016. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In NIPS .\n",
      "\n",
      "Li, J.; Monroe, W.; Shi, T.; Ritter, A.; and Jurafsky, D. 2017. Adversarial learning for neural dialogue generation. arXiv preprint arXiv:1701.06547 .\n",
      "\n",
      "Lin, K.; Li, D.; He, X.; Zhang, Z.; and Sun, M.-T. 2017. Adversarial ranking for language generation. arXiv preprint arXiv:1705.11001 .\n",
      "\n",
      "Mauldin, M. L. 1984. Semantic rule based text generation. In Proceedings of the 10th International Conference on Computational Linguistics and 22nd annual meeting on Association for Computational Linguistics .\n",
      "\n",
      "Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: a method for automatic evaluation of machine translation. In ACL .\n",
      "\n",
      "Peng, B.; Li, X.; Li, L.; Gao, J.; Celikyilmaz, A.; Lee, S.; and Wong, K.-F. 2017. Composite task-completion dialogue system via hierarchical deep reinforcement learning. arXiv preprint arXiv:1704.03084 .\n",
      "\n",
      "Rajeswar, S.; Subramanian, S.; Dutil, F.; Pal, C.; and Courville, A. 2017. Adversarial generation of natural language. arXiv preprint arXiv:1705.10929 .\n",
      "\n",
      "Srivastava, N.; Hinton, G. E.; Krizhevsky, A.; Sutskever, I.; and Salakhutdinov, R. 2014. Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning research .\n",
      "\n",
      "Sutton, R. S.; McAllester, D. A.; Singh, S. P.; Mansour, Y.; et al. 1999. Policy gradient methods for reinforcement learning with function approximation. In NIPS .\n",
      "\n",
      "Sutton, R. S.; McAllester, D. A.; Singh, S. P.; and Mansour, Y. 2000. Policy gradient methods for reinforcement learning with function approximation. In NIPS .\n",
      "\n",
      "Sutton, R. S.; Precup, D.; and Singh, S. 1999. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence .\n",
      "\n",
      "Vezhnevets, A. S.; Osindero, S.; Schaul, T.; Heess, N.; Jaderberg, M.; Silver, D.; and Kavukcuoglu, K. 2017. Feudal networks for hierarchical reinforcement learning. arXiv preprint arXiv:1703.01161 .\n",
      "\n",
      "Vinyals, O.; Toshev, A.; Bengio, S.; and Erhan, D. 2015. Show and tell: A neural image caption generator. In CVPR .\n",
      "\n",
      "Williams, R. J. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning .\n",
      "\n",
      "Yang, Z.; Chen, W.; Wang, F.; and Xu, B. 2017. Improving neural machine translation with conditional sequence generative adversarial nets. arXiv preprint arXiv:1703.04887 .\n",
      "\n",
      "Yu, L.; Zhang, W.; Wang, J.; and Yu, Y. 2017. Seqgan: Sequence generative adversarial nets with policy gradient. In AAAI .\n",
      "\n",
      "Zhang, X., and Lapata, M. 2014. Chinese poetry generation with recurrent neural networks. In EMNLP .\n",
      "\n",
      "Zhang, X., and LeCun, Y. 2015. Text understanding from scratch. arXiv preprint arXiv:1502.01710 .\n",
      "\n",
      "Zhang, Y.; Gan, Z.; Fan, K.; Chen, Z.; Henao, R.; Shen, D.; and Carin, L. 2017. Adversarial feature matching for text generation. arXiv preprint arXiv:1706.03850 .\n",
      "\n",
      "## Formulas for Reference\n",
      "\n",
      "## Discriminator\n",
      "\n",
      "## MANAGER of Generator\n",
      "\n",
      "## WORKER of Generator\n",
      "\n",
      "## Pseudo Code\n",
      "\n",
      "## Appendix\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "## Algorithm 1 Adversarial Training with Leaked Information\n",
      "\n",
      "Require: Hierachical policy G θ m ,θ w ; discriminator D φ ; a sequence dataset S = { X 1: T }\n",
      "\n",
      "- 2: Pre-train D φ (i.e. the feature extractor F ( · ; φ f ) and the output layer sigmoid ( φ l , · ) ) using S as positive samples and output from G θ m ,θ w as negative samples.\n",
      "- 1: Initialize G θ m ,θ w , D φ with random weights θ m , θ w , φ .\n",
      "- 3: Pre-train G θ m ,θ w using leaked information from D φ\n",
      "- 5: repeat\n",
      "- 4: Perform the two parts of pre-training interleavingly until convergence.\n",
      "- 6: for g-steps do\n",
      "- 8: for t in 1 : T do\n",
      "\n",
      "7:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "9:\n",
      "\n",
      "Store leaked information f t from D φ\n",
      "\n",
      "11:\n",
      "\n",
      "10:\n",
      "\n",
      "Get Q( f t , g t ) by Monte Carlo Search via Eq. (8)\n",
      "\n",
      "12:\n",
      "\n",
      "Get the computed direction g t from MANAGER\n",
      "\n",
      "13:\n",
      "\n",
      "Update WORKER parameters θ w , ψ, softmax via Eq. (10)\n",
      "\n",
      "- 14: end for\n",
      "\n",
      "Update MANAGER parameters θ m via Eq. (9)\n",
      "\n",
      "- 15: end for\n",
      "- 17: Use current G θ m ,θ w to generate negative examples and combine with given positive examples S\n",
      "- 16: for d-steps do\n",
      "- 18: Train discriminator D φ for k epochs by Eq. (2)\n",
      "- 20: until LeakGAN converges\n",
      "- 19: end for\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "## Discussions\n",
      "\n",
      "The Necessity of the Hierarchical Architecture The hierarchical architecture in LeakGAN serves as the mechanism of incorporating leaked information from D into G . However, in the body part, we haven't shown whether the explotation of hierachical architecture is a must. Actually, what we have to point out is, the explotation of hierarchical reinforment learning is not a must, but a good choice in sequence decision scenarios.\n",
      "\n",
      "We attempt to replace the hierarchical architecture by a fully connected layer. However, the model is so numerically sensitive that we cannot operate a stable training on it the original training settings. A possible reason is that, since the feature space of CNN changes rapidly during the training procedure, linear transformation without any normalization may not be able to incorporate the information contained in the feature vector leaked from D .\n",
      "\n",
      "Figure 1: The feature extractor's architecture (without the highway and dropout layer)\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Experiment Settings\n",
      "\n",
      "For synthetic data with length 20, the learning rate for MANAGER and WORKER is set to 0.001. The goal dimension size k is set to 16. The embedding size of the LSTM-RNNs is set to 32. For the discriminative model, we set the hyperparameters of the CNN as Table 1\n",
      "\n",
      "For synthetic data with length 40, the learning rate for MANAGER and WORKER is set to 0.0005. The goal dimension size k is set to 16. The embedding size of the LSTM-RNNs is set to 32. For the discriminative model, we set the hyperparameters of the CNN as Table 1\n",
      "\n",
      "Table 1: Convolutional layer structures.\n",
      "\n",
      "|   Sequence length | (window size, kernel numbers)                                                                                                     |\n",
      "|-------------------|-----------------------------------------------------------------------------------------------------------------------------------|\n",
      "|                20 | (1, 100),(2, 200),(3, 200),(4, 200),(5, 200) (6, 100),(7, 100),(8, 100),(9, 100),(10, 100) (15, 160),(20, 160)                    |\n",
      "|                40 | (1, 100),(2, 200),(3, 200),(4, 200),(5, 200) (6, 100),(7, 100),(8, 100),(9, 100),(10, 100) (16, 160),(20, 160),(30, 160),(40,160) |\n",
      "\n",
      "## Illustration of WORKER and MANAGER's Behaviors\n",
      "\n",
      "Figure 2: Illustration of WORKER and MANAGER's behaviors during a generation. (Dimension-wise Product of Worker and Manager)\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Here we present more examples for illustrating the interaction of WORKER and MANAGER to support our claims in the main text as below. Each curve shows a subscore of the token of that time step. Each dimension of the score, i.e. each subscore measures a specific feature of the token in that context.\n",
      "\n",
      "- (i) The 5th dimension stands for current token's divergence from an entity token. If the 5th value is high, the token would most possibly be a structural token, such as a modal verb, an article or a preposition.\n",
      "- (ii) The 6th dimension suggests how long the suffix from current step will be. If a peak occurs in the curve, there must be some token that triggers a long suffix. A frequently occurring example is the formal subject.\n",
      "- (iii) Although hard to observe, we do find connections of the 7th dimension and the substructure of a sentence. For example, when the start or end of a sub-sentence occurs, there is an observable fluctuation in the 7th dimension. This indicates that the token is most likely to be a punctuation or a conjuction.\n",
      "\n",
      "## Illustration of Feature Trace\n",
      "\n",
      "Figure 3: Feature traces (SeqGAN, RankGAN and LeakGAN) and features of real data (all compressed to 2-dim by PCA) on WMTNews.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "As we can see, during the generation process, in LeakGAN, the feature vector gradually approaches the real data feature vector region. However, previous models, i.e. SeqGAN and RankGAN, fail to match the features even when the generation is completed. This indicates that the proposed LeakGAN does finish its designed purpose of exploiting the leaked information from D φ to better match the feature vector distributions of real data.\n",
      "\n",
      "Table 2: Appendix 1 - COCO Examples in the Questionaire\n",
      "\n",
      "| Sources   | Example                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|-----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| Real data | A blue and white bathroom with butterfly themed wall tiles. The vanity contains two sinks with a towel for each. Several metal balls sit in the sand near a group of people. A surfer, a woman, and a child walk on the beach. A kitchen with a countertop that includes an Apple phone. A closeup of a red fire hydrant including the chains. People standing around many silver round balls on the ground. A person on a bicycle is riding in front of a car. A kitchen with a tile floor has cabinets with no doors, a dishwasher, a sink, and a refrigerator. The top of a kitchen cabinet covered with brass pots and pans. A woman is shaving her face while sitting on a wooden bench. A stuffed animal is laying on the bed by a window. A wooden toilet seat sits open in an empty bathroom. A person is taking a photo of a cat in a car. A phone lies on the counter in a modern kitchen. silver balls laying on the ground around a smaller red ball. A man riding a bicycle on a road carrying a surf board. A man using his bicycle to go down a street. A set table with silverware, glasses and a bottle of wine.                                                                                                                                                    |\n",
      "| LeakGAN   | A large kite in the shape of the bottom half of a woman. A woman holding an umbrella while standing against a sidewalk. A bathroom with a toilet and sink and mirror. A train rides along the tracks in a train yard. A man with a racket stands in front of a shop window. A red and white photo of a train station. The bathroom is clean and ready for us to use . A man is walking with his dog on the boardwalk by the beach. A man in a shirt and tie standing next to a woman. A couple of luggage cart filled with bags on a shelf. Large white and clean bathroom with white tile floors and white walls . A group of people fly kites in the sky on a clear day. A man wearing a suit and coat holds a tie through and wood pants. Two men are working on a laptop in a room . A man who is standing next to a brown and white horse. A street sign with a red stop sign on the street pole. A cat is laying on a keyboard and mouse in the air. A man with a rainbow - colored shirt and a black dog. A crowd of people standing around or standing on a sidewalk. A man is sitting on his desk holding an umbrella.                                                                                                                                                      |\n",
      "| SeqGAN    | A woman is riding a bike on the street next to a bus. A silver stove, the refrigerator, sitting in a kitchen. A guy doing tricks on a skateboard while a man is standing on a cellphone. A bunch of birds that are sitting in the sand. A bathroom with tiled walls and a shower on it. A couple of people are riding bikes down an asphalt road. An old photo of a man riding on a motorcycle with some people. A beautiful young girl in the bathroom has one has wine glasses and bottles above the counters. A person in a helmet standing next to a red street. An empty clean bathroom with a toilet and sink and tub. A kid in a black shirt and dog arms in a restaurant kitchen. A bathroom has a toilet, a sink and mirror. Two bicycles are parked outside inside a small brown field. The large rug is on the city under the city. A bathroom that is has a picture above and a sink. A small child jumping with glasses to a motor scooter. A white bathroom with a toilet, television and bathtub and a sink. A baby in a blue dress standing in front of a Frisbee. A cat and a woman standing by two computer preparing food. A pair of skis and pedestrians in a parking area near some different go. Two bikes in a parking lot with a dog that has a back on her. |\n",
      "\n",
      "Table 3: Appendix 2 - News Examples in the Questionaire\n",
      "\n",
      "| Sources   | Example                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|-----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| Real data | Out of those who came last year, 69 per cent were men, 18 per cent were children and just 13 per cent were women. ' Sometimes I think about leaving sex work, but because I am alone living costs are really expensive,' she said. ' I was then stuck in the house for nearly two years only going out for short periods of time,' she said. He has not played for Tottenham's first team since and it is now nearly two years since he completed a full Premier League match for the club. This is a part of the population that is notorious for its lack of interest in actually showing up when the political process takes place. I was paid far too little to pick up a dead off of the ground and put it back in the box. Local media reported the group were not looking to hurt anybody, but they would not rule out violence if police tried to remove them. The 55 to 43 vote was largely split down party lines and fell short of the 60 votes needed for the bill to advance. We got to a bus station in the evening, but our connection didn't leave until the following morning. It's actually something that I had to add, because I was getting really frustrated losing to my hitting partner all the time. Taiwan's Defence Ministry said it was 'aware of the information,' and declined further immediate comment, Reuters reported. Her response to the international refugee crisis gave a million refugees hope that they may be able to begin a new life. I'm racing against a guy who I lost a medal to - but am I ever going to get that medal back ?                                                                                                                                                                                                                                                                             |\n",
      "| LeakGAN   | A man has been arrested at age 28 , a resident in Seattle , which was widely reported in 2007 . I also think that ' s a good place for us , I ' msure that this would be a good opportunity for me to get in touch . What is the biggest problem for Clinton is that Donald Trump will be in the race and he ' s unlikely to be the nominee . ' We ' re going to do and we ' re going to put it out and get the ball ,' he said . ' I would be afraid to blame the girls to go back but I was just disappointed with the race,' he said. ' I'm not going to work together with a different role and we can win the game,' he added. The couple's lives are still missing and they have been killed in the city's way to play against them, and because I came out there. For the last three years, we've got a lot of things that we need to do with this is based on the financial markets. Don't ask me, but I know, if I' ll be able to be out of Hillary Clinton, I think it's being made for the Congress. ' I am proud to be able to move forward because we don't have to look at about,' he said. That ' s why we ' re the most important people for the African American community and we ' ve made a good response . But the move will be only in a fight against them, as well as likely to prevent an agreement to remain in the EU. The American Medical Association said that the militants had been arrested in connection with the murder of the same incident. The two - year - old girl has been charged with a suspect who was in the vehicle to the police station. It is hard to buy on the Olympics, but we probably don't see a lot of it. ' I'm not going to be very proud of the other countries,' he said. He said the U. N. intelligence industry will not comment on the ground, which would be sensitive to the European Union. |\n",
      "| SeqGAN    | You only certainly might not rush it down for those circumstances where we are when they were the heads, and when she's name. ' I think you should really really leave for because we hadn't been busy, where it goes to one,' he wrote. All the study knew was that they are, so they continue to provide support service and it doesn't exist. ' It can say become up with nothing sales have reached the charge for the other any evidence that been virtually well below the $ 800. Three times before the start of the season is much early on 2015 we are in the third training every year. That's the idea of strength that decision they said, we haven't already lost four or seven, or Liverpool's team. That is not the time for the cost of changing the system and it was pushing for $ 20 million. We had to take it a good day for a military, but nearly 6, 000 ] and prepare for them through. I actually didn't tell the background check the difference after my hour was to be recalled... and it was great. We are thinking about 40, 000 and jobs in what is wrong in the coming and you know. That is out how working you can't set out some pretty tight... or what I'm going through. ' I wanted to be made you decided to have a crisis that way up and get some sort of weapon, not much to give birth to for an American room. She had been fined almost 200, 000 with couple of asylum seekers in Syria and Iraq. Perhaps not, in looking for, housing officials would help the frustration of Government, with an FBI shortly before 2020. Once we got to real show for the young man since I'm sure she went to love it just, whether to be late later last year. But, after a holiday period we might have to go on a total - out debate like that could have happened to us.                                                |\n",
      "Document 16:\n",
      "## BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "\n",
      "Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\n",
      "\n",
      "Google AI Language\n",
      "\n",
      "{ jacobdevlin,mingweichang,kentonl,kristout } @google.com\n",
      "\n",
      "## Abstract\n",
      "\n",
      "We introduce a new language representation model called BERT , which stands for B idirectional E ncoder R epresentations from T ransformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.\n",
      "\n",
      "BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).\n",
      "\n",
      "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning . The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.\n",
      "\n",
      "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017). Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.\n",
      "\n",
      "In this paper, we improve the fine-tuning based approaches by proposing BERT: B idirectional E ncoder R epresentations from T ransformers. BERT alleviates the previously mentioned unidirectionality constraint by using a 'masked language model' (MLM) pre-training objective, inspired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a 'next sentence prediction' task that jointly pretrains text-pair representations. The contributions of our paper are as follows:\n",
      "\n",
      "- We demonstrate the importance of bidirectional pre-training for language representations. Unlike Radford et al. (2018), which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.\n",
      "- Weshow that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.\n",
      "- BERT advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at https://github.com/ google-research/bert .\n",
      "\n",
      "## 2 Related Work\n",
      "\n",
      "There is a long history of pre-training general language representations, and we briefly review the most widely-used approaches in this section.\n",
      "\n",
      "## 2.1 Unsupervised Feature-based Approaches\n",
      "\n",
      "Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods. Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch (Turian et al., 2010). To pretrain word embedding vectors, left-to-right language modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to discriminate correct from incorrect words in left and right context (Mikolov et al., 2013).\n",
      "\n",
      "These approaches have been generalized to coarser granularities, such as sentence embeddings (Kiros et al., 2015; Logeswaran and Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014). To train sentence representations, prior work has used objectives to rank candidate next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), left-to-right generation of next sentence words given a representation of the previous sentence (Kiros et al., 2015), or denoising autoencoder derived objectives (Hill et al., 2016).\n",
      "\n",
      "ELMo and its predecessor (Peters et al., 2017, 2018a) generalize traditional word embedding research along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks (Peters et al., 2018a) including question answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al. (2016) proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. Fedus et al. (2018) shows that the cloze task can be used to improve the robustness of text generation models.\n",
      "\n",
      "## 2.2 Unsupervised Fine-tuning Approaches\n",
      "\n",
      "As with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text (Collobert and Weston, 2008).\n",
      "\n",
      "More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved previously state-of-the-art results on many sentencelevel tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language model-\n",
      "\n",
      "Figure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architectures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating questions/answers).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "ing and auto-encoder objectives have been used for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).\n",
      "\n",
      "## 2.3 Transfer Learning from Supervised Data\n",
      "\n",
      "There has also been work showing effective transfer from supervised tasks with large datasets, such as natural language inference (Conneau et al., 2017) and machine translation (McCann et al., 2017). Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models, where an effective recipe is to fine-tune models pre-trained with ImageNet (Deng et al., 2009; Yosinski et al., 2014).\n",
      "\n",
      "## 3 BERT\n",
      "\n",
      "We introduce BERT and its detailed implementation in this section. There are two steps in our framework: pre-training and fine-tuning . During pre-training, the model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.\n",
      "\n",
      "A distinctive feature of BERT is its unified architecture across different tasks. There is mini- mal difference between the pre-trained architecture and the final downstream architecture.\n",
      "\n",
      "Model Architecture BERT's model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the tensor2tensor library. 1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as 'The Annotated Transformer.' 2\n",
      "\n",
      "In this work, we denote the number of layers (i.e., Transformer blocks) as L , the hidden size as H , and the number of self-attention heads as A . 3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).\n",
      "\n",
      "BERTBASE was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left. 4\n",
      "\n",
      "1 https://github.com/tensorflow/tensor2tensor\n",
      "\n",
      "2 http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
      "\n",
      "3\n",
      "\n",
      "In all cases we set the feed-forward/filter size to be 4 H , i.e., 3072 for the H = 768 and 4096 for the H = 1024 .\n",
      "\n",
      "4 We note that in the literature the bidirectional Trans-\n",
      "\n",
      "Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., 〈 Question, Answer 〉 ) in one token sequence. Throughout this work, a 'sentence' can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A 'sequence' refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.\n",
      "\n",
      "We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ( [CLS] ). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ( [SEP] ). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B . As shown in Figure 1, we denote input embedding as E , the final hidden vector of the special [CLS] token as C ∈ R H , and the final hidden vector for the i th input token as T i ∈ R H .\n",
      "\n",
      "For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2.\n",
      "\n",
      "## 3.1 Pre-training BERT\n",
      "\n",
      "Unlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure 1.\n",
      "\n",
      "Task #1: Masked LM Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-toright and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly 'see itself', and the model could trivially predict the target word in a multi-layered context.\n",
      "\n",
      "former is often referred to as a 'Transformer encoder' while the left-context-only version is referred to as a 'Transformer decoder' since it can be used for text generation.\n",
      "\n",
      "In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a 'masked LM' (MLM), although it is often referred to as a Cloze task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than reconstructing the entire input.\n",
      "\n",
      "Although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning. To mitigate this, we do not always replace 'masked' words with the actual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i -th token is chosen, we replace the i -th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i -th token 10% of the time. Then, T i will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2.\n",
      "\n",
      "Task #2: Next Sentence Prediction (NSP) Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext ), and 50% of the time it is a random sentence from the corpus (labeled as NotNext ). As we show in Figure 1, C is used for next sentence prediction (NSP). 5 Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI. 6\n",
      "\n",
      "5 The final model achieves 97%-98% accuracy on NSP.\n",
      "\n",
      "6 The vector C is not a meaningful sentence representation without fine-tuning, since it was trained with NSP.\n",
      "\n",
      "Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "The NSP task is closely related to representationlearning objectives used in Jernite et al. (2017) and Logeswaran and Lee (2018). However, in prior work, only sentence embeddings are transferred to down-stream tasks, where BERT transfers all parameters to initialize end-task model parameters.\n",
      "\n",
      "Pre-training data The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences.\n",
      "\n",
      "## 3.2 Fine-tuning BERT\n",
      "\n",
      "Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as Parikh et al. (2016); Seo et al. (2017). BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.\n",
      "\n",
      "For each task, we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end. At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and\n",
      "\n",
      "(4) a degenerate text-∅ pair in text classification or sequence tagging. At the output, the token representations are fed into an output layer for tokenlevel tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis.\n",
      "\n",
      "Compared to pre-training, fine-tuning is relatively inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model. 7 We describe the task-specific details in the corresponding subsections of Section 4. More details can be found in Appendix A.5.\n",
      "\n",
      "## 4 Experiments\n",
      "\n",
      "In this section, we present BERT fine-tuning results on 11 NLP tasks.\n",
      "\n",
      "## 4.1 GLUE\n",
      "\n",
      "The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a) is a collection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1.\n",
      "\n",
      "To fine-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the final hidden vector C ∈ R H corresponding to the first input token ( [CLS] ) as the aggregate representation. The only new parameters introduced during fine-tuning are classification layer weights W ∈ R K × H , where K is the number of labels. We compute a standard classification loss with C and W , i.e., log(softmax( CW T )) .\n",
      "\n",
      "7 For example, the BERT SQuAD model can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%.\n",
      "\n",
      "8 See (10) in https://gluebenchmark.com/faq .\n",
      "\n",
      "Table 1: GLUE Test results, scored by the evaluation server ( https://gluebenchmark.com/leaderboard ). The number below each task denotes the number of training examples. The 'Average' column is slightly different than the official GLUE score, since we exclude the problematic WNLI set. 8 BERT and OpenAI GPT are singlemodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\n",
      "\n",
      "| System           | MNLI-(m/mm) 392k   |   QQP 363k |   QNLI 108k |   SST-2 67k |   CoLA 8.5k |   STS-B 5.7k |   MRPC 3.5k |   RTE 2.5k |   Average - |\n",
      "|------------------|--------------------|------------|-------------|-------------|-------------|--------------|-------------|------------|-------------|\n",
      "| Pre-OpenAI SOTA  | 80.6/80.1          |       66.1 |        82.3 |        93.2 |        35   |         81   |        86   |       61.7 |        74   |\n",
      "| BiLSTM+ELMo+Attn | 76.4/76.1          |       64.8 |        79.8 |        90.4 |        36   |         73.3 |        84.9 |       56.8 |        71   |\n",
      "| OpenAI GPT       | 82.1/81.4          |       70.3 |        87.4 |        91.3 |        45.4 |         80   |        82.3 |       56   |        75.1 |\n",
      "| BERT BASE        | 84.6/83.4          |       71.2 |        90.5 |        93.5 |        52.1 |         85.8 |        88.9 |       66.4 |        79.6 |\n",
      "| BERT LARGE       | 86.7/85.9          |       72.1 |        92.7 |        94.9 |        60.5 |         86.5 |        89.3 |       70.1 |        82.1 |\n",
      "\n",
      "We use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERTLARGE we found that finetuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but perform different fine-tuning data shuffling and classifier layer initialization. 9\n",
      "\n",
      "Results are presented in Table 1. Both BERTBASE and BERTLARGE outperform all systems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy improvement over the prior state of the art. Note that BERTBASE and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the official GLUEleaderboard 10 , BERTLARGE obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing.\n",
      "\n",
      "We find that BERTLARGE significantly outperforms BERTBASE across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2.\n",
      "\n",
      "## 4.2 SQuAD v1.1\n",
      "\n",
      "The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowdsourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from\n",
      "\n",
      "9 The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE.\n",
      "\n",
      "10 https://gluebenchmark.com/leaderboard\n",
      "\n",
      "Wikipedia containing the answer, the task is to predict the answer text span in the passage.\n",
      "\n",
      "As shown in Figure 1, in the question answering task, we represent the input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding. We only introduce a start vector S ∈ R H and an end vector E ∈ R H during fine-tuning. The probability of word i being the start of the answer span is computed as a dot product between T i and S followed by a softmax over all of the words in the paragraph: P i = e S · T i ∑ j e S · T j . The analogous formula is used for the end of the answer span. The score of a candidate span from position i to position j is defined as S · T i + E · T j , and the maximum scoring span where j ≥ i is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.\n",
      "\n",
      "Table 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available, 11 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) befor fine-tuning on SQuAD.\n",
      "\n",
      "Our best performing system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system. In fact, our single BERT model outperforms the top ensemble system in terms of F1 score. Without TriviaQA fine-\n",
      "\n",
      "11 QANet is described in Yu et al. (2018), but the system has improved substantially after publication.\n",
      "\n",
      "Table 2: SQuAD 1.1 results. The BERT ensemble is 7x systems which use different pre-training checkpoints and fine-tuning seeds.\n",
      "\n",
      "| System                                   | Dev                                      | Dev                                      | Test                                     | Test                                     |\n",
      "|------------------------------------------|------------------------------------------|------------------------------------------|------------------------------------------|------------------------------------------|\n",
      "|                                          | EM                                       | F1                                       | EM                                       | F1                                       |\n",
      "| Top Leaderboard Systems (Dec 10th, 2018) | Top Leaderboard Systems (Dec 10th, 2018) | Top Leaderboard Systems (Dec 10th, 2018) | Top Leaderboard Systems (Dec 10th, 2018) | Top Leaderboard Systems (Dec 10th, 2018) |\n",
      "| Human                                    | -                                        | -                                        | 82.3                                     | 91.2                                     |\n",
      "| #1 Ensemble - nlnet                      | -                                        | -                                        | 86.0                                     | 91.7                                     |\n",
      "| #2 Ensemble - QANet                      | -                                        | -                                        | 84.5                                     | 90.5                                     |\n",
      "| Published                                | Published                                | Published                                | Published                                | Published                                |\n",
      "| BiDAF+ELMo (Single)                      | -                                        | 85.6                                     | -                                        | 85.8                                     |\n",
      "| R.M. Reader (Ensemble)                   | 81.2                                     | 87.9                                     | 82.3                                     | 88.5                                     |\n",
      "| Ours                                     | Ours                                     | Ours                                     | Ours                                     | Ours                                     |\n",
      "| BERT BASE (Single)                       | 80.8                                     | 88.5                                     | -                                        | -                                        |\n",
      "| BERT LARGE (Single)                      | 84.1                                     | 90.9                                     | -                                        | -                                        |\n",
      "| BERT LARGE (Ensemble)                    | 85.8                                     | 91.8                                     | -                                        | -                                        |\n",
      "| BERT LARGE (Sgl.+TriviaQA)               | 84.2                                     | 91.1                                     | 85.1                                     | 91.8                                     |\n",
      "| BERT LARGE (Ens.+TriviaQA)               | 86.2                                     | 92.2                                     | 87.4                                     | 93.2                                     |\n",
      "\n",
      "Table 3: SQuAD 2.0 results. We exclude entries that use BERT as one of their components.\n",
      "\n",
      "| System              | System            | Dev       | Dev       | Test      | Test      |\n",
      "|---------------------|-------------------|-----------|-----------|-----------|-----------|\n",
      "|                     |                   | EM        | F1        | EM        | F1        |\n",
      "| Top Leaderboard     | Systems           | (Dec      | 10th,     | 2018)     |           |\n",
      "| Human               |                   | 86.3      | 89.0      | 86.9      | 89.5      |\n",
      "| #1 Single           | - MIR-MRC (F-Net) | -         | -         | 74.8      | 78.0      |\n",
      "| #2 Single -         | nlnet             | -         | -         | 74.2      | 77.1      |\n",
      "| Published           | Published         | Published | Published | Published | Published |\n",
      "| unet (Ensemble)     |                   | -         | -         | 71.4      | 74.9      |\n",
      "| SLQA+ (Single)      |                   | -         |           | 71.4      | 74.4      |\n",
      "| BERT LARGE (Single) | Ours              | 78.7      | 81.9      | 80.0      | 83.1      |\n",
      "\n",
      "tuning data, we only lose 0.1-0.4 F1, still outperforming all existing systems by a wide margin. 12\n",
      "\n",
      "## 4.3 SQuAD v2.0\n",
      "\n",
      "The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph, making the problem more realistic.\n",
      "\n",
      "Weuse a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat questions that do not have an answer as having an answer span with start and end at the [CLS] token. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, we compare the score of the no-answer span: s null = S · C + E · C to the score of the best non-null span\n",
      "\n",
      "12 The TriviaQA data we used consists of paragraphs from TriviaQA-Wiki formed of the first 400 tokens in documents, that contain at least one of the provided possible answers.\n",
      "\n",
      "Table 4: SWAG Dev and Test accuracies. † Human performance is measured with 100 samples, as reported in the SWAG paper.\n",
      "\n",
      "| System                  | Dev   | Test   |\n",
      "|-------------------------|-------|--------|\n",
      "| ESIM+GloVe              | 51.9  | 52.7   |\n",
      "| ESIM+ELMo               | 59.1  | 59.2   |\n",
      "| OpenAI GPT              | -     | 78.0   |\n",
      "| BERT BASE               | 81.6  | -      |\n",
      "| BERT LARGE              | 86.6  | 86.3   |\n",
      "| Human (expert) †        | -     | 85.0   |\n",
      "| Human (5 annotations) † | -     | 88.0   |\n",
      "\n",
      "ˆ s i,j = max j ≥ i S · T i + E · T j . We predict a non-null answer when ˆ s i,j &gt; s null + τ , where the threshold τ is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.\n",
      "\n",
      "The results compared to prior leaderboard entries and top published work (Sun et al., 2018; Wang et al., 2018b) are shown in Table 3, excluding systems that use BERT as one of their components. We observe a +5.1 F1 improvement over the previous best system.\n",
      "\n",
      "## 4.4 SWAG\n",
      "\n",
      "The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference (Zellers et al., 2018). Given a sentence, the task is to choose the most plausible continuation among four choices.\n",
      "\n",
      "When fine-tuning on the SWAG dataset, we construct four input sequences, each containing the concatenation of the given sentence (sentence A ) and a possible continuation (sentence B ). The only task-specific parameters introduced is a vector whose dot product with the [CLS] token representation C denotes a score for each choice which is normalized with a softmax layer.\n",
      "\n",
      "We fine-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16. Results are presented in Table 4. BERTLARGE outperforms the authors' baseline ESIM+ELMo system by +27.1% and OpenAI GPT by 8.3%.\n",
      "\n",
      "## 5 Ablation Studies\n",
      "\n",
      "In this section, we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance. Additional\n",
      "\n",
      "Table 5: Ablation over the pre-training tasks using the BERTBASE architecture. 'No NSP' is trained without the next sentence prediction task. 'LTR &amp; No NSP' is trained as a left-to-right LM without the next sentence prediction, like OpenAI GPT. '+ BiLSTM' adds a randomly initialized BiLSTM on top of the 'LTR + No NSP' model during fine-tuning.\n",
      "\n",
      "|             | Dev Set      | Dev Set    | Dev Set    | Dev Set     | Dev Set    |\n",
      "|-------------|--------------|------------|------------|-------------|------------|\n",
      "| Tasks       | MNLI-m (Acc) | QNLI (Acc) | MRPC (Acc) | SST-2 (Acc) | SQuAD (F1) |\n",
      "| BERT BASE   | 84.4         | 88.4       | 86.7       | 92.7        | 88.5       |\n",
      "| No NSP      | 83.9         | 84.9       | 86.5       | 92.6        | 87.9       |\n",
      "| LTR &No NSP | 82.1         | 84.3       | 77.5       | 92.1        | 77.8       |\n",
      "| + BiLSTM    | 82.1         | 84.1       | 75.7       | 91.6        | 84.9       |\n",
      "\n",
      "ablation studies can be found in Appendix C.\n",
      "\n",
      "## 5.1 Effect of Pre-training Tasks\n",
      "\n",
      "We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data, fine-tuning scheme, and hyperparameters as BERTBASE:\n",
      "\n",
      "No NSP : A bidirectional model which is trained using the 'masked LM' (MLM) but without the 'next sentence prediction' (NSP) task.\n",
      "\n",
      "LTR&amp;NoNSP : A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input representation, and our fine-tuning scheme.\n",
      "\n",
      "Wefirst examine the impact brought by the NSP task. In Table 5, we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by comparing 'No NSP' to 'LTR &amp; No NSP'. The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.\n",
      "\n",
      "For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no rightside context. In order to make a good faith attempt at strengthening the LTR system, we added a randomly initialized BiLSTM on top. This does significantly improve results on SQuAD, but the results are still far worse than those of the pretrained bidirectional models. The BiLSTM hurts performance on the GLUE tasks.\n",
      "\n",
      "We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models, as ELMo does. However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer.\n",
      "\n",
      "## 5.2 Effect of Model Size\n",
      "\n",
      "In this section, we explore the effect of model size on fine-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training procedure as described previously.\n",
      "\n",
      "Results on selected GLUE tasks are shown in Table 6. In this table, we report the average Dev Set accuracy from 5 random restarts of fine-tuning. We can see that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks. It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature. For example, the largest Transformer explored in Vaswani et al. (2017) is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters (Al-Rfou et al., 2018). By contrast, BERTBASE contains 110M parameters and BERTLARGE contains 340M parameters.\n",
      "\n",
      "It has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6. However, we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained. Peters et al. (2018b) presented mixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to four layers and Melamud et al. (2016) mentioned in passing that increasing hidden dimension size from 200 to 600 helped, but increasing further to 1,000 did not bring further improvements. Both of these prior works used a featurebased approach - we hypothesize that when the model is fine-tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters, the taskspecific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small.\n",
      "\n",
      "## 5.3 Feature-based Approach with BERT\n",
      "\n",
      "All of the BERT results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pretrained model, has certain advantages. First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.\n",
      "\n",
      "In this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we formulate this as a tagging task but do not use a CRF\n",
      "\n",
      "Table 6: Ablation over BERT model size. #L = the number of layers; #H = hidden size; #A = number of attention heads. 'LM (ppl)' is the masked LM perplexity of held-out training data.\n",
      "\n",
      "| Hyperparams   | Hyperparams   | Hyperparams   | Dev Set Accuracy   | Dev Set Accuracy   | Dev Set Accuracy   | Dev Set Accuracy   |\n",
      "|---------------|---------------|---------------|--------------------|--------------------|--------------------|--------------------|\n",
      "| #L            | #H            | #A            | LM (ppl)           | MNLI-m             | MRPC               | SST-2              |\n",
      "| 3             | 768           | 12            | 5.84               | 77.9               | 79.8               | 88.4               |\n",
      "| 6             | 768           | 3             | 5.24               | 80.6               | 82.2               | 90.7               |\n",
      "| 6             | 768           | 12            | 4.68               | 81.9               | 84.8               | 91.3               |\n",
      "| 12            | 768           | 12            | 3.99               | 84.4               | 86.7               | 92.9               |\n",
      "| 12            | 1024          | 16            | 3.54               | 85.7               | 86.9               | 93.3               |\n",
      "| 24            | 1024          | 16            | 3.23               | 86.6               | 87.8               | 93.7               |\n",
      "\n",
      "Table 7: CoNLL-2003 Named Entity Recognition results. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5 random restarts using those hyperparameters.\n",
      "\n",
      "| System                              | Dev F1   | Test F1   |\n",
      "|-------------------------------------|----------|-----------|\n",
      "| ELMo (Peters et al., 2018a)         | 95.7     | 92.2      |\n",
      "| CVT (Clark et al., 2018)            | -        | 92.6      |\n",
      "| CSE (Akbik et al., 2018)            | -        | 93.1      |\n",
      "| Fine-tuning approach                |          |           |\n",
      "| BERT LARGE                          | 96.6     | 92.8      |\n",
      "| BERT BASE                           | 96.4     | 92.4      |\n",
      "| Feature-based approach (BERT BASE ) |          |           |\n",
      "| Embeddings                          | 91.0     | -         |\n",
      "| Second-to-Last Hidden               | 95.6     | -         |\n",
      "| Last Hidden                         | 94.9     | -         |\n",
      "| Weighted Sum Last Four Hidden       | 95.9     | -         |\n",
      "| Concat Last Four Hidden             | 96.1     | -         |\n",
      "| Weighted Sum All 12 Layers          | 95.5     | -         |\n",
      "\n",
      "layer in the output. We use the representation of the first sub-token as the input to the token-level classifier over the NER label set.\n",
      "\n",
      "To ablate the fine-tuning approach, we apply the feature-based approach by extracting the activations from one or more layers without fine-tuning any parameters of BERT. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer.\n",
      "\n",
      "Results are presented in Table 7. BERTLARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both finetuning and feature-based approaches.\n",
      "\n",
      "## 6 Conclusion\n",
      "\n",
      "Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks.\n",
      "\n",
      "## References\n",
      "\n",
      "- Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In Proceedings of the 27th International Conference on Computational Linguistics , pages 1638-1649.\n",
      "- Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018. Character-level language modeling with deeper self-attention. arXiv preprint arXiv:1808.04444 .\n",
      "- Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research , 6(Nov):1817-1853.\n",
      "- Luisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. 2009. The fifth PASCAL recognizing textual entailment challenge. In TAC . NIST.\n",
      "- John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 conference on empirical methods in natural language processing , pages 120-128. Association for Computational Linguistics.\n",
      "- Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In EMNLP . Association for Computational Linguistics.\n",
      "- Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics , 18(4):467-479.\n",
      "- Daniel Cer, Mona Diab, Eneko Agirre, Inigo LopezGazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017) , pages 1-14, Vancouver, Canada. Association for Computational Linguistics.\n",
      "- Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2013. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005 .\n",
      "- Z. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018. Quora question pairs.\n",
      "- Christopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehension. In ACL .\n",
      "- Kevin Clark, Minh-Thang Luong, Christopher D Manning, and Quoc Le. 2018. Semi-supervised sequence modeling with cross-view training. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 19141925.\n",
      "- Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning , pages 160-167. ACM.\n",
      "- Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ ıc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 670-680, Copenhagen, Denmark. Association for Computational Linguistics.\n",
      "- Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In Advances in neural information processing systems , pages 3079-3087.\n",
      "- J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09 .\n",
      "- William B Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005) .\n",
      "- William Fedus, Ian Goodfellow, and Andrew M Dai. 2018. Maskgan: Better text generation via filling in the . arXiv preprint arXiv:1801.07736 .\n",
      "- Dan Hendrycks and Kevin Gimpel. 2016. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. CoRR , abs/1606.08415.\n",
      "- Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational Linguistics.\n",
      "- Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In ACL . Association for Computational Linguistics.\n",
      "- Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. 2018. Reinforced mnemonic reader for machine reading comprehension. In IJCAI .\n",
      "- Yacine Jernite, Samuel R. Bowman, and David Sontag. 2017. Discourse-based objectives for fast unsupervised sentence representation learning. CoRR , abs/1705.00557.\n",
      "- Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL .\n",
      "- Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in neural information processing systems , pages 3294-3302.\n",
      "- Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In International Conference on Machine Learning , pages 1188-1196.\n",
      "- Hector J Levesque, Ernest Davis, and Leora Morgenstern. 2011. The winograd schema challenge. In Aaai spring symposium: Logical formalizations of commonsense reasoning , volume 46, page 47.\n",
      "- Lajanugen Logeswaran and Honglak Lee. 2018. An efficient framework for learning sentence representations. In International Conference on Learning Representations .\n",
      "- Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Contextualized word vectors. In NIPS .\n",
      "- Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context embedding with bidirectional LSTM. In CoNLL .\n",
      "- Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26 , pages 3111-3119. Curran Associates, Inc.\n",
      "- Andriy Mnih and Geoffrey E Hinton. 2009. A scalable hierarchical distributed language model. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21 , pages 1081-1088. Curran Associates, Inc.\n",
      "- Ankur P Parikh, Oscar T¨ ackstr¨ om, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In EMNLP .\n",
      "- Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP) , pages 15321543.\n",
      "- Matthew Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL .\n",
      "- Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word representations. In NAACL .\n",
      "- Matthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018b. Dissecting contextual word embeddings: Architecture and representation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 1499-1509.\n",
      "- Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding with unsupervised learning. Technical report, OpenAI.\n",
      "- Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 2383-2392.\n",
      "- Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In ICLR .\n",
      "- Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing , pages 1631-1642.\n",
      "- Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu. 2018. U-net: Machine reading comprehension with unanswerable questions. arXiv preprint arXiv:1810.06638 .\n",
      "- Wilson L Taylor. 1953. Cloze procedure: A new tool for measuring readability. Journalism Bulletin , 30(4):415-433.\n",
      "- Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In CoNLL .\n",
      "- Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics , ACL '10, pages 384-394.\n",
      "- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems , pages 6000-6010.\n",
      "- Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning , pages 1096-1103. ACM.\n",
      "- Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018a. Glue: A multi-task benchmark and analysis platform\n",
      "\n",
      "for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 353-355.\n",
      "\n",
      "Wei Wang, Ming Yan, and Chen Wu. 2018b. Multigranularity hierarchical attention fusion networks for reading comprehension and question answering. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics.\n",
      "\n",
      "- Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2018. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471 .\n",
      "- Adina Williams, Nikita Nangia, and Samuel R Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL .\n",
      "- Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144 .\n",
      "- Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks? In Advances in neural information processing systems , pages 3320-3328.\n",
      "- Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. QANet: Combining local convolution with global self-attention for reading comprehension. In ICLR .\n",
      "- Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP) .\n",
      "- Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision , pages 19-27.\n",
      "\n",
      "## Appendix for 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'\n",
      "\n",
      "We organize the appendix into three sections:\n",
      "\n",
      "- Additional implementation details for BERT are presented in Appendix A;\n",
      "- Additional details for our experiments are presented in Appendix B; and\n",
      "- Additional ablation studies are presented in Appendix C.\n",
      "\n",
      "We present additional ablation studies for BERT including:\n",
      "\n",
      "- -Effect of Number of Training Steps; and\n",
      "- -Ablation for Different Masking Procedures.\n",
      "\n",
      "## A Additional Details for BERT\n",
      "\n",
      "## A.1 Illustration of the Pre-training Tasks\n",
      "\n",
      "We provide examples of the pre-training tasks in the following.\n",
      "\n",
      "Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy , and during the random masking procedure we chose the 4-th token (which corresponding to hairy ), our masking procedure can be further illustrated by\n",
      "\n",
      "- 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy → my dog is [MASK]\n",
      "- 10% of the time: Replace the word with a random word, e.g., my dog is hairy → my dog is apple\n",
      "- 10% of the time: Keep the word unchanged, e.g., my dog is hairy → my dog is hairy . The purpose of this is to bias the representation towards the actual observed word.\n",
      "\n",
      "The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words, so it is forced to keep a distributional contextual representation of every input token. Additionally, because random replacement only occurs for 1.5% of all tokens (i.e., 10% of 15%), this does not seem to harm the model's language understanding capability. In Section C.2, we evaluate the impact this procedure.\n",
      "\n",
      "Compared to standard langauge model training, the masked LM only make predictions on 15% of tokens in each batch, which suggests that more pre-training steps may be required for the model\n",
      "\n",
      "Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-toleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly conditioned on both left and right context in all layers. In addition to the architecture differences, BERT and OpenAI GPT are fine-tuning approaches, while ELMo is a feature-based approach.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "to converge. In Section C.1 we demonstrate that MLMdoesconverge marginally slower than a leftto-right model (which predicts every token), but the empirical improvements of the MLM model far outweigh the increased training cost.\n",
      "\n",
      "Next Sentence Prediction The next sentence prediction task can be illustrated in the following examples.\n",
      "\n",
      "```\n",
      "Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP] Label = IsNext Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP] Label = NotNext\n",
      "```\n",
      "\n",
      "## A.2 Pre-training Procedure\n",
      "\n",
      "To generate each training input sequence, we sample two spans of text from the corpus, which we refer to as 'sentences' even though they are typically much longer than single sentences (but can be shorter also). The first sentence receives the A embedding and the second receives the B embedding. 50% of the time B is the actual next sentence that follows A and 50% of the time it is a random sentence, which is done for the 'next sentence prediction' task. They are sampled such that the combined length is ≤ 512 tokens. The LM masking is applied after WordPiece tokenization with a uniform masking rate of 15%, and no special consideration given to partial word pieces.\n",
      "\n",
      "We train with batch size of 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40\n",
      "\n",
      "epochs over the 3.3 billion word corpus. We use Adam with learning rate of 1e-4, β 1 = 0 . 9 , β 2 = 0 . 999 , L2 weight decay of 0 . 01 , learning rate warmup over the first 10,000 steps, and linear decay of the learning rate. We use a dropout probability of 0.1 on all layers. We use a gelu activation (Hendrycks and Gimpel, 2016) rather than the standard relu , following OpenAI GPT. The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.\n",
      "\n",
      "Training of BERTBASE was performed on 4 Cloud TPUs in Pod configuration (16 TPU chips total). 13 Training of BERTLARGE was performed on 16 Cloud TPUs (64 TPU chips total). Each pretraining took 4 days to complete.\n",
      "\n",
      "Longer sequences are disproportionately expensive because attention is quadratic to the sequence length. To speed up pretraing in our experiments, we pre-train the model with sequence length of 128 for 90% of the steps. Then, we train the rest 10% of the steps of sequence of 512 to learn the positional embeddings.\n",
      "\n",
      "## A.3 Fine-tuning Procedure\n",
      "\n",
      "For fine-tuning, most model hyperparameters are the same as in pre-training, with the exception of the batch size, learning rate, and number of training epochs. The dropout probability was always kept at 0.1. The optimal hyperparameter values are task-specific, but we found the following range of possible values to work well across all tasks:\n",
      "\n",
      "- Batch size : 16, 32\n",
      "\n",
      "13 https://cloudplatform.googleblog.com/2018/06/CloudTPU-now-offers-preemptible-pricing-and-globalavailability.html\n",
      "\n",
      "- Learning rate (Adam) : 5e-5, 3e-5, 2e-5\n",
      "\n",
      "- Number of epochs : 2, 3, 4\n",
      "\n",
      "We also observed that large data sets (e.g., 100k+ labeled training examples) were far less sensitive to hyperparameter choice than small data sets. Fine-tuning is typically very fast, so it is reasonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set.\n",
      "\n",
      "## A.4 Comparison of BERT, ELMo ,and OpenAI GPT\n",
      "\n",
      "Here we studies the differences in recent popular representation learning models including ELMo, OpenAI GPT and BERT. The comparisons between the model architectures are shown visually in Figure 3. Note that in addition to the architecture differences, BERT and OpenAI GPT are finetuning approaches, while ELMo is a feature-based approach.\n",
      "\n",
      "The most comparable existing pre-training method to BERT is OpenAI GPT, which trains a left-to-right Transformer LM on a large text corpus. In fact, many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared. The core argument of this work is that the bi-directionality and the two pretraining tasks presented in Section 3.1 account for the majority of the empirical improvements, but we do note that there are several other differences between how BERT and GPT were trained:\n",
      "\n",
      "- GPT is trained on the BooksCorpus (800M words); BERT is trained on the BooksCorpus (800M words) and Wikipedia (2,500M words).\n",
      "- GPT uses a sentence separator ( [SEP] ) and classifier token ( [CLS] ) which are only introduced at fine-tuning time; BERT learns [SEP] , [CLS] and sentence A / B embeddings during pre-training.\n",
      "- GPT was trained for 1M steps with a batch size of 32,000 words; BERT was trained for 1M steps with a batch size of 128,000 words.\n",
      "- GPT used the same learning rate of 5e-5 for all fine-tuning experiments; BERT chooses a task-specific fine-tuning learning rate which performs the best on the development set.\n",
      "\n",
      "To isolate the effect of these differences, we perform ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable.\n",
      "\n",
      "## A.5 Illustrations of Fine-tuning on Different Tasks\n",
      "\n",
      "The illustration of fine-tuning BERT on different tasks can be seen in Figure 4. Our task-specific models are formed by incorporating BERT with one additional output layer, so a minimal number of parameters need to be learned from scratch. Among the tasks, (a) and (b) are sequence-level tasks while (c) and (d) are token-level tasks. In the figure, E represents the input embedding, T i represents the contextual representation of token i , [CLS] is the special symbol for classification output, and [SEP] is the special symbol to separate non-consecutive token sequences.\n",
      "\n",
      "## B Detailed Experimental Setup\n",
      "\n",
      "## B.1 Detailed Descriptions for the GLUE Benchmark Experiments.\n",
      "\n",
      "Our GLUE results in Table1 are obtained from https://gluebenchmark.com/ leaderboard and https://blog. openai.com/language-unsupervised .\n",
      "\n",
      "The GLUE benchmark includes the following datasets, the descriptions of which were originally summarized in Wang et al. (2018a):\n",
      "\n",
      "MNLI Multi-Genre Natural Language Inference is a large-scale, crowdsourced entailment classification task (Williams et al., 2018). Given a pair of sentences, the goal is to predict whether the second sentence is an entailment , contradiction , or neutral with respect to the first one.\n",
      "\n",
      "QQP Quora Question Pairs is a binary classification task where the goal is to determine if two questions asked on Quora are semantically equivalent (Chen et al., 2018).\n",
      "\n",
      "QNLI Question Natural Language Inference is a version of the Stanford Question Answering Dataset (Rajpurkar et al., 2016) which has been converted to a binary classification task (Wang et al., 2018a). The positive examples are (question, sentence) pairs which do contain the correct answer, and the negative examples are (question, sentence) from the same paragraph which do not contain the answer.\n",
      "\n",
      "Figure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "SST-2 The Stanford Sentiment Treebank is a binary single-sentence classification task consisting of sentences extracted from movie reviews with human annotations of their sentiment (Socher et al., 2013).\n",
      "\n",
      "CoLA The Corpus of Linguistic Acceptability is a binary single-sentence classification task, where the goal is to predict whether an English sentence is linguistically 'acceptable' or not (Warstadt et al., 2018).\n",
      "\n",
      "STS-B The Semantic Textual Similarity Benchmark is a collection of sentence pairs drawn from news headlines and other sources (Cer et al., 2017). They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning.\n",
      "\n",
      "MRPC Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent (Dolan and Brockett, 2005).\n",
      "\n",
      "RTE Recognizing Textual Entailment is a binary entailment task similar to MNLI, but with much less training data (Bentivogli et al., 2009). 14\n",
      "\n",
      "WNLI Winograd NLI is a small natural language inference dataset (Levesque et al., 2011). The GLUE webpage notes that there are issues with the construction of this dataset, 15 and every trained system that's been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class. We therefore exclude this set to be fair to OpenAI GPT. For our GLUE submission, we always predicted the ma-\n",
      "\n",
      "14 Note that we only report single-task fine-tuning results in this paper. A multitask fine-tuning approach could potentially push the performance even further. For example, we did observe substantial improvements on RTE from multitask training with MNLI.\n",
      "\n",
      "15 https://gluebenchmark.com/faq\n",
      "\n",
      "## C Additional Ablation Studies\n",
      "\n",
      "## C.1 Effect of Number of Training Steps\n",
      "\n",
      "Figure 5 presents MNLI Dev accuracy after finetuning from a checkpoint that has been pre-trained for k steps. This allows us to answer the following questions:\n",
      "\n",
      "1. Question: Does BERT really need such a large amount of pre-training (128,000 words/batch * 1,000,000 steps) to achieve high fine-tuning accuracy?\n",
      "\n",
      "Answer: Yes, BERTBASE achieves almost 1.0% additional accuracy on MNLI when trained on 1M steps compared to 500k steps.\n",
      "\n",
      "2. Question: Does MLM pre-training converge slower than LTR pre-training, since only 15% of words are predicted in each batch rather than every word?\n",
      "\n",
      "Answer: The MLM model does converge slightly slower than the LTR model. However, in terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately.\n",
      "\n",
      "## C.2 Ablation for Different Masking Procedures\n",
      "\n",
      "In Section 3.1, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective. The following is an ablation study to evaluate the effect of different masking strategies.\n",
      "\n",
      "Figure 5: Ablation over number of training steps. This shows the MNLI accuracy after fine-tuning, starting from model parameters that have been pre-trained for k steps. The x-axis is the value of k .\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and fine-tuning, as the [MASK] symbol never appears during the fine-tuning stage. We report the Dev results for both MNLI and NER. For NER, we report both fine-tuning and feature-based approaches, as we expect the mismatch will be amplified for the feature-based approach as the model will not have the chance to adjust the representations.\n",
      "\n",
      "Table 8: Ablation over different masking strategies.\n",
      "\n",
      "| Masking Rates   | Masking Rates   | Masking Rates   | Dev Set Results   | Dev Set Results   | Dev Set Results   |\n",
      "|-----------------|-----------------|-----------------|-------------------|-------------------|-------------------|\n",
      "| MASK            | SAME            | RND             | MNLI              | NER               | NER               |\n",
      "|                 |                 |                 | Fine-tune         | Fine-tune         | Feature-based     |\n",
      "| 80%             | 10%             | 10%             | 84.2              | 95.4              | 94.9              |\n",
      "| 100%            | 0%              | 0%              | 84.3              | 94.9              | 94.0              |\n",
      "| 80%             | 0%              | 20%             | 84.1              | 95.2              | 94.6              |\n",
      "| 80%             | 20%             | 0%              | 84.4              | 95.2              | 94.7              |\n",
      "| 0%              | 20%             | 80%             | 83.7              | 94.8              | 94.6              |\n",
      "| 0%              | 0%              | 100%            | 83.6              | 94.9              | 94.6              |\n",
      "\n",
      "The results are presented in Table 8. In the table, MASK means that we replace the target token with the [MASK] symbol for MLM; SAME means that we keep the target token as is; RND means that we replace the target token with another random token.\n",
      "\n",
      "The numbers in the left part of the table represent the probabilities of the specific strategies used during MLM pre-training (BERT uses 80%, 10%, 10%). The right part of the paper represents the Dev set results. For the feature-based approach, we concatenate the last 4 layers of BERT as the features, which was shown to be the best approach in Section 5.3.\n",
      "\n",
      "From the table it can be seen that fine-tuning is surprisingly robust to different masking strategies. However, as expected, using only the MASK strategy was problematic when applying the featurebased approach to NER. Interestingly, using only the RND strategy performs much worse than our strategy as well.\n",
      "Document 17:\n",
      "## Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n",
      "\n",
      "Sergey Ioffe\n",
      "\n",
      "Google Inc., sioffe@google.com\n",
      "\n",
      "Christian Szegedy\n",
      "\n",
      "Google Inc., szegedy@google.com\n",
      "\n",
      "Using mini-batches of examples, as opposed to one example at a time, is helpful in several ways. First, the gradient of the loss over a mini-batch is an estimate of the gradient over the training set, whose quality improves as the batch size increases. Second, computation over a batch can be much more efficient than m computations for individual examples, due to the parallelism afforded by the modern computing platforms.\n",
      "\n",
      "While stochastic gradient is simple and effective, it requires careful tuning of the model hyper-parameters, specifically the learning rate used in optimization, as well as the initial values for the model parameters. The training is complicated by the fact that the inputs to each layer are affected by the parameters of all preceding layers - so that small changes to the network parameters amplify as the network becomes deeper.\n",
      "\n",
      "The change in the distributions of layers' inputs presents a problem because the layers need to continuously adapt to the new distribution. When the input distribution to a learning system changes, it is said to experience covariate shift (Shimodaira, 2000). This is typically handled via domain adaptation (Jiang, 2008). However, the notion of covariate shift can be extended beyond the learning system as a whole, to apply to its parts, such as a sub-network or a layer. Consider a network computing\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where F 1 and F 2 are arbitrary transformations, and the parameters Θ 1 , Θ 2 are to be learned so as to minimize the loss /lscript . Learning Θ 2 can be viewed as if the inputs x = F 1 (u , Θ 1 ) are fed into the sub-network\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "For example, a gradient descent step\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "(for batch size m and learning rate α ) is exactly equivalent to that for a stand-alone network F 2 with input x . Therefore, the input distribution properties that make training more efficient - such as having the same distribution between the training and test data - apply to training the sub-network as well. As such it is advantageous for the distribution of x to remain fixed over time. Then, Θ 2 does\n",
      "\n",
      "## Abstract\n",
      "\n",
      "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift , and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch . Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batchnormalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Deep learning has dramatically advanced the state of the art in vision, speech, and many other areas. Stochastic gradient descent (SGD) has proved to be an effective way of training deep networks, and SGD variants such as momentum (Sutskever et al., 2013) and Adagrad (Duchi et al., 2011) have been used to achieve state of the art performance. SGD optimizes the parameters Θ of the network, so as to minimize the loss\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where x 1 ...N is the training data set. With SGD, the training proceeds in steps, and at each step we consider a minibatch x 1 ...m of size m . The mini-batch is used to approximate the gradient of the loss function with respect to the parameters, by computing\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "not have to readjust to compensate for the change in the distribution of x .\n",
      "\n",
      "Fixed distribution of inputs to a sub-network would have positive consequences for the layers outside the subnetwork, as well. Consider a layer with a sigmoid activation function z = g ( W u + b) where u is the layer input, the weight matrix W and bias vector b are the layer parameters to be learned, and g ( x ) = 1 1+exp( -x ) . As | x | increases, g ′ ( x ) tends to zero. This means that for all dimensions of x = W u+b except those with small absolute values, the gradient flowing down to u will vanish and the model will train slowly. However, since x is affected by W, b and the parameters of all the layers below, changes to those parameters during training will likely move many dimensions of x into the saturated regime of the nonlinearity and slow down the convergence. This effect is amplified as the network depth increases. In practice, the saturation problem and the resulting vanishing gradients are usually addressed by using Rectified Linear Units (Nair &amp; Hinton, 2010) ReLU ( x ) = max( x, 0) , careful initialization (Bengio &amp; Glorot, 2010; Saxe et al., 2013), and small learning rates. If, however, we could ensure that the distribution of nonlinearity inputs remains more stable as the network trains, then the optimizer would be less likely to get stuck in the saturated regime, and the training would accelerate.\n",
      "\n",
      "We refer to the change in the distributions of internal nodes of a deep network, in the course of training, as Internal Covariate Shift . Eliminating it offers a promise of faster training. We propose a new mechanism, which we call Batch Normalization , that takes a step towards reducing internal covariate shift, and in doing so dramatically accelerates the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows us to use much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for Dropout (Srivastava et al., 2014). Finally, Batch Normalization makes it possible to use saturating nonlinearities by preventing the network from getting stuck in the saturated modes.\n",
      "\n",
      "In Sec. 4.2, we apply Batch Normalization to the bestperforming ImageNet classification network, and show that we can match its performance using only 7% of the training steps, and can further exceed its accuracy by a substantial margin. Using an ensemble of such networks trained with Batch Normalization, we achieve the top-5 error rate that improves upon the best known results on ImageNet classification.\n",
      "\n",
      "## 2 Towards Reducing Internal Covariate Shift\n",
      "\n",
      "We define Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training. To improve the training, we seek to reduce the internal covariate shift. By fixing the distribution of the layer inputs x as the training progresses, we expect to improve the training speed. It has been long known (LeCun et al., 1998b; Wiesler &amp; Ney, 2011) that the network training converges faster if its inputs are whitened - i.e., linearly transformed to have zero means and unit variances, and decorrelated. As each layer observes the inputs produced by the layers below, it would be advantageous to achieve the same whitening of the inputs of each layer. By whitening the inputs to each layer, we would take a step towards achieving the fixed distributions of inputs that would remove the ill effects of the internal covariate shift.\n",
      "\n",
      "We could consider whitening activations at every training step or at some interval, either by modifying the network directly or by changing the parameters of the optimization algorithm to depend on the network activation values (Wiesler et al., 2014; Raiko et al., 2012; Povey et al., 2014; Desjardins &amp; Kavukcuoglu). However, if these modifications are interspersed with the optimization steps, then the gradient descent step may attempt to update the parameters in a way that requires the normalization to be updated, which reduces the effect of the gradient step. For example, consider a layer with the input u that adds the learned bias b , and normalizes the result by subtracting the mean of the activation computed over the training data: ̂ x = x -E [ x ] where x = u + b , X = { x 1 ...N } is the set of values of x over the training set, and E [ x ] = 1 N ∑ N i =1 x i . If a gradient descent step ignores the dependence of E [ x ] on b , then it will update b ← b + ∆ b , where ∆ b ∝ -∂/lscript/∂ ̂ x . Then u + ( b + ∆ b ) -E [ u + ( b + ∆ b )] = u + b -E [ u + b ] . Thus, the combination of the update to b and subsequent change in normalization led to no change in the output of the layer nor, consequently, the loss. As the training continues, b will grow indefinitely while the loss remains fixed. This problem can get worse if the normalization not only centers but also scales the activations. We have observed this empirically in initial experiments, where the model blows up when the normalization parameters are computed outside the gradient descent step.\n",
      "\n",
      "The issue with the above approach is that the gradient descent optimization does not take into account the fact that the normalization takes place. To address this issue, we would like to ensure that, for any parameter values, the network always produces activations with the desired distribution. Doing so would allow the gradient of the loss with respect to the model parameters to account for the normalization, and for its dependence on the model parameters Θ . Let again x be a layer input, treated as a\n",
      "\n",
      "vector, and X be the set of these inputs over the training data set. The normalization can then be written as a transformation\n",
      "\n",
      "̂ x = Norm (x , X ) which depends not only on the given training example x but on all examples X - each of which depends on Θ if x is generated by another layer. For backpropagation, we would need to compute the Jacobians\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "ignoring the latter term would lead to the explosion described above. Within this framework, whitening the layer inputs is expensive, as it requires computing the covariance matrix Cov [x] = E x ∈X [xx T ] -E [x] E [x] T and its inverse square root, to produce the whitened activations Cov [x] -1 / 2 (x -E [x]) , as well as the derivatives of these transforms for backpropagation. This motivates us to seek an alternative that performs input normalization in a way that is differentiable and does not require the analysis of the entire training set after every parameter update.\n",
      "\n",
      "Some of the previous approaches (e.g. (Lyu &amp; Simoncelli, 2008)) use statistics computed over a single training example, or, in the case of image networks, over different feature maps at a given location. However, this changes the representation ability of a network by discarding the absolute scale of activations. We want to a preserve the information in the network, by normalizing the activations in a training example relative to the statistics of the entire training data.\n",
      "\n",
      "## 3 Normalization via Mini-Batch Statistics\n",
      "\n",
      "Since the full whitening of each layer's inputs is costly and not everywhere differentiable, we make two necessary simplifications. The first is that instead of whitening the features in layer inputs and outputs jointly, we will normalize each scalar feature independently, by making it have the mean of zero and the variance of 1. For a layer with d -dimensional input x = ( x (1) . . . x ( d ) ) , we will normalize each dimension\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where the expectation and variance are computed over the training data set. As shown in (LeCun et al., 1998b), such normalization speeds up convergence, even when the features are not decorrelated.\n",
      "\n",
      "Note that simply normalizing each input of a layer may change what the layer can represent. For instance, normalizing the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity. To address this, we make sure that the transformation inserted in the network can represent the identity transform . To accomplish this, weintroduce, for each activation x ( k ) , a pair of parameters γ ( k ) , β ( k ) , which scale and shift the normalized value:\n",
      "\n",
      "y ( k ) = γ ( k ) ̂ x ( k ) + β ( k ) . These parameters are learned along with the original model parameters, and restore the representation power of the network. Indeed, by setting γ ( k ) = √ Var [ x ( k ) ] and β ( k ) = E [ x ( k ) ] , we could recover the original activations, if that were the optimal thing to do.\n",
      "\n",
      "In the batch setting where each training step is based on the entire training set, we would use the whole set to normalize activations. However, this is impractical when using stochastic optimization. Therefore, we make the second simplification: since we use mini-batches in stochastic gradient training, each mini-batch produces estimates of the mean and variance of each activation. This way, the statistics used for normalization can fully participate in the gradient backpropagation. Note that the use of minibatches is enabled by computation of per-dimension variances rather than joint covariances; in the joint case, regularization would be required since the mini-batch size is likely to be smaller than the number of activations being whitened, resulting in singular covariance matrices.\n",
      "\n",
      "Consider a mini-batch B of size m . Since the normalization is applied to each activation independently, let us focus on a particular activation x ( k ) and omit k for clarity. We have m values of this activation in the mini-batch,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Let the normalized values be ̂ x 1 ...m , and their linear transformations be y 1 ...m . We refer to the transform\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "as the Batch Normalizing Transform . We present the BN Transform in Algorithm 1. In the algorithm, /epsilon1 is a constant added to the mini-batch variance for numerical stability.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Algorithm 1: Batch Normalizing Transform, applied to activation x over a mini-batch.\n",
      "\n",
      "The BN transform can be added to a network to manipulate any activation. In the notation y = BN γ,β ( x ) , we\n",
      "\n",
      "indicate that the parameters γ and β are to be learned, but it should be noted that the BN transform does not independently process the activation in each training example. Rather, BN γ,β ( x ) depends both on the training example and the other examples in the mini-batch . The scaled and shifted values y are passed to other network layers. The normalized activations ̂ x are internal to our transformation, but their presence is crucial. The distributions of values of any ̂ x has the expected value of 0 and the variance of 1 , as long as the elements of each mini-batch are sampled from the same distribution, and if we neglect /epsilon1 . This can be seen by observing that ∑ m i =1 ̂ x i = 0 and 1 m ∑ m i =1 ̂ x 2 i = 1 , and taking expectations. Each normalized activation ̂ x ( k ) can be viewed as an input to a sub-network composed of the linear transform y ( k ) = γ ( k ) ̂ x ( k ) + β ( k ) , followed by the other processing done by the original network. These sub-network inputs all have fixed means and variances, and although the joint distribution of these normalized ̂ x ( k ) can change over the course of training, we expect that the introduction of normalized inputs accelerates the training of the sub-network and, consequently, the network as a whole.\n",
      "\n",
      "During training we need to backpropagate the gradient of loss /lscript through this transformation, as well as compute the gradients with respect to the parameters of the BN transform. We use chain rule, as follows (before simplification):\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Thus, BN transform is a differentiable transformation that introduces normalized activations into the network. This ensures that as the model is training, layers can continue learning on input distributions that exhibit less internal covariate shift, thus accelerating the training. Furthermore, the learned affine transform applied to these normalized activations allows the BN transform to represent the identity transformation and preserves the network capacity.\n",
      "\n",
      "## 3.1 Training and Inference with BatchNormalized Networks\n",
      "\n",
      "To Batch-Normalize a network, we specify a subset of activations and insert the BN transform for each of them, according to Alg. 1. Any layer that previously received x as the input, now receives BN ( x ) . A model employing Batch Normalization can be trained using batch gradient descent, or Stochastic Gradient Descent with a mini-batch size m &gt; 1 , or with any of its variants such as Adagrad\n",
      "\n",
      "(Duchi et al., 2011). The normalization of activations that depends on the mini-batch allows efficient training, but is neither necessary nor desirable during inference; we want the output to depend only on the input, deterministically. For this, once the network has been trained, we use the normalization using the population, rather than mini-batch, statistics. Neglecting /epsilon1 , these normalized activations have the same mean 0 and variance 1 as during training. We use the unbiased variance estimate Var [ x ] = m m -1 · E B [ σ 2 B ] , where the expectation is over training mini-batches of size m and σ 2 B are their sample variances. Using moving averages instead, we can track the accuracy of a model as it trains. Since the means and variances are fixed during inference, the normalization is simply a linear transform applied to each activation. It may further be composed with the scaling by γ and shift by β , to yield a single linear transform that replaces BN ( x ) . Algorithm 2 summarizes the procedure for training batch-normalized networks.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Input: Network N with trainable parameters Θ ; subset of activations { x ( k ) } K k =1\n",
      "\n",
      "tr\n",
      "\n",
      "Output: Batch-normalized network for inference, N inf BN\n",
      "\n",
      "// Training BN network\n",
      "\n",
      "- 1: N BN ← N 2: for k = 1 . . . K do\n",
      "- 3: Add transformation y ( k ) = BN γ ( k ) ,β ( k ) ( x ( k ) ) to N tr BN (Alg. 1)\n",
      "- 4: Modify each layer in N tr BN with input x ( k ) to take y ( k ) instead\n",
      "- 5: end for\n",
      "- 6: Train N tr BN to optimize the parameters Θ ∪ { γ ( k ) , β ( k ) } K k =1\n",
      "- 7: N inf BN ← N tr BN // Inference BN network with frozen // parameters\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "- 10: Process multiple training mini-batches B , each of size m , and average over them:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Algorithm 2: Training a Batch-Normalized Network\n",
      "\n",
      "## 3.2 Batch-Normalized Convolutional Networks\n",
      "\n",
      "Batch Normalization can be applied to any set of activations in the network. Here, we focus on transforms\n",
      "\n",
      "that consist of an affine transformation followed by an element-wise nonlinearity:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where W and b are learned parameters of the model, and g ( · ) is the nonlinearity such as sigmoid or ReLU. This formulation covers both fully-connected and convolutional layers. We add the BN transform immediately before the nonlinearity, by normalizing x = W u+b . We could have also normalized the layer inputs u , but since u is likely the output of another nonlinearity, the shape of its distribution is likely to change during training, and constraining its first and second moments would not eliminate the covariate shift. In contrast, W u + b is more likely to have a symmetric, non-sparse distribution, that is 'more Gaussian' (Hyv¨ arinen &amp; Oja, 2000); normalizing it is likely to produce activations with a stable distribution.\n",
      "\n",
      "Note that, since we normalize W u+b , the bias b can be ignored since its effect will be canceled by the subsequent mean subtraction (the role of the bias is subsumed by β in Alg. 1). Thus, z = g ( W u + b) is replaced with\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where the BN transform is applied independently to each dimension of x = W u , with a separate pair of learned parameters γ ( k ) , β ( k ) per dimension.\n",
      "\n",
      "For convolutional layers, we additionally want the normalization to obey the convolutional property - so that different elements of the same feature map, at different locations, are normalized in the same way. To achieve this, we jointly normalize all the activations in a minibatch, over all locations. In Alg. 1, we let B be the set of all values in a feature map across both the elements of a mini-batch and spatial locations - so for a mini-batch of size m and feature maps of size p × q , we use the effective mini-batch of size m ′ = |B| = m · p q . We learn a pair of parameters γ ( k ) and β ( k ) per feature map, rather than per activation. Alg. 2 is modified similarly, so that during inference the BN transform applies the same linear transformation to each activation in a given feature map.\n",
      "\n",
      "## 3.3 Batch Normalization enables higher learning rates\n",
      "\n",
      "In traditional deep networks, too-high learning rate may result in the gradients that explode or vanish, as well as getting stuck in poor local minima. Batch Normalization helps address these issues. By normalizing activations throughout the network, it prevents small changes to the parameters from amplifying into larger and suboptimal changes in activations in gradients; for instance, it prevents the training from getting stuck in the saturated regimes of nonlinearities.\n",
      "\n",
      "Batch Normalization also makes training more resilient to the parameter scale. Normally, large learning rates may increase the scale of layer parameters, which then amplify the gradient during backpropagation and lead to the model explosion. However, with Batch Normalization, backpropagation through a layer is unaffected by the scale of its parameters. Indeed, for a scalar a ,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "and we can show that\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "The scale does not affect the layer Jacobian nor, consequently, the gradient propagation. Moreover, larger weights lead to smaller gradients, and Batch Normalization will stabilize the parameter growth.\n",
      "\n",
      "We further conjecture that Batch Normalization may lead the layer Jacobians to have singular values close to 1, which is known to be beneficial for training (Saxe et al., 2013). Consider two consecutive layers with normalized inputs, and the transformation between these normalized vectors: ̂ z = F ( ̂ x) . If we assume that ̂ x and ̂ z are Gaussian and uncorrelated, and that F ( ̂ x) ≈ J ̂ x is a linear transformation for the given model parameters, then both ̂ x and ̂ z have unit covariances, and I = Cov [ ̂ z] = J Cov [ ̂ x] J T = JJ T . Thus, JJ T = I , and so all singular values of J are equal to 1, which preserves the gradient magnitudes during backpropagation. In reality, the transformation is not linear, and the normalized values are not guaranteed to be Gaussian nor independent, but we nevertheless expect Batch Normalization to help make gradient propagation better behaved. The precise effect of Batch Normalization on gradient propagation remains an area of further study.\n",
      "\n",
      "## 3.4 Batch Normalization regularizes the model\n",
      "\n",
      "When training with Batch Normalization, a training example is seen in conjunction with other examples in the mini-batch, and the training network no longer producing deterministic values for a given training example. In our experiments, we found this effect to be advantageous to the generalization of the network. Whereas Dropout (Srivastava et al., 2014) is typically used to reduce overfitting, in a batch-normalized network we found that it can be either removed or reduced in strength.\n",
      "\n",
      "## 4 Experiments\n",
      "\n",
      "## 4.1 Activations over time\n",
      "\n",
      "To verify the effects of internal covariate shift on training, and the ability of Batch Normalization to combat it, we considered the problem of predicting the digit class on the MNIST dataset (LeCun et al., 1998a). We used a very simple network, with a 28x28 binary image as input, and\n",
      "\n",
      "Figure 1: (a) The test accuracy of the MNIST network trained with and without Batch Normalization, vs. the number of training steps. Batch Normalization helps the network train faster and achieve higher accuracy. (b, c) The evolution of input distributions to a typical sigmoid, over the course of training, shown as { 15 , 50 , 85 } th percentiles. Batch Normalization makes the distribution more stable and reduces the internal covariate shift.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "3 fully-connected hidden layers with 100 activations each. Each hidden layer computes y = g ( W u+b) with sigmoid nonlinearity, and the weights W initialized to small random Gaussian values. The last hidden layer is followed by a fully-connected layer with 10 activations (one per class) and cross-entropy loss. We trained the network for 50000 steps, with 60 examples per mini-batch. We added Batch Normalization to each hidden layer of the network, as in Sec. 3.1. We were interested in the comparison between the baseline and batch-normalized networks, rather than achieving the state of the art performance on MNIST (which the described architecture does not).\n",
      "\n",
      "Figure 1(a) shows the fraction of correct predictions by the two networks on held-out test data, as training progresses. The batch-normalized network enjoys the higher test accuracy. To investigate why, we studied inputs to the sigmoid, in the original network N and batchnormalized network N tr BN (Alg. 2) over the course of training. In Fig. 1(b,c) we show, for one typical activation from the last hidden layer of each network, how its distribution evolves. The distributions in the original network change significantly over time, both in their mean and the variance, which complicates the training of the subsequent layers. In contrast, the distributions in the batchnormalized network are much more stable as training progresses, which aids the training.\n",
      "\n",
      "## 4.2 ImageNet classification\n",
      "\n",
      "We applied Batch Normalization to a new variant of the Inception network (Szegedy et al., 2014), trained on the ImageNet classification task (Russakovsky et al., 2014). The network has a large number of convolutional and pooling layers, with a softmax layer to predict the image class, out of 1000 possibilities. Convolutional layers use ReLU as the nonlinearity. The main difference to the network described in (Szegedy et al., 2014) is that the 5 × 5 convolutional layers are replaced by two consecutive layers of 3 × 3 convolutions with up to 128 filters. The network contains 13 . 6 · 10 6 parameters, and, other than the top softmax layer, has no fully-connected layers. More details are given in the Appendix. We refer to this model as Inception in the rest of the text. The model was trained using a version of Stochastic Gradient Descent with momentum(Sutskever et al., 2013), using the mini-batch size of 32. The training was performed using a large-scale, distributed architecture (similar to (Dean et al., 2012)). All networks are evaluated as training progresses by computing the validation accuracy @1 , i.e. the probability of predicting the correct label out of 1000 possibilities, on a held-out set, using a single crop per image.\n",
      "\n",
      "In our experiments, we evaluated several modifications of Inception with Batch Normalization. In all cases, Batch Normalization was applied to the input of each nonlinearity, in a convolutional way, as described in section 3.2, while keeping the rest of the architecture constant.\n",
      "\n",
      "## 4.2.1 Accelerating BN Networks\n",
      "\n",
      "Simply adding Batch Normalization to a network does not take full advantage of our method. To do so, we further changed the network and its training parameters, as follows:\n",
      "\n",
      "Increase learning rate. In a batch-normalized model, we have been able to achieve a training speedup from higher learning rates, with no ill side effects (Sec. 3.3).\n",
      "\n",
      "Remove Dropout. As described in Sec. 3.4, Batch Normalization fulfills some of the same goals as Dropout. Removing Dropout from Modified BN-Inception speeds up training, without increasing overfitting.\n",
      "\n",
      "Reduce the L 2 weight regularization. While in Inception an L 2 loss on the model parameters controls overfitting, in Modified BN-Inception the weight of this loss is reduced by a factor of 5. We find that this improves the accuracy on the held-out validation data.\n",
      "\n",
      "Accelerate the learning rate decay. In training Inception, learning rate was decayed exponentially. Because our network trains faster than Inception, we lower the learning rate 6 times faster.\n",
      "\n",
      "Remove Local Response Normalization While Inception and other networks (Srivastava et al., 2014) benefit from it, we found that with Batch Normalization it is not necessary.\n",
      "\n",
      "Shuffle training examples more thoroughly. We enabled within-shard shuffling of the training data, which prevents the same examples from always appearing in a mini-batch together. This led to about 1% improvements in the validation accuracy, which is consistent with the view of Batch Normalization as a regularizer (Sec. 3.4): the randomization inherent in our method should be most beneficial when it affects an example differently each time it is seen.\n",
      "\n",
      "Reduce the photometric distortions. Because batchnormalized networks train faster and observe each training example fewer times, we let the trainer focus on more 'real' images by distorting them less.\n",
      "\n",
      "Figure 2: Single crop validation accuracy of Inception and its batch-normalized variants, vs. the number of training steps.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## 4.2.2 Single-Network Classification\n",
      "\n",
      "We evaluated the following networks, all trained on the LSVRC2012 training data, and tested on the validation data:\n",
      "\n",
      "Inception: the network described at the beginning of Section 4.2, trained with the initial learning rate of 0.0015.\n",
      "\n",
      "BN-Baseline: Same as Inception with Batch Normalization before each nonlinearity.\n",
      "\n",
      "BN-x5: Inception with Batch Normalization and the modifications in Sec. 4.2.1. The initial learning rate was increased by a factor of 5, to 0.0075. The same learning rate increase with original Inception caused the model parameters to reach machine infinity.\n",
      "\n",
      "BN-x30: Like BN-x5, but with the initial learning rate 0.045 (30 times that of Inception).\n",
      "\n",
      "BN-x5-Sigmoid: Like BN-x5, but with sigmoid nonlinearity g ( t ) = 1 1+exp( -x ) instead of ReLU. We also attempted to train the original Inception with sigmoid, but the model remained at the accuracy equivalent to chance.\n",
      "\n",
      "In Figure 2, we show the validation accuracy of the networks, as a function of the number of training steps. Inception reached the accuracy of 72.2% after 31 · 10 6 training steps. The Figure 3 shows, for each network, the number of training steps required to reach the same 72.2% accuracy, as well as the maximum validation accuracy reached by the network and the number of steps to reach it.\n",
      "\n",
      "By only using Batch Normalization (BN-Baseline), we match the accuracy of Inception in less than half the number of training steps. By applying the modifications in Sec. 4.2.1, we significantly increase the training speed of the network. BN-x5 needs 14 times fewer steps than Inception to reach the 72.2% accuracy. Interestingly, increasing the learning rate further (BN-x30) causes the model to train somewhat slower initially, but allows it to reach a higher final accuracy. It reaches 74.8% after 6 · 10 6 steps, i.e. 5 times fewer steps than required by Inception to reach 72.2%.\n",
      "\n",
      "We also verified that the reduction in internal covariate shift allows deep networks with Batch Normalization\n",
      "\n",
      "Figure 3: For Inception and the batch-normalized variants, the number of training steps required to reach the maximum accuracy of Inception (72.2%), and the maximum accuracy achieved by the network.\n",
      "\n",
      "| Model         | Steps to 72.2%   | Max accuracy   |\n",
      "|---------------|------------------|----------------|\n",
      "| Inception     | 31 . 0 · 10 6    | 72.2%          |\n",
      "| BN-Baseline   | 13 . 3 · 10 6    | 72.7%          |\n",
      "| BN-x5         | 2 . 1 · 10 6     | 73.0%          |\n",
      "| BN-x30        | 2 . 7 · 10 6     | 74.8%          |\n",
      "| BN-x5-Sigmoid |                  | 69.8%          |\n",
      "\n",
      "to be trained when sigmoid is used as the nonlinearity, despite the well-known difficulty of training such networks. Indeed, BN-x5-Sigmoid achieves the accuracy of 69.8%. Without Batch Normalization, Inception with sigmoid never achieves better than 1 / 1000 accuracy.\n",
      "\n",
      "## 4.2.3 Ensemble Classification\n",
      "\n",
      "The current reported best results on the ImageNet Large Scale Visual Recognition Competition are reached by the Deep Image ensemble of traditional models (Wu et al., 2015) and the ensemble model of (He et al., 2015). The latter reports the top-5 error of 4.94%, as evaluated by the ILSVRC server. Here we report a top-5 validation error of 4.9%, and test error of 4.82% (according to the ILSVRC server). This improves upon the previous best result, and exceeds the estimated accuracy of human raters according to (Russakovsky et al., 2014).\n",
      "\n",
      "For our ensemble, we used 6 networks. Each was based onBN-x30, modified via some of the following: increased initial weights in the convolutional layers; using Dropout (with the Dropout probability of 5% or 10%, vs. 40% for the original Inception); and using non-convolutional, per-activation Batch Normalization with last hidden layers of the model. Each network achieved its maximum accuracy after about 6 · 10 6 training steps. The ensemble prediction was based on the arithmetic average of class probabilities predicted by the constituent networks. The details of ensemble and multicrop inference are similar to (Szegedy et al., 2014).\n",
      "\n",
      "We demonstrate in Fig. 4 that batch normalization allows us to set new state-of-the-art by a healthy margin on the ImageNet classification challenge benchmarks.\n",
      "\n",
      "## 5 Conclusion\n",
      "\n",
      "We have presented a novel mechanism for dramatically accelerating the training of deep networks. It is based on the premise that covariate shift, which is known to complicate the training of machine learning systems, also ap-\n",
      "\n",
      "Figure 4: Batch-Normalized Inception comparison with previous state of the art on the provided validation set comprising 50000 images. *BN-Inception ensemble has reached 4.82% top-5 error on the 100000 images of the test set of the ImageNet as reported by the test server.\n",
      "\n",
      "| Model                    | Resolution   | Crops   | Models   | Top-1 error   | Top-5 error   |\n",
      "|--------------------------|--------------|---------|----------|---------------|---------------|\n",
      "| GoogLeNet ensemble       | 224          | 144     | 7        | -             | 6.67%         |\n",
      "| Deep Image low-res       | 256          | -       | 1        | -             | 7.96%         |\n",
      "| Deep Image high-res      | 512          | -       | 1        | 24.88         | 7.42%         |\n",
      "| Deep Image ensemble      | variable     | -       | -        | -             | 5.98%         |\n",
      "| BN-Inception single crop | 224          | 1       | 1        | 25.2%         | 7.82%         |\n",
      "| BN-Inception multicrop   | 224          | 144     | 1        | 21.99%        | 5.82%         |\n",
      "| BN-Inception ensemble    | 224          | 144     | 6        | 20.1%         | 4.9% *        |\n",
      "\n",
      "plies to sub-networks and layers, and removing it from internal activations of the network may aid in training. Our proposed method draws its power from normalizing activations, and from incorporating this normalization in the network architecture itself. This ensures that the normalization is appropriately handled by any optimization method that is being used to train the network. To enable stochastic optimization methods commonly used in deep network training, we perform the normalization for each mini-batch, and backpropagate the gradients through the normalization parameters. Batch Normalization adds only two extra parameters per activation, and in doing so preserves the representation ability of the network. We presented an algorithm for constructing, training, and performing inference with batch-normalized networks. The resulting networks can be trained with saturating nonlinearities, are more tolerant to increased training rates, and often do not require Dropout for regularization.\n",
      "\n",
      "Merely adding Batch Normalization to a state-of-theart image classification model yields a substantial speedup in training. By further increasing the learning rates, removing Dropout, and applying other modifications afforded by Batch Normalization, we reach the previous state of the art with only a small fraction of training steps - and then beat the state of the art in single-network image classification. Furthermore, by combining multiple models trained with Batch Normalization, we perform better than the best known system on ImageNet, by a significant margin.\n",
      "\n",
      "Interestingly, our method bears similarity to the standardization layer of (G¨ ulc ¸ehre &amp; Bengio, 2013), though the two methods stem from very different goals, and perform different tasks. The goal of Batch Normalization is to achieve a stable distribution of activation values throughout training, and in our experiments we apply it before the nonlinearity since that is where matching the first and second moments is more likely to result in a stable distribution. On the contrary, (G¨ ulc ¸ehre &amp; Bengio , 2013) apply the standardization layer to the output of the nonlinearity, which results in sparser activations. In our large-scale image classification experiments, we have not observed the nonlinearity inputs to be sparse, neither with nor without Batch Normalization. Other notable differ- entiating characteristics of Batch Normalization include the learned scale and shift that allow the BN transform to represent identity (the standardization layer did not require this since it was followed by the learned linear transform that, conceptually, absorbs the necessary scale and shift), handling of convolutional layers, deterministic inference that does not depend on the mini-batch, and batchnormalizing each convolutional layer in the network.\n",
      "\n",
      "In this work, we have not explored the full range of possibilities that Batch Normalization potentially enables. Our future work includes applications of our method to Recurrent Neural Networks (Pascanu et al., 2013), where the internal covariate shift and the vanishing or exploding gradients may be especially severe, and which would allow us to more thoroughly test the hypothesis that normalization improves gradient propagation (Sec. 3.3). We plan to investigate whether Batch Normalization can help with domain adaptation, in its traditional sense - i.e. whether the normalization performed by the network would allow it to more easily generalize to new data distributions, perhaps with just a recomputation of the population means and variances (Alg. 2). Finally, we believe that further theoretical analysis of the algorithm would allow still more improvements and applications.\n",
      "\n",
      "## References\n",
      "\n",
      "Bengio, Yoshua and Glorot, Xavier. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of AISTATS 2010 , volume 9, pp. 249256, May 2010.\n",
      "\n",
      "Dean, Jeffrey, Corrado, Greg S., Monga, Rajat, Chen, Kai, Devin, Matthieu, Le, Quoc V., Mao, Mark Z., Ranzato, Marc'Aurelio, Senior, Andrew, Tucker, Paul, Yang, Ke, and Ng, Andrew Y. Large scale distributed deep networks. In NIPS , 2012.\n",
      "\n",
      "Desjardins, Guillaume and Kavukcuoglu, Koray. Natural neural networks. (unpublished).\n",
      "\n",
      "Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic\n",
      "\n",
      "- optimization. J. Mach. Learn. Res. , 12:2121-2159,July 2011. ISSN 1532-4435.\n",
      "- G¨ ulc ¸ehre, C ¸ aglar and Bengio, Yoshua. Knowledge matters: Importance of prior information for optimization. CoRR , abs/1301.4083, 2013.\n",
      "- He, K., Zhang, X., Ren, S., and Sun, J. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. ArXiv e-prints , February 2015.\n",
      "\n",
      "Hyv¨ arinen, A. and Oja, E. Independent component analysis: Algorithms and applications. Neural Netw. , 13 (4-5):411-430, May 2000.\n",
      "\n",
      "Jiang, Jing. A literature survey on domain adaptation of statistical classifiers, 2008.\n",
      "\n",
      "- LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. Proceedings of the IEEE , 86(11):2278-2324, November 1998a.\n",
      "- LeCun, Y., Bottou, L., Orr, G., and Muller, K. Efficient backprop. In Orr, G. and K., Muller (eds.), Neural Networks: Tricks of the trade . Springer, 1998b.\n",
      "\n",
      "Lyu, S and Simoncelli, E P. Nonlinear image representation using divisive normalization. In Proc. Computer Vision and Pattern Recognition , pp. 1-8. IEEE Computer Society, Jun 23-28 2008. doi: 10.1109/CVPR. 2008.4587821.\n",
      "\n",
      "- Nair, Vinod and Hinton, Geoffrey E. Rectified linear units improve restricted boltzmann machines. In ICML , pp. 807-814. Omnipress, 2010.\n",
      "\n",
      "Pascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua. On the difficulty of training recurrent neural networks. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 1621 June 2013 , pp. 1310-1318, 2013.\n",
      "\n",
      "Povey, Daniel, Zhang, Xiaohui, and Khudanpur, Sanjeev. Parallel training of deep neural networks with natural gradient and parameter averaging. CoRR , abs/1410.7455, 2014.\n",
      "\n",
      "- Raiko, Tapani, Valpola, Harri, and LeCun, Yann. Deep learning made easier by linear transformations in perceptrons. In International Conference on Artificial Intelligence and Statistics (AISTATS) , pp. 924-932, 2012.\n",
      "- Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg, Alexander C., and Fei-Fei, Li. ImageNet Large Scale Visual Recognition Challenge, 2014.\n",
      "- Saxe, Andrew M., McClelland, James L., and Ganguli, Surya. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. CoRR , abs/1312.6120, 2013.\n",
      "\n",
      "Shimodaira, Hidetoshi. Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of Statistical Planning and Inference , 90(2):227-244, October 2000.\n",
      "\n",
      "Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: A simple way to prevent neural networks from overfitting. J. Mach. Learn. Res. , 15(1):1929-1958, January 2014.\n",
      "\n",
      "Sutskever, Ilya, Martens, James, Dahl, George E., and Hinton, Geoffrey E. On the importance of initialization and momentum in deep learning. In ICML (3) , volume 28 of JMLR Proceedings , pp. 1139-1147. JMLR.org, 2013.\n",
      "\n",
      "Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. CoRR , abs/1409.4842, 2014.\n",
      "\n",
      "Wiesler, Simon and Ney, Hermann. A convergence analysis of log-linear training. In Shawe-Taylor, J., Zemel, R.S., Bartlett, P., Pereira, F.C.N., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 24 , pp. 657-665, Granada, Spain, December 2011.\n",
      "\n",
      "Wiesler, Simon, Richard, Alexander, Schl¨ uter, Ralf, and Ney, Hermann. Mean-normalized stochastic gradient for large-scale deep learning. In IEEE International Conference on Acoustics, Speech, and Signal Processing , pp. 180-184, Florence, Italy, May 2014.\n",
      "\n",
      "Wu, Ren, Yan, Shengen, Shan, Yi, Dang, Qingqing, and Sun, Gang. Deep image: Scaling up image recognition, 2015.\n",
      "\n",
      "## Appendix\n",
      "\n",
      "## Variant of the Inception Model Used\n",
      "\n",
      "Figure 5 documents the changes that were performed compared to the architecture with respect to the GoogleNet archictecture. For the interpretation of this table, please consult (Szegedy et al., 2014). The notable architecture changes compared to the GoogLeNet model include:\n",
      "\n",
      "- The 5 × 5 convolutional layers are replaced by two consecutive 3 × 3 convolutional layers. This increases the maximum depth of the network by 9\n",
      "\n",
      "weight layers. Also it increases the number of parameters by 25% and the computational cost is increased by about 30%.\n",
      "\n",
      "- The number 28 × 28 inception modules is increased from 2 to 3.\n",
      "- Inside the modules, sometimes average, sometimes maximum-pooling is employed. This is indicated in the entries corresponding to the pooling layers of the table.\n",
      "- There are no across the board pooling layers between any two Inception modules, but stride-2 convolution/pooling layers are employed before the filter concatenation in the modules 3c, 4e.\n",
      "\n",
      "Our model employed separable convolution with depth multiplier 8 on the first convolutional layer. This reduces the computational cost while increasing the memory consumption at training time.\n",
      "\n",
      "Figure 5: Inception architecture\n",
      "\n",
      "| type           | patch size/ stride   | output size    |   depth | #1 × 1   | #3 × 3 reduce   | #3 × 3   | double #3 × 3 reduce   | double #3 × 3   | Pool +proj         |\n",
      "|----------------|----------------------|----------------|---------|----------|-----------------|----------|------------------------|-----------------|--------------------|\n",
      "| convolution*   | 7 × 7 / 2            | 112 × 112 × 64 |       1 |          |                 |          |                        |                 |                    |\n",
      "| max pool       | 3 × 3 / 2            | 56 × 56 × 64   |       0 |          |                 |          |                        |                 |                    |\n",
      "| convolution    | 3 × 3 / 1            | 56 × 56 × 192  |       1 |          | 64              | 192      |                        |                 |                    |\n",
      "| max pool       | 3 × 3 / 2            | 28 × 28 × 192  |       0 |          |                 |          |                        |                 |                    |\n",
      "| inception (3a) |                      | 28 × 28 × 256  |       3 | 64       | 64              | 64       | 64                     | 96              | avg + 32           |\n",
      "| inception (3b) |                      | 28 × 28 × 320  |       3 | 64       | 64              | 96       | 64                     | 96              | avg + 64           |\n",
      "| inception (3c) | stride 2             | 28 × 28 × 576  |       3 | 0        | 128             | 160      | 64                     | 96              | max + pass through |\n",
      "| inception (4a) |                      | 14 × 14 × 576  |       3 | 224      | 64              | 96       | 96                     | 128             | avg + 128          |\n",
      "| inception (4b) |                      | 14 × 14 × 576  |       3 | 192      | 96              | 128      | 96                     | 128             | avg + 128          |\n",
      "| inception (4c) |                      | 14 × 14 × 576  |       3 | 160      | 128             | 160      | 128                    | 160             | avg + 128          |\n",
      "| inception (4d) |                      | 14 × 14 × 576  |       3 | 96       | 128             | 192      | 160                    | 192             | avg + 128          |\n",
      "| inception (4e) | stride 2             | 14 × 14 × 1024 |       3 | 0        | 128             | 192      | 192                    | 256             | max + pass through |\n",
      "| inception (5a) |                      | 7 × 7 × 1024   |       3 | 352      | 192             | 320      | 160                    | 224             | avg + 128          |\n",
      "| inception (5b) |                      | 7 × 7 × 1024   |       3 | 352      | 192             | 320      | 192                    | 224             | max + 128          |\n",
      "| avg pool       | 7 × 7 / 1            | 1 × 1 × 1024   |       0 |          |                 |          |                        |                 |                    |\n",
      "Document 18:\n",
      "## TransCT: Dual-path Transformer for Low Dose Computed Tomography\n",
      "\n",
      "Zhicheng Zhang 1( GLYPH&lt;12&gt; ) , Lequan Yu 1 , 2 , Xiaokun Liang 1 , Wei Zhao 1 , and Lei Xing 1\n",
      "\n",
      "1\n",
      "\n",
      "Department of Radiation Oncology, Stanford University, Stanford, USA.\n",
      "\n",
      "zzc623@stanford.edu\n",
      "\n",
      "2 Department of Statistics and Actuarial Science, The University of Hong Kong\n",
      "\n",
      "Abstract. Low dose computed tomography (LDCT) has attracted more and more attention in routine clinical diagnosis assessment, therapy planning, etc. , which can reduce the dose of X-ray radiation to patients. However, the noise caused by low X-ray exposure degrades the CT image quality and then affects clinical diagnosis accuracy. In this paper, we train a transformer-based neural network to enhance the final CT image quality. To be specific, we first decompose the noisy LDCT image into two parts: high-frequency (HF) and low-frequency (LF) compositions. Then, we extract content features ( X L c ) and latent texture features ( X L t ) from the LF part, as well as HF embeddings ( X H f ) from the HF part. Further, we feed X L t and X H f into a modified transformer with three encoders and decoders to obtain well-refined HF texture features. After that, we combine these well-refined HF texture features with the pre-extracted X L c to encourage the restoration of high-quality LDCT images with the assistance of piecewise reconstruction. Extensive experiments on Mayo LDCT dataset show that our method produces superior results and outperforms other methods.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Computed tomography (CT) system, as noninvasive imaging equipment, has been widely used for medical diagnosis and treatment [18,16]. However, concerns about the increase of X-ray radiation risk have become an unavoidable problem for all CT vendors and medical institutions [2]. Since x-ray imaging is mainly based on a photon-noise-dominated process [27], lowering the X-ray dose will result in degraded CT images. Therefore, on the premise of ensuring CT image quality, how to reduce the X-ray radiation dose as far as possible becomes a promising and significant research topic [2].\n",
      "\n",
      "Compared to sparse or limited-view CT [33] and other hardware-based strategies [34], lowering single X-ray exposure dose [11,22] is the most convenient and affordable method. To obtain high-quality LDCT images, previous works can be mainly classified into two categories: model-based and data-driven methods. The key to model-based methods is to use a mathematical model for the description of each process of CT imaging: noise characteristics in the sinogram domain [15,30], image prior information in the image domain, such as sparsity\n",
      "\n",
      "in gradient domain [13] and low rank [3], as well as defects in CT hardware systems [32]. This kind of methods are independent of a large training dataset, while the accuracy of the model depiction limits its performance.\n",
      "\n",
      "With the development of deep learning in medical image reconstruction and analysis [29], many data-driven works have been proposed to reconstruct LDCT images with convolution neural network (CNN) [25]. Kang et al. proposed a CNN-based neural network with the assistance of directional wavelets, suggesting the potential of deep learning technique in LDCT. Similarly, Chen et al. employed residual learning to extract noise in the LDCT images and obtain superior performance [5]. However, these methods need FBP-reconstructed LDCT images as the inputs, which belong to image post-processing. To get rid of the influence of traditional analytic algorithms ( e.g. FBP), Zhu et al. suggested that 'AUTOMAP' was a direct reconstruction method from the measurement data to the final image [35]. Then again, the first fully-connected layer as domain transform has a huge memory requirement, which makes AUTOMAP unavailable for large-scale CT reconstruction [24]. Besides, many works with the combination of iterative reconstruction and deep learning have been proposed as deep unrolled approaches. This kind of method used CNNs as special regularizations plugged into conventional iterative reconstruction. They not only inherit the advantages of the convenient calculation of system matrix in conventional algorithms but also get rid of the complicated manual design regularization [10,7,11].\n",
      "\n",
      "Despite the success of CNNs in LDCT reconstruction, CNN-based methods heavily rely on cascaded convolution layers to extract high-level features since the convolution operation has its disadvantage of a limited receptive field that only perceives local areas. Moreover, this disadvantage makes it difficult for CNN-based methods to make full of the similarity across large regions [26,31], which makes CNN-based methods less efficient in modeling various structural information in CT images [14]. To overcome this limitation, Transformers [23], which solely depend on attention mechanisms instead, have emerged as a powerful architectures in many fields, such as natural language processing (NLP) [8], image segmentation [6],image recognition [9], etc. In addition to these high-level tasks, Transformer has also been tentatively investigated for some lower-level tasks [28,4], which can model all pairwise interactions between image regions and capture long-range dependencies by computing interactions between any two positions, regardless of their positional distance.\n",
      "\n",
      "For image denoising, noise is mainly contained in the high-frequency subband. Moreover, the remaining low-frequency sub-band not only contains the main image content, but also contains the weakened image textures, which are noise-free. These weakened image textures can be used to help noise removal in the high-frequency sub-band. Inspired by this observation, in this paper, we present the first work, TransCT, to explore the potential of transformers in LDCT imaging. Firstly, we decompose the noisy LDCT image into highfrequency (HF) and low-frequency (LF) parts. To remove the image noise on the premise of retaining the image content, we extract the corresponding content features ( X L c ) and latent texture features ( X L t ) from the LF part. Simultane-\n",
      "\n",
      "Fig. 1. The overall architecture of the proposed TransCT. ' n 64 s 2' means the convolution layer has 64 kernels with stride 2. Sub-Pixel layer is the upsampling layer [21].\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "ously, we extract the corresponding embeddings ( X H f ) from the HF part. Since transformers can only use sequences as input, we then convert X L t and X H f into separated sequences as the input of transformer encoder and decoder, respectively. To preserve the fine details of the final LDCT images, we integrate the output of the transformer decoder and some specific features from the LF part and then piecewise reconstruct high-quality and high-resolution LDCT images by stages. Extensive experiments on Mayo LDCT dataset demonstrate the superiority of our method over other methods.\n",
      "\n",
      "## 2 Method\n",
      "\n",
      "Fig 1 illustrates the overview of our proposed framework. For image denoising, an intuitive solution is to decompose the noisy image into HF and LF parts, and then the noise is mainly left in the HF part, which also contains plenty of image textures. However, noise removal only in the HF part breaks the relationship between the HF and LF parts since there are also weakened latent textures in the LF part with reduced noise. Therefore, we can remove the noise in the HF part with the assistance of the latent textures from the LF part. In this work, given the noisy LDCT image X with the size of H × W , we first use a Gaussian filter with a standard deviation of 1 . 5 to decompose the LDCT image into two compositions: HF part X H and LF part X L .\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "To use the latent textures in X L , we firstly extract the corresponding content fetatures X L c and texture features X L t from X L using shallow two CNNs. Further, we use these texture features and embeddings from X H to train a transformer and get high-level features of X H , combined with content features from X L to reconstruct the final high-quality LDCT image.\n",
      "\n",
      "## 2.1 TransCT\n",
      "\n",
      "Sequence Similar with what other works have done [6], we firstly employ two convolution layers with stride 2 to obtain low-resolution features from X L , and then set two paths to extract content features X L c 1 ( H 8 × W 8 × 64), X L c 2 ( H 16 × W 16 × 256) and latent texture feature X L t ( H 32 × W 32 × 256), respectively. For X H , we employ sub-pixel layer to make X H to be low-resolution images ( H 16 × W 16 × 256), and final high-level features X H f can be obtained with three convolution layers. The goal is to get a sequence of moderate dimensions eventually. To take advantage of the characteristic of long-range dependencies of transformers, we perform tokenization by reshaping X L t and X H f into two sequences S L , S H , respectively.\n",
      "\n",
      "Transformer In this work, we employ a modified transformer with three encoders and three decoders, each encoder includes a multi-head attention module (MHSA) and a feed-forward layer (MLP) and each decoder consists of two multihead attention modules and a feed-forward layer, as can be seen in Fig 1. For transformer encoder, we use S L ( WH 1024 × 256) as the input token, followed by a multi-head attention module to seek the global relationship across large regions, and then we use two fully-connected layers (whose number of the node are 8 c and c , respectively. c is the dimension of the input sequence) to increase the expressive power of the entire network.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "After acquiring the latent texture features S 3 L from X L , we feed S H ( WH 256 × 256) into the first multi-head attention module and treat S 3 L as the key and value of each transformer decoder in the second multi-head attention module.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Piecewise Reconstruction Since the transformer only output features Y , we combine Y with X L c 1 , X L c 2 to piecewise reconstruct the final high-quality LDCT images. In our work, the output of the transformer has the size of H 16 × W 16 × 256. Here, we reconstruct the high-resolution LDCT image piecewise. In the first step, we add Y and X L c 2 and then feed the output into a ResNet with two 'Conv2d + Leaky-ReLU(lrelu)' layers, followed by a sub-pixel layer which results in higher-resolution features with size of H 8 × W 8 × 64. Similarly, we add these higher-resolution features and X L c 1 . After another ResNet with two 'Conv2d\n",
      "\n",
      "+ lrelu' layers and sub-pixel layer, we can get the final output with the size of H × W\n",
      "\n",
      "## 2.2 Loss Function\n",
      "\n",
      "The MSE measures the difference between the output and normal dose CT images (NDCT), which reduces the noise in the input LDCT images. Formally, the MSE is defined as follows:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Where I ND is the NDCT image and I LD is the LDCT image, F is the proposed model and θ denotes the network parameters.\n",
      "\n",
      "## 2.3 Implementation\n",
      "\n",
      "In this work, the proposed framework was implemented in python based on Tensorflow [1] library. We used the Adam [12] optimizer to optimize all the parameters of the framework. We totally trained 300 epochs with a mini-batch size of 8. The learning rate was set as 0.0001 in the first 180 epochs and then reduced to 0.00001 for the next 120 epochs. The configuration of our computational platform is Intel(R) Core(Tm) i7-7700K CPU @4.20GHZ, 32 GB RAM, and a GeForce GTX TITAN X GPU with 12 GB RAM. We initialized all the variations with xavier initialization. Our code is publicly available at https://github.com/zzc623/TransCT\n",
      "\n",
      "## 3 Experiments\n",
      "\n",
      "Datasets In this work, we used a publicly released dataset for the 2016 NIHAAPM-Mayo Clinic Low-Dose CT Grand Challenge 3 [17]. In this dataset, normaldose abdominal CT images of 1 mm slice thickness were taken from 10 anonymous patients and the corresponding quarter-dose CT images were simulated by inserting Poisson noise into the projection data. To better train the proposed TransCT, we divided the original 10 training patient cases into 7/1/2 cases, related to the training/validation/testing datasets, respectively. Before network training, we converted CT value of each pixel into its corresponding attenuation value under the assumption that the x-ray source was monochromatic at 60 keV.\n",
      "\n",
      "Comparison with other methods We compared our method with baseline methods: Non-local Mean (NLM), RED-CNN [5], MAP-NN [19], which are the high-performance LDCT methods. NLM can be found in the scikit-image library 4 . Since there is no public well-trained model for RED-CNN and MAP-NN, we re-train these methods with the same dataset.\n",
      "\n",
      "3 https://www.aapm.org/GrandChallenge/LowDoseCT/\n",
      "\n",
      "4 https://scikit-image.org/\n",
      "\n",
      "Fig. 2. Visual comparisons from Mayo testing dataset. (A) NDCT, (B) LDCT, (C) NLM, (D) RED-CNN, (E) MAP-NN, (F) TransCT. The display window is [-160, 240] HU .\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Fig 2 shows the results randomly selected from the testing dataset. As compared to LDCT (Fig 2 (B)), NLM and all the DL-based methods can remove noise to a certain extent, while our proposed TransCT is more close to NDCT. By investigating the local region in Fig 3, we can see that the blood vessels (red arrows) are not obvious with NLM in (Fig 3 (C)). RED-CNN and MAP-NN generate, more or less, some additional light tissues (yellow arrow in (Fig 3 (D))) and shadows (green arrow in (Fig 3 (E))), respectively.\n",
      "\n",
      "Quantitative Analysis To quantitatively compare all the related methods, we conducted 5-fold cross-validation for all methods on Mayo dataset and employed Root Mean Square Error (RMSE), Structural Similarity (SSIM), and Visual Information Fidelity (VIF) [20] as image quality metrics. Among the three metrics, RMSE and SSIM mainly focus on pixel-wise similarity, and VIF uses natural statistics models to evaluate psychovisual features of the human visual system. From table 1, we can see that all the related methods improve the image quality on all three metrics. To be specific, Red-CNN is superior to MAP-NN at the pixel-wise level while inferior to MAP-NN in terms of VIF. As compared to LDCT, our TransCT can decrease RMSE by 40.5%, improve SSIM by 12.3%, and VIF by 93.7%. For clinical evaluation, limited by clinical ethics, we evaluated all the methods on clinical CBCT images from a real pig head.\n",
      "\n",
      "Fig. 3. The zoomed regions marked by the red box in Fig. 2 (A). (A) NDCT, (B) LDCT, (C) NLM, (D) RED-CNN, (E) MAP-NN, (F) TransCT. The display window is [-160, 240] HU .\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "The tube current was: 80 mA for NDCT and 20 mA for LDCT. From table 1, our method outperforms others with superior robustness.\n",
      "\n",
      "## 3.1 Ablation study\n",
      "\n",
      "On the Influence of Piecewise Reconstruction In this work, after the output of transformer decoder, we used two resnet blocks and two sub-pixel layers to piecewise reconstruct the high-quality high-resolution LDCT image. The goal is to restore image detail more finely. To evaluate the influence of piecewise reconstruction, we modified the proposed TransCT and removed the piecewise reconstruction. After the output of the third transformer decoder, we used a sub-pixel layer to directly reconstruct the noise-free high-resolution HF texture, and then we added this HF texture and X L to obtain the final LDCT image. Specifically, we have removed six convolution layers, including the path of content extraction ( X L c 1 and X L c 2 ) and four convolution layers in the final two\n",
      "\n",
      "Table 1. Quantitative results (MEAN ± SDs) associated with different methods. Red and blue indicate the best and the second-best results, respectively.\n",
      "\n",
      "| Dataset   |                     | LDCT                | NLM            | RED-CNN        | MAP-NN         | TransCT        |\n",
      "|-----------|---------------------|---------------------|----------------|----------------|----------------|----------------|\n",
      "| Mayo      | RMSE 37.167 ± 7.245 | RMSE 37.167 ± 7.245 | 25.115 ± 4.54  | 22.204 ± 3.89  | 22.492 ± 3.897 | 22.123 ± 3.784 |\n",
      "| Mayo      | SSIM                | 0.822 ± 0.053       | 0.908 ± 0.031  | 0.922 ± 0.025  | 0.921 ± 0.025  | 0.923 ± 0.024  |\n",
      "| Mayo      | VIF                 | 0.079 ± 0.032       | 0.133 ± 0.037  | 0.152 ± 0.037  | 0.150 ± 0.038  | 0.153 ± 0.039  |\n",
      "| Pig       | RMSE                | 50.776 ± 3.7        | 42.952 ± 5.971 | 37.551 ± 5.334 | 37.744 ± 4.883 | 36.999 ± 5.25  |\n",
      "| Pig       | SSIM                | 0.701 ± 0.02        | 0.799 ± 0.043  | 0.861 ± 0.03   | 0.86 ± 0.027   | 0.87 ± 0.029   |\n",
      "| Pig       | VIF                 | 0.023 ± 0.002       | 0.040 ± 0.004  | 0.066 ± 0.006  | 0.063 ± 0.006  | 0.069 ± 0.007  |\n",
      "\n",
      "Fig. 4. RMSE results on the validation dataset during the network trainings.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "resnet blocks. Fig 4 (a) shows the RMSE value on the validation dataset at each epoch. We can see that in about the first 20 epochs, the RMSE from modified TransCT decreases faster since its model scale is smaller than our TransCT, while the convergence was inferior to our TransCT with piecewise reconstruction.\n",
      "\n",
      "On the Influence of Model Size Generally, larger network size will lead to stronger neural network learning ability. In terms of each transformer encoder and decoder, which includes a two-layer feed-forward network, respectively, when the dimension of the input sequence is fixed, the dimension of the hidden layer in the feed-forward network will determine the network size. Here, we adjusted the dimension of the hidden layer { c, 2 c, 4 c } to investigate the influence of model size. From Fig 4 (b), we can see that the smaller the dimension of the hidden layer is, the larger the fluctuation of the convergence curve is, the larger the final convergent value will be. Therefore, we conclude that larger model results in a better performance. In this work, we set the dimension of the hidden layer in the feed-forward network at 8 c .\n",
      "\n",
      "Ablation studies on Transformer Module and Dual-path Module To investigate the effectiveness of the transformer module and dual-path module, we conducted two additional experiments. First, we used a revised module ('Conv+3 × ResNet blocks') to replace the transformer module. We concatenated X H f and the output from the fourth Conv layer (n128s2, before X L t ) and then inputted it into the revised module. As for the dual-path module, we discarded the HF path and inputted the X L c 2 into 3 transformer encoders, whose output will be combined with X L c 1 and X L c 2 in the piecewise reconstruction stage. The results on the validation dataset were shown in table 2, we can see that our TransCT with transformer module and dual-path module can obtain better performance.\n",
      "\n",
      "## 4 Conclusion\n",
      "\n",
      "Inspired by the internal similarity of the LDCT image, we present the first transformer-based neural network for LDCT, which can explore large-range dependencies between LDCT pixels. To ease the impact of noise on high-frequency\n",
      "\n",
      "Table 2. Ablation studies on transformer module and dual-path module conducted on the validation dataset.\n",
      "\n",
      "|                                             | RMSE           | SSIM          | VIF           |\n",
      "|---------------------------------------------|----------------|---------------|---------------|\n",
      "| w/o transformer module w/o dual-path module | 22.62 ± 2.068  | 0.927 ± 0.013 | 0.13 ± 0.023  |\n",
      "|                                             | 21.711 ± 1.997 | 0.931 ± 0.012 | 0.14 ± 0.025  |\n",
      "| TransCT                                     | 21.199 ± 2.054 | 0.933 ± 0.012 | 0.144 ± 0.025 |\n",
      "\n",
      "texture recovery, we employ a transformer encoder to further excavate the lowfrequency part of the latent texture features and then use these texture features to restore the high-frequency features from noisy high-frequency parts of LDCT image. The final high-quality LDCT image can be piecewise reconstructed with the combination of low-frequency content and high-frequency features. In the future, we will further explore the learning ability of TransCT and introduce self-supervised learning to lower the need for the training dataset.\n",
      "\n",
      "Acknowledgements. This work was partially supported by NIH (1 R01CA227713) and a Faculty Research Award from Google Inc.\n",
      "\n",
      "## References\n",
      "\n",
      "1. Abadi, M., Barham, P., et al.: Tensorflow: A system for large-scale machine learning. In: OSDI. pp. 265-283 (2016)\n",
      "2. Brenner, D.J., Hall, E.J.: Computed tomography-an increasing source of radiation exposure. N. Engl. J. Med. 357 (22), 2277-2284 (2007)\n",
      "3. Cai, J.F., Jia, X., et al.: Cine cone beam ct reconstruction using low-rank matrix factorization: algorithm and a proof-of-principle study. IEEE Trans. Med. Imag. 33 (8), 1581-1591 (2014)\n",
      "4. Chen, H., Wang, Y., et al.: Pre-trained image processing transformer. In: CVPR. pp. 12299-12310 (2021)\n",
      "5. Chen, H., Zhang, Y., et al.: Low-dose ct with a residual encoder-decoder convolutional neural network. IEEE Trans. Med. Imag. 36 (12), 2524-2535 (2017)\n",
      "6. Chen, J., Lu, Y., et al.: Transunet: Transformers make strong encoders for medical image segmentation. arXiv:2102.04306 (2021)\n",
      "7. Chun, I.Y., Zheng, X., et al.: Bcd-net for low-dose ct reconstruction: Acceleration, convergence, and generalization. In: MICCAI. pp. 31-40. Springer (2019)\n",
      "8. Devlin, J., Chang, M.W., et al.: Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805 (2018)\n",
      "9. Dosovitskiy, A., Beyer, L., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. arXiv:2010.11929 (2020)\n",
      "10. Gupta, H., Jin, K.H., et al.: Cnn-based projected gradient descent for consistent ct image reconstruction. IEEE Trans. Med. Imag. 37 (6), 1440-1453 (2018)\n",
      "11. He, J., Yang, Y., et al.: Optimizing a parameterized plug-and-play admm for iterative low-dose ct reconstruction. IEEE Trans. Med. Imag. 38 (2), 371-382 (2018)\n",
      "12. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv:1412.6980 (2014)\n",
      "\n",
      "13. LaRoque, S.J., Sidky, E.Y., Pan, X.: Accurate image reconstruction from few-view and limited-angle data in diffraction tomography. JOSA A 25 (7), 1772-1782 (2008)\n",
      "14. Li, M., Hsu, W., et al.: Sacnn: self-attention convolutional neural network for lowdose ct denoising with self-supervised perceptual loss network. IEEE Trans. Med. Imag. 39 (7), 2289-2301 (2020)\n",
      "15. Manduca, A., Yu, L., et al.: Projection space denoising with bilateral filtering and ct noise modeling for dose reduction in ct. Med. phys. 36 (11), 4911-4919 (2009)\n",
      "16. Mathews, J.P., Campbell, Q.P., et al.: A review of the application of x-ray computed tomography to the study of coal. Fuel 209 , 10-24 (2017)\n",
      "17. McCollough, C.H., Bartley, A.C., et al.: Low-dose ct for the detection and classification of metastatic liver lesions: Results of the 2016 low dose ct grand challenge. Med. phys. 44 (10), e339-e352 (2017)\n",
      "18. Seeram, E.: Computed tomography: physical principles, clinical applications, and quality control. Elsevier Health Sciences (2015)\n",
      "19. Shan, H., Padole, A., et al.: Competitive performance of a modularized deep neural network compared to commercial algorithms for low-dose ct image reconstruction. Nat. Mach. Intell. 1 (6), 269-276 (2019)\n",
      "20. Sheikh, H.R., Bovik, A.C.: Image information and visual quality. IEEE Trans. Image Process. 15 (2), 430-444 (2006)\n",
      "21. Shi, W., Caballero, J., et al.: Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In: CVPR. pp. 18741883 (2016)\n",
      "22. Tian, Z., Jia, X., et al.: Low-dose ct reconstruction via edge-preserving total variation regularization. Phys. Med. Biol. 56 (18), 5949 (2011)\n",
      "23. Vaswani, A., Shazeer, N., et al.: Attention is all you need. arXiv:1706.03762 (2017)\n",
      "24. Wang, G., Ye, J.C., et al.: Image reconstruction is a new frontier of machine learning. IEEE Trans. Med. Imag. 37 (6), 1289-1296 (2018)\n",
      "25. Wang, G., Ye, J.C., De Man, B.: Deep learning for tomographic image reconstruction. Nat. Mach. Intell. 2 (12), 737-748 (2020)\n",
      "26. Wang, X., Girshick, R., et al.: Non-local neural networks. In: CVPR. pp. 7794-7803 (2018)\n",
      "27. Xu, Q., Yu, H., et al.: Low-dose x-ray ct reconstruction via dictionary learning. IEEE Trans. Med. Imag. 31 (9), 1682-1697 (2012)\n",
      "28. Yang, F., Yang, H., et al.: Learning texture transformer network for image superresolution. In: CVPR. pp. 5791-5800 (2020)\n",
      "29. Yu, L., Zhang, Z., et al.: Deep sinogram completion with image prior for metal artifact reduction in ct images. IEEE Trans. Med. Imag. 40 (1), 228-238 (2020)\n",
      "30. Yu, L., Manduca, A., et al.: Sinogram smoothing with bilateral filtering for low-dose ct. In: Medical Imaging 2008: Physics of Medical Imaging. vol. 6913, p. 691329\n",
      "31. Zhang, H., Goodfellow, I., et al.: Self-attention generative adversarial networks. In: ICML. pp. 7354-7363 (2019)\n",
      "32. Zhang, Z., Yu, L., et al.: Modularized data-driven reconstruction framework for non-ideal focal spot effect elimination in computed tomography. Med. Phys. (2021)\n",
      "33. Zhang, Z., Liang, X., et al.: A sparse-view ct reconstruction method based on combination of densenet and deconvolution. IEEE Trans. Med. Imag. 37 (6), 14071417 (2018)\n",
      "34. Zhang, Z., Yu, S., et al.: A novel design of ultrafast micro-ct system based on carbon nanotube: a feasibility study in phantom. Phys. Med. 32 (10), 1302-1307 (2016)\n",
      "35. Zhu, B., Liu, J.Z., et al.: Image reconstruction by domain-transform manifold learning. Nature 555 (7697), 487-492 (2018)\n",
      "Document 19:\n",
      "## 3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data\n",
      "\n",
      "## Maurice Weiler*\n",
      "\n",
      "University of Amsterdam m.weiler@uva.nl\n",
      "\n",
      "## Max Welling\n",
      "\n",
      "University of Amsterdam, CIFAR, Qualcomm AI Research m.welling@uva.nl\n",
      "\n",
      "Mario Geiger*\n",
      "\n",
      "EPFL mario.geiger@epfl.ch\n",
      "\n",
      "## Wouter Boomsma\n",
      "\n",
      "University of Copenhagen wb@di.ku.dk\n",
      "\n",
      "## Abstract\n",
      "\n",
      "We present a convolutional network that is equivariant to rigid body motions. The model uses scalar-, vector-, and tensor fields over 3D Euclidean space to represent data, and equivariant convolutions to map between such representations. These SE(3) -equivariant convolutions utilize kernels which are parameterized as a linear combination of a complete steerable kernel basis, which is derived analytically in this paper. We prove that equivariant convolutions are the most general equivariant linear maps between fields over R 3 . Our experimental results confirm the effectiveness of 3D Steerable CNNs for the problem of amino acid propensity prediction and protein structure classification, both of which have inherent SE(3) symmetry.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Increasingly, machine learning techniques are being applied in the natural sciences. Many problems in this domain, such as the analysis of protein structure, exhibit exact or approximate symmetries. It has long been understood that the equations that define a model or natural law should respect the symmetries of the system under study, and that knowledge of symmetries provides a powerful constraint on the space of admissible models. Indeed, in theoretical physics, this idea is enshrined as a fundamental principle, known as Einstein's principle of general covariance. Machine learning, which is, like physics, concerned with the induction of predictive models, is no different: our models must respect known symmetries in order to produce physically meaningful results.\n",
      "\n",
      "A lot of recent work, reviewed in Sec. 2, has focused on the problem of developing equivariant networks, which respect some known symmetry. In this paper, we develop the theory of SE(3) -equivariant networks. This is far from trivial, because SE(3) is both non-commutative and noncompact. Nevertheless, at run-time, all that is required to make a 3D convolution equivariant using our method, is to parameterize the convolution kernel as a linear combination of pre-computed steerable basis kernels. Hence, the 3D Steerable CNN incorporates equivariance to symmetry transformations without deviating far from current engineering best practices.\n",
      "\n",
      "The architectures presented here fall within the framework of Steerable G-CNNs [8, 10, 41, 46], which represent their input as fields over a homogeneous space ( R 3 in this case), and use steerable\n",
      "\n",
      "* Equal Contribution. MG initiated the project, derived the kernel space constraint, wrote the first network implementation and ran the Shrec17 experiment. MW solved the kernel constraint analytically, designed the anti-aliased kernel sampling in discrete space and coded / ran many of the CATH experiments.\n",
      "\n",
      "Source code is available at https://github.com/mariogeiger/se3cnn .\n",
      "\n",
      "32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.\n",
      "\n",
      "Taco Cohen Qualcomm AI Research taco.cohen@gmail.com filters [15, 38] to map between such representations. In this paper, the convolution kernel is modeled as a tensor field satisfying an equivariance constraint, from which steerable filters arise automatically.\n",
      "\n",
      "Weevaluate the 3D Steerable CNN on two challenging problems: prediction of amino acid preferences from atomic environments, and classification of protein structure. We show that a 3D Steerable CNN improves upon state of the art performance on the former task. For the latter task, we introduce a new and challenging dataset, and show that the 3D Steerable CNN consistently outperforms a strong CNN baseline over a wide range of trainingset sizes.\n",
      "\n",
      "## 2 Related Work\n",
      "\n",
      "There is a rapidly growing body of work on neural networks that are equivariant to some group of symmetries [3, 9, 10, 12, 19, 20, 29, 31-33, 37, 43, 47]. At a high level, these models can be categorized along two axes: the group of symmetries they are equivariant to, and the type of geometrical features they use [8]. The class of regular G-CNNs represents the input signal in terms of scalar fields on a group G (e.g. SE(3) ) or homogeneous space G/H (e.g. R 3 = SE(3) / SO(3)) and maps between feature spaces of consecutive layers via group convolutions [9, 30]. Regular G-CNNs can be seen as a special case of steerable (or induced) G-CNNs which represent features in terms of more general fields over a homogeneous space [8, 10, 28, 31, 41]. The models described in this paper are of the steerable kind, since they use general fields over R 3 . These fields typically consist of multiple independently transforming geometrical quantities (vectors, tensors, etc.), and can thus be seen as a formalization of the idea of convolutional capsules [18, 35].\n",
      "\n",
      "Regular 3D G-CNNs operating on voxelized data via group convolutions were proposed in [44, 45]. These architectures were shown to achieve superior data efficiency over conventional 3D CNNs in tasks like medical imaging and 3D model recognition. In contrast to 3D Steerable CNNs, both networks are equivariant to certain discrete rotations only.\n",
      "\n",
      "The most closely related works achieving full SE(3) equivariance are the Tensor Field Network (TFN) [41] and the N-Body networks (NBNs) [27]. The main difference between 3D Steerable CNNs and both TFN and NBN is that the latter work on irregular point clouds, whereas our model operates on regular 3D grids. Point clouds are more general, but regular grids can be processed more efficiently on current hardware. The second difference is that whereas the TFN and NBN use Clebsch-Gordan coefficients to parameterize the network, we simply parameterize the convolution kernel as a linear combination of steerable basis filters. Clebsch-Gordan coefficient tensors have 6 indices, and depend on various phase and normalization conventions, making them tricky to work with. Our implementation requires only a very minimal change from the conventional 3D CNN. Specifically, we compute conventional 3D convolutions with filters that are a linear combination of pre-computed basis filters. Further, in contrast to TFN, we derive this filter basis directly from an equivariance constraint and can therefore prove its completeness.\n",
      "\n",
      "The two dimensional analog of our work is the SE(2) equivariant harmonic network [46]. The harmonic network and 3D steerable CNN use features that transform under irreducible representations of SO(2) resp. SO(3) , and use filters related to the circular resp. spherical harmonics.\n",
      "\n",
      "SE(3) equivariant models were already investigated in classical computer vision and signal processing. In [34, 39], a spherical tensor algebra was utilized to expand signals in terms of spherical tensor fields. In contrast to 3D Steerable CNNs, this expansion is fixed and not learned. Similar approaches were used for detection and crossing preserving enhancement of fibrous structures in volumetric biomedical images [13, 22, 23].\n",
      "\n",
      "## 3 Convolutional feature spaces as fields\n",
      "\n",
      "A convolutional network produces a stack of K n feature maps f k in each layer n . In 3D, we can model the feature maps as (well-behaved) functions f k : R 3 → R . Written another way, we have a map f : R 3 → R K n that assigns to each position x a feature vector f ( x ) that lives in what we call the fiber R K n at x . In practice f will have compact support, meaning that f ( x ) = 0 outside of some compact domain Ω ∈ R 3 . We thus define the feature space F n as the vector space of continuous maps from R 3 to R K n with compact support.\n",
      "\n",
      "In this paper, we impose additional structure on the fibers. Specifically, we assume the fiber consists of a number of geometrical quantities, such as scalars, vectors, and tensors, stacked into a single\n",
      "\n",
      "K n -dimensional vector. The assignment of such a geometrical quantity to each point in space is called a field . Thus, the feature spaces consist of a number of fields, each of which consists of a number of channels (dimensions).\n",
      "\n",
      "Before deriving SE(3) -equivariant networks in Sec. 4 we discuss the transformation properties of fields and the kinds of fields we use in 3D Steerable CNNs.\n",
      "\n",
      "## 3.1 Fields, Transformations and Disentangling\n",
      "\n",
      "What makes a geometrical quantity (e.g. a vector) anything more than an arbitrary grouping of feature channels? The answer is that under rigid body motions, information flows within the channels of a single geometrical quantity, but not between different quantities. This idea is known as Weyl's principle, and has been proposed as a way of formalizing the notion of disentangling [6, 24].\n",
      "\n",
      "Figure 1: To transform a vector field (L) by a 90 ◦ rotation g , first move each arrow to its new position (C), keeping its orientation the same, then rotate the vector itself (R). This is described by the induced representation π = Ind SE(2) SO(3) ρ , where ρ ( g ) is a 3 × 3 rotation matrix that mixes the three coordinate channels.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "As an example, consider the three-dimensional vector field over R 3 , shown in Figure 1. At each point x ∈ R 3 there is a vector f ( x ) of dimension K = 3 . If the field is translated by t , each vector x -t would simply move to a new (translated) position x . When the field is rotated, however, two things happen: the vector at r -1 x is moved to a new (rotated) position x , and each vector is itself rotated by a 3 × 3 rotation matrix ρ ( r ) . Thus, the rotation operator π ( r ) for vector fields is defined as [ π ( r ) f ]( x ) := ρ ( r ) f ( r -1 x ) . Notice that in order to rotate this field, we need all three channels: we cannot rotate each channel independently, because ρ introduces a functional dependency between them. For contrast, consider the common situation where in the input space we have an RGB image with K = 3 channels. Then f ( x ) ∈ R 3 , and the rotation can be described using the same formula ρ ( r ) f ( r -1 x ) if we choose ρ ( r ) = I 3 to be the 3 × 3 identity matrix for all r . Since ρ ( r ) is diagonal for all r , the channels do not get mixed, and so in geometrical terms, we would describe this feature space as consisting of three scalar fields, not a 3D vector field. The RGB channels each have an independent physical meaning, while the x and y coordinate channels of a vector do not.\n",
      "\n",
      "The RGB and 3D-vector cases constitute two examples of fields, each one determined by a different choice of ρ . As one might guess, there is a one-to-one correspondence between the type of field and the type of transformation law (group representation) ρ . Hence, we can speak of a ρ -field.\n",
      "\n",
      "So far, we have concentrated on the behaviour of a field under rotations and translations separately. A 3D rigid body motion g ∈ SE(3) can always be decomposed into a rotation r ∈ SO(3) and a translation t ∈ R 3 , written as g = tr . So the transformation law for a ρ -field is given by the formula\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "The map π is known as the representation of SE(3) induced by the representation ρ of SO(3) , which is denoted by π = Ind SE(3) SO(3) ρ . For more information on induced representations, see [5, 8, 17].\n",
      "\n",
      "## 3.2 Irreducible SO(3) features\n",
      "\n",
      "We have seen that there is a correspondence between the type of field and the type of inducing representation ρ , which describes the rotation behaviour of a single fiber. To get a better understanding of the space of possible fields, we will now define precisely what it means to be a representation of SO(3) , and explain how any such representation can be constructed from elementary building blocks called irreducible representations.\n",
      "\n",
      "A group representation ρ assigns to each element in the group an invertible n × n matrix. Here n is the dimension of the representation, which can be any positive integer (or even infinite). For ρ to be called a representation of G , it has to satisfy ρ ( gg ′ ) = ρ ( g ) ρ ( g ′ ) , where gg ′ denotes the composition of two transformations g, g ′ ∈ G , and ρ ( g ) ρ ( g ′ ) denotes matrix multiplication.\n",
      "\n",
      "To make this more concrete, and to introduce the concept of an irreducible representation, we consider the classical example of a rank-2 tensor (i.e. matrix). A 3 × 3 matrix A transforms under rotations as A ↦→ R ( r ) AR ( r ) T , where R ( r ) is the 3 × 3 rotation matrix representation of the abstract group element r ∈ SO(3) . This can be written in matrix-vector form using the Kronecker / tensor product: vec( A ) ↦→ [ R ( r ) ⊗ R ( r )] vec( A ) ≡ ρ ( r ) vec( A ) . This is a 9 -dimensional representation of SO(3) .\n",
      "\n",
      "One can easily verify that the symmetric and anti-symmetric parts of A remain symmetric respectively anti-symmetric under rotations. This splits R 3 × 3 into 6 - and 3 -dimensional linear subspaces that transform independently. According to Weyl's principle, these may be considered as distinct quantities, even if it is not immediately visible by looking at the coordinates A ij . The 6 -dimensional space can be further broken down, because scalar matrices A ij = αδ ij (which are invariant under rotation) and traceless symmetric matrices also transform independently. Thus a rank-2 tensor decomposes into representations of dimension 1 (trace), 3 (anti-symmetric part), and 5 (traceless symmetric part). In representation-theoretic terms, we have reduced the 9 -dimensional representation ρ into irreducible representations of dimension 1 , 3 and 5 . We can write this as\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where we use ⊕ to denote the construction of a block-diagonal matrix with blocks D l ( r ) , and Q is a change of basis matrix that extracts the trace, symmetric-traceless and anti-symmetric parts of A .\n",
      "\n",
      "More generally, it can be shown that any representation of SO(3) can be decomposed into irreducible representations of dimension 2 l +1 , for l = 0 , 1 , 2 , . . . , ∞ . The irreducible representation acting on this 2 l +1 dimensional space is known as the Wigner-D matrix of order l , denoted D l ( r ) . Note that the Wigner-D matrix of order 4 is a representation of dimension 9, it has the same dimension as the representation ρ acting on A but these are two different representations.\n",
      "\n",
      "Since any SO(3) representation can be decomposed into irreducibles, we only use irreducible features in our networks. This means that the feature vector f ( x ) in layer n is a stack of F n features f i ( x ) ∈ R 2 l i +1 , so that K n = ∑ F n i =1 2 l in +1 .\n",
      "\n",
      "## 4 SE(3) -Equivariant Networks\n",
      "\n",
      "Our general approach to building SE(3) -equivariant networks will be as follows: First, we will specify for each layer n a linear transformation law π n ( g ) : F n → F n , which describes how the feature space F n transforms under transformations of the input by g ∈ SE(3) . Then, we will study the vector space Hom SE(3) ( F n , F n +1 ) of equivariant linear maps (intertwiners) Φ between adjacent feature spaces:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Here Hom( F n , F n +1 ) is the space of linear (not necessarily equivariant) maps from F n to F n +1 .\n",
      "\n",
      "By finding a basis for the space of intertwiners and parameterizing Φ n as a linear combination of basis maps, we can make sure that layer n +1 transforms according to π n +1 if layer n transforms according to π n , thus guaranteeing equivariance of the whole network by induction.\n",
      "\n",
      "As explained in the previous section, fields transform according to induced representations [5, 8, 10, 17]. In this section we show that equivariant maps between induced representations of SE(3) can always be expressed as convolutions with equivariant / steerable filter banks. The space of equivariant filter banks turns out to be a linear subspace of the space of filter banks of a conventional 3D CNN. The filter banks of our network are expanded in terms of a basis of this subspace with parameters corresponding to expansion coefficients.\n",
      "\n",
      "Sec. 4.1 derives the linear constraint on the kernel space for arbitrary induced representations. From Sec. 4.2 on we specialize to representations induced from irreducible representations of SO(3) and derive a basis of the equivariant kernel space for this choice analytically. Subsequent sections discuss choices of equivariant nonlinearities and the actual discretized implementation.\n",
      "\n",
      "## 4.1 The Subspace of Equivariant Kernels\n",
      "\n",
      "A continuous linear map between F n and F n +1 can be written using a continuous kernel κ with signature κ : R 3 × R 3 → R K n +1 × K n , as follows:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Lemma 1. The map f ↦→ κ · f is equivariant if and only if for all g ∈ SE(3) ,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Proof. For this map to be equivariant, it must satisfy κ · [ π 1 ( g ) f ] = π 2 ( g )[ κ · f ] . Expanding the left hand side of this constraint, using g = tr , and the substitution y ↦→ gy , we find:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Equating these, and using that the equality has to hold for arbitrary f ∈ F n , we conclude:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Substitution of x ↦→ gx and right-multiplication by ρ 1 ( r ) -1 yields the result.\n",
      "\n",
      "Theorem 2. A linear map from F n to F n +1 is equivariant if and only if it is a cross-correlation with a rotation-steerable kernel.\n",
      "\n",
      "Proof. Lemma 1 implies that we can write κ in terms of a one-argument kernel, since for g = -x :\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Substituting this into Equation 4, we find\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Cross-correlation is always translation-equivariant, but Eq. 5 still constrains κ rotationally:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "A kernel satisfying this constraint is called rotation-steerable .\n",
      "\n",
      "We note that κglyph[star] f (Eq. 10) is exactly the operation used in a conventional convolutional network, just written in an unconventional form, using a matrix-valued kernel ('propagator') κ : R 3 → R K n +1 × K n .\n",
      "\n",
      "Since Eq. 11 is a linear constraint on the correlation kernel κ , the space of equivariant kernels (i.e. those satisfying Eq. 11) forms a vector space. We will now proceed to compute a basis for this space, so that we can parameterize the kernel as a linear combination of basis kernels.\n",
      "\n",
      "## 4.2 Solving for the Equivariant Kernel Basis\n",
      "\n",
      "As mentioned before, we assume that the K n -dimensional feature vectors f ( x ) = ⊕ i f i ( x ) consist of irreducible features f i ( x ) of dimension 2 l in +1 . In other words, the representation ρ n ( r ) that acts on fibers in layer n is block-diagonal, with irreducible representation D l in ( r ) as the i -th block. This implies that the kernel κ : R 3 → R K n +1 × K n splits into blocks 1 κ jl : R 3 → R (2 j +1) × (2 l +1) mapping between irreducible features. The blocks themselves are by Eq. 11 constrained to transform as\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "1 For more details on the block structure see Sec. 2.7 of [10]\n",
      "\n",
      "For the right hand side,\n",
      "\n",
      "Figure 2: Angular part of the basis for the space of steerable kernels κ jl (for j = l = 1 , i.e. 3D vector fields as input and output). From left to right we plot three 3 × 3 matrices, for j -l ≤ J ≤ j + l i.e. J = 0 , 1 , 2 . Each 3 × 3 matrix corresponds to one learnable parameter per radial basis function ϕ m . A seasoned eye will see the identity, the curl ( ∇∧ ) and the gradient of the divergence ( ∇∇· ).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "To bring this constraint into a more manageable form, we vectorize these kernel blocks to vec( κ jl ( x )) , so that we can rewrite the constraint as a matrix-vector equation 2\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where we used the orthogonality of D l . The tensor product of representations is itself a representation, and hence can be decomposed into irreducible representations. For irreducible SO(3) representations D j and D l of order j and l it is well known [17] that D j ⊗ D l can be decomposed in terms of 2 min( j, l ) + 1 irreducible representations of order 3 | j -l | ≤ J ≤ j + l . That is, we can find a change of basis matrix 4 Q of shape (2 l +1)(2 j +1) × (2 l +1)(2 j +1) such that the representation becomes block diagonal:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Thus, we can change the basis to η jl ( x ) := Q vec( κ jl ( x )) such that constraint 12 becomes\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "The block diagonal form of the representation in this basis reveals that η jl decomposes into 2 min( j, l ) + 1 invariant subspaces of dimension 2 J +1 with separated constraints:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "This is a famous equation for which the unique and complete solution is well-known to be given by the spherical harmonics Y J ( x ) = ( Y J -J ( x ) , . . . , Y J J ( x )) ∈ R 2 J +1 . More specifically, since x lives in R 3 instead of the sphere, the constraint only restricts the angular part of η jl but leaves its radial part free. Therefore, the solutions are given by spherical harmonics modulated by an arbitrary continuous radial function ϕ : R + → R as η jl,J ( x ) = ϕ ( ‖ x ‖ ) Y J ( x/ ‖ x ‖ ) .\n",
      "\n",
      "To obtain a complete basis, we can choose a set of radial basis functions ϕ m : R + → R , and define kernel basis functions η jl,Jm ( x ) = ϕ m ( ‖ x ‖ ) Y J ( x/ ‖ x ‖ ) . Following [43], we choose a Gaussian radial shell ϕ m ( ‖ x ‖ ) = exp ( -1 2 ( ‖ x ‖ -m ) 2 /σ 2 ) in our implementation. The angular dependency at a fixed radius of the basis for j = l = 1 is shown in Figure 2.\n",
      "\n",
      "By mapping each η jl,Jm back to the original basis via Q T and unvectorizing, we obtain a basis κ jl,Jm for the space of equivariant kernels between features of order j and l . This basis is indexed by the radial index m and frequency index J . In the forward pass, we linearly combine the basis kernels as κ jl = ∑ Jm w jl,Jm κ jl,Jm using learnable weights w , and stack them into a complete kernel κ , which is passed to a standard 3D convolution routine.\n",
      "\n",
      "## 4.3 Equivariant Nonlinearities\n",
      "\n",
      "In order for the whole network to be equivariant, every layer, including the nonlinearities, must be equivariant. In a regular G-CNN, any elementwise nonlinearity will be equivariant because the regular representation acts by permuting the activations. In a steerable G-CNN however, special equivariant nonlinearities are required.\n",
      "\n",
      "2 vectorize correspond to flatten it in numpy and the tensor product correspond to np.kron\n",
      "\n",
      "3 There is a fascinating analogy with the quantum states of a two particle system for which the angular momentum states decompose in a similar fashion.\n",
      "\n",
      "4 Q can be expressed in terms of Clebsch-Gordan coefficients, but here we only need to know it exists.\n",
      "\n",
      "Trivial irreducible features, corresponding to scalar fields, do not transform under rotations. So for these features we use conventional nonlinearities like ReLUs or sigmoids. For higher order features we considered tensor product nonlinearities [27] and norm nonlinearities [46], but settled on a novel gated nonlinearity. For each non-scalar irreducible feature κ i n glyph[star] f n -1 ( x ) = f i n ( x ) ∈ R 2 l in +1 in layer n , we produce a scalar gate σ ( γ i n glyph[star] f n -1 ( x )) , where σ denotes the sigmoid function and γ i n is another learnable rotation-steerable kernel. Then, we multiply the feature (a non-scalar field) by the gate (a scalar field): f i n ( x ) σ ( γ i n glyph[star] f n -1 ( x )) . Since γ i n glyph[star] f n -1 is a scalar field, σ ( γ i n glyph[star] f n -1 ) is a scalar field, and multiplying any feature by a scalar is equivariant. See Section 1.3 and Figure 5 in the Supplementary Material for details.\n",
      "\n",
      "## 4.4 Discretized Implementation\n",
      "\n",
      "In a computer implementation of SE(3) equivariant networks, we need to sample both the fields / feature maps and the kernel on a discrete sampling grid in Z 3 . Since this could introduce aliasing artifacts, care is required to make sure that high-frequency filters, corresponding to large values of J , are not sampled on a grid of low spatial resolution. This is particularly important for small radii since near the origin only a small number of pixels is covered per solid angle. In order to prevent aliasing we hence introduce a radially dependent angular frequency cutoff. Aliasing effect originating from the radial part of the kernel basis are counteracted by choosing a smooth Gaussian radial profile as described above. Below we describe how our implementation works in detail.\n",
      "\n",
      "## 4.4.1 Kernel space precomputation\n",
      "\n",
      "Before training, we compute basis kernels κ jl,Jm ( x i ) sampled on a s × s × s cubic grid of points x i ∈ Z 3 , as follows. For each pair of output and input orders j and l we first sample spherical harmonics Y J , | j -l | ≤ J ≤ j + l in a radially independent manner in an array of shape (2 J +1) × s × s × s . Then, we transform the spherical harmonics back to the original basis by multiplying by Q J ∈ R (2 j +1)(2 l +1) × (2 J +1) , consisting of 2 J +1 adjacent columns of Q , and unvectorize the resulting array to unvec( Q J Y J ( x i )) which has shape (2 j +1) × (2 l +1) × s × s × s .\n",
      "\n",
      "The matrix Q itself could be expressed in terms of Clebsch-Gordan coefficients [17], but we find it easier to compute it by numerically solving Eq. 14.\n",
      "\n",
      "The radial dependence is introduced by multiplying the cubes with each windowing function ϕ m . We use integer means m = 0 , . . . , glyph[floorleft] s/ 2 glyph[floorright] and a fixed width of σ = 0 . 6 for the radial Gaussian windows.\n",
      "\n",
      "Sampling high-order spherical harmonics will introduce aliasing effects, particularly near the origin. Hence, we introduce a radius-dependent bandlimit J m max , and create basis functions only for | j -l | ≤ J ≤ J m max . Each basis kernel is scaled to unit norm for effective signal propagation [43]. In total we get B = ∑ glyph[floorleft] s/ 2 glyph[floorright] m =0 ∑ J m max | j -l | 1 ≤ ( glyph[floorleft] s/ 2 glyph[floorright] +1)(2min( j, l ) + 1) basis kernels mapping between fields of order j and l , and thus a basis array of shape B × (2 j +1) × (2 l +1) × s × s × s .\n",
      "\n",
      "## 4.4.2 Spatial dimension reduction\n",
      "\n",
      "We found that the performance of the Steerable CNN models depends critically on the way of downsampling the fields. In particular, the standard procedure of downsampling via strided convolutions performed poorly compared to smoothing features maps before subsampling. We followed [1] and experiment with applying a low pass filtering before performing the downsampling step which can be implemented either via an additional strided convolution with a Gaussian kernel or via an average pooling. We observed significant improvements of the rotational equivariance by doing so. See Table 2 in the Supplementary Material for a comparison between performances with and without low pass filtering.\n",
      "\n",
      "## 4.4.3 Forward pass\n",
      "\n",
      "At training time, we linearly combine the basis kernels using learned weights, and stack them together into a full filter bank of shape K n +1 × K n × s × s × s , which is used in a standard convolution routine. Once the network is trained, we can convert the network to a standard 3D CNN by linearly combining the basis kernels with the learned weights, and storing only the resulting filter bank.\n",
      "\n",
      "## 5 Experiments\n",
      "\n",
      "We performed several experiments to gauge the performance and data efficiency of our model.\n",
      "\n",
      "## 5.1 Tetris\n",
      "\n",
      "In order to confirm the equivariance of our model, we performed a variant of the Tetris experiments reported by [41]. We constructed a 4-layer 3D Steerable CNN and trained it to classify 8 kinds of Tetris blocks, stored as voxel grids, in a fixed orientation. Then we test on Tetris blocks rotated by random rotations in SO(3) . As expected, the 3D Steerable CNN generalizes over rotations and achieves 99 ± 2% accuracy on the test set. In contrast, a conventional CNN is not able to generalize over larger unseen rotations and gets a result of only 27 ± 7% . For both networks we repeated the experiment over 17 runs.\n",
      "\n",
      "## 5.2 3D model classification\n",
      "\n",
      "Moving beyond the simple Tetris blocks, we next considered classification of more complex 3D objects. The SHREC17 task [36], which contains 51300 models of 3D shapes belonging to 55 classes (chair, table, light, oven, keyboard, etc), has a 'perturbed' category where images are arbitrarily rotated, making it a well-suited test case for our model. We converted the input into voxel grids of size 64x64x64, and used an architecture similar to the Tetris case, but with an increased number of layers (see Table 3 in the Supplementary Material). Although we have not done extensive fine-tuning on this dataset, we find our model to perform comparably to the current state of the art, see Figure 3 and Table 4 in the Supplementary Material.\n",
      "\n",
      "## 5.3 Visualization of the equivariance property\n",
      "\n",
      "We made a movie to show the action of rotating the input on the internal fields. We found that the action are remarkably stable. A visualization is provided in https://youtu.be/ENLJACPHSEA .\n",
      "\n",
      "## 5.4 Amino acid environments\n",
      "\n",
      "Next, we considered the task of predicting amino acid preferences from the atomic environments, a problem which has been studied by several groups in the last year [4, 42]. Since physical forces are primarily a function of distance, one of the previous studies argued for the use of a concentric grid, investigated strategies for conducting convolutions on such grids, and reported substantial gains when using such convolutions over a standard 3D convolution in a regular grid ( 0 . 56 vs 0 . 50 accuracy) [4].\n",
      "\n",
      "Since the classification of molecular environments involves the recognition of particular interactions between atoms (e.g. hydrogen bonds), one would expect rotational equivariant convolutions to be more suitable for the extraction of relevant features. We tested this hypothesis by constructing the exact same network as used in the original study, merely replacing the conventional convolutional layers with equivalent 3D steerable convolutional layers. Since the latter use substantially fewer parameters per channel, we chose to use the same number of fields as the number of channels in the original model, which still only corresponds to roughly half the number of parameters (32.6M vs 61.1M (regular grid), and 75.3M (concentric representation)). Without any alterations to the model and using the same training procedure (apart from adjustment of learning rate and regularization factor), we obtained a test accuracy of 0 . 58 , substantially outperforming the conventional CNN on this task, and also providing an improvement over the state-of-the-art on this problem.\n",
      "\n",
      "## 5.5 CATH: Protein structure classification\n",
      "\n",
      "The molecular environments considered in the task above are oriented based on the protein backbone. Similar to standard images, this implies that the images have a natural orientation. For the final experiment, we wished to investigate the performance of our Steerable 3D convolutions on a problem domain with full rotational invariance, i.e. where the images have no inherent orientation. For this purpose, we consider the task of classifying the overall shape of protein structures.\n",
      "\n",
      "Figure 3: Shrec17 results[2, 7, 14, 16, 25, 36, 40]. Comparison of different architectures by number of parameters and score. See Table 4 in the Supplementary Material for all the details.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "We constructed a new data set, based on the CATH protein structure classification database [11], version 4.2 (see http://cathdb.info/browse/tree ). The database is a classification hierarchy containing millions of experimentally determined protein domains at different levels of structural detail. For this experiment, we considered the CATH classification-level of \"architecture\", which splits proteins based on how protein secondary structure elements are organized in three dimensional space. Predicting the architecture from the raw protein structure thus poses a particularly challenging task for the model, which is required to not only detect the secondary structure elements at any orientation in the 3D volume, but also detect how these secondary structures orient themselves relative to one another. We limited ourselves to architectures with at least 500 proteins, which left us with 10 categories. For each of these, we balanced the data set so that all categories are represented by the same number of structures (711), also ensuring that no two proteins within the set have more than 40% sequence identity. See Supplementary Material for details. The new dataset is available at https://github.com/wouterboomsma/cath\\_datasets .\n",
      "\n",
      "We first established a state-of-the-art baseline consisting of a conventional 3D CNN, by conducting a range of experiments with various architectures. We converged on a ResNet34-inspired architecture with half as many channels as the original, and global pooling at the end. The final model consists of 15 , 878 , 764 parameters. For details on the experiments done to obtain the baseline, see Supplementary Material.\n",
      "\n",
      "Following the same ResNet template, we then constructed a 3D Steerable network by replacing each layer by an equivariant version, keeping the number of 3D channels fixed. The channels are allocated such that there is an equal number of fields of order l = 0 , 1 , 2 , 3 in each layer except the last, where we only used scalar fields ( l = 0 ). This network contains only 143 , 560 parameters, more than a factor hundred less than the baseline.\n",
      "\n",
      "We used the first seven of the ten splits for training, the eighth for validation and the last two for testing. The data set was augmented by randomly rotating the input proteins whenever they were presented to the model during training. Note that due to their rotational equivariance, 3D Steerable CNNs benefit only marginally from rotational data augmentation compared to the baseline CNN. We train the models for 100 epochs using the Adam optimizer [26], with an exponential learning rate decay of 0 . 94 per epoch starting after an initial burn-in phase of 40 epochs.\n",
      "\n",
      "Despite having 100 times fewer parameters, a comparison between the accuracy on the test set shows a clear benefit to the 3DSteerable CNN on this dataset (Figure 4, leftmost value). We proceeded with an investigation of the dependency of this performance on the size of the dataset by considering reductions of the size of each training split in the dataset by increasing powers of two, maintaining the same network architecture but re-optimizing the regularization parameters of the networks. We found that the proposed model outperforms the baseline even when trained on a fraction of the training set size. The results further demonstrate the accuracy improvements across these reductions to be robust (Figure 4).\n",
      "\n",
      "## 6 Conclusion\n",
      "\n",
      "In this paper we have presented 3D Steerable CNNs, a class of SE(3) -equivariant networks which represents data in terms of various kinds of fields over R 3 . We have presented a comprehensive theory of 3D Steerable CNNs, and have proven that convolutions with SO(3) -steerable filters provide the most general way of mapping between fields in an equivariant manner, thus establishing SE(3) -equivariant networks as a universal class of architectures. 3D Steerable CNNs require only a minor adaptation to the code of a 3D CNN, and can be converted to a conventional 3D CNN after training. Our results show that 3D Steerable CNNs are indeed equivariant, and that they show excellent accuracy and data efficiency in amino acid propensity prediction and protein structure classification.\n",
      "\n",
      "Figure 4: Accuracy on the CATH test set as a function of increasing reduction in training set size.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## References\n",
      "\n",
      "- [1] Aharon Azulay and Yair Weiss. Why do deep convolutional networks generalize so poorly to small image transformations? arXiv preprint arXiv:1805.12177 , abs/1805.12177, 2018.\n",
      "- [2] Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, and Longin Jan Latecki. Gift: A real-time and scalable 3d shape search engine. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2016.\n",
      "- [3] Erik J Bekkers, Maxime W Lafarge, Mitko Veta, Koen AJ Eppenhof, and Josien PW Pluim. Roto-translation covariant convolutional networks for medical image analysis. arXiv preprint arXiv:1804.03393 , 2018.\n",
      "- [4] Wouter Boomsma and Jes Frellsen. Spherical convolutions and their application in molecular modelling. In Advances in Neural Information Processing Systems 30 , pages 3436-3446. 2017.\n",
      "- [5] Tullio Ceccherini-Silberstein, A Machì, Fabio Scarabotti, and Filippo Tolli. Induced representations and mackey theory. Journal of Mathematical Sciences , 156(1):11-28, 2009.\n",
      "- [6] Taco Cohen and Max Welling. Learning the irreducible representations of commutative lie groups. In Proceedings of the 31st International Conference on Machine Learning (ICML) , volume 31, pages 1755-1763, 2014.\n",
      "- [7] Taco S. Cohen, Mario Geiger, Jonas Köhler, and Max Welling. Spherical CNNs. In International Conference on Learning Representations (ICLR) , 2018.\n",
      "- [8] Taco S Cohen, Mario Geiger, and Maurice Weiler. Intertwiners between induced representations (with applications to the theory of equivariant neural networks). arXiv preprint arXiv:1803.10743 , 2018.\n",
      "- [9] Taco S Cohen and Max Welling. Group equivariant convolutional networks. In Proceedings of The 33rd International Conference on Machine Learning (ICML) , volume 48, pages 2990-2999, 2016.\n",
      "- [10] Taco S Cohen and Max Welling. Steerable CNNs. In International Conference on Learning Representations (ICLR) , 2017.\n",
      "- [11] Natalie L Dawson, Tony E Lewis, Sayoni Das, Jonathan G Lees, David Lee, Paul Ashford, Christine A Orengo, and Ian Sillitoe. CATH: an expanded resource to predict protein function through structure and sequence. Nucleic acids research , 45(D1):D289-D295, 2016.\n",
      "- [12] Sander Dieleman, Jeffrey De Fauw, and Koray Kavukcuoglu. Exploiting cyclic symmetry in convolutional neural networks. In International Conference on Machine Learning (ICML) , 2016.\n",
      "- [13] Remco Duits and Erik Franken. Left-invariant diffusions on the space of positions and orientations and their application to crossing-preserving smoothing of hardi images. International Journal of Computer Vision , 92(3):231-264, 2011.\n",
      "- [14] Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, and Kostas Daniilidis. 3D object classification and retrieval with Spherical CNNs. arXiv preprint arXiv:1711.06721 , abs/1711.06721, 2017.\n",
      "- [15] William T. Freeman and Edward H Adelson. The design and use of steerable filters. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence , (9):891-906, 1991.\n",
      "- [16] Takahiko Furuya and Ryutarou Ohbuchi. Deep aggregation of local 3d geometric features for 3d model retrieval. In Proceedings of the British Machine Vision Conference (BMVC) , pages 121.1-121.12, September 2016.\n",
      "- [17] David Gurarie. Symmetries and Laplacians: Introduction to Harmonic Analysis, Group Representations and Applications . 1992.\n",
      "- [18] Geoffrey Hinton, Nicholas Frosst, and Sabour Sara. Matrix capsules with EM routing. In International Conference on Learning Representations (ICLR) , 2018.\n",
      "\n",
      "- [19] Emiel Hoogeboom, Jorn W T Peters, Taco S Cohen, and Max Welling. HexaConv. In International Conference on Learning Representations (ICLR) , 2018.\n",
      "- [20] Truong Son Hy, Shubhendu Trivedi, Horace Pan, Brandon M. Anderson, and Risi Kondor. Predicting molecular properties with covariant compositional networks. The Journal of Chemical Physics , 148(24):241745, 2018.\n",
      "- [21] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning , volume 37 of Proceedings of Machine Learning Research , pages 448-456, Lille, France, 07-09 Jul 2015. PMLR.\n",
      "- [22] Michiel HJ Janssen, Tom CJ Dela Haije, Frank C Martin, Erik J Bekkers, and Remco Duits. The hessian of axially symmetric functions on se (3) and application in 3d image analysis. In International Conference on Scale Space and Variational Methods in Computer Vision , pages 643-655. Springer, 2017.\n",
      "- [23] Michiel HJ Janssen, Augustus JEM Janssen, Erik J Bekkers, Javier Oliván Bescós, and Remco Duits. Design and processing of invertible orientation scores of 3d images. Journal of Mathematical Imaging and Vision , pages 1-32, 2018.\n",
      "- [24] Kenichi Kanatani. Group-Theoretical Methods in Image Understanding . Springer-Verlag New York, Inc., Secaucus, NJ, USA, 1990.\n",
      "- [25] Asako Kanezaki, Yasuyuki Matsushita, and Yoshifumi Nishida. Rotationnet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints, 2018.\n",
      "- [26] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations (ICLR) , 2015.\n",
      "- [27] Risi Kondor. N-body networks: a covariant hierarchical neural network architecture for learning atomic potentials. arXiv preprint arXiv:1803.01588 , 2018.\n",
      "- [28] Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch-gordan nets: a fully fourier space spherical convolutional neural network. In Neural Information Processing Systems (NIPS) , 2018.\n",
      "- [29] Risi Kondor, Hy Truong Son, Horace Pan, Brandon Anderson, and Shubhendu Trivedi. Covariant compositional networks for learning graphs. In International Conference on Learning Representations (ICLR) , 2018.\n",
      "- [30] Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural networks to the action of compact groups. arXiv preprint arXiv:1802.03690 , 2018.\n",
      "- [31] Diego Marcos, Michele Volpi, Nikos Komodakis, and Devis Tuia. Rotation equivariant vector field networks. In International Conference on Computer Vision (ICCV) , 2017.\n",
      "- [32] Chris Olah. Groups and group convolutions. https://colah.github.io/posts/ 2014-12-Groups-Convolution/ , 2014.\n",
      "- [33] Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos. Equivariance through parametersharing. arXiv preprint arXiv:1702.08389 , 2017.\n",
      "- [34] Marco Reisert and Hans Burkhardt. Efficient tensor voting with 3d tensorial harmonics. In Computer Vision and Pattern Recognition Workshops, 2008. CVPRW'08. IEEE Computer Society Conference on , pages 1-7. IEEE, 2008.\n",
      "- [35] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. In Advances in Neural Information Processing Systems 30 , pages 3856-3866. 2017.\n",
      "- [36] Manolis Savva, Fisher Yu, Hao Su, Asako Kanezaki, Takahiko Furuya, Ryutarou Ohbuchi, Zhichao Zhou, Rui Yu, Song Bai, Xiang Bai, Masaki Aono, Atsushi Tatsuma, S. Thermos, A. Axenopoulos, G. Th. Papadopoulos, P. Daras, Xiao Deng, Zhouhui Lian, Bo Li, Henry Johan, Yijuan Lu, and Sanjeev Mk. Large-Scale 3D Shape Retrieval from ShapeNet Core55. In Ioannis Pratikakis, Florent Dupont, and Maks Ovsjanikov, editors, Eurographics Workshop on 3D Object Retrieval . The Eurographics Association, 2017.\n",
      "\n",
      "- [37] Laurent Sifre and Stephane Mallat. Rotation, scaling and deformation invariant scattering for texture discrimination. IEEE conference on Computer Vision and Pattern Recognition (CVPR) , 2013.\n",
      "- [38] Eero P Simoncelli and William T Freeman. The steerable pyramid: A flexible architecture for multi-scale derivative computation. In Image Processing, 1995. Proceedings., International Conference on , volume 3, pages 444-447. IEEE, 1995.\n",
      "- [39] Henrik Skibbe. Spherical Tensor Algebra for Biomedical Image Analysis . PhD thesis, 2013.\n",
      "- [40] Atsushi Tatsuma and Masaki Aono. Multi-fourier spectra descriptor and augmentation with spectral clustering for 3d shape retrieval. The Visual Computer , 25(8):785-804, Aug 2009.\n",
      "- [41] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor Field Networks: Rotation-and Translation-Equivariant Neural Networks for 3D Point Clouds. arXiv preprint arXiv:1802.08219 , 2018.\n",
      "- [42] Wen Torng and Russ B Altman. 3D deep convolutional neural networks for amino acid environment similarity analysis. BMC Bioinformatics , 18(1):302, June 2017.\n",
      "- [43] Maurice Weiler, Fred A Hamprecht, and Martin Storath. Learning steerable filters for rotation equivariant CNNs. In Computer Vision and Pattern Recognition (CVPR) , 2018.\n",
      "- [44] Marysia Winkels and Taco S Cohen. 3D G-CNNs for Pulmonary Nodule Detection. arXiv preprint arXiv:1804.04656 , 2018.\n",
      "- [45] Daniel Worrall and Gabriel Brostow. CubeNet: Equivariance to 3D Rotation and Translation. arXiv preprint arXiv:1804.04458 , 2018.\n",
      "- [46] Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Harmonic networks: Deep translation and rotation equivariance. In Computer Vision and Pattern Recognition (CVPR) , 2017.\n",
      "- [47] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov, and Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems , pages 3391-3401, 2017.\n",
      "\n",
      "## Supplementary material 3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data\n",
      "\n",
      "## 1 Design choices\n",
      "\n",
      "## 1.1 Feature types and multiplicities\n",
      "\n",
      "The choice of the types and multiplicities of the features is a hyperparameter of our network comparable to the choice of channels in a conventional CNN. As in the latter we follow the logic of doubling the number of multiplicities when downsampling the feature maps. The types and multiplicities of the network's input and output are prescribed by the problem to be solved. If one uses only scalar fields, then the kernels can only be isotropic, higher order representation allows more complex kernels. A more detailed investigation of the choice of these hyperparameters is left open for future work.\n",
      "\n",
      "## 1.2 Normalization\n",
      "\n",
      "We implemented an equivariant version of batch normalization [21]. For scalar fields, our implementation matches with the usual batch normalization. For the nonscalar fields we normalize them with the average of their norms:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where B is the batch and i, j are the batch indices.\n",
      "\n",
      "In order to reduce the memory consumption, we merged the batch normalization operation with the convolution\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "## 1.3 Nonlinearities\n",
      "\n",
      "The nonlinearities of an equivariant network need to be adapted to be equivariant themselves. Note that the domain and codomain of the nonlinearities might transform under different representations. We give an overview over the nonlinearities with which we experimented in the following paragraphs.\n",
      "\n",
      "Elementwise nonlinearities Scalar features do not transform under rotations. As a consequence, they can be acted on by elementwise nonlinearities as in conventional CNNs. We chose ReLU nonlinearities for all scalar features except those which are used as gates (see below).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Norm nonlinearity The representations we are considering are all orthogonal and hence preserve the norm of feature vectors:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "It follows that any nonlinearity applied to the norm of the feature commutes with the group transformation. Denoting a positive bias by β ∈ R + , we experimented with norm nonlinearites of the\n",
      "\n",
      "form\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Intuitively, the bias acts as a threshold on the norm of the feature vectors, setting small vectors to zero and preserving the orientation of large feature vectors. In practice, this kind of nonlinearity tended to converge slower than the gated nonlinearities, therefore we did not use them in our final experiments. This issue might be related to the problem of finding a suitable initialization of the learned biases for which we could not derive a proper scale. Norm nonlinearities were considered before in [46].\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Tensor product nonlinearity The tensor product of two fields f 1 and f 2 is in index notation defined by\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "This operation is nonlinear and equivariant and hence can be used in neural networks. We denote this nonlinearity by\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Note that the output of this operation transforms under the tensor product representation ρ ⊗ ρ of the input representations ρ . In our framework we could perform a change of basis Q defined by Q [ ρ ⊗ ρ ] Q -1 = ⊕ j D j to obtain features transforming under irreducible representations.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Gated nonlinearity The gated nonlinearity acts on any feature vector by scaling it with a data dependent gate. We compute the gating scalars for each output feature via a sigmoid nonlinearity σ : F scalar n →F scalar n acting on an associated scalar feature. Figure 5 shows how the gated nonlinaritiy is coupled with the convolution operation. One can view the gated nonlinearity as a special case of the norm nonlinearity since it operates by changing the length of the feature vector. Simultaneously it can also be seen as a tensor product nonlinearity where one of the two fields as a scalar field. We found that the gated nonlinearities work in practice better than the the other options described above.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Figure 5: A gated nonlinearity requires one extra scalar field (represented by gray circles with an I ) per nonscalar output fields (represented by circles with a ρ ). Specifically, the number of scalar output channels for the preceding convolution operator is increased by the number of features acted on by gated nonlinearities, and the extra scalar fields are computed in the same way as any other scalar field. We use sigmoid for the gate fields. In this picture, there is one scalar field in the output. It is activated with a ReLU.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## 2 Reduced parameter cost of 3D Steerable CNNs\n",
      "\n",
      "In the main paper, we demonstrated that the 3D Steerable CNN outperforms a conventional CNN despite having many fewer parameters. To ensure that the reduced number of parameters would not be an advantage also for the conventional CNN (due to overfitting with the highcapacity network), we trained a series of conventional CNNs with reduced number of filters in each layer (Figure 6). Note that the relative performance gain of our model increases dramatically if we restrict the conventional CNN to use the same number of parameters as the Steerable CNN.\n",
      "\n",
      "## 3 The Tetris experiment\n",
      "\n",
      "The architecture used for the Tetris experiment has 4 hidden layers, the kernel size is 5 and the padding is 4. We didn't use batch normalization. Table 1 shows the multiplicities of the fields representations and the sizes of the fields. We compare with a regular CNN that has the same feature map sizes. The CNN is like the SE3 network simply without the constraint of being equivariant for rotation. It has therefore much more parameters since its kernels are unconstrained. The SE3 network has 41k parameters and the CNN has 6M parameters.\n",
      "\n",
      "Table 1: Architecture of the network for the Tetris experiment. Between layer 1-2 and 2-3 there is a stride of 2. Between layer 4 and the output there is a global average pooling.\n",
      "\n",
      "|         |   l = 0 | l = 1   | l = 2   | l = 3   | size   |   CNN features |\n",
      "|---------|---------|---------|---------|---------|--------|----------------|\n",
      "| input   |       1 |         |         |         | 36 3   |              1 |\n",
      "| layer 1 |       4 | 4       | 4       | 1       | 40 3   |             43 |\n",
      "| layer 2 |      16 | 16      | 16      |         | 22 3   |            144 |\n",
      "| layer 3 |      32 | 16      | 16      |         | 13 3   |            160 |\n",
      "| layer 4 |     128 |         |         |         | 17 3   |            128 |\n",
      "| output  |       8 |         |         |         | 1      |              8 |\n",
      "\n",
      "Figure 6: Performance of our 3D Steerable CNN compared to a conventional 3D CNN with varying numbers of filters.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Table 2: Test accuracy to classify rotated pieces of Tetris. Average and standard deviation over 17 runs.\n",
      "\n",
      "| low pass filter   | disabled   | enabled   |\n",
      "|-------------------|------------|-----------|\n",
      "| CNN               | 24% ± 4%   | 27% ± 7%  |\n",
      "| SE3               | 36% ± 6%   | 99% ± 2%  |\n",
      "\n",
      "## 4 3D Model classification\n",
      "\n",
      "To find the model we ran 10 different models by changing depth, multiplicities, dropout, low pass filter or stride and two initialization method.\n",
      "\n",
      "For this experiment we used a kernel size of 5 and a padding of 4. We used batch normalization. In this architecture we did't used the low pass filters. Table 3 shows the multiplicities of the fields representations and the sizes of the fields. This network has 142k parameters.\n",
      "\n",
      "We converted the 3d models into voxels of size 64 × 64 × 64 with the following code https: //github.com/mariogeiger/obj2voxel .\n",
      "\n",
      "Table 4 compares our results with results of the original competition and two other articles [7, 14].\n",
      "\n",
      "Table 3: Architecture of the network for the 3D Model experiment. Where the size decrease we used a stride of 2. Between the last hidden layer and the output there is a global average pooling.\n",
      "\n",
      "|         |   l = 0 | l =   | l =   | size   |\n",
      "|---------|---------|-------|-------|--------|\n",
      "| input   |       1 |       |       | 64 3   |\n",
      "| layer 1 |       8 | 4     | 2     | 34 3   |\n",
      "| layer 2 |       8 | 4     | 2     | 38 3   |\n",
      "| layer 3 |      16 | 8     | 4     | 21 3   |\n",
      "| layer 4 |      16 | 8     | 4     | 25 3   |\n",
      "| layer 5 |      32 | 16    | 8     | 15 3   |\n",
      "| layer 6 |      32 | 16    | 8     | 19 3   |\n",
      "| layer 7 |      32 | 16    | 8     | 12 3   |\n",
      "| layer 8 |     512 |       |       | 16 3   |\n",
      "| output  |      55 |       |       | 1      |\n",
      "\n",
      "Table 4: Results of the SHREC17 experiment.\n",
      "\n",
      "|               | micro   | micro   | micro   | macro   | macro   | macro   | total   | input size   |        |\n",
      "|---------------|---------|---------|---------|---------|---------|---------|---------|--------------|--------|\n",
      "|               | P@R     | R@N     | mAP     | P@R     | R@N     | mAP     | score   |              | params |\n",
      "| Furuya [16]   | 0.814   | 0.683   | 0.656   | 0.607   | 0.539   | 0.476   | 1.13    | 126 × 10 3   | 8.4M   |\n",
      "| Esteves [14]  | 0.717   | 0.737   | 0.685   | 0.450   | 0.550   | 0.444   | 1.13    | 2 × 64 2     | 0.5M   |\n",
      "| Tatsuma [40]  | 0.705   | 0.769   | 0.696   | 0.424   | 0.563   | 0.418   | 1.11    | 38 × 224 2   | 3M     |\n",
      "| Ours          | 0.704   | 0.706   | 0.661   | 0.490   | 0.549   | 0.449   | 1.11    | 1 × 64 3     | 142k   |\n",
      "| Cohen [7]     | 0.701   | 0.711   | 0.676   | -       | -       | -       | -       | 6 × 128 2    | 1.4M   |\n",
      "| Zhou [2]      | 0.660   | 0.650   | 0.567   | 0.443   | 0.508   | 0.406   | 0.97    | 50 × 224 2   | 36M    |\n",
      "| Kanezaki [25] | 0.655   | 0.652   | 0.606   | 0.372   | 0.393   | 0.327   | 0.93    | -            | 61M    |\n",
      "| Deng [36]     | 0.418   | 0.717   | 0.540   | 0.122   | 0.667   | 0.339   | 0.85    | -            | 138M   |\n",
      "\n",
      "## 5 The CATH experiment\n",
      "\n",
      "## 5.1 The data set\n",
      "\n",
      "The protein structures used in the CATH study were simplified to include only C α atoms (one atom per amino acid in the backbone), and placed at the center of a 50 3 vx grid, where each voxel spans 0 . 2 nm. The values of the voxels were set to the densities arising from placing a Gaussian at each atom position, with a standard deviation of half the voxel width. Since we limit ourselves to grids of size 5 nm, we exclude proteins which expand beyond a 5 nm sphere centered around their center of mass. This constraint is only violated by a small fraction of the original dataset, and thus constitutes no severe restriction.\n",
      "\n",
      "For training purposes, we constructed a 10-fold split of the data. To rule out any overlap between the splits (in addition to the 40% homology reduction), we further introduce a constraint that any two\n",
      "\n",
      "members from different splits are guaranteed to originate from different categories at the \"superfamily\" level in the CATH hierarchy (the lowest level in the hierarchy), and all splits are guaranteed to have members from all 10 architectures. Further details about the data set are provided on the website ( https://github.com/wouterboomsma/cath\\_datasets ).\n",
      "\n",
      "## 5.2 Establishing a state-of-the-art baseline\n",
      "\n",
      "The baseline 3D CNN architecture for the CATH task was determined through a range of experiments, ultimately converging on a ResNet34-like architecture, with half the number of channels compared to the original implementation (but with an extra spatial dimension), and using a global pooling at the end to obtain translational invariance. After establishing the architecture, we conducted additional experiments to establish good values for the learning and drop-out rates (both in the linear and in the convolutional layers). We settled on a 0 . 01 dropout rate in the convolutional layers, and L1 and L2 regularization values of 10 -7 . The final model consists of 15 , 878 , 764 parameters.\n",
      "\n",
      "## 5.3 Architecture details\n",
      "\n",
      "Following the same ResNet template, we then constructed a 3D Steerable network, by replacing each layer with its equivariant equivalent. In contrast to the model architecture for the amino acid environment, we here opted for a minimal architecture, where we use exactly the same number of 3D channels as in the baseline model, which leads to a model with the following block structure: (2 , 2 , 2 , 2) , (((2 , 2 , 2 , 2) × 2) × 3) , (((4 , 4 , 4 , 4) × 2) × 4) , (((8 , 8 , 8 , 8) × 2) × 6) , (((16 , 16 , 16 , 16) × 2) × 2 + ((256 , 0 , 0 , 0)) . Here the 4-tuples represent fields of order l = 0 , 1 , 2 , 3 , respectively. The final block deviates slightly from the rest, since we wish to reduce to a scalar representation prior to the pooling. Optimal regularization settings were found to be a capsule-wide convolutional dropout rate of 0 . 1 , and L1 and L2 regularization values of 10 -8 . 5 . In this minimal setup, the model contains only 143 , 560 parameters, more than a factor hundred less than the baseline.\n",
      "Document 20:\n",
      "## TransCT: Dual-path Transformer for Low Dose Computed Tomography\n",
      "\n",
      "Zhicheng Zhang 1( GLYPH&lt;12&gt; ) , Lequan Yu 1 , 2 , Xiaokun Liang 1 , Wei Zhao 1 , and Lei Xing 1\n",
      "\n",
      "1\n",
      "\n",
      "Department of Radiation Oncology, Stanford University, Stanford, USA.\n",
      "\n",
      "zzc623@stanford.edu\n",
      "\n",
      "2 Department of Statistics and Actuarial Science, The University of Hong Kong\n",
      "\n",
      "Abstract. Low dose computed tomography (LDCT) has attracted more and more attention in routine clinical diagnosis assessment, therapy planning, etc. , which can reduce the dose of X-ray radiation to patients. However, the noise caused by low X-ray exposure degrades the CT image quality and then affects clinical diagnosis accuracy. In this paper, we train a transformer-based neural network to enhance the final CT image quality. To be specific, we first decompose the noisy LDCT image into two parts: high-frequency (HF) and low-frequency (LF) compositions. Then, we extract content features ( X L c ) and latent texture features ( X L t ) from the LF part, as well as HF embeddings ( X H f ) from the HF part. Further, we feed X L t and X H f into a modified transformer with three encoders and decoders to obtain well-refined HF texture features. After that, we combine these well-refined HF texture features with the pre-extracted X L c to encourage the restoration of high-quality LDCT images with the assistance of piecewise reconstruction. Extensive experiments on Mayo LDCT dataset show that our method produces superior results and outperforms other methods.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Computed tomography (CT) system, as noninvasive imaging equipment, has been widely used for medical diagnosis and treatment [18,16]. However, concerns about the increase of X-ray radiation risk have become an unavoidable problem for all CT vendors and medical institutions [2]. Since x-ray imaging is mainly based on a photon-noise-dominated process [27], lowering the X-ray dose will result in degraded CT images. Therefore, on the premise of ensuring CT image quality, how to reduce the X-ray radiation dose as far as possible becomes a promising and significant research topic [2].\n",
      "\n",
      "Compared to sparse or limited-view CT [33] and other hardware-based strategies [34], lowering single X-ray exposure dose [11,22] is the most convenient and affordable method. To obtain high-quality LDCT images, previous works can be mainly classified into two categories: model-based and data-driven methods. The key to model-based methods is to use a mathematical model for the description of each process of CT imaging: noise characteristics in the sinogram domain [15,30], image prior information in the image domain, such as sparsity\n",
      "\n",
      "in gradient domain [13] and low rank [3], as well as defects in CT hardware systems [32]. This kind of methods are independent of a large training dataset, while the accuracy of the model depiction limits its performance.\n",
      "\n",
      "With the development of deep learning in medical image reconstruction and analysis [29], many data-driven works have been proposed to reconstruct LDCT images with convolution neural network (CNN) [25]. Kang et al. proposed a CNN-based neural network with the assistance of directional wavelets, suggesting the potential of deep learning technique in LDCT. Similarly, Chen et al. employed residual learning to extract noise in the LDCT images and obtain superior performance [5]. However, these methods need FBP-reconstructed LDCT images as the inputs, which belong to image post-processing. To get rid of the influence of traditional analytic algorithms ( e.g. FBP), Zhu et al. suggested that 'AUTOMAP' was a direct reconstruction method from the measurement data to the final image [35]. Then again, the first fully-connected layer as domain transform has a huge memory requirement, which makes AUTOMAP unavailable for large-scale CT reconstruction [24]. Besides, many works with the combination of iterative reconstruction and deep learning have been proposed as deep unrolled approaches. This kind of method used CNNs as special regularizations plugged into conventional iterative reconstruction. They not only inherit the advantages of the convenient calculation of system matrix in conventional algorithms but also get rid of the complicated manual design regularization [10,7,11].\n",
      "\n",
      "Despite the success of CNNs in LDCT reconstruction, CNN-based methods heavily rely on cascaded convolution layers to extract high-level features since the convolution operation has its disadvantage of a limited receptive field that only perceives local areas. Moreover, this disadvantage makes it difficult for CNN-based methods to make full of the similarity across large regions [26,31], which makes CNN-based methods less efficient in modeling various structural information in CT images [14]. To overcome this limitation, Transformers [23], which solely depend on attention mechanisms instead, have emerged as a powerful architectures in many fields, such as natural language processing (NLP) [8], image segmentation [6],image recognition [9], etc. In addition to these high-level tasks, Transformer has also been tentatively investigated for some lower-level tasks [28,4], which can model all pairwise interactions between image regions and capture long-range dependencies by computing interactions between any two positions, regardless of their positional distance.\n",
      "\n",
      "For image denoising, noise is mainly contained in the high-frequency subband. Moreover, the remaining low-frequency sub-band not only contains the main image content, but also contains the weakened image textures, which are noise-free. These weakened image textures can be used to help noise removal in the high-frequency sub-band. Inspired by this observation, in this paper, we present the first work, TransCT, to explore the potential of transformers in LDCT imaging. Firstly, we decompose the noisy LDCT image into highfrequency (HF) and low-frequency (LF) parts. To remove the image noise on the premise of retaining the image content, we extract the corresponding content features ( X L c ) and latent texture features ( X L t ) from the LF part. Simultane-\n",
      "\n",
      "Fig. 1. The overall architecture of the proposed TransCT. ' n 64 s 2' means the convolution layer has 64 kernels with stride 2. Sub-Pixel layer is the upsampling layer [21].\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "ously, we extract the corresponding embeddings ( X H f ) from the HF part. Since transformers can only use sequences as input, we then convert X L t and X H f into separated sequences as the input of transformer encoder and decoder, respectively. To preserve the fine details of the final LDCT images, we integrate the output of the transformer decoder and some specific features from the LF part and then piecewise reconstruct high-quality and high-resolution LDCT images by stages. Extensive experiments on Mayo LDCT dataset demonstrate the superiority of our method over other methods.\n",
      "\n",
      "## 2 Method\n",
      "\n",
      "Fig 1 illustrates the overview of our proposed framework. For image denoising, an intuitive solution is to decompose the noisy image into HF and LF parts, and then the noise is mainly left in the HF part, which also contains plenty of image textures. However, noise removal only in the HF part breaks the relationship between the HF and LF parts since there are also weakened latent textures in the LF part with reduced noise. Therefore, we can remove the noise in the HF part with the assistance of the latent textures from the LF part. In this work, given the noisy LDCT image X with the size of H × W , we first use a Gaussian filter with a standard deviation of 1 . 5 to decompose the LDCT image into two compositions: HF part X H and LF part X L .\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "To use the latent textures in X L , we firstly extract the corresponding content fetatures X L c and texture features X L t from X L using shallow two CNNs. Further, we use these texture features and embeddings from X H to train a transformer and get high-level features of X H , combined with content features from X L to reconstruct the final high-quality LDCT image.\n",
      "\n",
      "## 2.1 TransCT\n",
      "\n",
      "Sequence Similar with what other works have done [6], we firstly employ two convolution layers with stride 2 to obtain low-resolution features from X L , and then set two paths to extract content features X L c 1 ( H 8 × W 8 × 64), X L c 2 ( H 16 × W 16 × 256) and latent texture feature X L t ( H 32 × W 32 × 256), respectively. For X H , we employ sub-pixel layer to make X H to be low-resolution images ( H 16 × W 16 × 256), and final high-level features X H f can be obtained with three convolution layers. The goal is to get a sequence of moderate dimensions eventually. To take advantage of the characteristic of long-range dependencies of transformers, we perform tokenization by reshaping X L t and X H f into two sequences S L , S H , respectively.\n",
      "\n",
      "Transformer In this work, we employ a modified transformer with three encoders and three decoders, each encoder includes a multi-head attention module (MHSA) and a feed-forward layer (MLP) and each decoder consists of two multihead attention modules and a feed-forward layer, as can be seen in Fig 1. For transformer encoder, we use S L ( WH 1024 × 256) as the input token, followed by a multi-head attention module to seek the global relationship across large regions, and then we use two fully-connected layers (whose number of the node are 8 c and c , respectively. c is the dimension of the input sequence) to increase the expressive power of the entire network.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "After acquiring the latent texture features S 3 L from X L , we feed S H ( WH 256 × 256) into the first multi-head attention module and treat S 3 L as the key and value of each transformer decoder in the second multi-head attention module.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Piecewise Reconstruction Since the transformer only output features Y , we combine Y with X L c 1 , X L c 2 to piecewise reconstruct the final high-quality LDCT images. In our work, the output of the transformer has the size of H 16 × W 16 × 256. Here, we reconstruct the high-resolution LDCT image piecewise. In the first step, we add Y and X L c 2 and then feed the output into a ResNet with two 'Conv2d + Leaky-ReLU(lrelu)' layers, followed by a sub-pixel layer which results in higher-resolution features with size of H 8 × W 8 × 64. Similarly, we add these higher-resolution features and X L c 1 . After another ResNet with two 'Conv2d\n",
      "\n",
      "+ lrelu' layers and sub-pixel layer, we can get the final output with the size of H × W\n",
      "\n",
      "## 2.2 Loss Function\n",
      "\n",
      "The MSE measures the difference between the output and normal dose CT images (NDCT), which reduces the noise in the input LDCT images. Formally, the MSE is defined as follows:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Where I ND is the NDCT image and I LD is the LDCT image, F is the proposed model and θ denotes the network parameters.\n",
      "\n",
      "## 2.3 Implementation\n",
      "\n",
      "In this work, the proposed framework was implemented in python based on Tensorflow [1] library. We used the Adam [12] optimizer to optimize all the parameters of the framework. We totally trained 300 epochs with a mini-batch size of 8. The learning rate was set as 0.0001 in the first 180 epochs and then reduced to 0.00001 for the next 120 epochs. The configuration of our computational platform is Intel(R) Core(Tm) i7-7700K CPU @4.20GHZ, 32 GB RAM, and a GeForce GTX TITAN X GPU with 12 GB RAM. We initialized all the variations with xavier initialization. Our code is publicly available at https://github.com/zzc623/TransCT\n",
      "\n",
      "## 3 Experiments\n",
      "\n",
      "Datasets In this work, we used a publicly released dataset for the 2016 NIHAAPM-Mayo Clinic Low-Dose CT Grand Challenge 3 [17]. In this dataset, normaldose abdominal CT images of 1 mm slice thickness were taken from 10 anonymous patients and the corresponding quarter-dose CT images were simulated by inserting Poisson noise into the projection data. To better train the proposed TransCT, we divided the original 10 training patient cases into 7/1/2 cases, related to the training/validation/testing datasets, respectively. Before network training, we converted CT value of each pixel into its corresponding attenuation value under the assumption that the x-ray source was monochromatic at 60 keV.\n",
      "\n",
      "Comparison with other methods We compared our method with baseline methods: Non-local Mean (NLM), RED-CNN [5], MAP-NN [19], which are the high-performance LDCT methods. NLM can be found in the scikit-image library 4 . Since there is no public well-trained model for RED-CNN and MAP-NN, we re-train these methods with the same dataset.\n",
      "\n",
      "3 https://www.aapm.org/GrandChallenge/LowDoseCT/\n",
      "\n",
      "4 https://scikit-image.org/\n",
      "\n",
      "Fig. 2. Visual comparisons from Mayo testing dataset. (A) NDCT, (B) LDCT, (C) NLM, (D) RED-CNN, (E) MAP-NN, (F) TransCT. The display window is [-160, 240] HU .\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Fig 2 shows the results randomly selected from the testing dataset. As compared to LDCT (Fig 2 (B)), NLM and all the DL-based methods can remove noise to a certain extent, while our proposed TransCT is more close to NDCT. By investigating the local region in Fig 3, we can see that the blood vessels (red arrows) are not obvious with NLM in (Fig 3 (C)). RED-CNN and MAP-NN generate, more or less, some additional light tissues (yellow arrow in (Fig 3 (D))) and shadows (green arrow in (Fig 3 (E))), respectively.\n",
      "\n",
      "Quantitative Analysis To quantitatively compare all the related methods, we conducted 5-fold cross-validation for all methods on Mayo dataset and employed Root Mean Square Error (RMSE), Structural Similarity (SSIM), and Visual Information Fidelity (VIF) [20] as image quality metrics. Among the three metrics, RMSE and SSIM mainly focus on pixel-wise similarity, and VIF uses natural statistics models to evaluate psychovisual features of the human visual system. From table 1, we can see that all the related methods improve the image quality on all three metrics. To be specific, Red-CNN is superior to MAP-NN at the pixel-wise level while inferior to MAP-NN in terms of VIF. As compared to LDCT, our TransCT can decrease RMSE by 40.5%, improve SSIM by 12.3%, and VIF by 93.7%. For clinical evaluation, limited by clinical ethics, we evaluated all the methods on clinical CBCT images from a real pig head.\n",
      "\n",
      "Fig. 3. The zoomed regions marked by the red box in Fig. 2 (A). (A) NDCT, (B) LDCT, (C) NLM, (D) RED-CNN, (E) MAP-NN, (F) TransCT. The display window is [-160, 240] HU .\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "The tube current was: 80 mA for NDCT and 20 mA for LDCT. From table 1, our method outperforms others with superior robustness.\n",
      "\n",
      "## 3.1 Ablation study\n",
      "\n",
      "On the Influence of Piecewise Reconstruction In this work, after the output of transformer decoder, we used two resnet blocks and two sub-pixel layers to piecewise reconstruct the high-quality high-resolution LDCT image. The goal is to restore image detail more finely. To evaluate the influence of piecewise reconstruction, we modified the proposed TransCT and removed the piecewise reconstruction. After the output of the third transformer decoder, we used a sub-pixel layer to directly reconstruct the noise-free high-resolution HF texture, and then we added this HF texture and X L to obtain the final LDCT image. Specifically, we have removed six convolution layers, including the path of content extraction ( X L c 1 and X L c 2 ) and four convolution layers in the final two\n",
      "\n",
      "Table 1. Quantitative results (MEAN ± SDs) associated with different methods. Red and blue indicate the best and the second-best results, respectively.\n",
      "\n",
      "| Dataset   |                     | LDCT                | NLM            | RED-CNN        | MAP-NN         | TransCT        |\n",
      "|-----------|---------------------|---------------------|----------------|----------------|----------------|----------------|\n",
      "| Mayo      | RMSE 37.167 ± 7.245 | RMSE 37.167 ± 7.245 | 25.115 ± 4.54  | 22.204 ± 3.89  | 22.492 ± 3.897 | 22.123 ± 3.784 |\n",
      "| Mayo      | SSIM                | 0.822 ± 0.053       | 0.908 ± 0.031  | 0.922 ± 0.025  | 0.921 ± 0.025  | 0.923 ± 0.024  |\n",
      "| Mayo      | VIF                 | 0.079 ± 0.032       | 0.133 ± 0.037  | 0.152 ± 0.037  | 0.150 ± 0.038  | 0.153 ± 0.039  |\n",
      "| Pig       | RMSE                | 50.776 ± 3.7        | 42.952 ± 5.971 | 37.551 ± 5.334 | 37.744 ± 4.883 | 36.999 ± 5.25  |\n",
      "| Pig       | SSIM                | 0.701 ± 0.02        | 0.799 ± 0.043  | 0.861 ± 0.03   | 0.86 ± 0.027   | 0.87 ± 0.029   |\n",
      "| Pig       | VIF                 | 0.023 ± 0.002       | 0.040 ± 0.004  | 0.066 ± 0.006  | 0.063 ± 0.006  | 0.069 ± 0.007  |\n",
      "\n",
      "Fig. 4. RMSE results on the validation dataset during the network trainings.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "resnet blocks. Fig 4 (a) shows the RMSE value on the validation dataset at each epoch. We can see that in about the first 20 epochs, the RMSE from modified TransCT decreases faster since its model scale is smaller than our TransCT, while the convergence was inferior to our TransCT with piecewise reconstruction.\n",
      "\n",
      "On the Influence of Model Size Generally, larger network size will lead to stronger neural network learning ability. In terms of each transformer encoder and decoder, which includes a two-layer feed-forward network, respectively, when the dimension of the input sequence is fixed, the dimension of the hidden layer in the feed-forward network will determine the network size. Here, we adjusted the dimension of the hidden layer { c, 2 c, 4 c } to investigate the influence of model size. From Fig 4 (b), we can see that the smaller the dimension of the hidden layer is, the larger the fluctuation of the convergence curve is, the larger the final convergent value will be. Therefore, we conclude that larger model results in a better performance. In this work, we set the dimension of the hidden layer in the feed-forward network at 8 c .\n",
      "\n",
      "Ablation studies on Transformer Module and Dual-path Module To investigate the effectiveness of the transformer module and dual-path module, we conducted two additional experiments. First, we used a revised module ('Conv+3 × ResNet blocks') to replace the transformer module. We concatenated X H f and the output from the fourth Conv layer (n128s2, before X L t ) and then inputted it into the revised module. As for the dual-path module, we discarded the HF path and inputted the X L c 2 into 3 transformer encoders, whose output will be combined with X L c 1 and X L c 2 in the piecewise reconstruction stage. The results on the validation dataset were shown in table 2, we can see that our TransCT with transformer module and dual-path module can obtain better performance.\n",
      "\n",
      "## 4 Conclusion\n",
      "\n",
      "Inspired by the internal similarity of the LDCT image, we present the first transformer-based neural network for LDCT, which can explore large-range dependencies between LDCT pixels. To ease the impact of noise on high-frequency\n",
      "\n",
      "Table 2. Ablation studies on transformer module and dual-path module conducted on the validation dataset.\n",
      "\n",
      "|                                             | RMSE           | SSIM          | VIF           |\n",
      "|---------------------------------------------|----------------|---------------|---------------|\n",
      "| w/o transformer module w/o dual-path module | 22.62 ± 2.068  | 0.927 ± 0.013 | 0.13 ± 0.023  |\n",
      "|                                             | 21.711 ± 1.997 | 0.931 ± 0.012 | 0.14 ± 0.025  |\n",
      "| TransCT                                     | 21.199 ± 2.054 | 0.933 ± 0.012 | 0.144 ± 0.025 |\n",
      "\n",
      "texture recovery, we employ a transformer encoder to further excavate the lowfrequency part of the latent texture features and then use these texture features to restore the high-frequency features from noisy high-frequency parts of LDCT image. The final high-quality LDCT image can be piecewise reconstructed with the combination of low-frequency content and high-frequency features. In the future, we will further explore the learning ability of TransCT and introduce self-supervised learning to lower the need for the training dataset.\n",
      "\n",
      "Acknowledgements. This work was partially supported by NIH (1 R01CA227713) and a Faculty Research Award from Google Inc.\n",
      "\n",
      "## References\n",
      "\n",
      "1. Abadi, M., Barham, P., et al.: Tensorflow: A system for large-scale machine learning. In: OSDI. pp. 265-283 (2016)\n",
      "2. Brenner, D.J., Hall, E.J.: Computed tomography-an increasing source of radiation exposure. N. Engl. J. Med. 357 (22), 2277-2284 (2007)\n",
      "3. Cai, J.F., Jia, X., et al.: Cine cone beam ct reconstruction using low-rank matrix factorization: algorithm and a proof-of-principle study. IEEE Trans. Med. Imag. 33 (8), 1581-1591 (2014)\n",
      "4. Chen, H., Wang, Y., et al.: Pre-trained image processing transformer. In: CVPR. pp. 12299-12310 (2021)\n",
      "5. Chen, H., Zhang, Y., et al.: Low-dose ct with a residual encoder-decoder convolutional neural network. IEEE Trans. Med. Imag. 36 (12), 2524-2535 (2017)\n",
      "6. Chen, J., Lu, Y., et al.: Transunet: Transformers make strong encoders for medical image segmentation. arXiv:2102.04306 (2021)\n",
      "7. Chun, I.Y., Zheng, X., et al.: Bcd-net for low-dose ct reconstruction: Acceleration, convergence, and generalization. In: MICCAI. pp. 31-40. Springer (2019)\n",
      "8. Devlin, J., Chang, M.W., et al.: Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805 (2018)\n",
      "9. Dosovitskiy, A., Beyer, L., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. arXiv:2010.11929 (2020)\n",
      "10. Gupta, H., Jin, K.H., et al.: Cnn-based projected gradient descent for consistent ct image reconstruction. IEEE Trans. Med. Imag. 37 (6), 1440-1453 (2018)\n",
      "11. He, J., Yang, Y., et al.: Optimizing a parameterized plug-and-play admm for iterative low-dose ct reconstruction. IEEE Trans. Med. Imag. 38 (2), 371-382 (2018)\n",
      "12. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv:1412.6980 (2014)\n",
      "\n",
      "13. LaRoque, S.J., Sidky, E.Y., Pan, X.: Accurate image reconstruction from few-view and limited-angle data in diffraction tomography. JOSA A 25 (7), 1772-1782 (2008)\n",
      "14. Li, M., Hsu, W., et al.: Sacnn: self-attention convolutional neural network for lowdose ct denoising with self-supervised perceptual loss network. IEEE Trans. Med. Imag. 39 (7), 2289-2301 (2020)\n",
      "15. Manduca, A., Yu, L., et al.: Projection space denoising with bilateral filtering and ct noise modeling for dose reduction in ct. Med. phys. 36 (11), 4911-4919 (2009)\n",
      "16. Mathews, J.P., Campbell, Q.P., et al.: A review of the application of x-ray computed tomography to the study of coal. Fuel 209 , 10-24 (2017)\n",
      "17. McCollough, C.H., Bartley, A.C., et al.: Low-dose ct for the detection and classification of metastatic liver lesions: Results of the 2016 low dose ct grand challenge. Med. phys. 44 (10), e339-e352 (2017)\n",
      "18. Seeram, E.: Computed tomography: physical principles, clinical applications, and quality control. Elsevier Health Sciences (2015)\n",
      "19. Shan, H., Padole, A., et al.: Competitive performance of a modularized deep neural network compared to commercial algorithms for low-dose ct image reconstruction. Nat. Mach. Intell. 1 (6), 269-276 (2019)\n",
      "20. Sheikh, H.R., Bovik, A.C.: Image information and visual quality. IEEE Trans. Image Process. 15 (2), 430-444 (2006)\n",
      "21. Shi, W., Caballero, J., et al.: Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In: CVPR. pp. 18741883 (2016)\n",
      "22. Tian, Z., Jia, X., et al.: Low-dose ct reconstruction via edge-preserving total variation regularization. Phys. Med. Biol. 56 (18), 5949 (2011)\n",
      "23. Vaswani, A., Shazeer, N., et al.: Attention is all you need. arXiv:1706.03762 (2017)\n",
      "24. Wang, G., Ye, J.C., et al.: Image reconstruction is a new frontier of machine learning. IEEE Trans. Med. Imag. 37 (6), 1289-1296 (2018)\n",
      "25. Wang, G., Ye, J.C., De Man, B.: Deep learning for tomographic image reconstruction. Nat. Mach. Intell. 2 (12), 737-748 (2020)\n",
      "26. Wang, X., Girshick, R., et al.: Non-local neural networks. In: CVPR. pp. 7794-7803 (2018)\n",
      "27. Xu, Q., Yu, H., et al.: Low-dose x-ray ct reconstruction via dictionary learning. IEEE Trans. Med. Imag. 31 (9), 1682-1697 (2012)\n",
      "28. Yang, F., Yang, H., et al.: Learning texture transformer network for image superresolution. In: CVPR. pp. 5791-5800 (2020)\n",
      "29. Yu, L., Zhang, Z., et al.: Deep sinogram completion with image prior for metal artifact reduction in ct images. IEEE Trans. Med. Imag. 40 (1), 228-238 (2020)\n",
      "30. Yu, L., Manduca, A., et al.: Sinogram smoothing with bilateral filtering for low-dose ct. In: Medical Imaging 2008: Physics of Medical Imaging. vol. 6913, p. 691329\n",
      "31. Zhang, H., Goodfellow, I., et al.: Self-attention generative adversarial networks. In: ICML. pp. 7354-7363 (2019)\n",
      "32. Zhang, Z., Yu, L., et al.: Modularized data-driven reconstruction framework for non-ideal focal spot effect elimination in computed tomography. Med. Phys. (2021)\n",
      "33. Zhang, Z., Liang, X., et al.: A sparse-view ct reconstruction method based on combination of densenet and deconvolution. IEEE Trans. Med. Imag. 37 (6), 14071417 (2018)\n",
      "34. Zhang, Z., Yu, S., et al.: A novel design of ultrafast micro-ct system based on carbon nanotube: a feasibility study in phantom. Phys. Med. 32 (10), 1302-1307 (2016)\n",
      "35. Zhu, B., Liu, J.Z., et al.: Image reconstruction by domain-transform manifold learning. Nature 555 (7697), 487-492 (2018)\n",
      "CPU times: user 294 ms, sys: 198 ms, total: 492 ms\n",
      "Wall time: 13min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from parallel_exec import convert_list\n",
    "\n",
    "converted_docs = convert_list(sources)\n",
    "\n",
    "# In kết quả markdown của từng tài liệu\n",
    "for i, doc in enumerate(converted_docs):\n",
    "    if doc:\n",
    "        print(f\"Document {i + 1}:\")\n",
    "        print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761692f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 08:01:54,959 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 08:01:54,983 - INFO - Going to convert document batch...\n",
      "2025-12-15 08:01:54,984 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 08:01:55,005 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-15 08:01:55,007 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-12-15 08:01:55,030 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-15 08:01:55,035 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-12-15 08:01:55,036 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 08:01:55,037 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 08:01:55,206 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 08:01:55,226 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:01:55,229 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:01:55,242 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:01:55,243 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:01:55,450 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:01:55,451 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:01:55,453 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:01:55,453 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:01:55,534 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:01:55,535 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:01:55,557 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:01:55,558 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 08:01:55,783 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 08:01:55,793 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-15 08:01:55,796 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2025-12-15 08:01:55,801 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:01:56,411 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-15 08:01:56,412 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2025-12-15 08:01:56,656 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:01:57,022 - INFO - Processing document 2408.09869v5.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 08:02:39,473 - INFO - Finished converting document 2408.09869v5.pdf in 44.63 sec.\n",
      "2025-12-15 08:02:39,601 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 08:02:39,604 - INFO - Going to convert document batch...\n",
      "2025-12-15 08:02:39,605 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 08:02:39,606 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 08:02:39,606 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 08:02:39,607 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 08:02:39,625 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:02:39,626 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:02:39,638 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:02:39,639 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!-- image -->\n",
      "\n",
      "## Docling Technical Report\n",
      "\n",
      "## Version 1.0\n",
      "\n",
      "Christoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\n",
      "\n",
      "AI4K Group, IBM Research R¨ uschlikon, Switzerland\n",
      "\n",
      "## Abstract\n",
      "\n",
      "This technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Converting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\n",
      "\n",
      "With Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\n",
      "\n",
      "Here is what Docling delivers today:\n",
      "\n",
      "- Converts PDF documents to JSON or Markdown format, stable and lightning fast\n",
      "- Understands detailed page layout, reading order, locates figures and recovers table structures\n",
      "- Extracts metadata from the document, such as title, authors, references and language\n",
      "- Optionally applies OCR, e.g. for scanned PDFs\n",
      "- Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\n",
      "- Can leverage different accelerators (GPU, MPS, etc).\n",
      "\n",
      "## 2 Getting Started\n",
      "\n",
      "To use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.\n",
      "\n",
      "Docling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.\n",
      "\n",
      "from docling.document\\_converter import DocumentConverter\n",
      "\n",
      "```\n",
      "source = \"https://arxiv.org/pdf/2206.01062\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \"## DocLayNet: A Large Human -Annotated Dataset for Document -Layout Analysis [...]\"\n",
      "```\n",
      "\n",
      "Optionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.\n",
      "\n",
      "## 3 Processing pipeline\n",
      "\n",
      "Docling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.\n",
      "\n",
      "## 3.1 PDF backends\n",
      "\n",
      "Two basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling's PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive\n",
      "\n",
      "1 see huggingface.co/ds4sd/docling-models/\n",
      "\n",
      "Figure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "licensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14].\n",
      "\n",
      "We therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings.\n",
      "\n",
      "## 3.2 AI models\n",
      "\n",
      "As part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.\n",
      "\n",
      "## Layout Analysis Model\n",
      "\n",
      "Our layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\n",
      "\n",
      "The Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.\n",
      "\n",
      "## Table Structure Recognition\n",
      "\n",
      "The TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\n",
      "\n",
      "The Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.\n",
      "\n",
      "## OCR\n",
      "\n",
      "Docling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page).\n",
      "\n",
      "We are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements.\n",
      "\n",
      "## 3.3 Assembly\n",
      "\n",
      "In the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request.\n",
      "\n",
      "## 3.4 Extensibility\n",
      "\n",
      "Docling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.\n",
      "\n",
      "Implementations of model classes must satisfy the python Callable interface. The \\_\\_call\\_\\_ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.\n",
      "\n",
      "## 4 Performance\n",
      "\n",
      "In this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable ) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1.\n",
      "\n",
      "If you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery.\n",
      "\n",
      "Establishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and\n",
      "\n",
      "torch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report.\n",
      "\n",
      "Table 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.\n",
      "\n",
      "| CPU                     | Thread budget   | native backend   | native backend   | native backend   | pypdfium backend   | pypdfium backend   | pypdfium backend   |\n",
      "|-------------------------|-----------------|------------------|------------------|------------------|--------------------|--------------------|--------------------|\n",
      "|                         |                 | TTS              | Pages/s          | Mem              | TTS                | Pages/s            | Mem                |\n",
      "| Apple M3 Max (16 cores) | 4 16            | 177 s 167 s      | 1.27 1.34        | 6.20 GB          | 103 s 92 s         | 2.18 2.45          | 2.56 GB            |\n",
      "| Intel(R) Xeon E5-2690   | 4 16            | 375 s 244 s      | 0.60 0.92        | 6.16 GB          | 239 s 143 s        | 0.94 1.57          | 2.42 GB            |\n",
      "\n",
      "## 5 Applications\n",
      "\n",
      "Thanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.\n",
      "\n",
      "## 6 Future work and contributions\n",
      "\n",
      "Docling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\n",
      "\n",
      "We encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.\n",
      "\n",
      "## References\n",
      "\n",
      "- [1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\n",
      "- [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster\n",
      "\n",
      "machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24) . ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf .\n",
      "\n",
      "- [3] C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing (CLOUD) , pages 363-373. IEEE, 2022.\n",
      "- [4] J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf .\n",
      "- [5] O. R. developers. Onnx runtime. https://onnxruntime.ai/ , 2024. Version: 1.18.1.\n",
      "- [6] IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit .\n",
      "- [7] A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF .\n",
      "- [8] J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama\\_index .\n",
      "- [9] M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San Jos´ e, CA, USA, August 21-26, 2023, Proceedings, Part II , pages 37-50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8\\_3 .\n",
      "- [10] L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. Statements: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christiaen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024) , pages 193-214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15 .\n",
      "- [11] L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of chemical structures in patent documents. Nature Communications , 15(1):6532, August 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/ s41467-024-50779-y .\n",
      "- [12] A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4614-4623, 2022.\n",
      "- [13] B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large humanannotated dataset for document-layout segmentation. pages 3743-3751, 2022.\n",
      "- [14] pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/ py-pdf/pypdf .\n",
      "- [15] P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/ pypdfium2-team/pypdfium2 .\n",
      "- [16] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on real-time object detection, 2023.\n",
      "\n",
      "## Appendix\n",
      "\n",
      "In this section, we illustrate a few examples of Docling's output in Markdown and JSON.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1. Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation (not shown).\n",
      "\n",
      "KDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\n",
      "\n",
      "Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.\n",
      "\n",
      "|                                                                                                        | human                                                                   | MRCNN R50 R101                                                                                                          | FRCNN R101                                                  | YOLO v5x6                                                   |\n",
      "|--------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------|\n",
      "| Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All | 84-89 83-91 83-85 87-88 93-94 85-89 69-71 83-84 77-81 84-86 60-72 82-83 | 68.4 71.5 70.9 71.8 60.1 63.4 81.2 80.8 61.6 59.3 71.9 70.0 71.7 72.7 67.6 69.3 82.2 82.9 84.6 85.8 76.7 80.4 72.4 73.5 | 70.1 73.7 63.5 81.0 58.9 72.0 72.0 68.4 82.2 85.4 79.9 73.4 | 77.7 77.2 66.2 86.2 61.1 67.9 77.1 74.6 86.3 88.1 82.7 76.8 |\n",
      "\n",
      "to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\n",
      "\n",
      "## 5 EXPERIMENTS\n",
      "\n",
      "The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\n",
      "\n",
      "Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "paper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\n",
      "\n",
      "In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].\n",
      "\n",
      "## Baselines for Object Detection\n",
      "\n",
      "In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\n",
      "\n",
      "Table 2: Prediction perlormance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 beckbone were rained based on the network architecturesfrom the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utized was YOLOv5x6 [13]. All models were initialised using pre-trained weightsfrom the COCO 2017 dataset.\n",
      "\n",
      "|                | human   |   MRCNN |   MRCNNFRCNN |      |   YOLO |\n",
      "|----------------|---------|---------|--------------|------|--------|\n",
      "| Caption        | 84-89   |    68.4 |         71.5 | 70.1 |   77.7 |\n",
      "| Footnote       | 83-91   |    70.9 |         71.8 | 73.7 |   77.2 |\n",
      "| Formula        | 83-85   |    60.1 |         63.4 | 63.5 |   66.2 |\n",
      "| List-item      | 89-48   |    81.2 |         80.8 | 81   |   86.2 |\n",
      "| Page-ooer      | 93-94   |    61.6 |         59.3 | 58.9 |   61.1 |\n",
      "| Page-header    | 85-89   |    71.9 |         70   | 72   |   67.9 |\n",
      "| Picture        | 69-71   |    71.7 |         72.7 | 72   |   77.1 |\n",
      "| Section-header | 83-84   |    67.6 |         69.3 | 68.4 |   74.6 |\n",
      "| Table          | 77-81   |    82.2 |         82.9 | 82.2 |   86.3 |\n",
      "| x1             | 84-86   |    84.6 |         85.8 | 85.4 |   88.1 |\n",
      "| Title          | 60-72   |    76.7 |         80.4 | 79.9 |   82.7 |\n",
      "| AlIl           | 82-83   |    72.4 |         73.5 | 73.4 |   76.8 |\n",
      "\n",
      "to avoid this at any cost in order to have clear, unbiased baseline numbers for human dcument-layout annotation. hird, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS -pxa fjeund je o sjoo-xe posopu oug punoe xoq-upunoq wnwu o o xoq umep-jesnne syuus fgeogewone joo uogeqouue based segments, which excludes only Table and Picture For the later, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cellsis that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\n",
      "\n",
      "## 5EXPERIMENTS\n",
      "\n",
      "The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthemore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\n",
      "\n",
      "Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNN network with ResNet50 backbone trained on increasing fractions of the DocLayNet dataset. The leaming curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with simlar data ill not yield significantly better predictions.\n",
      "\n",
      "paper and leave the detaled evaluation of more reoent methods mentioned in Section 2 for future work.\n",
      "\n",
      "In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in 0 S'0 wo ofue pu sdeμano o  (dvw) uospeud oene ueu Susn suogoped e po Agenb u aenje m om oeqnd 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API[16]\n",
      "\n",
      "## BaselinesforObjectDetection\n",
      "\n",
      "In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pbxels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overallbetween 6 and 0 eug uogeopu poo6 e senj6 sL sobed popeoue-edjμ uo suogegoue ueunq esujed oug wo. pnduoo dvw oug ueg emo %01 DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixelupoau auou ou puey saugo au uo suogopeud sogeq ueqo o djou lou sep soxoq-ugpunoq wou panuap uogeuoubes oteu poseq Yolov5x model does very welland even out-performs humans on selected labels such as Text, Table and Picture  Thisis not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\n",
      "\n",
      "Figure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing first under the title. Elements recognized as page headers or footers are suppressed in Markdown to deliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph in '5. Experiments' wrapping over the column end is broken up in two and interrupted by the table.\n",
      "\n",
      "KDD '22, August 14-18, 2022, Washington, DC, USA\n",
      "\n",
      "Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\n",
      "\n",
      "Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as %\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "of row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric we distributed the annotation workload and performed continuous\n",
      "\n",
      "only. For phases three and four, a group of 40 dedicated annotators quality controls. Phase one and two required a small team of experts\n",
      "\n",
      "were assembled and supervised.\n",
      "\n",
      "while coverage ensures that all meaningful items on a page can to a document category, such as\n",
      "\n",
      "be annotated. We refrained from class labels that are very specific\n",
      "\n",
      "Abstract in the\n",
      "\n",
      "Scientific Articles semantics of the text. Labels such as\n",
      "\n",
      "category. We also avoided class labels that are tightly linked to the\n",
      "\n",
      "Author\n",
      "\n",
      "Affiliation\n",
      "\n",
      "teria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources in DocBank, are often only distinguishable by discriminating on 3 https://arxiv.org/ Figure 4: Table 1 from the DocLayNet paper in the original PDF (A), as rendered Markdown (B) and in JSON representation (C). Spanning table cells, such as the multi-column header 'triple interannotator mAP@0.5-0.95 (%)', is repeated for each column in the Markdown representation (B), which guarantees that every data point can be traced back to row and column headings only by its grid coordinates in the table. In the JSON representation, the span information is reflected in the fields of each table cell (C).\n",
      "\n",
      "and\n",
      "\n",
      ", as seen\n",
      "\n",
      "Phase 1: Data selection and preparation.\n",
      "\n",
      "Our inclusion cri-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 08:02:39,824 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:02:39,825 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:02:39,827 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:02:39,827 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:02:39,915 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:02:39,916 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:02:39,939 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:02:39,940 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 08:02:40,170 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 08:02:40,171 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:02:40,881 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:02:41,167 - INFO - Processing document 2311.04155v3.pdf\n",
      "2025-12-15 08:04:36,550 - INFO - Finished converting document 2311.04155v3.pdf in 117.01 sec.\n",
      "2025-12-15 08:04:36,695 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 08:04:36,699 - INFO - Going to convert document batch...\n",
      "2025-12-15 08:04:36,700 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 08:04:36,701 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 08:04:36,701 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 08:04:36,702 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 08:04:36,720 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:04:36,720 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:04:36,732 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:04:36,733 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Black-Box Prompt Optimization: Aligning Large Language Models without Model Training\n",
      "\n",
      "Jiale Cheng 1 , 2 * , Xiao Liu 3 , 2 * , Kehan Zheng 1 , Pei Ke 1 , Hongning Wang 1 , Yuxiao Dong 3 , Jie Tang 3 , Minlie Huang 1 †\n",
      "\n",
      "1 The Conversational Artificial Intelligence (CoAI) Group, Tsinghua University 2 Zhipu AI\n",
      "\n",
      "3\n",
      "\n",
      "The Knowledge Engineering Group (KEG), Tsinghua University chengjl23@mails.tsinghua.edu.cn, shawliu9@gmail.com, aihuang@tsinghua.edu.cn\n",
      "\n",
      "## Abstract\n",
      "\n",
      "Large language models (LLMs) have shown impressive success in various applications. However, these models are often not well aligned with human intents, which calls for additional treatments on them; that is, the alignment problem. To make LLMs better follow user instructions, existing alignment methods primarily focus on further training them. However, the extra training of LLMs is usually expensive in terms of GPU computing; even worse, some LLMs are not accessible for userdemanded training, such as GPTs. In this work, we take a different perspectiveBlack-Box Prompt Optimization (BPO)-to perform alignments. The idea is to optimize user prompts to suit LLMs' input understanding, so as to best realize users' intents without updating LLMs' parameters. BPO leverages human preferences to optimize prompts, thus making it superior to LLM (e.g., ChatGPT) as a prompt engineer. Moreover, BPO is model-agnostic, and the empirical results demonstrate that the BPOaligned ChatGPT yields a 22% increase in the win rate against its original version and 10% for GPT-4. Notably, the BPO-aligned LLMs can outperform the same models aligned by PPO and DPO, and it also brings additional performance gains when combining BPO with PPO or DPO. Code and datasets are released at https://github.com/thu-coai/BPO .\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Recently, the field of Natural Language Processing has made remarkable progress, largely thanks to the advent of Large Language Models (LLMs) (Brown et al., 2020b; Chowdhery et al., 2022; Zhang et al., 2022; Zeng et al., 2022; Touvron et al., 2023). After elaborate alignment (Gabriel, 2020; Ji et al., 2023), these models have demonstrated a strong ability of\n",
      "\n",
      "* JC and XL made equal contributions.\n",
      "\n",
      "† Corresponding author.\n",
      "\n",
      "2 Work done when JC interned at Zhipu AI.\n",
      "\n",
      "Figure 1: (Upper) Two directions of LLM alignment: Black-Box Prompt Optimization (BPO) and Learning from Feedback (PPO, DPO). BPO offers a conceptually new perspective to bridge the gap between humans and LLMs. (Lower) On Vicuna Eval's pairwise evaluation, we show that BPO further aligns gpt-3.5-turbo and claude-2 without training. It also outperforms both PPO &amp; DPO and presents orthogonal improvements.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "instruction-following and human preference understanding, yielding products like ChatGPT (OpenAI, 2022) that have attracted widespread attention.\n",
      "\n",
      "However, aligning LLMs to human preferences is not trivial. The major challenge lies in narrowing the gap between human intents (conveyed by prompts ) and LLMs' understanding of them. Significant effort has been focused on steering LLMs to approach human preference, including reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022), reinforcement learning from AI feedback (RLAIF) (Bai et al., 2022b; Lee et al., 2023), or Direct Preference Optimization (DPO) (Rafailov et al., 2023). Nevertheless, these methods suffer from various deficiencies:\n",
      "\n",
      "- Efficiency: As LLMs grow larger, it becomes far more expensive and difficult to train these\n",
      "- models, especially when using notoriously unstable RL algorithms for the purpose.\n",
      "- Accessibility: As most best-performing LLMs, such as GPT-4 (OpenAI, 2023) and Claude2 (Anthropic, 2023a), are close-sourced and only can be accessed by API, these trainingbased methods are not applicable for users outside the organization to enhance alignment.\n",
      "- Interpretability: The modeling and exact consequent improvements of human preference are uninterpretable when using these approaches.\n",
      "\n",
      "Distinct from the aforementioned alignment methods, we propose to steer human prompts to accommodate LLMs' understanding . While the idea is closely related to ' prompt engineering ', its automated prototypes would trace back to AutoPrompt (Shin et al., 2020) and prompt tuning (i.e., P-Tuning) (Liu et al., 2021; Lester et al., 2021), where prompts are optimized to improve task performance without training the LMs. Our new alignment method, Black-Box Prompt Optimization (BPO) , presents an efficient and interpretable paradigm that aligns LLMs without modifying these models. The central idea behind BPO is to create an automatic prompt optimizer that rewrites human prompts, which are usually less organized or ambiguous, to prompts that better deliver human intent. Consequently, these prompts could be more LLM-preferred and yield better human-preferred responses.\n",
      "\n",
      "In BPO, the prompt preference optimizer is learned from preference comparisons. We curate a subset of publicly available SFT datasets with either human or AI preferences. Each instance of our training data contains a prompt along with a pair of favorable and unfavorable responses. We then employ LLMs to delineate and criticize the paired responses, and subsequently ask the LLMs to refine the input prompt to explicitly incorporate the features that shift the responses from unfavorable to favorable. In this way, we construct 14K pairs of the original instruction and its optimized version to train a sequence-to-sequence model that optimizes user instructions.\n",
      "\n",
      "Our extensive experiments demonstrate that without LLM training, BPO can improve the alignment of both API-based and open-sourced LLMs remarkably: increasing win rates by 8.8% to 22.0% on gpt-3.5-turbo , gpt-4 , claude-2 , llama-2-chat , vicuna etc. Moreover, we show that BPO not only outperforms RLHF via\n",
      "\n",
      "PPO (Schulman et al., 2017) and DPO (Rafailov et al., 2023) but also further improves LLMs' alignment after these RLHF's training. We also show that BPO can align LLMs in supervised fine-tuning by optimizing response quality in the experiment of Alpaca. In addition, we have demonstrated the superiority of BPO over the direct use of LLM as a prompt engineer, highlighting the importance of incorporating human feedback.\n",
      "\n",
      "Our contributions can be summarized as follows:\n",
      "\n",
      "- We propose a novel prompt optimization method BPO, which enhances LLMs' alignment to human preferences without training these models, demonstrating improvements over a wide variety of LLMs, including APIbased and open-sourced ones.\n",
      "- We empirically justify that BPO is a novel and competitive alignment approach, in addition to existing RLHF and preference learning methods, outperforming PPO and DPO on extensive experiments. Moreover, we show that it is orthogonal to RLHF's alignment, which adds additional gain on top of conventional alignment pipelines.\n",
      "- We systematically analyze how BPO refines the original prompts from the perspectives of prompt explanation, clarification, enrichment, and safety enhancement. We demonstrate its better interpretability than existing preference learning algorithms when aligning LLMs.\n",
      "\n",
      "## 2 Related Work\n",
      "\n",
      "LLMs pre-trained on massive corpus can generate fluent text but are not well aligned to follow users' instructions. Therefore, aligning LLMs with human intents has become an important research problem. Existing efforts in alignment mostly follow the paradigm proposed by Ouyang et al. (2022), consisting of two main stages: SFT and RLHF.\n",
      "\n",
      "Supervised Fine-tuning (SFT). SFT alignment endows LLMs with preliminary instruction-following abilities. Nonetheless, it heavily relies on abundant high-quality fine-tuning data. Since the high cost of human-written data, self-instruct data augmentation (Wang et al., 2022) based on a small human-created seed set has become a predominant approach in academia (Taori et al., 2023; BELLEGroup, 2023). However, SFT alignment still suffers from hallucinations, inferior scalability, and poor understanding of human preference.\n",
      "\n",
      "Reinforcement Learning from Human Feedback\n",
      "\n",
      "Figure 2: BPO consists of three main steps: collecting feedback data (we adopt open-sourced feedback data), constructing prompt optimization pairs based on the feedback data, and building a prompt optimization model using these pairs. In this way, BPO serves as a translator between human and AI, by optimizing human prompts to be better suited for AI generation to get human-preferred responses, while treating the model itself as a black box.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "(RLHF). RLHF alignment is proposed to further align LLMs with scalable feedback. The standard framework (Stiennon et al., 2020; Ouyang et al., 2022) consists of reward modeling and policy training. Due to the significant cost of manual effort (Ouyang et al., 2022; Ji et al., 2024b), several studies have explored incorporating AI feedback and shown impressive results (Bai et al., 2022b; Lee et al., 2023). Moreover, considering the cumbersome procedures and unstable RL training, some works have sought other methods beyond RLHF to learn from preference feedback. Rafailov et al. (2023) introduces feedback into the design of the loss function. Furthermore, some studies also explore self-improvement (Yuan et al., 2024; Xu et al., 2024) and alignment of agents (Lai et al., 2024).\n",
      "\n",
      "Prompt Engineering and Prompt Tuning. Since the pre-trained language models are proposed, leveraging prompt tuning to accomplish NLP tasks has gradually become a new paradigm (Brown et al., 2020a; Liu et al., 2021). There are two main types of prompt tuning: hard and soft. Hard prompt tuning, or prompt engineering, often requires extensive manual effort. Therefore, many works explore how to automate this process, which can be traced back to AutoPrompt (Shin et al., 2020). Recently, with the advent of LLMs, utilizing language models for automated prompt engineering has demonstrated remarkable performance (Zhou et al., 2022; Yang et al., 2023; Pryzant et al., 2023; Pan et al.,\n",
      "\n",
      "2023; Li et al., 2024). However, existing methods primarily focus on specific tasks rather than alignment and require searching for each task. In addition, these methods necessitate optimization for an individual model, rendering them not universally applicable across all models, which further limits their usability. Soft prompt tuning (Liu et al., 2021; Lester et al., 2021; Li and Liang, 2021) further improves effectiveness by enabling optimization in the embedding space rather than limited token vocabulary, but it requires tuning of the model parameters, which is not as flexible as hard prompting.\n",
      "\n",
      "Prompt tuning and model training have been two parallel ways to improve pre-trained model performance. Current alignment strategies primarily focus on adjusting models to follow user intents and instructions, and few works have explored plugand-play alignment tools (Ji et al., 2024a). Under the context of LLMs, models have become huge and difficult to train or even obtain (e.g. API-based models). Therefore, we argue that prompt optimization desires its attention, and LLM alignment can also be achieved by optimizing the input prompt without modifying the LLMs.\n",
      "\n",
      "## 3 Black-Box Prompt Optimization\n",
      "\n",
      "The overall process of BPO is shown in Figure 2. BPO is to enhance the alignment between model output and human preference by optimizing the input prompt. To this end, we first collect several\n",
      "\n",
      "Table 1: Preference data statistics. We sampled prompts from open-sourced prompt datasets and filter them to form the preference training dataset.\n",
      "\n",
      "| Dataset       | Sampled   | Sampled      | Generating &Filtering   | Generating &Filtering   |\n",
      "|---------------|-----------|--------------|-------------------------|-------------------------|\n",
      "|               | Number    | Distinct-4 ↑ | Number                  | Distinct-4 ↑            |\n",
      "| OASST1        | 3000      | 0.953        | 2940                    | 0.963                   |\n",
      "| HH-RLHF       | 2000      | 0.957        | 1961                    | 0.957                   |\n",
      "| Chatbot Arena | 5000      | 0.804        | 4494                    | 0.899                   |\n",
      "| Alpaca-GPT4   | 5000      | 0.938        | 5000                    | 0.938                   |\n",
      "| Overall       | 15000     | 0.860        | 14395                   | 0.913                   |\n",
      "\n",
      "instruction-tuning datasets with human preference annotations, carefully curate and filter low-quality data. Subsequently, we employ an LLM to capture the difference between responses favored and disfavored by human, based on which we leverage the LLM to refine the input. We then get a pair of original instruction and its improved version, using which we further train a sequence-to-sequence model to automatically optimize user inputs.\n",
      "\n",
      "## 3.1 Task Definition\n",
      "\n",
      "As discussed above, our task is to optimize user input to help LLMs generate better responses. Formally, we denote user input as X user . Our goal is to build a function F that maps X user to its optimized version, denoted as X opt . In order to get this, we introduce annotated human preferences, as the preferred response indicates good model output, while the other one suggests inferior output. By capturing the differences between these preference data, we can incorporate the attributes human favor into user instructions to make them more aligned with what LLMs can do, thus bringing LLMs' outputs better into alignment with human preferences. Inspired by recent work utilizing LLMs as evaluators (Wang et al., 2023; Zheng et al., 2023), we believe that LLMs possess the capacity to understand different features within various responses. Consequently, we leverage LLMs to get X opt . Specifically, each sample is represented as ( X user , Y good , Y bad ) , where Y good stands for the favorable response and Y bad is for the unfavorable one. Thus, the prompt optimization process with LLM can be expressed as X opt = LLM ( X user , Y good , Y bad ) . Finally, we build the F function by training a smaller sequenceto-sequence model over the pairs of ( X user , X opt ) .\n",
      "\n",
      "## 3.2 Training Data Construction\n",
      "\n",
      "To construct the optimized prompts, we begin by collecting datasets with human preferences. In to- tal, we employ four instruction-tuning datasets with human preference annotations, as shown in Table 1. The detailed description of these datasets can be found in Appendix A. After collecting and reformatting these datasets, we carefully eliminate low-quality instances with manually crafted rules (e.g. too short instructions tend to be low quality) and use self-bleu to perform a strict diversity filtering. Finally, we get 14k diverse samples in the format of ( X user , Y good , Y bad ) . In this work, we mainly focus on single-turn response generation and leave the multi-turn setting for our future work.\n",
      "\n",
      "Subsequently, we leverage ChatGPT (OpenAI, 2022) to refine these instructions. After meticulous prompt engineering efforts, we employ two types of prompts for different data formats as illustrated in Appendix B. Then, we conduct quality filtering by rule-based methods to drop wrong optimizations (e.g., wrong format). Following the whole procedure, our dataset comprises about 14k pairs of instruction before and after optimization, with the final distribution shown in Table 1. The overall distinct score (Li et al., 2016) demonstrates the high diversity of our dataset.\n",
      "\n",
      "## 3.3 Model Training\n",
      "\n",
      "Based on the constructed dataset, we learn a small sequence-to-sequence model to automatically optimize user instruction. Formally, we generate X opt conditioned on the given input X user , where the loss function is specified as,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where N is the length of X opt and x t represents the t -th token in X opt . In this work, we choose to use llama2-7b-chat as the backbone model, as we believe a stronger model can learn the implicit preference mapping between X user and X opt better. Meanwhile, the number of parameters in a 7B model is small among LLMs, which can be more efficient for training and inference. And we leave the model scaling explorations to future work.\n",
      "\n",
      "## 3.4 Comparison with Existing Methods\n",
      "\n",
      "As shown in Table 2, BPO exhibits several preferred advantages compared to existing alignment methods. While the ultimate goal is to align LLMs' outputs with human preferences, RLHF (Ouyang\n",
      "\n",
      "Table 2: Comparison to RLHF (PPO), DPO, OPRO. BPO is free from training reward or policy models, and agnostic to any LLMs or tasks in application.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "| Method                      | Reward -free   | Policy -free   | LLM -agnostic   | Task -agnostic   |\n",
      "|-----------------------------|----------------|----------------|-----------------|------------------|\n",
      "| PPO (Ouyang et al., 2022)   |                |                |                 | ✓                |\n",
      "| DPO (Rafailov et al., 2023) | ✓              |                |                 | ✓                |\n",
      "| OPRO (Yang et al., 2023)    | ✓              | ✓              |                 |                  |\n",
      "| BPO (ours)                  | ✓              | ✓              | ✓               | ✓                |\n",
      "\n",
      "et al., 2022) and DPO (Rafailov et al., 2023) modify the LLMs' parameters to fit human preferences. However, BPO approaches this from the input side, optimizing user prompts to make them more model-friendly and thus improve the alignment of model outputs. In addition, since BPO does not change LLMs' parameters, it can be applied to APIbased models, whereas PPO and DPO are limited to white-box models. Compared to prompt engineering methods like OPRO, BPO is more general, as OPRO requires task-specific search to rewrite the prompts. Moreover, OPRO does not do samplelevel optimization: it uses the same learned prompt for all samples in each task, which can cause low stability. Furthermore, PPO, DPO, and OPRO only optimize specific LLMs, but BPO, once learned, is model-agnostic. As stated in section Section 3.1, we aim to learn a universal mapping from user prompts to optimized prompts following human preferences, which is achieved by incorporating multiple LLMs models' generations in the training data. The incorporation of human preferences allows BPO to outperform prompt optimization using LLM (e.g., ChatGPT) directly.\n",
      "\n",
      "## 4 Experiments\n",
      "\n",
      "To comprehensively showcase the capabilities of BPO, we have conducted extensive experiments encompassing diverse aspects, including alignment on black-box models, comparisons with existing feedback learning techniques (DPO &amp; PPO), SFT data quality enhancement capability, iterative improvement capability, comparisons with prompt engineering method (Appendix H), and ablation study on feedback. Implementation details can be found in Appendix C.\n",
      "\n",
      "## 4.1 Evaluation of Alignment\n",
      "\n",
      "As it remains a significant challenge to comprehensively evaluate a language model's alignment quality, in this work, we adopt the widely-used setting of employing strong LLMs to evaluate the model's performance on instruction-following datasets.\n",
      "\n",
      "Test Datasets In order to evaluate the quality of alignment more accurately, we selected multiple instruction datasets for assessment.\n",
      "\n",
      "- Dolly Eval is a subset of 200 instances randomly sampled from the dolly (Conover et al., 2023) dataset, which is human-generated and contains eight categories of tasks.\n",
      "- Vicuna Eval (Chiang et al., 2023) contains 80 diverse questions in 8 categories.\n",
      "- Self-Instruct Eval is the human evaluation dataset created by Wang et al. (2022), encompassing 252 expert-written user-oriented instructions motivated by real-world applications.\n",
      "- BPO-test Eval is a split of our dataset, containing 200 samples from the four datasets we used when constructing the training set.\n",
      "\n",
      "Evaluation Methods As existing studies (Wang et al., 2023; Zheng et al., 2023) demonstrated, strong LLMs can be good evaluators. Following Li et al. (2023), we use both GPT-4 (OpenAI, 2023) and Claude (Anthropic, 2023b) for evaluation and, we employ a pairwise scoring setup to intuitively show the alignment capability differences. The prompt for GPT-4 scoring is from MT-bench (Zheng et al., 2023), and the prompt for Claude scoring is from Alpaca Eval (Li et al., 2023), which can be found in Appendix D. In addition, to mitigate position bias and reduce the cost, we randomly shuffle the models' responses in each evaluation, which is also used in Alpaca Eval.\n",
      "\n",
      "## 4.2 Black-Box Alignment Results\n",
      "\n",
      "Detailed experiment results can be found in Table 3 and Table 4. Our method achieves a higher win rate on all datasets across all models with our optimized prompts vs. original prompts. Notably, on gpt-3.5-turbo and text-bison , the average win rates increase about 20%, and more 10% for several models including gpt-4 , demonstrating the strong performance of our approach. Moreover, consistent gains are achieved across models of varying capabilities, from smaller open-sourced models like llama2-7b-chat and vicuna-7b to powerful large-scale models like gpt-4 and claude-2 , highlighting BPO's robust generalization for various models. Additionally, across these four test sets, the most significant gain occurs on VicunaEval, where under the GPT-4's evaluation, many\n",
      "\n",
      "Table 3: Win rates between BPO-aligned and original LLM APIs, evaluated by gpt-4 (Cf. Table 8 for claude-v1.3 's evaluation). Without training these LLMs, BPO can significantly improve block-box LLM APIs' alignment. ('ori.' denotes 'original', and 'WR' denotes 'win rates').\n",
      "\n",
      "|                    | Method   | Method   | Vicuna Eval   | Vicuna Eval   | Self-instruct Eval   | Self-instruct Eval   | Self-instruct Eval   | Dolly Eval   | Dolly Eval   | Dolly Eval   | BPO-test Eval   | BPO-test Eval   | BPO-test Eval   |       |\n",
      "|--------------------|----------|----------|---------------|---------------|----------------------|----------------------|----------------------|--------------|--------------|--------------|-----------------|-----------------|-----------------|-------|\n",
      "| Base LLM           | A B      |          | A win         | tie B win     | A win                | tie                  | B win                | A win        | tie          | B win        | A win           | tie             | B win           | ∆ WR  |\n",
      "| gpt-3.5-turbo      | BPO      | ori.     | 60.0 8.7      | 31.3          | 50.4                 | 12.3                 | 37.3                 | 55.0         | 16.0         | 29.0         | 51.0            | 18.0            | 31.0            | +22.0 |\n",
      "| gpt-4              | BPO ori. | 41.3     | 23.7          | 35.0          | 39.7                 | 22.6                 | 37.7                 | 51.0         | 26.0         | 23.0         | 39.0            | 26.0            | 35.0            | +10.1 |\n",
      "| claude-instant-1.2 | BPO ori. | 66.3     | 5.0           | 28.7          | 50.0                 | 9.1                  | 40.9                 | 45.0         | 14.5         | 40.5         | 45.0            | 10.5            | 44.5            | +12.9 |\n",
      "| claude-2           | BPO ori. | 57.5     | 5.0           | 37.5          | 48.8                 | 12.7                 | 38.5                 | 44.5         | 13.0         | 42.5         | 45.0            | 13.0            | 42.0            | +8.8  |\n",
      "| text-bison         | BPO ori. | 65.0     | 10.0          | 25.0          | 47.0                 | 21.9                 | 31.1                 | 42.0         | 30.5         | 27.5         | 50.5            | 10.5            | 39.0            | +20.5 |\n",
      "\n",
      "Table 4: Win rates between BPO-aligned and original llama-2-chat and vicuna-v1.3 LLMs, evaluated by gpt-4 (Cf. Table 9 for claude-v1.3 's evaluation). Training-free BPO improves alignment substantially, even making llama-2-13b-chat outperform llama-2-70b-chat . ('WR' denotes 'win rates').\n",
      "\n",
      "| Base LLM      | Method    | Method   | Method   | Vicuna Eval   | Vicuna Eval   | Vicuna Eval   | Self-instruct Eval   | Self-instruct Eval   | Self-instruct Eval   | Dolly Eval   | Dolly Eval   | Dolly Eval   | BPO-test Eval   | BPO-test Eval   | BPO-test Eval   |       |\n",
      "|---------------|-----------|----------|----------|---------------|---------------|---------------|----------------------|----------------------|----------------------|--------------|--------------|--------------|-----------------|-----------------|-----------------|-------|\n",
      "| Base LLM      | A         | B        | A win    | tie           | B win         | A win         | tie                  | B win                | A                    | win          | tie B        | win          | A win           | tie             | B win           | ∆ WR  |\n",
      "| llama-2 -chat | 7B + BPO  | 7B       | 60.0     | 2.5           | 37.5          | 53.6          | 9.9                  | 36.5                 | 52.0                 | 9.5          | 38.5         |              | 53.0            | 10.5            | 36.5            | +17.4 |\n",
      "| llama-2 -chat | 13B + BPO | 13B      | 61.3     | 2.5           | 36.2          | 51.2          | 11.9                 | 36.9                 | 50.5                 | 13.5         | 36.0         |              | 53.0            | 12.5            | 34.5            | +18.1 |\n",
      "| llama-2 -chat | 7B + BPO  | 70B      | 48.8     | 3.7           | 47.5          | 40.1          | 5.1                  | 54.8                 | 49.0                 | 2.0          | 49.0         |              | 40.0            | 5.0             | 55.0            | -7.1  |\n",
      "| llama-2 -chat | 13B + BPO | 70B      | 61.3     | 0.0           | 38.7          | 48.4          | 4.8                  | 46.8                 | 54.0                 | 6.5          | 39.5         |              | 51.0            | 7.0             | 42.0            | +11.9 |\n",
      "| llama-2 -chat | 70B + BPO | 70B      | 59.3     | 5.5           | 35.2          | 46.0          | 13.1                 | 40.9                 | 51.0                 | 18.0         | 31.0         |              | 53.5            | 11.0            | 35.5            | +16.8 |\n",
      "| vicuna        | 7B + BPO  | 7B       | 65.0     | 8.7           | 26.3          | 42.0          | 21.1                 | 36.9                 | 47.0                 | 22.0         | 31.0         |              | 46.0            | 22.0            | 32.0            | +18.5 |\n",
      "| -v1.3         | 13B + BPO | 13B      | 52.5     | 3.7           | 43.8          | 46.4          | 13.9                 | 39.7                 | 52.0                 | 8.0          | 40.0         |              | 59.5            | 6.0             | 34.5            | +13.1 |\n",
      "\n",
      "BPO-aligned models achieve over 60%:40% preference ratio (20% win rate increase), with some even reaching 70%:30% win rates (40% win rate increase). This suggests that BPO can achieve greater alignment gain on open-ended instructions. BPO can significantly enhance the comprehensiveness of responses in these open-ended tasks (§5). However, the benefits of BPO are not limited to these tasks. In closed tasks within these evaluation sets, such as mathematics, reasoning, and coding, BPO also demonstrates excellent performance, achieving an average improvement in win rate of over 10%.\n",
      "\n",
      "Furthermore, we conduct a scaling experiment, as shown in Figure 7. We compare LLaMA2-chat models of varying sizes with our optimized instructions against the original llama2-70b-chat model. Remarkably, BPO boosts smaller model llama2-7b-chat to match or even outperform the 10x larger model on some datasets. And under Claude's evaluation, llama2-7b-chat with BPO alignment nearly reaches the performance of llama2-70b-chat . For the llama2-13b-chat model, BPO enables it to substantially surpass the 70b model, demonstrating the potential of BPO to boost smaller models beyond much larger ones.\n",
      "\n",
      "## 4.3 RLHF Results\n",
      "\n",
      "As shown in Table 5, PPO, DPO, and BPO all successfully improve the performance of vicuna-7b and vicuna-13b . Moreover, the SFT model with BPO outperforms PPO and DPO aligned models, which highlights BPO's advantage. As mentioned before, BPO is model-agnostic and can be applied to LLMs with different capabilities. Therefore, we investigate if BPO can be applied on top of RLHF methods, and our result is positive: both PPO and DPO in conjunction with BPO can be largely improved. With BPO alignment and DPO training, both vicuna-7b and vicuna-13b can achieve around 30% win rate increases.\n",
      "\n",
      "## 4.4 BPO for Data Augmentation\n",
      "\n",
      "BPO can also be applied to construct high-quality data by leveraging the optimized prompts to get high-quality responses. We validate its applicability on the Alpaca (Taori et al., 2023) dataset: we first optimize the original instructions with BPO and use these optimized instructions as inputs for text-davinci-003 to generate responses. This gives us a refined Alpaca dataset, and we train llama-7b and llama-13b with this new dataset. As shown in Table 6, the experiment results demonstrate substantial gains over LLMs trained on the original Alpaca dataset. Notably, on Vicuna Eval, llama-13b trained with 52k BPO reproduced data can achieve 93.8%:1.2% win rate against the one trained with the original dataset. Furthermore, using just 1k reproduced data, the trained model can surpass the original model, which is trained with\n",
      "\n",
      "| Method   | Method   | Vicuna Eval   | Vicuna Eval   | Vicuna Eval   | Self-instruct Eval   | Self-instruct Eval   | Self-instruct Eval   | Dolly Eval   | Dolly Eval   | Dolly Eval   | BPO-test Eval   | BPO-test Eval   | BPO-test Eval   | ∆ WR   |\n",
      "|----------|----------|---------------|---------------|---------------|----------------------|----------------------|----------------------|--------------|--------------|--------------|-----------------|-----------------|-----------------|--------|\n",
      "| A        | B        | A win         | tie           | B win         | A win                | tie                  | B win                | A win        | tie          | B win        | A win           | tie             | B win           | ∆ WR   |\n",
      "| PPO      | ori.     | 47.5          | 10.0          | 42.5          | 49.6                 | 10.3                 | 40.1                 | 46.0         | 13.9         | 38.5         | 42.0            | 19.5            | 36.0            | +7.0   |\n",
      "| BPO      | PPO      | 61.3          | 6.2           | 32.5          | 49.6                 | 11.9                 | 38.5                 | 49.0         | 12.5         | 41.5         | 47.5            | 13.0            | 39.5            | +13.8  |\n",
      "| BPO+PPO  | ori.     | 55.0          | 7.5           | 37.5          | 50.0                 | 10.3                 | 39.7                 | 52.5         | 9.0          | 38.5         | 54.5            | 10.0            | 35.5            | +15.2  |\n",
      "| BPO+PPO  | PPO      | 56.3          | 11.2          | 32.5          | 44.4                 | 20.7                 | 34.9                 | 43.0         | 29.0         | 28.0         | 44.0            | 23.0            | 33.0            | +14.8  |\n",
      "| DPO      | ori.     | 58.8          | 6.2           | 35.0          | 53.6                 | 11.5                 | 34.9                 | 50.0         | 19.0         | 31.0         | 51.0            | 18.0            | 31.0            | +20.4  |\n",
      "| BPO      | DPO      | 53.8          | 3.7           | 42.5          | 40.1                 | 8.3                  | 51.6                 | 45.0         | 10.0         | 45.0         | 45.0            | 11.0            | 44.0            | +0.2   |\n",
      "| BPO+DPO  | ori.     | 65.0          | 5.0           | 30.0          | 60.3                 | 10.7                 | 29.0                 | 54.0         | 17.0         | 29.0         | 56.0            | 13.0            | 31.0            | +29.1  |\n",
      "| BPO+DPO  | DPO      | 63.8          | 2.5           | 33.7          | 49.6                 | 9.9                  | 40.5                 | 46.0         | 14.0         | 40.0         | 45.0            | 16.0            | 39.0            | +12.8  |\n",
      "| PPO      | ori.     | 53.8          | 3.7           | 42.5          | 49.2                 | 11.1                 | 39.7                 | 49.0         | 14.5         | 36.5         | 42.0            | 17.5            | 40.5            | +8.7   |\n",
      "| BPO      | PPO      | 52.5          | 3.7           | 43.7          | 44.4                 | 6.4                  | 49.2                 | 50.0         | 9.0          | 41.0         | 53.5            | 11.5            | 35.0            | +7.9   |\n",
      "| BPO+PPO  | ori.     | 55.0          | 7.5           | 37.5          | 49.6                 | 9.9                  | 40.5                 | 54.0         | 11.0         | 35.0         | 55.5            | 11.5            | 33.0            | +17.0  |\n",
      "| BPO+PPO  | PPO      | 55.0          | 5.0           | 40.0          | 49.6                 | 5.6                  | 44.8                 | 49.5         | 9.5          | 41.0         | 55.0            | 11.0            | 34.0            | +12.3  |\n",
      "| DPO      | ori.     | 50.0          | 3.7           | 46.3          | 55.6                 | 6.3                  | 38.1                 | 58.5         | 6.5          | 35.0         | 58.0            | 11.5            | 30.5            | +18.1  |\n",
      "| BPO      | DPO      | 53.8          | 2.5           | 43.7          | 44.0                 | 8.4                  | 47.6                 | 45.0         | 5.0          | 50.0         | 43.0            | 16.0            | 41.0            | +0.9   |\n",
      "| BPO+DPO  | ori.     | 71.3          | 2.5           | 26.2          | 61.1                 | 7.2                  | 31.7                 | 58.0         | 9.0          | 33.0         | 62.0            | 8.0             | 30.0            | +32.9  |\n",
      "| BPO+DPO  | DPO      | 60.0          | 2.5           | 37.5          | 48.8                 | 9.1                  | 42.1                 | 48.0         | 8.5          | 43.5         | 50.0            | 11.0            | 39.0            | +11.2  |\n",
      "\n",
      "Table 5: Win rates between PPO, DPO, and BPO-aligned vicuna-v1.3 series LLMs, evaluated by gpt-4 (Cf. Table 10 for claude-v1.3 's evaluation). BPO not only outperforms both PPO and DPO, and could yield additional bonus over PPO and DPO-aligned LLMs. ('ori.' denotes 'original', and 'WR' denotes 'win rates').\n",
      "\n",
      "| Base LLM   | Method         | Method            | Vicuna Eval   | Vicuna Eval   | Vicuna Eval   | Self-instruct Eval   | Self-instruct Eval   | Self-instruct Eval   | Dolly Eval   | Dolly Eval   | Dolly Eval   | BPO-test Eval   | BPO-test Eval   | BPO-test Eval   | ∆ WR        |\n",
      "|------------|----------------|-------------------|---------------|---------------|---------------|----------------------|----------------------|----------------------|--------------|--------------|--------------|-----------------|-----------------|-----------------|-------------|\n",
      "| Base LLM   | A              | B                 | A win         | tie           | B win         | A win                | tie                  | B win                | A win        | tie          | B win        | A win           | tie             | B win           | ∆ WR        |\n",
      "| llama-7b   | BPO-1k BPO-52k | ori.-52k          | 72.5          | 10.0          | 17.5          | 45.2 47.2            | 14.7                 | 40.1                 | 57.0         | 13.0         | 30.0         | 44.5 50.0       | 13.5 20.0       | 42.0            | +22.4 +26.7 |\n",
      "|            |                | ori.-52k          | 75.0          | 7.5           | 17.5          | 55.2                 | 13.9                 | 38.9                 | 58.0         | 5.0          | 37.0         | 58.5            | 16.0            | 30.0            |             |\n",
      "| llama-13b  | BPO-1k BPO-52k | ori.-52k ori.-52k | 78.8 93.8     | 6.2 5.0       | 15.0 1.2      | 68.7                 | 10.7 8.3             | 34.1 23.0            | 56.5 56.0    | 15.0 12.0    | 28.5 32.0    | 67.0            | 19.0            | 25.5 14.0       | +36.5 +53.8 |\n",
      "\n",
      "Table 6: Win rates between BPO reproduced and original alpaca dataset tuned llama-1 series LLMs, evaluated by gpt-4 (Cf. Table 11 for claude-v1.3 's evaluation). -1k means training the LLM with 1k randomly sampled data, -52k means using the whole dataset. ('ori.' denotes 'original', and 'WR' denotes 'win rates').\n",
      "\n",
      "52k samples. These results underscore the importance of high-quality data and verify that BPO can assist in producing high-quality training data.\n",
      "\n",
      "## 4.5 Iterative Prompt Optimization\n",
      "\n",
      "Since BPO can optimize the user prompt for better response, a natural idea is whether we can iteratively improve a prompt, progressively enhancing an LLM's output. We thus conduct this experiment with gpt-3.5-turbo on the Vicuna Eval dataset. Specifically, we iteratively optimize the original instruction five times and compare the win rate against the original instruction. As shown in Figure 3, ∆ WR achieves noticeable improvement through four iterations, with a small decline on the fifth iteration. Appendix G presents a case study of a prompt after each optimization iteration. Furthermore, we also find that BPO exhibits good retention, which has a high probability of preserving the input prompt when it is already good enough. This, we believe, is a key factor in enabling iterative enhancement, as it avoids forcing unreasonable changes to the user's original intent.\n",
      "\n",
      "Figure 3: Difference of win rate and lose rate in each iteration (iteration 0 means the original) scored by gpt-4 and claude-v1.3 .\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## 4.6 Ablation Study\n",
      "\n",
      "One critical component of BPO is to leverage feedback to optimize user instructions. To investigate how much feedback contributes to BPO's prompt optimization, we conduct an ablation experiment to compare feedback-learned optimization (BPO) and directly using gpt-3.5-turbo for prompt optimization. As shown in Table 7, direct optimization can improve model performance, which val-\n",
      "\n",
      "Table 7: Win rates between BPO and directly using gpt-3.5-turbo for prompt optimization (w/o FDBK), evaluated by gpt-4 (Cf. Table 12 for claude-v1.3 's evaluation). While BPO largely improves model performance, w/o FDBK improves little. ('ori.' denotes 'original', and 'WR' denotes 'win rates', 'FDBK' denotes 'feedback').\n",
      "\n",
      "| Base LLM       | Method           | Method             | Vicuna Eval    | Vicuna Eval   | Vicuna Eval    | Self-instruct Eval   | Self-instruct Eval   | Self-instruct Eval   | Dolly Eval     | Dolly Eval     | Dolly Eval     | BPO-test Eval   | BPO-test Eval   | BPO-test Eval   | ∆ WR             |\n",
      "|----------------|------------------|--------------------|----------------|---------------|----------------|----------------------|----------------------|----------------------|----------------|----------------|----------------|-----------------|-----------------|-----------------|------------------|\n",
      "| Base LLM       | A                | B                  | A win          | tie           | B win          | A win                | tie                  | B win                | A win          | tie            | B win          | A win           | tie             | B win           | ∆ WR             |\n",
      "| gpt-3.5 -turbo | BPO w/o FDBK BPO | ori. ori. w/o FDBK | 60.0 58.8 52.5 | 8.7 8.7 6.2   | 31.3 32.5 41.3 | 50.4 36.9 57.9       | 12.3 7.5 5.6         | 37.3 55.6 36.5       | 55.0 43.5 52.0 | 16.0 16.0 16.0 | 29.0 40.5 32.0 | 51.0 46.0 49.0  | 18.0 16.0 13.0  | 31.0 38.0 38.0  | +22.0 +4.6 +15.9 |\n",
      "\n",
      "Figure 4: BPO Optimization types and examples. Due to space limitations, we omit some examples and refer to Figure 11 for the complete results.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "idates the potential for LLMs to be good prompt engineers. BPO provides further improvements beyond direct optimization. The results suggest that incorporating feedback allows LLMs to refine prompts in line with demonstrated user preferences, enabling more effective prompt optimization.\n",
      "\n",
      "## 5 Interpretability of BPO\n",
      "\n",
      "Compared with model-training-based alignment methods like PPO or DPO, BPO has a distinct advantage in its strong interpretability, as we can directly compare the instructions before and after optimization to find out how BPO works. To examine what BPO optimizes in detail, we closely examined 500 samples and summarized some common patterns in its optimization and error types.\n",
      "\n",
      "As shown in Figure 4, we summarize four common optimization strategies exhibited in BPO's results, including Explanation Generation (green box), Prompt Elaboration (orange box), Providing Hint (blue box) and Safety Enhancement (pink box). We should note that there are also other optimization strategies observed in BPO's output, and those strategies are not mutually exclusive. These presented examples are only typical instances in these four categories.\n",
      "\n",
      "- Explanation Generation is a common way that BPO employs to instruct LLMs to generate rea-\n",
      "\n",
      "soning steps or detailed explanations, which helps to form a more logical and understandable response.\n",
      "\n",
      "- Prompt Elaboration includes various methods to help models better understand user intentions and generate comprehensive responses, as users often give unclear, over-concise instructions and even with errors.\n",
      "- Providing Hint adds specific hints to the user's prompt. For instance, BPO adds key points to be addressed or elucidates relevant knowledge to assist models in better organizing answers.\n",
      "- Safety Enhancement is critical in alignment. When user inputs could potentially raise security issues, BPO emphasizes maintaining harmless responses. Moreover, BPO enables interpretable security enhancements, as it can refine the unsafe request to require the model to output relevant harmless advice. In this way, we can better prevent safety issues while still keeping responses helpful.\n",
      "\n",
      "Error analysis is shown in Appendix I.\n",
      "\n",
      "## 6 Conclusion\n",
      "\n",
      "In this work, we present BPO, a black-box alignment method that automatically optimizes user inputs to better suit LLMs' preference for improved responses. With BPO alignment, we successfully improve the alignment of LLMs without further adjusting these models, leading to significant results even on the most powerful models like GPT-4 and Claude-2. Moreover, extensive experiments show that BPO can reach or surpass the performance of current mainstream alignment techniques on Vicuna models and further improve these alignment methods. Our findings demonstrate that tailoring inputs to best suit LLMs is a promising technical direction to obtain interpretable and controllable alignment in parallel to existing model-trainingbased solutions, and there is still great room to further explore in depth.\n",
      "\n",
      "## Limitations\n",
      "\n",
      "Despite BPO's effectiveness and strong potential for wider applications, we want to discuss some known limitations of this work, which require further research and efforts to improve.\n",
      "\n",
      "Require more data and training. Though we show that BPO can effectively improve alignment on established benchmarks including Vicuna Eval (Chiang et al., 2023), Self-Instruct Eval (Wang et al., 2022), and our sampled Dolly Eval (Conover et al., 2023), BPO-test Eval, our prompt preference optimizer is only trained on 14k pairs of optimized prompts deriving from the combination of few existing academic feedback datasets. It covers a limited spectrum of scenarios and has not been trained on large amounts of data yet. Thus, the currently released optimizer may not be as good as expected for very general usage.\n",
      "\n",
      "Adaptation to long-context and math-related inputs. Another thing we notice is that due to the few academic feedback datasets we adopt, there is an imbalance in the prompt's topic distribution and length. One is the lack of long-context prompts. Take the summarization task as an example; due to the lack of related training data, our prompt optimizer tends to alter the instructional prompt as well as the original passage for summarization (which should not be changed). Another case is mathrelated problems. Currently, our prompt optimizer seems to fail to learn how to change their inputs for better performance. We believe such a problem could be improved if we pay more attention to related topics in the dataset construction.\n",
      "\n",
      "## Ethical Considerations\n",
      "\n",
      "In this work, we leveraged several available datasets for training BPO. The OASST1 (Köpf et al., 2023) dataset is under Apache license; the HH-RLHF (Bai et al., 2022a) dataset is under MIT license; Chatbot Arena Conversations (Zheng et al., 2023) dataset and Alpaca-GPT4 (Peng et al., 2023) dataset is under Creative Commons license. In these datasets, there exists some instructions with security issues. However, in BPO training, we constructed optimized prompt pairs that provide safety enhancements to these unsafe instructions, further mitigating the security issues.\n",
      "\n",
      "## ACKNOWLEDGEMENT\n",
      "\n",
      "This work was supported by the National Key Research and Development Program of China (No. 2021ZD0113304). This work was supported by the National Science Foundation for Distinguished Young Scholars (with No. 62125604). This work was supported by the NSFC projects (with No. 62306160). This work was also supported by China National Postdoctoral Program for Innovative Talents (No. BX20230194) and China Postdoctoral Science Foundation (No. 2023M731952). We would also like to thank Zhipu AI for sponsoring GPU computing and API cost consumed in this study.\n",
      "\n",
      "## References\n",
      "\n",
      "Anthropic. 2023a. Claude 2.\n",
      "\n",
      "Anthropic. 2023b. Introducing claude.\n",
      "\n",
      "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 .\n",
      "\n",
      "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022b. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073 .\n",
      "\n",
      "BELLEGroup. 2023. Belle: Be everyone's large language model engine. https://github.com/ LianjiaTech/BELLE .\n",
      "\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020a. Language models are few-shot learners. Advances in neural information processing systems , 33:1877-1901.\n",
      "\n",
      "- Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020b. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems , NIPS'20, Red Hook, NY, USA. Curran Associates Inc.\n",
      "- Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality.\n",
      "- Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 .\n",
      "- Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023. Free dolly: Introducing the world's first truly open instructiontuned llm.\n",
      "- Iason Gabriel. 2020. Artificial intelligence, values, and alignment. Minds and machines , 30(3):411-437.\n",
      "- Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, and Yaodong Yang. 2024a. Aligner: Achieving efficient alignment through weak-to-strong correction. arXiv preprint arXiv:2402.02416 .\n",
      "- Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 2024b. Beavertails: Towards improved safety alignment of llm via a humanpreference dataset. Advances in Neural Information Processing Systems , 36.\n",
      "- Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et al. 2023. Ai alignment: A comprehensive survey. arXiv preprint arXiv:2310.19852 .\n",
      "- Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, et al. 2023. Openassistant conversations-democratizing large language model alignment. arXiv preprint arXiv:2304.07327 .\n",
      "- Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang,\n",
      "- Xiaohan Zhang, Yuxiao Dong, et al. 2024. Autowebglm: Bootstrap and reinforce a large language model-based web navigating agent. arXiv preprint arXiv:2404.03648 .\n",
      "- Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. 2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267 .\n",
      "- Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691 .\n",
      "- Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and William B Dolan. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 110-119.\n",
      "- Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 45824597.\n",
      "- Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca\\_eval .\n",
      "- Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, and Xifeng Yan. 2024. Guiding large language models via directional stimulus prompting. Advances in Neural Information Processing Systems , 36.\n",
      "- Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt understands, too. arXiv:2103.10385 .\n",
      "- Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 .\n",
      "- OpenAI. 2022. Introducing chatgpt.\n",
      "- OpenAI. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774 .\n",
      "- Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35:27730-27744.\n",
      "- Rui Pan, Shuo Xing, Shizhe Diao, Xiang Liu, Kashun Shum, Jipeng Zhang, and Tong Zhang. 2023. Plum: Prompt learning using metaheuristic. arXiv preprint arXiv:2311.08364 .\n",
      "- Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277 .\n",
      "- Reid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang Zhu, and Michael Zeng. 2023. Automatic prompt optimization with 'gradient descent' and beam search. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 7957-7968.\n",
      "- Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290 .\n",
      "- Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining , pages 3505-3506.\n",
      "- John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 .\n",
      "- Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 4222-4235.\n",
      "- Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. Advances in Neural Information Processing Systems , 33:30083021.\n",
      "- Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford\\_alpaca .\n",
      "- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 .\n",
      "- Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. 2023. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087 .\n",
      "- Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560 .\n",
      "- Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pages 38-45, Online. Association for Computational Linguistics.\n",
      "- Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang, Zihan Wang, Aohan Zeng, Zhengxiao Du, Wenyi Zhao, et al. 2024. Chatglmmath: Improving math problem-solving in large language models with a self-critique pipeline. arXiv preprint arXiv:2404.02893 .\n",
      "- Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. 2023. Large language models as optimizers. arXiv preprint arXiv:2309.03409 .\n",
      "- Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, et al. 2023. Deepspeedchat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales. arXiv preprint arXiv:2308.01320 .\n",
      "- Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024. Self-rewarding language models. arXiv preprint arXiv:2401.10020 .\n",
      "- Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 .\n",
      "- Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 .\n",
      "- Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685 .\n",
      "- Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910 .\n",
      "\n",
      "## A Datasets for Traning\n",
      "\n",
      "The training data construction includes four preference-annotated datasets.\n",
      "\n",
      "- The OASST1 (Köpf et al., 2023) dataset is a crowd-sourced instruction dataset with human-annotated response quality ratings. Under each instruction, we choose the response with the highest score as the good response and the one with the lowest score as the bad response.\n",
      "- The HH-RLHF (Bai et al., 2022a) dataset contains human preference over the responses' helpfulness and harmfulness.\n",
      "- The Chatbot Arena Conversations (Zheng et al., 2023) dataset is collected from human on the Chatbot Arena leaderboard 1 platform.\n",
      "- In addition, we use the comparison data subset of the Alpaca-GPT4 (Peng et al., 2023) dataset, where the preference is generated by GPT4 (OpenAI, 2023). To ensure data quality, we only keep samples where gpt-4 outperforms text-davinci-003 .\n",
      "\n",
      "## B Data Construction Prompts\n",
      "\n",
      "Since our data construction process involves four datasets and the data formats are not the same, we design two prompts to construct the optimized prompts as shown in Figure 5. For OASST1, HHRLHF, and Chatbot Arena Conversations, we adopt the prompt without context; for Alpaca-GPT4, we adopt the prompt with context.\n",
      "\n",
      "## C Implementation Details\n",
      "\n",
      "For BPO, we use Llama-2-7b-chat-hf 2 as backbone model, trained for three epochs on our dataset. And we simply take the final checkpoint. In the training stage, we utilize AdamW (Loshchilov and Hutter, 2017) optimizer with β 1 = 0 . 9 and β 2 = 0 . 999 . We set the learning rate to 2e-5, with 0.1 ratio warm-up steps and linear decay. The training batch size is 4 per GPU, and we leverage Huggingface Transformers (Wolf et al., 2020) and DeepSpeed (Rasley et al., 2020) framework for the Zero-2 strategy. For the RLHF training, we employed the\n",
      "\n",
      "1\n",
      "\n",
      "https://huggingface.co/spaces/lmsys/\n",
      "\n",
      "chatbot-arena-leaderboard\n",
      "\n",
      "2\n",
      "\n",
      "https://huggingface.co/meta-llama/\n",
      "\n",
      "DeepSpeed-Chat (Yao et al., 2023) framework, running just one epoch for reward model learning and PPO optimization as recommended. Our reward model achieves 80% accuracy on the in-distribution test set. The 16k data for PPO optimization is also from the combined OASST1 (Köpf et al., 2023), HH-RLHF (Bai et al., 2022a), Chatbot Area Conversations (Zheng et al., 2023) and Alpaca-GPT4 (Peng et al., 2023). All experiments are conducted on 8 × 80GB NVIDIA A800 GPUs. BPO adopts Top-p 0.9 and temperature 0.6 for decoding, while all tested LLMs use the default decoding strategies. In LLM-based evaluation, we set the temperature to 0.\n",
      "\n",
      "## D Evaluation Prompts\n",
      "\n",
      "As existing works demonstrated (Zheng et al., 2023; Li et al., 2023), strong LLMs can be good evaluators and show high consistency with human. Therefore we adopt gpt-4 and claude-v1.3 for evaluation, evaluation prompt for gpt-4 is from MT-bench (Zheng et al., 2023), and the one for claude-v1.3 is from Alpaca Eval (Li et al., 2023), as shown in Figure 6.\n",
      "\n",
      "## E Model Scaling Experiments\n",
      "\n",
      "As shown in Figure 7, BPO-aligned llama2-13b-chat model outperforms the 70b version, and this shows the great potential of BPO to boost smaller LLMs to surpass much larger ones.\n",
      "\n",
      "## F Experimental Results of Claude Evaluation\n",
      "\n",
      "As shown in Table 8 and Table 9, the evaluation results of claude-v1.3 are consistent with the results of gpt-4 . For each model with vs. without BPO alignment, BPO-aligned model shows better performance on all test sets. For the scaling setting ( llama-2-chat series with BPO alignment vs. llama-2-70b-chat ), BPO-aligned llama-2-7b-chat nearly achieves the same performance as 10x larger llama-2-70b-chat , and BPO-aligned 13b version can surpass llama-2-70b-chat .\n",
      "\n",
      "Table 10 shows the results compared to RLHF through PPO and DPO. BPO outperforms both PPO and DPO and can further improve the PPO or DPO aligned models. For both vicuna-7b and vicuna-13b , BPO with DPO achieves over 20% win rate increases.\n",
      "\n",
      "Figure 5: Our data construction prompt for dataset with (like Alpaca) or without context (like Chatbot Area Conversations).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Claude Pairwise Scoring Prompt\n",
      "\n",
      "```\n",
      "Human: I want you to create a leaderboard of different of large-language models. To do so, I will give you the instructions (prompts) given to the models, and the responses of two models. Please rank the models based on which responses would be preferred by humans. All inputs and outputs should be python dictionaries. Here is the prompt: { \"instruction\": \"\"\"{instruction}\"\"\", } Here are the outputs of the models: [ { \"model\": \"model_1\", \"answer\": \"\"\"{output_1}\"\"\" }, { \"model\": \"model_2\", \"answer\": \"\"\"{output_2}\"\"\" } ] Now please rank the models by the quality of their answers, so that the model with rank 1 has the best output. Then return a list of the model names and ranks, i.e., produce the following output: [ {'model': <model-name>, 'rank': <model-rank>}, {'model': <model-name>, 'rank': <model-rank>} ] Your response must be a valid Python dictionary and should contain nothing else because we will directly execute it in Python. Please provide the ranking that the majority of humans would give. Assistant:\n",
      "```\n",
      "\n",
      "Figure 6: Pairwise scoring prompt for gpt-4 and claude-v1.3 .\n",
      "\n",
      "|                    | Method   | Vicuna Eval   | Vicuna Eval   | Self-inst. Eval   | Self-inst. Eval   | Dolly Eval   | Dolly Eval   | BPO-test Eval   | BPO-test Eval   | ∆ WR   |\n",
      "|--------------------|----------|---------------|---------------|-------------------|-------------------|--------------|--------------|-----------------|-----------------|--------|\n",
      "| Base LLM           | A B      | A win         | B win         | A win             | B win             | A win        | B win        | A win           | B win           | ∆ WR   |\n",
      "| gpt-3.5-turbo      | BPO ori. | 63.8          | 36.2          | 56.3              | 43.7              | 60.0         | 40.0         | 58.5            | 41.5            | +19.3  |\n",
      "| gpt-4              | BPO ori. | 53.8          | 46.2          | 51.2              | 48.8              | 62.0         | 38.0         | 51.5            | 48.5            | +9.2   |\n",
      "| claude-instant-1.2 | BPO ori. | 56.3          | 43.7          | 56.7              | 43.3              | 51.5         | 48.5         | 52.5            | 47.5            | +8.5   |\n",
      "| claude-2           | BPO ori. | 60.0          | 40.0          | 51.6              | 48.4              | 50.5         | 49.5         | 52.0            | 48.0            | +7.1   |\n",
      "| text-bison         | BPO ori. | 58.8          | 41.2          | 56.3              | 43.7              | 60.5         | 39.5         | 53.0            | 47.0            | +14.3  |\n",
      "\n",
      "Table 8: Win rates between BPO-aligned and original LLM APIs, evaluated by claude-v1.3 . Without training these LLMs, BPO can significantly improve block-box LLM APIs' alignment. ('Self-inst.' denotes 'Self-instruct', 'ori.' denotes 'original', and 'WR' denotes 'win rates').\n",
      "\n",
      "Table 9: Win rates between BPO-aligned and original llama-2-chat and vicuna-v1.3 LLMs, evaluated by claude-v1.3 . Training-free BPO improves alignment substantially, even making llama-2-13b-chat outperform llama-2-70b-chat . ('Self-inst.' denotes 'Self-instruct, and 'WR' denotes 'win rates').\n",
      "\n",
      "| Base LLM      | Method    | Method   | Vicuna Eval    | Vicuna Eval              | Self-inst. Eval   | Self-inst. Eval   | Dolly Eval               | Dolly Eval   | BPO-test Eval   | BPO-test Eval   | ∆ WR                  |\n",
      "|---------------|-----------|----------|----------------|--------------------------|-------------------|-------------------|--------------------------|--------------|-----------------|-----------------|-----------------------|\n",
      "| Base LLM      | A         | B        | A win          | B win                    | A win             | B win             | A win                    | B win        | A win           | B win           | ∆ WR                  |\n",
      "| llama-2 -chat | 7B + BPO  | 7B       | 55.0 52.5 48.8 | 45.0 47.5 51.2 53.7 47.5 | 52.0              | 48.0 43.7 52.0    | 56.0 57.0 51.0 62.0 56.0 | 44.0 43.0    | 58.0            | 42.0            | +10.5 +11.7 -0.6 +8.7 |\n",
      "| llama-2 -chat | 13B + BPO | 13B      |                |                          | 56.3              |                   |                          |              | 57.5            | 42.5            |                       |\n",
      "| llama-2 -chat | 7B + BPO  | 70B      |                |                          | 48.0              |                   |                          | 49.0         | 51.0            | 49.0            |                       |\n",
      "| llama-2 -chat | 13B + BPO | 70B      | 46.3           |                          | 55.6              | 44.4              |                          | 38.0         | 53.5            | 46.5            |                       |\n",
      "| llama-2 -chat | 70B + BPO | 70B      | 52.5           |                          | 52.4              | 47.6              |                          | 44.0         | 52.5            | 47.5            | +6.7                  |\n",
      "| vicuna        | 7B + BPO  | 7B       | 65.0           | 35.0                     | 56.7              | 43.3              | 54.0                     | 46.0         | 53.0            | 47.0            | +14.4                 |\n",
      "| -v1.3         | 13B + BPO | 13B      | 57.5           | 42.5                     | 54.0              | 46.0              | 56.5                     | 43.5         | 57.5            | 42.5            | +12.8                 |\n",
      "\n",
      "Table 10: Win rates between PPO, DPO, and BPO-aligned vicuna-v1.3 series LLMs, evaluated by claude-v1.3 . BPO not only outperforms both PPO and DPO, and could yield additional bonus over PPO and DPO-aligned LLMs. ('Self-inst.' denotes 'Self-instruct', 'ori.' denotes 'original', and 'WR' denotes 'win rates').\n",
      "\n",
      "| Method   | Method   | Vicuna Eval   | Vicuna Eval   | Self-inst. Eval   | Self-inst. Eval   | Dolly Eval   | Dolly Eval   | BPO-test Eval   | BPO-test Eval   | ∆ WR   |\n",
      "|----------|----------|---------------|---------------|-------------------|-------------------|--------------|--------------|-----------------|-----------------|--------|\n",
      "| A        | B        | A win         | B win         | A win             | B win             | A win        | B win        | A win           | B win           | ∆ WR   |\n",
      "| PPO      | ori.     | 53.8          | 46.2          | 48.8              | 51.2              | 52.5         | 47.5         | 52.5            | 47.5            | +3.8   |\n",
      "| BPO      | PPO      | 53.8          | 46.2          | 54.8              | 45.2              | 52.0         | 48.0         | 51.5            | 48.5            | +6.0   |\n",
      "| BPO+PPO  | ori.     | 57.5          | 42.5          | 51.2              | 48.8              | 57.5         | 42.5         | 56.5            | 43.5            | +11.4  |\n",
      "| BPO+PPO  | PPO      | 53.8          | 46.2          | 55.2              | 44.8              | 52.5         | 47.5         | 52.0            | 48.0            | +6.7   |\n",
      "| DPO      | ori.     | 53.8          | 46.2          | 54.8              | 45.2              | 55.0         | 45.0         | 58.0            | 42.0            | +10.8  |\n",
      "| BPO      | DPO      | 51.3          | 48.7          | 49.2              | 50.8              | 52.0         | 48.0         | 50.0            | 50.0            | +1.2   |\n",
      "| BPO+DPO  | ori.     | 62.5          | 37.5          | 62.3              | 37.7              | 57.5         | 42.5         | 62.0            | 38.0            | +22.2  |\n",
      "| BPO+DPO  | DPO      | 56.3          | 43.7          | 52.4              | 47.6              | 52.5         | 47.5         | 60.0            | 40.0            | +10.6  |\n",
      "| PPO      | ori.     | 47.5          | 52.5          | 55.2              | 44.8              | 61.5         | 38.5         | 51.0            | 49.0            | +7.6   |\n",
      "| BPO      | PPO      | 52.5          | 47.5          | 52.0              | 48.0              | 58.0         | 42.0         | 55.5            | 44.5            | +9.0   |\n",
      "| BPO+PPO  | ori.     | 57.5          | 42.5          | 60.3              | 39.7              | 62.0         | 38.0         | 57.5            | 42.5            | +18.7  |\n",
      "| BPO+PPO  | PPO      | 51.3          | 48.7          | 52.8              | 47.2              | 58.0         | 42.0         | 53.5            | 46.5            | +7.8   |\n",
      "| DPO      | ori.     | 48.8          | 51.2          | 54.0              | 46.0              | 58.0         | 42.0         | 58.0            | 42.0            | +9.4   |\n",
      "| BPO      | DPO      | 55.0          | 45.0          | 48.8              | 51.2              | 49.0         | 51.0         | 50.0            | 50.0            | +1.4   |\n",
      "| BPO+DPO  | ori.     | 57.5          | 42.5          | 60.7              | 39.3              | 60.5         | 39.5         | 62.0            | 38.0            | +20.4  |\n",
      "| BPO+DPO  | DPO      | 63.8          | 36.2          | 56.7              | 43.3              | 53.5         | 46.5         | 54.0            | 46.0            | +14.0  |\n",
      "\n",
      "| Base LLM   | Method   | Method            | Vicuna Eval   | Vicuna Eval   | Self-inst. Eval   | Self-inst. Eval   | Dolly Eval   | Dolly Eval   | BPO-test Eval   | BPO-test Eval   | ∆ WR        |\n",
      "|------------|----------|-------------------|---------------|---------------|-------------------|-------------------|--------------|--------------|-----------------|-----------------|-------------|\n",
      "| Base LLM   | A        | B                 | A win         | B win         | A win             | B win             | A win        | B win        | A win           | B win           | ∆ WR        |\n",
      "| llama-7b   | BPO-1k   | ori.-52k ori.-52k | 72.5          | 27.5          | 52.4              | 47.6              | 58.5         | 41.5         | 54.5            | 45.5            | +19.0 +22.2 |\n",
      "|            | BPO-52k  |                   | 76.3          | 23.7          | 53.2              | 46.8              | 57.0         | 43.0         | 58.0            | 42.0            |             |\n",
      "| llama-13b  | BPO-1k   | ori.-52k          | 77.5          | 22.5          | 61.1              | 38.9              | 61.5         | 38.5         | 64.0            | 36.0            | +32.1 +41.1 |\n",
      "|            | BPO-52k  | ori.-52k          | 86.3          | 13.7          | 69.0              | 31.0              | 57.5         | 42.5         | 69.5            | 30.5            |             |\n",
      "\n",
      "Table 11: Win rates between BPO reproduced and original alpaca dataset tuned llama-1 series LLMs, evaluated by claude-v1.3 . -1k means training the LLM with 1k randomly sampled data, -52k means using the whole dataset. ('Self-inst.' denotes 'Self-instruct, 'ori.' denotes 'original', and 'WR' denotes 'win rates').\n",
      "\n",
      "Table 12: Win rates between BPO optimization and directly using gpt-3.5-turbo for prompt optimization (w/o feedback), evaluated by claude-v1.3 . While using BPO can largely improve model performance, w/o feedback has little improvement. ('Self-inst.' denotes 'Self-instruct, 'ori.' denotes 'original', and 'WR' denotes 'win rates').\n",
      "\n",
      "|               | Method       | Method       | Method   | Vicuna Eval Self-inst.   | Vicuna Eval Self-inst.   | Eval   | Eval   | Dolly Eval   | Dolly Eval   | BPO-test Eval   | BPO-test Eval   | ∆ WR   |\n",
      "|---------------|--------------|--------------|----------|--------------------------|--------------------------|--------|--------|--------------|--------------|-----------------|-----------------|--------|\n",
      "|               | A            | B            | A win    | B win                    | A win                    | B win  | A win  | B win        | win          | A               | B win           | ∆ WR   |\n",
      "|               | BPO          | ori.         | 63.8     | 36.2                     | 56.3                     | 43.7   | 60.0   | 40.0         | 58.5         |                 | 41.5            | +19.3  |\n",
      "| gpt-3.5-turbo | w/o feedback | ori.         | 57.5     | 42.5                     | 44.4                     | 52.6   | 52.0   | 48.0         | 57.5         |                 | 42.5            | +6.5   |\n",
      "| gpt-3.5-turbo | BPO          | w/o feedback | 55.0     | 45.0                     | 53.6                     | 43.7   | 63.5   | 36.5         |              | 59.0            | 41.0            | +16.2  |\n",
      "\n",
      "Figure 7: Difference of win-lose rate of various versions of LLaMA-2-chat with BPO alignment v.s. LLaMA-2chat-70B scored by gpt-4 and claude-v1.3 .\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "The result of BPO for SFT data construction is shown in Table 11. Fine-tuning with BPO reproduced Alpaca dataset can largely enhance the alignment performance, with more than 40% win rate increase on llama-13b .\n",
      "\n",
      "As shown in Table 12, feedback is a critical component in BPO alignment. Optimization without feedback may bring a decline in some datasets, while BPO achieves significant gains on each test set.\n",
      "\n",
      "## G Iterative Prompt Optimization\n",
      "\n",
      "To show how the prompts are iteratively optimized, we cherry-pick an example in Figure 8. Comparing iteration 5 with the original prompt, we can see that the optimized prompt is more specific and complete, containing more possible scenarios about the question, which can prompt the LLM to give a more comprehensive and well-considered response.\n",
      "\n",
      "## H OPRO Experiments\n",
      "\n",
      "We compare BPO with one of the most recent prompt engineering methods, OPRO (Yang et al., 2023). OPRO, like other existing automated prompt engineering methods, requires a training dataset to perform its search for improved prompts; we sample 250 examples from each category of the Dolly (Conover et al., 2023) dataset, totaling 2000 instances. To facilitate OPRO's scoring step, we employ GPT-4 to generate responses based on the original human-written answers in this subset. Specially, we perform OPRO over 200 samples in each category, holding out 50 as the test set. Both scoring and the generation model used gpt-3.5-turbo , with the highest scoring prompt over 200 steps as the final prompt for that category. Leveraging the reproduced Dolly dataset, we adopt reference-based evaluation with gpt-4 . The scoring prompt is from (Zheng et al., 2023), shown in Figure 9. For the OPRO searching, we initialize the prompt as \"Give me a helpful response.\" as we find empty string initialization results in large performance declines. We should note BPO does not use any instances from the Dolly dataset for training,\n",
      "\n",
      "Figure 8: An example of iterative optimization. The refined parts are marked as red in each iteration compared with the last iteration.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "which also indicates BPO's better applicability in new tasks without the need for specific searching like OPRO.\n",
      "\n",
      "As shown in Figure 10, BPO achieves stable improvements across most categories, while OPRO degrades compared to the original performance on more than half the tasks with an average negative improvement across all tasks. In addition, BPO shows noticeable gains on General QA, which is an open-ended, topically diverse task, while OPRO exhibits largely performance declines. Our conjecture is that BPO performs sample-specific optimization and thus provides more tailored enhancement, while OPRO or other prompt engineering methods are task-specific and thus may be hurting the performance of some samples, which may also be one of the reasons why these methods are mostly un- stable. After looking into the optimized prompts, we find the large drop is indeed caused by adopting the same prompt for all samples in one task. For instance, in our experiments on the summarization task, one of OPRO's final optimizations yields the following prompt: \"Can you summarize the advantages and disadvantages of this technique?\" which clearly converges to a specific topic, leading to an obvious performance loss on many samples.\n",
      "\n",
      "## I Error Analysis\n",
      "\n",
      "Another advantage of strong interpretability is the ability to facilitate error analysis since iterative improvements can be made quickly from optimization failures. As shown in Figure 11, we present three illustrative examples of common errors (grey box). Error case 1 is over-specification, where the user's\n",
      "\n",
      "Figure 9: Reference-based evaluation prompt for gpt-4 .\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Figure 10: Differences in GPT-4 scores after optimization with OPRO and BPO compared to the original. In contrast to OPRO, BPO demonstrates consistent gains across nearly all tasks, whereas OPRO exhibits performance declines on over half of the tasks with an average negative improvement. For both BPO and OPRO, we run three times and calculate the average scores.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "instruction only provides general topics, but BPO turns the prompt into more specific ones. Such overspecification limits the LLM's output too much. Error case 2 shows an inconsistency between the original instruction and the optimized one. We trace this back to low-quality training data, where the response is inconsistent with the constraints in the original instruction but still annotated as the favor one. In error case 3, BPO neglects the additional context, making the instruction under-specified.\n",
      "\n",
      "Figure 11: BPO Optimization types and examples (above the line), as well as error cases (below the line).\n",
      "\n",
      "<!-- image -->\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 08:04:36,918 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:04:36,919 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:04:36,921 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:04:36,921 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:04:37,008 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:04:37,008 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:04:37,031 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:04:37,031 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 08:04:37,263 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 08:04:37,264 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:04:37,966 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:04:38,321 - INFO - Processing document 1706.03762v7.pdf\n",
      "2025-12-15 08:05:36,134 - INFO - Finished converting document 1706.03762v7.pdf in 59.51 sec.\n",
      "2025-12-15 08:05:36,280 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 08:05:36,283 - INFO - Going to convert document batch...\n",
      "2025-12-15 08:05:36,283 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 08:05:36,284 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 08:05:36,285 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 08:05:36,285 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 08:05:36,306 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:05:36,306 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:05:36,320 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:05:36,321 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\n",
      "\n",
      "## Attention Is All You Need\n",
      "\n",
      "Ashish Vaswani ∗ Google Brain avaswani@google.com\n",
      "\n",
      "Noam Shazeer ∗ Google Brain noam@google.com\n",
      "\n",
      "Llion Jones ∗ Google Research llion@google.com\n",
      "\n",
      "Niki Parmar ∗ Google Research nikip@google.com\n",
      "\n",
      "Aidan N. Gomez ∗ † University of Toronto aidan@cs.toronto.edu\n",
      "\n",
      "Jakob Uszkoreit ∗ Google Research usz@google.com\n",
      "\n",
      "Łukasz Kaiser ∗ Google Brain lukaszkaiser@google.com\n",
      "\n",
      "Illia Polosukhin ∗ ‡\n",
      "\n",
      "illia.polosukhin@gmail.com\n",
      "\n",
      "## Abstract\n",
      "\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
      "\n",
      "∗ Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n",
      "\n",
      "† Work performed while at Google Brain.\n",
      "\n",
      "‡ Work performed while at Google Research.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].\n",
      "\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h t , as a function of the previous hidden state h t -1 and the input for position t . This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\n",
      "\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\n",
      "\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "\n",
      "## 2 Background\n",
      "\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n",
      "\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\n",
      "\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "\n",
      "## 3 Model Architecture\n",
      "\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations ( x 1 , ..., x n ) to a sequence of continuous representations z = ( z 1 , ..., z n ) . Given z , the decoder then generates an output sequence ( y 1 , ..., y m ) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.\n",
      "\n",
      "Figure 1: The Transformer - model architecture.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\n",
      "\n",
      "## 3.1 Encoder and Decoder Stacks\n",
      "\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm( x +Sublayer( x )) , where Sublayer( x ) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d model = 512 .\n",
      "\n",
      "Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i .\n",
      "\n",
      "## 3.2 Attention\n",
      "\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "\n",
      "## Scaled Dot-Product Attention\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
      "\n",
      "## 3.2.1 Scaled Dot-Product Attention\n",
      "\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension d k , and values of dimension d v . We compute the dot products of the query with all keys, divide each by √ d k , and apply a softmax function to obtain the weights on the values.\n",
      "\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q . The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1 √ d k . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
      "\n",
      "While for small values of d k the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d k [3]. We suspect that for large values of d k , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4 . To counteract this effect, we scale the dot products by 1 √ d k .\n",
      "\n",
      "## 3.2.2 Multi-Head Attention\n",
      "\n",
      "Instead of performing a single attention function with d model-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k , d k and d v dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d v -dimensional\n",
      "\n",
      "4 To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1 . Then their dot product, q · k = ∑ d k i =1 q i k i , has mean 0 and variance d k .\n",
      "\n",
      "output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\n",
      "\n",
      "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Where the projections are parameter matrices W Q i ∈ R d model × d k , W i K ∈ R d model × d k , W V i ∈ R d model × d v and W O ∈ R hd v × d model .\n",
      "\n",
      "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use d k = d v = d model /h = 64 . Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
      "\n",
      "## 3.2.3 Applications of Attention in our Model\n",
      "\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "\n",
      "- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\n",
      "- The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
      "- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to -∞ ) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\n",
      "\n",
      "## 3.3 Position-wise Feed-Forward Networks\n",
      "\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is d model = 512 , and the inner-layer has dimensionality d ff = 2048 .\n",
      "\n",
      "## 3.4 Embeddings and Softmax\n",
      "\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √ d model.\n",
      "\n",
      "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\n",
      "\n",
      "| Layer Type                  | Complexity per Layer   | Sequential Operations   | Maximum Path Length   |\n",
      "|-----------------------------|------------------------|-------------------------|-----------------------|\n",
      "| Self-Attention              | O ( n 2 · d )          | O (1)                   | O (1)                 |\n",
      "| Recurrent                   | O ( n · d 2 )          | O ( n )                 | O ( n )               |\n",
      "| Convolutional               | O ( k · n · d 2 )      | O (1)                   | O ( log k ( n ))      |\n",
      "| Self-Attention (restricted) | O ( r · n · d )        | O (1)                   | O ( n/r )             |\n",
      "\n",
      "## 3.5 Positional Encoding\n",
      "\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d model as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\n",
      "\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2 π to 10000 · 2 π . We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k , PE pos + k can be represented as a linear function of PE pos .\n",
      "\n",
      "We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
      "\n",
      "## 4 Why Self-Attention\n",
      "\n",
      "In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations ( x 1 , ..., x n ) to another sequence of equal length ( z 1 , ..., z n ) , with x i , z i ∈ R d , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\n",
      "\n",
      "One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n",
      "\n",
      "The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n",
      "\n",
      "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O ( n ) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
      "\n",
      "length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O ( n/r ) . We plan to investigate this approach further in future work.\n",
      "\n",
      "A single convolutional layer with kernel width k &lt; n does not connect all pairs of input and output positions. Doing so requires a stack of O ( n/k ) convolutional layers in the case of contiguous kernels, or O ( log k ( n )) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k . Separable convolutions [6], however, decrease the complexity considerably, to O ( k · n · d + n · d 2 ) . Even with k = n , however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\n",
      "\n",
      "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\n",
      "\n",
      "## 5 Training\n",
      "\n",
      "This section describes the training regime for our models.\n",
      "\n",
      "## 5.1 Training Data and Batching\n",
      "\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n",
      "\n",
      "## 5.2 Hardware and Schedule\n",
      "\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\n",
      "\n",
      "## 5.3 Optimizer\n",
      "\n",
      "We used the Adam optimizer [20] with β 1 = 0 . 9 , β 2 = 0 . 98 and ϵ = 10 -9 . We varied the learning rate over the course of training, according to the formula:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "This corresponds to increasing the learning rate linearly for the first warmup \\_ steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup \\_ steps = 4000 .\n",
      "\n",
      "## 5.4 Regularization\n",
      "\n",
      "We employ three types of regularization during training:\n",
      "\n",
      "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      "\n",
      "| Model                           | BLEU   | BLEU   | Training Cost (FLOPs)   | Training Cost (FLOPs)   |\n",
      "|---------------------------------|--------|--------|-------------------------|-------------------------|\n",
      "|                                 | EN-DE  | EN-FR  | EN-DE                   | EN-FR                   |\n",
      "| ByteNet [18]                    | 23.75  |        |                         |                         |\n",
      "| Deep-Att + PosUnk [39]          |        | 39.2   |                         | 1 . 0 · 10 20           |\n",
      "| GNMT + RL [38]                  | 24.6   | 39.92  | 2 . 3 · 10 19           | 1 . 4 · 10 20           |\n",
      "| ConvS2S [9]                     | 25.16  | 40.46  | 9 . 6 · 10 18           | 1 . 5 · 10 20           |\n",
      "| MoE [32]                        | 26.03  | 40.56  | 2 . 0 · 10 19           | 1 . 2 · 10 20           |\n",
      "| Deep-Att + PosUnk Ensemble [39] |        | 40.4   |                         | 8 . 0 · 10 20           |\n",
      "| GNMT + RL Ensemble [38]         | 26.30  | 41.16  | 1 . 8 · 10 20           | 1 . 1 · 10 21           |\n",
      "| ConvS2S Ensemble [9]            | 26.36  | 41.29  | 7 . 7 · 10 19           | 1 . 2 · 10 21           |\n",
      "| Transformer (base model)        | 27.3   | 38.1   | 3 . 3 · 10 18           | 3 . 3 · 10 18           |\n",
      "| Transformer (big)               | 28.4   | 41.8   | 2 . 3 · 10 19           | 2 . 3 · 10 19           |\n",
      "\n",
      "Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P drop = 0 . 1 .\n",
      "\n",
      "Label Smoothing During training, we employed label smoothing of value ϵ ls = 0 . 1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "\n",
      "## 6 Results\n",
      "\n",
      "## 6.1 Machine Translation\n",
      "\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2 . 0 BLEU, establishing a new state-of-the-art BLEU score of 28 . 4 . The configuration of this model is listed in the bottom line of Table 3. Training took 3 . 5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n",
      "\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41 . 0 , outperforming all of the previously published single models, at less than 1 / 4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate P drop = 0 . 1 , instead of 0 . 3 .\n",
      "\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0 . 6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50 , but terminate early when possible [38].\n",
      "\n",
      "Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5 .\n",
      "\n",
      "## 6.2 Model Variations\n",
      "\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\n",
      "\n",
      "5 We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
      "\n",
      "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n",
      "\n",
      "|      | N                                         | d model                                   | d ff                                      | h                                         | d k                                       | d v                                       | P drop                                    | ϵ ls                                      | train steps   |   PPL (dev) |   BLEU (dev) | params × 10 6   |\n",
      "|------|-------------------------------------------|-------------------------------------------|-------------------------------------------|-------------------------------------------|-------------------------------------------|-------------------------------------------|-------------------------------------------|-------------------------------------------|---------------|-------------|--------------|-----------------|\n",
      "| base | 6                                         | 512                                       | 2048                                      | 8                                         | 64                                        | 64                                        | 0.1                                       | 0.1                                       | 100K          |        4.92 |         25.8 | 65              |\n",
      "|      |                                           |                                           |                                           | 1                                         | 512                                       | 512                                       |                                           |                                           |               |        5.29 |         24.9 |                 |\n",
      "|      |                                           |                                           |                                           | 4                                         | 128                                       | 128                                       |                                           |                                           |               |        5    |         25.5 |                 |\n",
      "| (A)  |                                           |                                           |                                           | 16                                        | 32                                        | 32                                        |                                           |                                           |               |        4.91 |         25.8 |                 |\n",
      "|      |                                           |                                           |                                           | 32                                        | 16                                        | 16                                        |                                           |                                           |               |        5.01 |         25.4 |                 |\n",
      "|      |                                           |                                           |                                           |                                           | 16                                        |                                           |                                           |                                           |               |        5.16 |         25.1 | 58              |\n",
      "| (B)  |                                           |                                           |                                           |                                           | 32                                        |                                           |                                           |                                           |               |        5.01 |         25.4 | 60              |\n",
      "|      | 2                                         |                                           |                                           |                                           |                                           |                                           |                                           |                                           |               |        6.11 |         23.7 | 36              |\n",
      "|      | 4                                         |                                           |                                           |                                           |                                           |                                           |                                           |                                           |               |        5.19 |         25.3 | 50              |\n",
      "|      | 8                                         |                                           |                                           |                                           |                                           |                                           |                                           |                                           |               |        4.88 |         25.5 | 80              |\n",
      "| (C)  |                                           | 256                                       |                                           |                                           | 32                                        | 32                                        |                                           |                                           |               |        5.75 |         24.5 | 28              |\n",
      "|      |                                           | 1024                                      |                                           |                                           | 128                                       | 128                                       |                                           |                                           |               |        4.66 |         26   | 168             |\n",
      "|      |                                           |                                           | 1024                                      |                                           |                                           |                                           |                                           |                                           |               |        5.12 |         25.4 | 53              |\n",
      "|      |                                           |                                           | 4096                                      |                                           |                                           |                                           |                                           |                                           |               |        4.75 |         26.2 | 90              |\n",
      "|      |                                           |                                           |                                           |                                           |                                           |                                           | 0.0                                       |                                           |               |        5.77 |         24.6 |                 |\n",
      "|      |                                           |                                           |                                           |                                           |                                           |                                           | 0.2                                       |                                           |               |        4.95 |         25.5 |                 |\n",
      "| (D)  |                                           |                                           |                                           |                                           |                                           |                                           |                                           | 0.0                                       |               |        4.67 |         25.3 |                 |\n",
      "|      |                                           |                                           |                                           |                                           |                                           |                                           |                                           | 0.2                                       |               |        5.47 |         25.7 |                 |\n",
      "| (E)  | positional embedding instead of sinusoids | positional embedding instead of sinusoids | positional embedding instead of sinusoids | positional embedding instead of sinusoids | positional embedding instead of sinusoids | positional embedding instead of sinusoids | positional embedding instead of sinusoids | positional embedding instead of sinusoids |               |        4.92 |         25.7 |                 |\n",
      "| big  | 6                                         | 1024                                      | 4096                                      | 16                                        |                                           |                                           | 0.3                                       |                                           | 300K          |        4.33 |         26.4 | 213             |\n",
      "\n",
      "development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\n",
      "\n",
      "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "\n",
      "In Table 3 rows (B), we observe that reducing the attention key size d k hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\n",
      "\n",
      "## 6.3 English Constituency Parsing\n",
      "\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
      "\n",
      "We trained a 4-layer transformer with d model = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n",
      "\n",
      "We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\n",
      "\n",
      "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\n",
      "\n",
      "| Parser                             | Training                 |   WSJ 23 F1 |\n",
      "|------------------------------------|--------------------------|-------------|\n",
      "| Vinyals &Kaiser el al. (2014) [37] | WSJ only, discriminative |        88.3 |\n",
      "| Petrov et al. (2006) [29]          | WSJ only, discriminative |        90.4 |\n",
      "| Zhu et al. (2013) [40]             | WSJ only, discriminative |        90.4 |\n",
      "| Dyer et al. (2016) [8]             | WSJ only, discriminative |        91.7 |\n",
      "| Transformer (4 layers)             | WSJ only, discriminative |        91.3 |\n",
      "| Zhu et al. (2013) [40]             | semi-supervised          |        91.3 |\n",
      "| Huang &Harper (2009) [14]          | semi-supervised          |        91.3 |\n",
      "| McClosky et al. (2006) [26]        | semi-supervised          |        92.1 |\n",
      "| Vinyals &Kaiser el al. (2014) [37] | semi-supervised          |        92.1 |\n",
      "| Transformer (4 layers)             | semi-supervised          |        92.7 |\n",
      "| Luong et al. (2015) [23]           | multi-task               |        93   |\n",
      "| Dyer et al. (2016) [8]             | generative               |        93.3 |\n",
      "\n",
      "increased the maximum output length to input length + 300 . We used a beam size of 21 and α = 0 . 3 for both WSJ only and the semi-supervised setting.\n",
      "\n",
      "Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\n",
      "\n",
      "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.\n",
      "\n",
      "## 7 Conclusion\n",
      "\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\n",
      "\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "\n",
      "The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor .\n",
      "\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\n",
      "\n",
      "## References\n",
      "\n",
      "- [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016.\n",
      "- [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR , abs/1409.0473, 2014.\n",
      "- [3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR , abs/1703.03906, 2017.\n",
      "- [4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733 , 2016.\n",
      "\n",
      "- [5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR , abs/1406.1078, 2014.\n",
      "- [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357 , 2016.\n",
      "- [7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\n",
      "- [8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL , 2016.\n",
      "- [9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\n",
      "- [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 , 2013.\n",
      "- [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770-778, 2016.\n",
      "- [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\n",
      "- [13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation , 9(8):1735-1780, 1997.\n",
      "- [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing , pages 832-841. ACL, August 2009.\n",
      "- [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\n",
      "- [16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS) , 2016.\n",
      "- [17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR) , 2016.\n",
      "- [18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 , 2017.\n",
      "- [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations , 2017.\n",
      "- [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\n",
      "- [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722 , 2017.\n",
      "- [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130 , 2017.\n",
      "- [23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\n",
      "- [24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attentionbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\n",
      "\n",
      "- [25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics , 19(2):313-330, 1993.\n",
      "- [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference , pages 152-159. ACL, June 2006.\n",
      "- [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing , 2016.\n",
      "- [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304 , 2017.\n",
      "- [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL , pages 433-440. ACL, July 2006.\n",
      "- [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859 , 2016.\n",
      "- [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909 , 2015.\n",
      "- [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 , 2017.\n",
      "- [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research , 15(1):1929-1958, 2014.\n",
      "- [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2440-2448. Curran Associates, Inc., 2015.\n",
      "- [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems , pages 3104-3112, 2014.\n",
      "- [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\n",
      "- [37] Vinyals &amp; Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems , 2015.\n",
      "- [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144 , 2016.\n",
      "- [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\n",
      "- [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers) , pages 434-443. ACL, August 2013.\n",
      "\n",
      "## Attention Visualizations Input-Input Layer5\n",
      "\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult'. Attentions here shown only for the word 'making'. Different colors represent different heads. Best viewed in color.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Input-Input Layer5\n",
      "\n",
      "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word 'its' for attention heads 5 and 6. Note that the attentions are very sharp for this word.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Input-Input Layer5\n",
      "\n",
      "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "\n",
      "<!-- image -->\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 08:05:36,508 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:05:36,508 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:05:36,510 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:05:36,511 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:05:36,590 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:05:36,590 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:05:36,613 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:05:36,613 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 08:05:36,832 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 08:05:36,833 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:05:37,516 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:05:37,812 - INFO - Processing document 2302.09664v3.pdf\n",
      "2025-12-15 08:06:13,685 - INFO - Finished converting document 2302.09664v3.pdf in 37.50 sec.\n",
      "2025-12-15 08:06:13,828 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 08:06:13,831 - INFO - Going to convert document batch...\n",
      "2025-12-15 08:06:13,832 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 08:06:13,833 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 08:06:13,834 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 08:06:13,835 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:13,857 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:13,858 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:13,871 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:13,871 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## SEMANTIC UNCERTAINTY: LINGUISTIC INVARIANCES FOR UNCERTAINTY ESTIMATION IN NATURAL LANGUAGE GENERATION\n",
      "\n",
      "Lorenz Kuhn, Yarin Gal, Sebastian Farquhar\n",
      "\n",
      "OATML Group, Department of Computer Science, University of Oxford lorenz.kuhn@cs.ox.ac.uk\n",
      "\n",
      "## ABSTRACT\n",
      "\n",
      "We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of 'semantic equivalence'-different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to 'off-the-shelf' language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.\n",
      "\n",
      "## 1 INTRODUCTION\n",
      "\n",
      "Despite progress in natural language generation (NLG) tasks like question answering or abstractive summarisation (Brown et al., 2020; Hoffmann et al., 2022; Chowdhery et al., 2022), there is little understanding of uncertainty in foundation models. Without measures of uncertainty in transformerbased systems it is hard to use generated language as a reliable source of information. Reliable measures of uncertainty have been identified as a key problem in building safer AI systems (Amodei et al., 2016; Hendrycks et al., 2022).\n",
      "\n",
      "Unfortunately, uncertainty in free-form NLG faces unique challenges. This limits how much we can learn from uncertainty estimation techniques in other applications of deep learning (Gal et al., 2016; Lakshminarayanan et al., 2017; Ovadia et al., 2019) which focuses especially on image classification (Kendall &amp; Gal, 2017) or regression in low-dimensional data spaces (Kuleshov et al., 2018).\n",
      "\n",
      "The key challenges come from the importance in language of meanings and form . This corresponds to what linguists and philosophers call the semantic content of a sentence and its syntactic or lexical form. Foundation models output token -likelihoods-representing lexical confidence. But for almost all applications we care about meanings! For example, a model which is uncertain about whether to generate 'France's capital is Paris' or 'Paris is France's capital' is not uncertain in any important sense. Yet, at a token-level the model is uncertain between two forms of the same meaning . Existing unsupervised methods (e.g., Malinin &amp; Gales (2020)) ignore this distinction.\n",
      "\n",
      "To address semantic equivalence, we estimate semantic likelihoods-probabilities attached to meanings of text rather than standard sequence-likelihoods. We introduce an algorithm for clustering sequences that mean the same thing based on the principle that two sentences mean the same thing if you can infer each from the other. We then use these semantic-likelihoods to estimate semantic uncertainty-uncertainty over different meanings. In particular, we compute the entropy of the probability distribution over meanings. Adjusting for semantic equivalence in this way offers better uncertainty estimation than standard entropy and also greatly improves over methods for model self-evaluation (Kadavath et al., 2022). In addition, semantic entropy scales better with model size and makes better use of increasing numbers of samples than baselines.\n",
      "\n",
      "We further analyse major challenges for measuring uncertainty in NLG. We show empirically how sampling a set of model answers to estimate entropies in NLG must balance sample accuracy and diversity, which significantly strengthens the baselines we compare against relative to prior imple-\n",
      "\n",
      "⋃˜⌉∐{√]̂(˜{√√}√glyph[arrowbt]({}√√√}\n",
      "\n",
      "\n",
      "\n",
      "⊗√⌉̂˜√(}˜(√∐√∐⌉˜√˜√√(}˜(⌉}̂˜⌈\n",
      "\n",
      "Figure 1: (a) Our semantic entropy (blue) predicts model accuracy better than baselines on the free-form question answering data set TriviaQA (30B parameter OPT model). Normalised entropy reimplements single-model variant of Malinin &amp; Gales (2020), lexical similarity measures the average Rouge-L in a sampled set of answers for a given question analogously to Fomicheva et al. (2020), entropy and p ( True ) reimplement Kadavath et al. (2022). (b) Our method's outperformance increases with model size while also doing well for smaller models.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "mentations. We also examine the situational heuristic of length-normalising predictive entropies. Our main contributions are thus as follows:\n",
      "\n",
      "- We explain why uncertainty in free-form NLG is different from other settings (Section 3).\n",
      "- We introduce semantic entropy -a novel entropy-based uncertainty measure which uses our algorithm for marginalising over semantically-equivalent samples (Section 4) and show that it outperforms comparable baselines in extensive ablations with both open- and closedbook free-form question answering using TriviaQA and CoQA (Section 6).\n",
      "- Through hyperparameter ablations we suggest how to balance the trade-off between sampling diverse and accurate generations for our method as well as baselines (Section 6.2) and show that far fewer samples are needed for effective uncertainty than prior work presumes.\n",
      "\n",
      "We focus on free-form question answering (QA) because it is a difficult and important use of NLG with high-stakes applications. At the same time, it is easier to establish a ground truth without expensive human evaluation than more nebulous tasks like summarisation.\n",
      "\n",
      "Ultimately, we show that semantic entropy is an effective unsupervised way to estimate uncertainty in NLG. As an unsupervised method, it requires no further training or data-gathering, unlike supervised methods including Lin et al. (2022a); Kadavath et al. (2022). Semantic entropy is designed to work with existing foundation and large language models with no modifications 'out-of-the-box'. Our experiments use OPT (Zhang et al., 2022) but semantic entropy works with any similar model.\n",
      "\n",
      "## 2 BACKGROUND ON UNCERTAINTY ESTIMATION\n",
      "\n",
      "Our method draws inspiration from probabilistic tools for uncertainty estimation, which have been extensively employed in settings like deep image classification (Gal et al., 2016). Although these methods are often used in Bayesian models, we emphasise that our method does not require any special training or architectural modifications and is not limited to Bayesian settings.\n",
      "\n",
      "The total uncertainty of a prediction can be understood as the predictive entropy of the output distribution. This measures the information one has about the output given the input. This entropy is highest when the output is minimally informative-predicting the same probability for all possible outcomes. The predictive entropy for a point x is the conditional entropy of the output random variable Y with realisation y given x\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "One can further distinguish aleatoric uncertainty-uncertainty in the underlying data distributionand epistemic uncertainty-resulting from missing information (Kendall &amp; Gal, 2017). Epistemic\n",
      "\n",
      "uncertainty, measured using a mutual information, can be useful but is hard to estimate, especially for very large models, requiring special methods and computational expense. Instead of estimating the epistemic uncertainty based on the model variance, the epistemic uncertainty can also be predicted directly using a second model (see e.g. Jain et al. (2021)). We do not use mutual information in this work, because our focus is on existing foundation models 'off-the-shelf'. Similarly, while, e.g., Malinin &amp; Gales (2020) use ensembles of models to estimate the integral in Eq. (1) we use samples from a single model's output distribution. Prior networks (Malinin &amp; Gales, 2018; Malinin et al., 2020) estimate model uncertainty by emulating an ensemble with a single model. This could be important for NLG because of large model sizes.\n",
      "\n",
      "For sequence-prediction tasks like NLG, the probability of the entire sequence, s , is the product of the conditional probabilities of new tokens given past tokens, whose resulting log-probability is log p ( s | x ) = ∑ i log p ( s i | s &lt;i ) , where s i is the i 'th output token and s &lt;i denotes the set of previous tokens. Sometimes, instead of the entropy of these probabilities, the geometric mean tokenprobability is used instead (Malinin &amp; Gales, 2020) becoming an arithmetic mean log-probability 1 N ∑ N i log p ( s i | s &lt;i ) . Despite empirical success Murray &amp; Chiang (2018), so far this has little theoretical justification.\n",
      "\n",
      "Direct application of language models to uncertainty. In contrast to our approach using probabilistic methods, recent work has sought to use the generating language model itself to estimate its own uncertainty. For example, Lin et al. (2022a) finetune language models to verbally describe their confidence. Meanwhile, Kadavath et al. (2022) sample multiple generations and return the completion to an NLG prompt asking if a proposed answer is true (further detail in Appendix B.5). Both Lin et al. (2022a) and Kadavath et al. (2022) also propose ways to finetune predictors on the embeddings of generating models to predict models uncertainty. While promising, these approaches need task-specific labels, additional training, and seem to be unreliable out-of-distribution (as shown in Figures 13 and 14 in Kadavath et al. (2022)).\n",
      "\n",
      "## 3 CHALLENGES IN UNCERTAINTY ESTIMATION FOR NLG\n",
      "\n",
      "Approaches to NLG uncertainty might treat the language model as a black-box (e.g., asking it if its answer is correct) or alternatively focus on the probabilistic model without accounting for the special characteristics of language (e.g., measuring predictive entropy).\n",
      "\n",
      "Our unsupervised approach instead uses the powerful tools of probabilistic modelling, but also recognises the unique challenges posed by free-form NLG. In this section, we critically analyse the probabilistic interpretation of language models in order to ground both our method and future exploration of the field.\n",
      "\n",
      "## 3.1 SEMANTIC EQUIVALENCE IN LANGUAGE OUTPUTS\n",
      "\n",
      "Most machine learning problems have mutually exclusive outputs. An image in class 17 is not class 29 as well; a regression output of 23.1 is not anything else; an RL agent going left does not go right. In contrast, for free-form text generation an output usually means the same thing as many other outputs. For example, 'The capital of France is Paris' means the same thing as 'France's capital is Paris'. Linguists and philosophers distinguish text's meaning-its semantic content-from its syntactic and lexical form. The syntax is the grammatical structure while its lexical form is the specific words used. Lexical equivalence entails the other two, but not the reverse.\n",
      "\n",
      "We almost always care about the semantic content of a sentence. For decision-problems relying on NLG, meaning is usually an invariance in output-space which is not present in the model specification. This is true for question answering, summarisation, artificial assistants. Meanings are especially important for trustworthiness: a system can be reliable even with many different ways to say the same thing but answering with inconsistent meanings shows poor reliability.\n",
      "\n",
      "We can formalize semantic equivalence mathematically. Let the space of tokens in a language be T . The space of all possible sequences of tokens of length N is then S N ≡ T N . For some sentence s ∈ S N , a sequence of tokens s i ∈ T there is an associated meaning. 1\n",
      "\n",
      "Let us introduce a placeholder semantic equivalence relation , E ( · , · ) , which holds of any two sentences that mean the same thing-we operationalise this in Section 4. Recall that an equivalence\n",
      "\n",
      "1 Theories of meaning are contested (Speaks, 2021). However, for specific models and deployment contexts many considerations can be set aside. Care should be taken comparing very different models and contexts.\n",
      "\n",
      "Table 1: Answers to the question 'What is the capital of France?' (a) When all generations from the model mean different things, semantic clustering has no effect-the entropy and semantic entropy are identical. (b) When some of the answers are semantically equivalent ('Paris' and 'It's Paris') the semantic entropy does a better job of capturing the actually low uncertainty.\n",
      "\n",
      "(a) Scenario 1: No semantic equivalence\n",
      "\n",
      "(b) Scenario 2: Some semantic equivalence\n",
      "\n",
      "| Answer s   |   Likelihood p ( s | x ) |   Semantic likelihood ∑ s ∈ c p ( s | x ) | Answer s   | Likelihood p ( s | x )   | Semantic likelihood ∑ s ∈ c p ( s | x )   |\n",
      "|------------|--------------------------|-------------------------------------------|------------|--------------------------|-------------------------------------------|\n",
      "| Paris      |                     0.5  |                                      0.5  | Paris      | 0.5 }                    | 0.9                                       |\n",
      "| Rome       |                     0.4  |                                      0.4  | It's Paris | 0.4                      |                                           |\n",
      "| London     |                     0.1  |                                      0.1  | London     | 0.1                      | 0.1                                       |\n",
      "| Entropy    |                     0.94 |                                      0.94 | Entropy    | 0.94                     | 0.33                                      |\n",
      "\n",
      "relation is any reflexive, symmetric, and transitive relation, and that any equivalence relation on a set corresponds to a set of equivalence classes. Each semantic equivalence class corresponds to one possible meaning that our text can have. That is, for the space of semantic equivalence classes C the sentences in the set c ∈ C all share a meaning such that ∀ s, s ′ ∈ c : E ( s, s ′ ) .\n",
      "\n",
      "Ordinarily, large language models produce conditional distributions over tokens and their resulting sequences. That is, the probability of the sequence conditioned on the context comes from conditional token probabilities p ( s | x ) = ∏ i p ( s i | s &lt;i , x ) . Instead, we focus on the probability of the model generating any sequence that shares some meaning. This can be written as\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Formally, this treats the output random variable whose event-space is C , a subσ -algebra of the standard event-space S .\n",
      "\n",
      "## 3.2 SAMPLING THE EXTREMELY HIGH-DIMENSIONAL LANGUAGE-SPACE\n",
      "\n",
      "Recall from Eq. (1) that estimating predictive entropy requires taking an expectation in output-space. However, the output-space of natural language has O ( |T | N ) dimensions. Moreover, while we can sample from our autoregressive token-model, we lack a normalized probability density function over sentences. The expectation must be approximated by Monte Carlo integration-sampling a finite set of sentences from the output distribution and averaging their likelihoods to compute the entropy. For entropies the average is dominated by low-probability sentences (whose logs are large and negative) making Monte Carlo integration difficult (Mackay, 2003).\n",
      "\n",
      "## 3.3 VARIABLE LENGTH GENERATIONS\n",
      "\n",
      "Sentences of natural language have different lengths. As is widely noted (Murray &amp; Chiang, 2018) and especially in the context of NLG uncertainty by Malinin &amp; Gales (2020), in expectation longer sequences have lower joint likelihoods because of the conditional independence of the token probabilities. The joint likelihood of a sequence of length N shrinks exponentially in N . Its negative log-probability therefore grows linearly in N , so longer sentences tend to contribute more to entropy.\n",
      "\n",
      "We therefore interpret length-normalising the log-probabilities when estimating the entropy as asserting that the expected uncertainty of generations is independent of sentence length. Sometimes, this is approximately valid. Other times, longer sentences may well be usually more uncertain (e.g., when the goal is to exactly match a typically short reference answer, such as for TriviaQA). In these cases, the advantages of length-normalisation become less clear-cut, as we show empirically in Section 6.1. This offers some guidance a priori on cases when length-normalisation is appropriate.\n",
      "\n",
      "## 4 SEMANTIC UNCERTAINTY\n",
      "\n",
      "We have introduced the idea that uncertainty over meanings is more important for most situations than uncertainty over the exact tokens used to express those meanings. Our method examines uncertainty in meaning-space-the entropy of the random variable representing the output distribution in the semantic event-space. This is in contrast to entropy in the usual token event-space. To do this we introduce a novel algorithm for estimating the semantic equivalence relation as well as a novel uncertainty estimation algorithm for semantic entropy. At a high level this involves three steps:\n",
      "\n",
      "1. Generation: Sample M sequences { s (1) , . . . , s ( M ) } from the predictive distribution of a large language model given a context x .\n",
      "2. Clustering: Cluster the sequences which mean the same thing using our bi-directional entailment algorithm.\n",
      "3. Entropy estimation: Approximate semantic entropy by summing probabilities that share a meaning following Eq. (2) and compute resulting entropy. This is illustrated in Table 1.\n",
      "\n",
      "## Step 1: Generating a set of answers from the model\n",
      "\n",
      "First we sample M sequences { s (1) , . . . , s ( M ) } which we will use later to estimate the uncertainty. These sequences must be sampled according to the distribution p ( s | x ) . In this paper, we sample these sequences only from a single model using either multinomial sampling or multinomial beam sampling. We show in Section 6.2, that the choice of sampling temperature and sampling method can have a significant impact on the performance of both our method and the baselines. Unlike Malinin &amp; Gales (2020), we do not use an ensemble of models. Ensembling would probably improve performance, but the cost of training multiple independent foundation models is often prohibitive.\n",
      "\n",
      "## Step 2: Clustering by semantic equivalence\n",
      "\n",
      "In Section 3.1, we formalised semantic equivalence by introducing the semantic equivalence relation, E ( · , · ) , which induces semantic equivalence classes which are sets of sequences that share a meaning. Recall that the equivalence class c is a set of sequences s such that ∀ s, s ′ ∈ c : E ( s, s ′ ) . We operationalise E ( · , · ) using the idea of bi-directional entailment. A sequence, s , means the same thing as a second sequence, s ′ , if and only if they entail (i.e. logically imply) each other. E.g., 'The capital of France is Paris.' entails 'Paris is the capital of France.' because they mean the same thing.\n",
      "\n",
      "Importantly, we require that the sequences mean the same thing with respect to the context-key meaning is sometimes contained within the context. For example, 'Paris.' does not entail 'The capital of France is Paris.' because 'Paris.' is not a declarative sentence without context. But within the context of the question, the one-word answer does entail the fuller answer.\n",
      "\n",
      "In general, any natural language inference classification system (NLI) can be used for our bidirectional entailment clustering algorithm. In our case, we use a Deberta-large model (He et al., 2020a) that is fine-tuned on the NLI data set MNLI (Williams et al., 2017). For each pair of sequences in our set of samples, s and s ′ , we detect whether it is possible to infer the concatenation of the context and s from the concatenation of the context and s ′ and vice versa. To do this we concatenate each of the two question/answer pairs, and then concatenate them both together separated by a special token. The Deberta model then classifies this sequence into one of: entailment , neutral , contradiction . We compute both directions, and the algorithm returns equivalent if and only if both directions were entailment . Algorithm pseudocode is provided in Appendix A.2.\n",
      "\n",
      "Because this component is novel, we confirm in Appendix B.2 that the bidirectional entailment classifier works by manually labelling 300 generations for semantic equivalence, finding an accuracy of 92.7% on TriviaQA and 95.5% on CoQA.\n",
      "\n",
      "Computational cost. The bidirectional equivalence algorithm is combinatorially complex in M , it requires ( M 2 ) -many comparisons in the worst-case. In practice, however, the computational cost is small compared to the cost of generating sequences. First, as we show in Section 6.2, M &lt; 20 is often sufficient for good uncertainty. Second, because the Deberta-large model is so much smaller than the main language model (1.5B parameters), each pair comparison is much faster than generating even one token from the main model. Third, because semantic equivalence is transitive we only need to compare one member of each equivalence class to remaining sequences (see Algorithm 1). Additionally, the number of semantic clusters in our tasks is empirically quite low, see Table 2.\n",
      "\n",
      "## Step 3: Computing the semantic entropy\n",
      "\n",
      "Having determined the clusters of generated sequences that mean the same thing, we add their likelihoods following Eq. (2) as a way of determining the likelihood of each meaning, rather than each sequence. We then compute the semantic entropy (SE) as the entropy over the meaning-distribution\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "We do not have access to every possible meaning-class c . Instead, we can only sample c from the sequence-generating distribution induced by the model. To handle this, we estimate the expectation in Eq. (3) using Monte Carlo integration over the semantic equivalence classes C from Algorithm 1\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "This is an unbiased estimator of the entropy in Eq. (3). In addition, in some cases we use lengthnormalisation as described in Section 3.3.\n",
      "\n",
      "## 4.1 HOW THE SEMANTIC ENTROPY ADDRESSES THE CHALLENGES OF NLG\n",
      "\n",
      "The main inspiration of semantic entropy is to address the semantic invariance of natural language head-on by converting the problem of uncertainty estimation into meaning-space. In addition, semantic entropy goes some way towards addressing unequal token importance. Generations whose meanings are the same but differ on unimportant tokens will be added together, which we expect will reduce the effect of the likelihoods of unimportant tokens although we do not demonstrate this empirically. However, this challenge is only partially addressed: semantic entropy will still pay too much attention to non-keyword likelihoods. This is one area where supervised language-modelbased uncertainty tools (Lin et al., 2022a; Kadavath et al., 2022) might enable future improvements that handle this challenge better. We address the challenges of sampling and variable-length generation using specific details of our sampling procedure in Section 4.\n",
      "\n",
      "## 5 RELATED WORK\n",
      "\n",
      "Prior work on uncertainty in foundation models for NLP has largely focused on the calibration of classifiers (Jiang et al., 2021; Desai &amp; Durrett, 2020) and text regressors (Glushkova et al., 2021; Wang et al., 2022). These settings, are analogous to classification or regression settings in other modalities like vision, and conventional uncertainty measures like MC dropout or Deep Ensembles can be applied without modification (see Section 2 for a discussion of uncertainty in deep learning in general). As we argue in Section 3, generative natural language poses important further challenges. Jiang et al. (2021) do examine calibration in generative question answering and find only a weak correlation between the log-likelihood models assign to their answer and the answer's correctness. In Section 6 we explain however why semantic equivalence in natural language makes calibration a problematic evaluation for generative language models. Reliable uncertainty can be useful on downstream tasks such as graph semantic parsing (Lin et al., 2022b).\n",
      "\n",
      "Some research has addressed uncertainty or calibration in NLG either by prompting the models to evaluate their own generations or by fine-tuning the generating model to predict its uncertainty (Mielke et al., 2020; Lin et al., 2022a; Kadavath et al., 2022). These methods need further training and supervision. Because they need additional training and supervision, they are hard to reproduce, expensive to create, and have been shown to be sensitive to distribution shift. For example, we were unable to implement one proposal by Kadavath et al. (2022) to train a language model to directly predict confidence due to hardware limitations. Our unsupervised method which uses models 'offthe-shelf' avoids these limitations.\n",
      "\n",
      "Many of the issues that make probabilistic uncertainty estimation in NLG difficult also make automatic evaluation of NLG difficult. Ott et al. (2018), for instance, study how the performance of machine translation models suffers because one sentence can be translated in multiple ways. Similarly, Sai et al. (2022) discuss how paraphrase detection can be used to evaluate NLG and other related methods might transfer to uncertainty estimation.\n",
      "\n",
      "Automatic paraphrase identification can be based on comparing lexical features of two given sequences (Fernando &amp; Stevenson, 2008; Issa et al., 2018) or on measuring the similarity between the embeddings of the two sequences (Yu et al., 2014; Socher et al., 2011). Recently, however, SotA paraphrase identification approaches have primarily used BERT-based models to classify pairs of sequences into the classes paraphrases and not paraphrases (He et al., 2020b; Tay et al., 2021). The idea of formalising semantic equivalence via textual entailment has a long history in linguistics (Culicover, 1968) and NLP (Pad´ o et al., 2009; Androutsopoulos &amp; Malakasiotis, 2010). Transformer-based paraphrase detection models such as EFL (Wang et al., 2021) achieve SotA performance on paraphrase detection benchmarks such as Quora Question Pairs Wang et al. (2017).\n",
      "\n",
      "Figure 2: (a) On CoQA open-book question answering semantic entropy demonstrates better uncertainty than ordinary predictive entropy with and without normalisation at larger model sizes. It also performs significantly better than p ( True ) . (b) TriviaQA shows similar results. Identical to Fig. 1b with the addition of p ( True ) , which was previously omitted to avoid stretching the scale.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## 6 EMPIRICAL EVALUATION\n",
      "\n",
      "We demonstrate that semantic entropy is an effective way to quantify the uncertainty of NLG on free-form QA tasks. Effective uncertainty measures should offer information about how reliable the model's answers are-that is, very uncertain generations should be less likely to be correct .\n",
      "\n",
      "Performance evaluation. Following prior work (e.g. Filos et al. (2019)), we evaluate uncertainty by treating uncertainty estimation as the problem of predicting whether to rely on a model generation for a given context-whether to trust an answer to a question. The area under the receiver operator characteristic curve (AUROC) metric is equivalent to the probability that a randomly chosen correct answer has a higher uncertainty score than a randomly chosen incorrect answer. Higher scores are better, with perfect uncertainty scoring 1 while a random uncertainty measure would score 0.5.\n",
      "\n",
      "The AUROC is a better measure of uncertainty for free-form question answering and NLG than calibration measures like the Brier score, which are often used in classification or for multiple choice QA. This is because the language model outputs a likelihood for a given token-sequence, but not for an entire meaning. In order to estimate the Brier score, we would need to estimate the entire probability mass assigned to any possible way of saying the correct answer. This is intractable for free form text where we do not have access to probabilities about meanings. In contrast, we can estimate the entropy because it is structured as an expected information, which makes Monte Carlo integration suitable.\n",
      "\n",
      "Model. We use the GPT-like OPT models (Zhang et al., 2022). We vary the size of the model between 2.7B, 6.7B, 13B and 30B parameters, while our headline results are all reported using the largest computationally feasible model, with 30B parameters. In all cases we use only a single unmodified model. There is no ensembling and no stochastic or Bayesian modification. We chose this because in most cases cutting-edge foundation models are not available as ensembles and are too large to efficiently perform approximate Bayesian inference with. We do not fine-tune these models on TriviaQA or CoQA but use them in their pre-trained form.\n",
      "\n",
      "Datasets. We use CoQA Reddy et al. (2019) as an open-book conversational question answering problem (in which the model answers a question using a supporting paragraph). We use the development split ( ∼ 8000 questions). We also use TriviaQA (Joshi et al., 2017) as a closed-book QA problem (in which the model must answer a question without access to a supporting paragraph). We use a subset of 8000 questions of the training split to match the size of CoQA.\n",
      "\n",
      "We evaluate correctness of our model's generations on the underlying dataset using the a fuzzy matching criterion: L ( s , s ′ ) = 1 RougeL ( s , s ′ ) &gt; 0 . 3 . That is, we consider an answer s to be correct if its Rouge-L (Lin &amp; Och, 2004) - a measure of the longest common subsequence - with regards to the reference answer is larger than 0.3. In Appendix B.3 we study other objective functions such as exact matching and Rouge-1 and find our method to be robust to these choices.\n",
      "\n",
      "Baselines. We compare our method against predictive entropy, length-normalised predictive entropy (Malinin &amp; Gales, 2020), p ( True ) (Kadavath et al., 2022), and lexical similarity (similar to (Fomicheva et al., 2020)). Predictive entropy is a widely used measure of uncertainty in other\n",
      "\n",
      "Table 2: Incorrectly answered questions have more semantically distinct answers than correct ones. On its own, this count is a reasonable uncertainty measure, though semantic entropy is better.\n",
      "\n",
      "| Dataset   | Average # of semantically distinct answers   | Average # of semantically distinct answers   | AUROC            | AUROC              |\n",
      "|-----------|----------------------------------------------|----------------------------------------------|------------------|--------------------|\n",
      "|           | Correctly answered                           | Incorrectly answered                         | Semantic entropy | # distinct answers |\n",
      "| CoQA      | 1.27                                         | 1.77                                         | 0.77             | 0.66               |\n",
      "| TriviaQA  | 1.89                                         | 3.89                                         | 0.83             | 0.79               |\n",
      "\n",
      "domains, and has been used as a baseline without length-normalisation in, e.g., Kadavath et al. (2022). Here, the score is just the predictive entropy of the output distribution as described in Eq. (1). Length-normalised predictive entropy divides the joint log-probability of each sequence by the length of the sequence, as proposed by Malinin &amp; Gales (2020) in the case of NLG uncertainty and further discussed in Section 3.3. Note that unlike Malinin &amp; Gales (2020), we use only a single model, not an ensemble, and use multinomial sampling as we do for all other methods. p ( True ) proposed by (Kadavath et al., 2022) as a way to estimate the probability that a model's generation is correct by 'asking' the model if its answer is correct. They propose sampling M answers and constructing a new natural language question using these possible answers as context before asking whether the proposed answer is correct and measuring the probability of the completion being True . An example of the format is provided in Appendix B. Note that our implementation of this uses OPT models with up to 30B parameters, while Kadavath et al. (2022) use a proprietary 52B parameter model. We are also limited computationally to 10-shot prompting while the original paper uses 20-shot prompting. Lexical similarity uses the average similarity of the answers in the answer set A : 1 C ∑ | A | i =1 ∑ | A | j =1 sim ( s i , s j ) , where C = | A | ∗ ( | A | -1) / 2 , and sim is Rouge-L. Additionally, we evaluate a margin-probability baseline (Lin et al., 2022b) in Appendix B.6, and study why it is not very predictive of model accuracy in this setting. All code and data used in our experiments is available at https://github.com/lorenzkuhn/semantic\\_uncertainty .\n",
      "\n",
      "## 6.1 SEMANTIC ENTROPY UNCERTAINTY\n",
      "\n",
      "For both TriviaQA and CoQA, semantic entropy improves over baselines in predicting whether a model's answer to a question is correct. For TriviaQA, using the largest model we show in Fig. 1a we show that semantic entropy has a significantly higher AUROC than entropy in sequence-probabilityspace with and without length-normalisation, as well as the lexical similarity baseline. At the same time, it performs dramatically better than p ( True ) . Similarly, we find in Fig. 1b that our method outperforms more for larger model sizes and continues to steadily improve as the model size increases, with the performance of the p ( True ) baseline added in Fig. 2b (not shown in the opening figure for visual clarity). For CoQA, in Fig. 2a we show that semantic entropy predicts model correctness significantly better than the baselines at larger model sizes.\n",
      "\n",
      "The ground truth answers for TriviaQA are generally single words or very short phrases, while CoQA contains both longer and shorter ground truth answers. This is why performing lengthnormalisation has a large effect for CoQA but no effect for TriviaQA (compare Fig. 2a and Fig. 2b). TriviaQA is also a more challenging dataset: accuracy of 50.6% against 82.3% for CoQA.\n",
      "\n",
      "We can better understand the mechanism of action for semantic entropy by examining the difference between incorrect and correct answers generated by the model. In Table 2 we show that the average number of semantically distinct clusters of answers ( | C | ) from the 30B parameter model is significantly greater for incorrectly answered questions than correctly answered ones. This fits our predictions, which is that the model is more likely to generate incorrect answers when it is uncertain about the most likely generation. There are 10 answers generated per question, so there is substantial overlap in meaning. We also show that simply using the number of semantically distinct answers as an uncertainty measure on its own performs reasonably well. Semantic entropy has a higher AUROC than the number of distinct answers, especially for CoQA whose difficulty causes greater spread in predicted probabilities between possible answers.\n",
      "\n",
      "Finally, we can see that much of the performance gain comes from making better use of more samples. In Fig. 3a we show that for both CoQA (top) and TriviaQA (bottom) the gap between semantic entropy and length-normalised entropy widens as the number of samples increases.\n",
      "\n",
      "## 6.2 HYPERPARAMETERS FOR EFFECTIVE SAMPLING\n",
      "\n",
      "Adjusting the temperature used for multinomial sampling has two competing effects on the generated sequences produced by the model. Increasing the temperature increases the diversity of samples\n",
      "\n",
      "〈]√˜√√]√glyph[arrowbt]\n",
      "\n",
      "Figure 3: (a) Semantic entropy makes better use of additional samples because it handles duplication better, the performance gap therefore continues to improve. (b) (bottom) Higher temperatures result in more diversity but less accurate generations. (top) The best performing uncertainty comes from an intermediate temperature that balances these two forces. Results on TriviaQA.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "glyph[negationslash]\n",
      "\n",
      "(Fig. 3b, bottom figure, solid line). One would expect more diverse generations to cover the space of possible meanings more fully. Here we measure the diversity using the average overlap of the longest sub-sequence among sampled answers ( 1 -( M 2 ) -1 ∑ s = s ′ ∈ C Rouge-L ( s , s ′ ) ). At the same time, reducing the temperature improves the average correctness of the answer (Fig. 3b, bottom figure, dashed line). Typically, more accurate models are also better at estimating uncertainty.\n",
      "\n",
      "In fact, we find that these two effects compete and the highest AUROC for semantic entropy and length-normalised entropy is optimised by an intermediate temperature of 0.5 (Fig. 3b, top figure). A lower temperature would improve accuracy, while a higher temperature would improve diversity. A similar figure for CoQA can be found in Appendix B. Note that prior work using predictive entropy as a baseline uses a temperature of 1.0 (Kadavath et al., 2022), which our evaluation suggests would significantly weaken the baseline relative to our implementation.\n",
      "\n",
      "## 7 DISCUSSION\n",
      "\n",
      "Many natural language problems display a crucial invariance: sequences of distinct tokens mean the same thing. Addressing this directly, we introduce semantic entropy-the entropy of the distribution over meanings rather than sequences-and show that this is more predictive of model accuracy on QA than strong baselines. Our unsupervised approach using 'out-of-the-box' models improves reproducibility and is easier to deploy. Unsupervised uncertainty may also help address the observation raised in prior work that supervised uncertainty measures struggle with distribution shift.\n",
      "\n",
      "For semantic entropy, we introduce a novel bidirectional entailment clustering algorithm which uses a smaller natural language inference model. Our method therefore represents a middle ground between fully probabilistic methods and methods that use language models to exploit aspects of natural language that are not transparently present in the model activations. We believe that this sort of joint approach is more promising than relying on either perspective on its own, especially as language models continue to improve. This will become more important in cases where language models are capable of deception, something which our method does not protect against, rather than merely being uncertain between many possible meaningful options. By combining model internals with model prediction one can hope to stay a step ahead of model capabilities.\n",
      "\n",
      "We focus on question answering because this is a particularly important free-form NLG problem with relatively clear ground truths. In the future, however, we hope our work on semantic equivalence can pave the way towards progress in settings like summarisation where correctness requires more human evaluation although additional progress on paraphrase identification in these settings is likely required first. Semantic likelihoods could also be extended to other tools for probabilistic uncertainty like mutual information, potentially offering new strategies for NLG uncertainty.\n",
      "\n",
      "## ACKNOWLEDGMENTS\n",
      "\n",
      "We are grateful to Geoffrey Irving, Kuba Perlin, Laura Rimell, and Miles Turpin for their advice and feedback on earlier drafts of this paper. We are also grateful to the members of the OATML group for helpful discussions about this project.\n",
      "\n",
      "## ETHICS STATEMENT\n",
      "\n",
      "Our aim is to work towards safer AI systems by enabling users to understand the confidence and reliability of language model generations. In principle, this could help mitigate many of the potential harms of NLG from foundation models such as generating false and harmful information in response to genuine questions about important topics like medical questions. However, this potential benefit comes with the risk that systematic mistakes in the assessment of uncertainty or its communication could cause unfounded and misplaced confidence. While this paper represents research progress in identifying new considerations and methods for uncertainty quantification in NLG, before deployment we advise that practitioners conduct extensive evaluations specific to the deployment context to make sure that uncertainty is communicated in a way that empowers users and is not misleading or confusing.\n",
      "\n",
      "## REPRODUCIBILITY STATEMENT\n",
      "\n",
      "Because of the computational cost of experimentation with foundation models, most of the relatively small amount of existing research into NLG uncertainty relies on proprietary models, finetuning of expensive models, and human evaluation. These factors put this kind of research out of reach for many academic groups. Our work takes advantage of the recently released, publicly available OPT models, and builds on this to provide an uncertainty quantification pipeline for NLG that uses entirely open source tools. Meanwhile our method requires no finetuning or training of foundation models and can work with 'off-the-shelf' existing models. We hope that this can facilitate more research on these important topics in the academic community as well as making our methods easier to replicate. We make all of our code, as well as the hand-labelled semantic equivalence dataset drawn from TriviaQA and CoQA, available under an MIT license.\n",
      "\n",
      "## REFERENCES\n",
      "\n",
      "- Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´ e. Concrete Problems in AI Safety. arXiv , 2016. 1\n",
      "- Ion Androutsopoulos and Prodromos Malakasiotis. A survey of paraphrasing and textual entailment methods. Journal of Artificial Intelligence Research , 38:135-187, 2010. 6\n",
      "- Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877-1901, 2020. 1\n",
      "- Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022. 1\n",
      "- Peter W Culicover. Paraphrase generation and information retrieval from stored text. Comput. Linguistics , 11(3-4):78-88, 1968. 6\n",
      "- Mech. Transl.\n",
      "- Shrey Desai and Greg Durrett. Calibration of pre-trained transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 295302, 2020. 6\n",
      "- Samuel Fernando and Mark Stevenson. A semantic similarity approach to paraphrase detection. In Proceedings of the 11th annual research colloquium of the UK special interest group for computational linguistics , pp. 45-52. Citeseer, 2008. 6\n",
      "- Angelos Filos, Sebastian Farquhar, Aidan N Gomez, Tim G J Rudner, Zachary Kenton, Lewis Smith, Milad Alizadeh, Arnoud de Kroon, and Yarin Gal. Benchmarking Bayesian Deep Learning with Diabetic Retinopathy Diagnosis. Bayesian Deep Learning Workshop at NeurIPS , 2019. 7\n",
      "\n",
      "- Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Fr´ ed´ eric Blain, Francisco Guzm´ an, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. Unsupervised quality estimation for neural machine translation. Transactions of the Association for Computational Linguistics , 8: 539-555, 2020. 2, 7\n",
      "- Yarin Gal et al. Uncertainty in deep learning. PhD thesis , 2016. 1, 2\n",
      "- Taisiya Glushkova, Chrysoula Zerva, Ricardo Rei, and Andr´ e FT Martins. Uncertainty-aware machine translation evaluation. arXiv preprint arXiv:2109.06352 , 2021. 6\n",
      "- Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654 , 2020a. 5\n",
      "- Ruining He, Anirudh Ravula, Bhargav Kanagal, and Joshua Ainslie. Realformer: Transformer likes residual attention. arXiv preprint arXiv:2012.11747 , 2020b. 6\n",
      "- Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved Problems in ML Safety. arXiv , 2022. 1\n",
      "- Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022. 1\n",
      "- Fuad Issa, Marco Damonte, Shay B Cohen, Xiaohui Yan, and Yi Chang. Abstract meaning representation for paraphrase detection. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pp. 442-452, 2018. 6\n",
      "- Moksh Jain, Salem Lahlou, Hadi Nekoei, Victor Butoi, Paul Bertin, Jarrid Rector-Brooks, Maksym Korablyov, and Yoshua Bengio. Deup: Direct epistemic uncertainty prediction. arXiv preprint arXiv:2102.08501 , 2021. 3\n",
      "- Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics , 9:962-977, 2021. 6\n",
      "- Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551 , 2017. 7\n",
      "- Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221 , 2022. 1, 2, 3, 6, 7, 8, 9, 18\n",
      "- Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? Advances in neural information processing systems , 30, 2017. 1, 2\n",
      "- Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning using calibrated regression. In International conference on machine learning , pp. 2796-2804. PMLR, 2018. 1\n",
      "- Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems , 30, 2017. 1\n",
      "- Chin-Yew Lin and Franz Josef Och. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04) , pp. 605-612, 2004. 7\n",
      "- Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334 , 2022a. 2, 3, 6\n",
      "\n",
      "- Zi Lin, Jeremiah Zhe Liu, and Jingbo Shang. Towards collaborative neural-symbolic graph semantic parsing via uncertainty. In Findings of the Association for Computational Linguistics: ACL 2022 , pp. 4160-4173, 2022b. 6, 8, 19\n",
      "- David Mackay. Information Theory, Inference and Learning Algorithms . Cambridge University Press, 2003. 4\n",
      "- Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. Advances in neural information processing systems , 31, 2018. 3\n",
      "- Andrey Malinin and Mark Gales. Uncertainty estimation in autoregressive structured prediction. arXiv preprint arXiv:2002.07650 , 2020. 1, 2, 3, 4, 5, 7, 8\n",
      "- Andrey Malinin, Sergey Chervontsev, Ivan Provilkov, and Mark Gales. Regression prior networks. arXiv preprint arXiv:2006.11590 , 2020. 3\n",
      "- Sabrina J Mielke, Arthur Szlam, Y-Lan Boureau, and Emily Dinan. Linguistic calibration through metacognition: aligning dialogue agent responses with expected correctness. arXiv preprint arXiv:2012.14983 , 2020. 6\n",
      "- Kenton Murray and David Chiang. Correcting length bias in neural machine translation. arXiv preprint arXiv:1808.10006 , 2018. 3, 4\n",
      "- Myle Ott, Michael Auli, David Grangier, and Marc'Aurelio Ranzato. Analyzing uncertainty in neural machine translation. In International Conference on Machine Learning , pp. 3956-3965. PMLR, 2018. 6\n",
      "- Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. Advances in neural information processing systems , 32, 2019. 1\n",
      "- Sebastian Pad´ o, Daniel Cer, Michel Galley, Dan Jurafsky, and Christopher D Manning. Measuring machine translation quality as semantic equivalence: A metric based on entailment features. Machine Translation , 23(2):181-193, 2009. 6\n",
      "- Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question answering challenge. Transactions of the Association for Computational Linguistics , 7:249-266, 2019. 7\n",
      "- Ananya B Sai, Akash Kumar Mohankumar, and Mitesh M Khapra. A survey of evaluation metrics used for nlg systems. ACM Computing Surveys (CSUR) , 55(2):1-39, 2022. 6\n",
      "- Richard Socher, Eric Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Ng. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. Advances in neural information processing systems , 24, 2011. 6\n",
      "- Jeff Speaks. Theories of Meaning. In Edward N. Zalta (ed.), The Stanford Encyclopedia of Philosophy . Metaphysics Research Lab, Stanford University, 2021. 3\n",
      "- Yi Tay, Vinh Q Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin, Simon Baumgartner, Cong Yu, and Donald Metzler. Charformer: Fast character transformers via gradient-based subword tokenization. arXiv preprint arXiv:2106.12672 , 2021. 6\n",
      "- Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao, and Hao Ma. Entailment as few-shot learner. arXiv preprint arXiv:2104.14690 , 2021. 6\n",
      "- Yuxia Wang, Daniel Beck, Timothy Baldwin, and Karin Verspoor. Uncertainty estimation and reduction of pre-trained models for text regression. Transactions of the Association for Computational Linguistics , 10:680-696, 2022. 6\n",
      "- Zhiguo Wang, Wael Hamza, and Radu Florian. Bilateral multi-perspective matching for natural language sentences. arXiv preprint arXiv:1702.03814 , 2017. 6\n",
      "- Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426 , 2017. 5\n",
      "\n",
      "- Lei Yu, Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman. Deep learning for answer sentence selection. arXiv preprint arXiv:1412.1632 , 2014. 6\n",
      "- Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022. 2, 7\n",
      "\n",
      "Table 3: Illustration of semantic, syntactic, and lexical equivalence. Work with foundation models implicitly focuses on lexical equivalence, which entails the others, but we usually care about semantic equivalence.\n",
      "\n",
      "|                                 |                                                                                             | Equivalence   | Equivalence   | Equivalence   |\n",
      "|---------------------------------|---------------------------------------------------------------------------------------------|---------------|---------------|---------------|\n",
      "| Sentence A                      | Sentence B                                                                                  | Lexical       | Syntactic     | Semantic      |\n",
      "| Paris is the capital of France. | Paris is the capital of France. Berlin is the capital of France. France's capital is Paris. | !             | ! !           | ! !           |\n",
      "\n",
      "## A FURTHER DETAILS ON SEMANTIC ENTROPY\n",
      "\n",
      "## A.1 FURTHER DISCUSSION OF SEMANTIC EQUIVALENCE\n",
      "\n",
      "We illustrate the distinction between different kinds of equivalence in Table 3. Lexically equivalent sequences use exactly the same symbols. They are always also semantically and syntactically equivalent (in a given context). Syntactically equivalent sentences have the same grammatical form. But they can have different meanings (not semantically equivalent) and can use different symbols (not lexically equivalent). Semantically equivalent sentences mean the same thing, but they might have different grammatical form (not syntactically equivalent) or symbols (not lexically equivalent). Two sentences can also be both syntactically and semantically equivalent but not lexically equivalent if they match up to a synonym.\n",
      "\n",
      "Soft equivalence and transitivity. Formally, semantic equivalence is transitive. That is, if E ( s , s ′ ) and E ( s ′ , s ′′ ) then it follows that E ( s , s ′′ ) . However, the implementation of our bidirectional equivalence algorithm permits some classification errors and it is slightly 'soft'-it will sometimes return equivalent for pairs that are not quite equivalent. As a result, it is not strictly true that our equivalence relation is transitive, and therefore not strictly true that there is a unique set of equivalence classes. For example, the clusters might depend on the order in which the comparisons are made. In practice, however, we find that this does not pose a noticeable problem-usually, inspecting the outputs shows that the equivalence appears clear cut. However, we acknowledge this potential issue as an area for improvement in future clustering algorithms.\n",
      "\n",
      "Unequal token importance. From the perspective of meaning, some tokens can matter more than others-key words. Naive methods like predictive entropy do distinguish between key words or unimportant tokens. Supervised uncertainty methods that make use of language models in the uncertainty evaluation can potentially take this into account better. In addition, our semantic entropy approach partly adjusts for this, as discussed in Section 4.1.\n",
      "\n",
      "## A.2 FURTHER ALGORITHMIC DETAILS\n",
      "\n",
      "In addition to the description of the method provided in the main body, in Algorithm 1 we provide the pseudocode for our bi-directional entailment algorithm.\n",
      "\n",
      "## A.3 IMPACT OF SAMPLING METHOD ON QUALITY OF UNCERTAINTY ESTIMATE\n",
      "\n",
      "In Section 4, we study the impact of the temperature hyper-parameter on the performance of the uncertainty measures. Here, we show a variant of Fig. 3b for the CoQA dataset showing an almost identical pattern. Like TriviaQA, the optimal temperature is 0.5 despite a significantly harder problem with lower accuracy, suggesting that this choice hyperparameter may generalize well. Unlike TriviaQA, normalised entropy outperforms semantic entropy at high temperatures.\n",
      "\n",
      "Beyond the temperature, there are a number of other design choices to be made when sampling: the sampling method and hyper-parameters such as top-p and top-k . Our contribution in this paper is to show the importance of these choices for uncertainty estimation which has been overlooked previously, and study the temperature in particular. While we leave the detailed study of these hyperparameters to future work, we do compare our default multinomial sampling method, to multinomial beam search sampling which focuses more on high-likelihood regions of the output space.\n",
      "\n",
      "## Algorithm 1 Bidirectional Entailment Clustering\n",
      "\n",
      "```\n",
      "Require: context x , set of seqs. { s (2) , . . . , s ( M ) } , NLI classifier M , set of meanings C = {{ s (1) }} for 2 ≤ m ≤ M do for c ∈ C do glyph[triangleright] Compare to already-processed meanings. s ( c ) ← c 0 glyph[triangleright] Use first sequence for each semantic-class. left ←M (cat( x, s ( c ) , ' < g/ > ' , x, s ( m ) )) glyph[triangleright] Does old sequence entail new one? right ←M (cat( x, s ( m ) , ' < g/ > ' , x, s ( c ) )) glyph[triangleright] Vice versa? if left is entailment and right is entailment then c ← c ⋃ s ( m ) glyph[triangleright] Put into existing class. end if end for C ← C ⋃ { s ( m ) } glyph[triangleright] Semantically distinct, gets own class. end for return C\n",
      "```\n",
      "\n",
      "〈]√˜√√]√glyph[arrowbt]\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "⋂˜⌉√˜√∐√√√˜\n",
      "\n",
      "Figure 4: CoQA temperature ablation. (bottom) Similar to TriviaQA, higher temperatures mean higher diversity and lower accuracy. (top) The best performance for both methods comes at a temperature of 0.5. Unlike TriviaQA, normalised entropy outperforms semantic entropy at high temperatures.\n",
      "\n",
      "In Table 4 we show that multinomial beam search sampling yields uncertainty measures that are less predictive of model accuracy than multinomial sampling. Beam search also generates much less diverse samples. We conjecture that multinomial beam search sampling focuses too much on the most likely sequences. The diversity of this beam search corresponds to the lowest temperature result in Fig. 4. As in the main body of the paper, we measure diversity as the average lexical overlap of the answers in the answer set. Additionally, we investigate, why the semantic entropy underperforms the length-normalised entropy at high temperatures. To that end, we manually inspect and label 100 classifications of our semantic equivalence method at T=1.5, and we find that at these temperatures, many of the generated model answers are nonsensical combinations of words from the context that is provided for the question. While the likelihood of these sequences still seems somewhat predictive of the model's accuracy, semantic clustering becomes very difficult and an unreliable signal for uncertainty estimation. At this temperature, the accuracy of the semantic equivalence methods is only 61%, whereas it is over 92% at lower temperatures (see Appendix B.2)\n",
      "\n",
      "Note, that at low-temperatures, where one does gets plausible and well-formed model generations, semantic entropy does clearly outperform the baselines. This finding further underlines the importance of choosing appropriate sampling hyper-parameters when using entropy-based uncertainty measures in NLG.\n",
      "\n",
      "Table 4: Multinomial beam search sampling produces sampled answers that are less diverse and thus less useful for uncertainty estimation than multinomial sampling.\n",
      "\n",
      "| Sampling method                  |   Semantic Entropy AUROC |   Diversity of answers |\n",
      "|----------------------------------|--------------------------|------------------------|\n",
      "| Multinomial sampling             |                    0.758 |                  0.49  |\n",
      "| Multinomial beam search sampling |                    0.735 |                  0.258 |\n",
      "\n",
      "## B EXPERIMENTAL DETAILS AND ABLATIONS\n",
      "\n",
      "We use both the OPT models 2 and the Deberta-large model 3 via the HuggingFace transformers library which can be easily adopted for reproducibility. All of our code is open-source and relies on no proprietary models.\n",
      "\n",
      "We use the following functions of the HuggingFace API to sample the most likely answers, and the set of answers:\n",
      "\n",
      "- To obtain the answer which is compared to the reference answer, which determines whether the question is correctly answered, we use beam search using the generate() function with num beams = 5 and do sample = True .\n",
      "- To obtain the answer set for uncertainty estimation, by default we use multinomial sampling, that is generate() using do sample = True and num beams = 1 . If indicated explicitly, we use beam multinomial sampling, that is generate() using num beams = 5 and do sample = True .\n",
      "\n",
      "We run all of our experiments on 80GB NVIDIA A100s.\n",
      "\n",
      "Testing up to 20 samples per answer on the 2.7B, 6.7B and 13B CoQA experiments, we conclude that using more than 10 samples does not significantly improve the performance of the uncertainty measure, we use 10 sampled answers per question in the remaining experiments on TriviaQA. Note, that in Table 2 we compare the 30B model on CoQA and TriviaQA where in both settings we use answer sets of size 10.\n",
      "\n",
      "We use the following prompts on CoQA and TriviaQA. We find that on CoQA, we obtain accurate model results with zero-shot prompting. While we have to use few-shot prompting to obtain accurate answers on closed-book TriviaQA. We use the following prompts for each of the settings:\n",
      "\n",
      "## CoQA:\n",
      "\n",
      "```\n",
      "[The provided context paragraph] [additional question-answer pairs] Q: [Provided question] A:\n",
      "```\n",
      "\n",
      "where additional question-answer pairs are preceding turns of the conversation about the paragraph consisting of questions and reference answers.\n",
      "\n",
      "## TriviaQA:\n",
      "\n",
      "For TriviaQA, we use a 10-shot prompt of the format:\n",
      "\n",
      "Q: Which Oscar-nominated film had You Sexy Thing as its theme song? A: The Full Monty Q: Which Joan's career revived in Whatever Happened to Baby Jane? A: Crawford Q: Which much-loved actor won the Best Actor Oscar for The Philadelphia Story? A: James Stewart (...) Q: In which river is the Boulder Dam? A:\n",
      "\n",
      "To account for generations where the model continues the Q:...A:... pattern after providing an answer to the given question, we trim all generations by pattern matching for a selection of stopwords that we observe in the generations: Q: , Question: , QUESTION: and questions: .\n",
      "\n",
      "2 https://huggingface.co/docs/transformers/model doc/opt\n",
      "\n",
      "3 https://huggingface.co/docs/transformers/model doc/opt\n",
      "\n",
      "Table 5: Automatic evaluation of question answering is highly accurate as compared to human evaluation. We evaluate how accurate the automatic evaluation metric. The predictions, in this settings are the automatically determined accuracy labels on our question answering task, and the ground truth are human labels for the accuracy of the provided model generation given the reference answer\n",
      "\n",
      "| Data set   |   Accuracy of automatic evaluation |\n",
      "|------------|------------------------------------|\n",
      "| CoQA       |                               0.89 |\n",
      "| TriviaQA   |                               0.96 |\n",
      "\n",
      "Table 6: TriviaQA: the exact choice of accuracy metric for the free-form QA task has little effect on the assessment of the quality of the uncertainty measure.\n",
      "\n",
      "| Metric                     | AUROC            | AUROC              | Accuracy   |\n",
      "|----------------------------|------------------|--------------------|------------|\n",
      "|                            | Semantic entropy | Normalised entropy |            |\n",
      "| Rouge-L ( y, y ′ ) > 0 . 3 | 0.828            | 0.802              | 0.506      |\n",
      "| Rouge-L ( y, y ′ ) > 0 . 5 | 0.835            | 0.810              | 0.456      |\n",
      "| Rouge-1 ( y, y ′ ) > 0 . 5 | 0.835            | 0.810              | 0.457      |\n",
      "| Exact matching             | 0.828            | 0.808              | 0.394      |\n",
      "\n",
      "## B.1 RELIABILITY OF ACCURACY METRIC AS COMPARED TO HUMAN EVALUATION\n",
      "\n",
      "In our experiments, we evaluate how well our uncertainty measures predict the model's accuracy when answering a given question. The choice of accuracy metric is thus a crucial component of our experimental setup. Generally, it has been shown to be difficult to develop automatic metrics for free-form generation that correlate well with human evaluations. We thus verify our choice of accuracy criterion: Rouge-L ( y, y ′ ) &gt; 0 . 3 , for a given reference answer y and a model generation y ′ . We manually evaluate the accuracy of 200 answers of the 30B parameter model on both COQA and on TriviaQA, and evaluate how closely the human evaluation matches the automatic evaluation. We find that on both data sets, the accuracy of the automatic labels as compared to the human labels as the ground truth is high, see Table 5.\n",
      "\n",
      "## B.2 TESTING THE BI-DIRECTIONAL ENTAILMENT CLASSIFIER\n",
      "\n",
      "To the best of our knowledge, this paper is the first application of the bi-directional entailment approach to identifying answers with the same meaning in question answering. Since this is a core component of our approach, we verify how accurately this approach identifies model answers with the same meaning. To this end, we manually label 300 samples for each of TriviaQA and CoQA produced by the 13B parameter model to provide a ground truth as to whether or not they mean the same thing. We find that the model achieves an accuracy of 92.7% and 95.3% respectively.\n",
      "\n",
      "## B.3 SENSITIVITY OF RESULTS TO ACCURACY METRIC\n",
      "\n",
      "In principle, the choice of metric to decide whether or not an answer is 'correct' might have a large effect on the assessment of our method and baselines. However, we find empirically that our results are relatively insensitive to the choice of accuracy metric.\n",
      "\n",
      "In Table 6 we show that for TriviaQA the choice of accuracy metric for the question answering has almost no effect on the measured AUROC of the uncertainty estimation, despite making the measured accuracy of the model's generation significantly different. In particular, the exact matching requirement reduces the accuracy significantly but has little effect on the AUROCs.\n",
      "\n",
      "For CoQA, which is an open-book QA task with greater answer variability and longer answers the results are broadly similar (see Table 7) except for the exact matching accuracy criterion which is too demanding because of the much larger variety of possible answers for this task.\n",
      "\n",
      "Table 7: CoQA: the exact choice of the accuracy metric for the free-form open-book QA task has little effect on the assessment of the quality of the uncertainty measure except for the use of exact matching. For CoQA, getting an exact match is significantly harder.\n",
      "\n",
      "| Metric                     | AUROC            | AUROC              | Accuracy   |\n",
      "|----------------------------|------------------|--------------------|------------|\n",
      "|                            | Semantic entropy | Normalised entropy |            |\n",
      "| Rouge-L ( y, y ′ ) > 0 . 3 | 0.7672           | 0.7533             | 0.8239     |\n",
      "| Rouge-L ( y, y ′ ) > 0 . 5 | 0.7379           | 0.7290             | 0.7657     |\n",
      "| Rouge-1 ( y, y ′ ) > 0 . 3 | 0.7672           | 0.7533             | 0.8239     |\n",
      "| Rouge-1 ( y, y ′ ) > 0 . 5 | 0.7397           | 0.7309             | 0.7677     |\n",
      "| Exact matching             | 0.6749           | 0.6727             | 0.6459     |\n",
      "\n",
      "Figure 5: Accuracy improves with model size, as does semantic entropy's uncertainty performance. At the smallest model size, both accuracy and uncertainty diminish.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## B.4 ACCURACY ABLATIONS WITH MODEL SIZE\n",
      "\n",
      "We confirm that increasing the model size improves the accuracy of the generations on both QA datasets (see Fig. 5a and Fig. 5b). Semantic entropy's uncertainty performance is also shown for context.\n",
      "\n",
      "## B.5 EXAMPLE P(TRUE) FORMAT\n",
      "\n",
      "The format of the prompt, reproduced here for convenient reference from the original source Kadavath et al. (2022), is:\n",
      "\n",
      "```\n",
      "Question: Who was the third president of the United States? Here are some brainstormed ideas: James Monroe Thomas Jefferson John Adams Thomas Jefferson George Washington Possible Answer: James Monroe Is the possible answer: (A) True (B) False The possible answer is:\n",
      "```\n",
      "\n",
      "where the 'brainstormed answers' are from the set of sampled answers A and P(True), i.e. the likelihood of the next token being True is taken as the uncertainty measure. The authors note that doing the above needs to be done in a few-shot manner and does not work well as in a zero-shot format. In our experiments, we use a few-shot prompt with 10 examples.\n",
      "\n",
      "\n",
      "\n",
      "⊗√⌉̂˜√(}˜(√∐√∐⌉˜√˜√√(}˜(⌉}̂˜⌈\n",
      "\n",
      "Figure 6: The margin probability, i.e. the difference between the likelihood of the most likely answer and the likelihood of the second most likely answer, is not very predictive of models' accuracy on CoQA open-book question answering (a) nor on TriviaQA (b). Identical to Fig. 2 with the addition of Margin probability which was previously omitted to avoid stretching the scale.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## B.6 MARGIN-PROBABILITY BASELINE\n",
      "\n",
      "We additionally compare our method to the margin probability method used for neural-symbolic parsing in Lin et al. (2022b):\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where y (1) is the top-1 beam search result and y (2) is the top-2 beam search result.\n",
      "\n",
      "Initially, running the method as proposed in Lin et al. (2022b) using a 13B parameter model on CoQA, we find that H margin is not very predictive of the model's accuracy on answering questions in CoQA achieving an AUROC of 0.54.\n",
      "\n",
      "We hypothesise that two factors contribute to this poor performance. First, since this measure only looks at the difference of likelihoods, the information about the magnitude of the likelihood of a given answer is lost. Second-analogously to the predictive entropy-it would be important to take semantic uncertainty into account when computing H margin . Manually inspecting model answers on CoQA, and the corresponding H margin , we see that the margin between two semantically equivalent answers and two semantically distinct answers is often similar. That is, this measure does not distinguish between uncertainty between paraphrases of the same meaning (in which case the model might actually be confident about meaning of the answer), and the model's uncertainty about which semantically distinct meaning is correct.\n",
      "\n",
      "We find that if instead of obtaining y (1) and y (2) by multinomial sampling (as in our other experiments) instead of by beam search, this second problem becomes less pronounced and H margin performs better while still being clearly outperformed by the other methods we study. We report our full results in Fig. 6.\n",
      "\n",
      "Table 8: Example of challenges for H margin . H margin does not distinguish between lexical and semantic uncertainty and thus can not distinguish cases where the model is certain about the correct answer (but uncertain about the precise formulation) as in row 1, and cases where the model is uncertain about the correct answer as in row 2. The semantic entropy correctly indicates low uncertainty in the first case and high uncertainty in the second case.\n",
      "\n",
      "| y (1)          | y (2)   |   H margin |   Semantic entropy |\n",
      "|----------------|---------|------------|--------------------|\n",
      "| Thomas Edison. | Edison. |       0.9  |               0.1  |\n",
      "| Thomas.        | George. |       0.36 |               0.87 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 08:06:14,057 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:14,058 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:14,060 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:14,061 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:14,145 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:14,146 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:14,168 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:14,169 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 08:06:14,406 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 08:06:14,407 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:06:15,134 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:06:15,422 - INFO - Processing document 2003.12771v2.pdf\n",
      "2025-12-15 08:06:25,113 - INFO - Finished converting document 2003.12771v2.pdf in 11.37 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Effective scalar-tensor description of regularized Lovelock gravity in four dimensions\n",
      "\n",
      "Tsutomu Kobayashi 1, ∗\n",
      "\n",
      "1 Department of Physics, Rikkyo University, Toshima, Tokyo 171-8501, Japan\n",
      "\n",
      "We reformulate the recently proposed regularized version of Lovelock gravity in four dimensions as a scalar-tensor theory. By promoting the warp factor of the internal space to a scalar degree of freedom by means of Kaluza-Klein reduction, we show that regularized Lovelock gravity can be described effectively by a certain subclass of the Horndeski theory. Cosmological aspects of this particular scalar-tensor theory are studied. It is found that the background with a scalar charge is generically allowed. The consequences of this scalar charge are briefly discussed.\n",
      "\n",
      "## I. INTRODUCTION\n",
      "\n",
      "Lovelock gravity [1] is the most general metric theory of gravity in higher dimensions retaining the second-order nature of field equations for the metric. The action for Lovelock gravity in D dimensions is given by\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where α p is the coupling constant and\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "The first three terms are written more explicitly as\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "In D = 4 dimensions, the Lovelock Lagrangian uniquely reduces to the Einstein-Hilbert term ( L 1 ) plus a cosmological constant ( L 0 ). This puts strong limitations on constructing the metric theory of gravity other than Einstein in four dimensions.\n",
      "\n",
      "Recently, a trick has been proposed to circumvent this limitation [2, 3]. (See also [4, 5] for earlier works.) The trick amounts to rescaling the coupling constants α p ( p ≥ 2) as\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "and then taking the D → 4 limit. This procedure leaves nonvanishing contributions in the gravitational field equations, and thus one ends up with a seemingly novel theory of gravity in four dimensions. Although how this 'regularization' works is not so evident at the level of the action or the covariant field equations, an explicit analysis of cosmological solutions, black hole solutions, and perturbations [2, 3, 6-11] shows that the Lovelock terms yield the factor of ( D -4) in the field equations to cancel ( D -4) in the denominator. See also Refs. [12-16] for aspects of black hole solutions in this regularized version of Lovelock gravity.\n",
      "\n",
      "Let us take a look at the cosmological spacetime studied in [2, 3]. The D -dimensional cosmological metric is assumed to take the form\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where i, j = 1 , 2 , 3 and indices a, b run through 4 , 5 , · · · , 3 + n with n = D -4. Substituting this metric to the gravitational field equations and then taking the D → 4 limit along with rescaling the coupling constants as Eq. (4), one obtains the modified background equations in four dimensions [2, 3]. Here some questions arise. What happens if one assumes a different scale factor of the n -dimensional internal space, b 2 ( t ) δ ab d x a d x b ? And then, what is the role of the ( a, b ) (i.e., the internal-space) components of the field equations in the D → 4 limit? More generically, what is\n",
      "\n",
      "∗ Email: tsutomu'at'rikkyo.ac.jp\n",
      "\n",
      "the explicit form of covariant equations to determine the four-dimensional metric after taking the D → 4 limit? Can the D → 4 limit be taken consistently for any metric? Concerning these points, some criticisms on the validity of taking the D → 4 limit have been raised [17-21].\n",
      "\n",
      "To address these issues, in this short paper we study the dynamics of regularized Lovelock gravity by assuming a general four-dimensional metric and a general warp factor of the internal space. We show that taking the D → 4 limit after the Kaluza-Klein reduction leaves a dynamical scalar degree of freedom in four dimensions, yielding a particular class of scalar-tensor theories within the Horndeski family. Although the validity of the original formulation is questioned, regularized Lovelock gravity can thus be reformulated in a consistent way as a well-defined fourdimensional theory. Employing this scalar-tensor reformulation, we revisit the dynamics of a cosmological spacetime in regularized Einstein-Gauss-Bonnet gravity in four dimensions.\n",
      "\n",
      "The rest of the paper is organized as follows. In the next section we clarify the relation between the Horndeski theory and the D → 4 limit of the Lovelock theory to provide the scalar-tensor reformulation of the latter theory. We then study the background dynamics and linear perturbations of a cosmological spacetime based on the scalar-tensor reformulation in Sec. III. A brief summary of the paper is presented in Sec. IV.\n",
      "\n",
      "## II. HORNDESKI AND REGULARIZED LOVELOCK\n",
      "\n",
      "For simplicity, we focus on the case of Einstein-Gauss-Bonnet gravity for the moment. Let us consider the (4 + n )-dimensional metric of the form\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where µ, ν = 0 , 1 , 2 , 3 and d σ 2 K is the line element of a n -dimensional maximally symmetric space with constant curvature K . We perform a Kaluza-Klein reduction starting from the metric (6). Substituting Eq. (6) to the GaussBonnet term and doing integration by parts, we obtain [22, 23]\n",
      "\n",
      "where χ µ = ∇ µ χ , X = -χ µ χ µ / 2, and O ( n 2 ) stands for the terms proportional to n 2 (or higher powers of n ). Here, R and G µν are the four-dimensional Ricci scalar and Einstein tensor, respectively, and G is the Gauss-Bonnet combination of the four-dimensional curvature tensors, G = R 2 -4 R µν R µν + R µνρσ R µνρσ . By rescaling the coupling constant α 2 as α 2 = α ′ 2 /n and taking the n → 0 limit, we arrive at the following alternative description of regularized Gauss-Bonnet gravity,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "This theory can be viewed as a particular subclass of the Horndeski theory [27] (see [28] for a review). The Lagrangian of the Horndeski theory is the sum of the following four Lagrangians [29, 30],\n",
      "\n",
      "As long as the (4 + n )-dimensional metric in underlying Einstein-Gauss-Bonnet gravity is assumed to be of the form of Eq. (6), the n → 0 ( D → 4) limit can be taken consistently and straightforwardly for any four-dimensional metric g µν , giving the above effective Lagrangian. 1 This is one of the main result of this paper. In the case of K = 0, the theory has the invariance under χ → χ +const.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "The Lagrangian (8) corresponds to the case with\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "1 After the first version of this paper was submitted, it was pointed out that the same scalar-tensor theory (with K = 0) is obtained also by introducing a counter term to the action to cancel the divergence in the D → 4 limit [24, 25]. Our result is also consistent with the analysis of amplitudes [26].\n",
      "\n",
      "Here, we used the fact that G µν χ µ χ ν can be written equivalently as XR + δ µ 1 µ 2 ν 1 ν 2 χ ν 1 µ 1 χ ν 2 µ 2 up to total derivatives. Note also that the nonminimal coupling to the Gauss-Bonnet term can be reproduced from the Horndeski functions including the ln X terms [30].\n",
      "\n",
      "The gravitational field equations derived from the Lagrangian (8) take the form\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where H ν µ is a χ -dependent tensor and T ν µ is the energy-momentum tensor of matter. One of the interesting properties of the above scalar-tensor theory is that a particular linear combination of the χ -field equation of motion and the trace of Eq. (17) reduce to\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "One can thus remove χ from the trace part of the gravitational field equations. Equation (18) itself is as expected from the original formulation of the theory [2], and our scalar-tensor reformulation can correctly reproduce the same result, though this equation is not trivial from the scalar-tensor viewpoint.\n",
      "\n",
      "Rewriting explicitly the p ≥ 3 Lovelock terms as the Horndeski theory is much more involved, though they must reduce to the form of the second-order scalar-tensor theory anyway [22]. For example, substituting the metric (6) with K = 0 to L 3 yields\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "The first line is of the form of the generalized Galileon [29],\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "which is a total derivative in four dimensions. The second line, the derivative coupling to the double dual Riemann tensor, can be written equivalently as L 5 {-48 X } [31]. It is easy to rearrange the other terms and we finally obtain, in the n → 0 limit,\n",
      "\n",
      "Similarly, the p -th Lovelock term yields L H 2 { X p } , L H 3 { X p -1 } , L H 5 { X p -1 } , and L H 2 { X p -2 } with particular coefficients. This confirms that the contributions from the higher-order Lovelock terms are high-energy corrections to Eq. (8). It is straightforward to include the curvature K .\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "## III. REVISITING COSMOLOGY IN REGULARIZED EINSTEIN-GAUSS-BONNET GRAVITY\n",
      "\n",
      "Let us study the cosmological dynamics, focusing again on the case of regularized Einstein-Gauss-Bonnet gravity and its scalar-tensor reformulation (8) with K = 0 for simplicity. Since χ is promoted to be a dynamical field in our scalar-tensor reformulation, we will emphasize its consequences on the background and perturbation dynamics.\n",
      "\n",
      "## A. Background Cosmology\n",
      "\n",
      "For the flat Friedmann-Lemaˆ ıtre-Robertson-Walker metric, d s 2 = -d t 2 + a 2 ( t ) δ ij d x i d x j , and χ = χ ( t ), we have\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where H = ˙ a/a is the Hubble parameter and a dot stands for differentiation with respect to t . The χ -field equation, or, equivalently, α ′ 2 ∇ ν H ν µ = 0, reduces to\n",
      "\n",
      "This can be integrated to give\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where the integration constant C is the scalar charge associated with the shift symmetry χ → χ +const. The case with C = 0 corresponds to the isotropically expanding solution, d s 2 = -d t 2 + a 2 δ ij d x i d x j + a 2 δ ab d x a d x b , which is assumed from the beginning in [2]. The scalar-tensor reformulation reveals that the background with the nonvanishing scalar charge is in fact allowed. In an accelerating universe, the second term in Eq. (25) decays quickly compared to the first, so that ˙ χ = H is a dynamical attractor. However, this is not the case in a decelerating universe.\n",
      "\n",
      "Substituting the solution (25) to Eqs. (22) and (23), we now have the following background cosmological equations,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "and ρ and P are the energy density and pressure of matter, respectively. It can be seen that the nonvanishing scalar charge gives rise to an extra radiation-like component in the background equations. This is a specific nature of the particular scalar-tensor theory (8) with K = 0. In the case of C = 0 the background equations derived in [2] are reproduced correctly.\n",
      "\n",
      "## B. Cosmological Perturbations\n",
      "\n",
      "Let us move to the perturbation dynamics. 2 As a matter field we simply add a canonical scalar field described by the Lagrangian L φ = -φ µ φ µ / 2 -V ( φ ). We fix the temporal gauge degree of freedom by imposing that φ ( t, x i ) = φ ( t ). The remaining spatial gauge degrees of freedom can be used to write the metric as\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "with ( e h ) ij = δ ij + h ij + h ik h kj / 2 + · · · . The χ -field also fluctuates,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Since all the gauge degrees of freedom have already been fixed, we cannot gauge away δχ . One would therefore expect that there are two dynamical modes in the scalar sector.\n",
      "\n",
      "It is straightforward to compute the quadratic Lagrangian for the tensor modes h ij :\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "2 It is worth emphasizing that we consider the perturbation dynamics within our scalar-tensor reformulation of the original theory. This effectively kills some of the perturbation modes associated to the internal space such as the Kaluza-Klein vector modes. In the presence of such modes, there is no guarantee that one can take the D → 4 limit consistently after perturbing the full D -dimensional metric.\n",
      "\n",
      "where where\n",
      "\n",
      "Clearly, this Lagrangian reproduces the equation of motion presented in [2] for C = 0. One might worry about ghost instabilities for a sufficiently large scalar charge. However, even if the scalar charge is as large as C /a /greaterorsimilar H , we have, from the background equations, that α 1 Γ H 2 ∼ α ′ 2 C 4 /a 4 /greaterorsimilar α ′ 2 H 2 C 2 /a 2 , which implies that ghost instabilities in the tensor sector are generically avoided.\n",
      "\n",
      "The propagation speed of tensor modes c GW is strongly constrained at low redshifts by the observation of GW170817 and its electromagnetic counterparts [32, 33] as | c 2 GW -1 | /lessorsimilar 10 -15 . From Eq. (32) we see that the propagation speed is given by\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "which deviates from 1 in general, | c 2 GW -1 | ∼ α ′ 2 ˙ H/α 1 , α ′ 2 C 2 /α 1 a 2 ( /lessorsimilar ( α ′ 2 ) 1 / 2 H/α 1 / 2 1 ). However, since H/α 1 / 2 1 is as small as 10 -60 in the present Universe (where we substituted the Planck mass to α 1 / 2 1 ), this does not lead to a meaningful constraint on the dimensionless parameter α ′ 2 .\n",
      "\n",
      "The quadratic Lagrangian for the scalar perturbations reads\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where\n",
      "\n",
      "The variation with respect to ψ gives\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Substituting this back to the above Lagrangian, we obtain the quadratic Lagrangian written in terms of ζ and δχ . The general expression is messy, and hence we expand the Lagrangian in terms of C assuming that the contribution of the scalar charge to the background is subdominant, leading to\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where /epsilon1 := -˙ H/H 2 . One sees that the sound speed of δχ is given by 1 / √ 3, signaling its radiation-like nature. On the C = 0 background, δχ apparently drops off from the Lagrangian and thereby the equation of motion in [2] is reproduced from our Lagrangian. Note, however, that the C → 0 limit must be taken with care because the disappearance of the kinetic term of δχ would indicate a strong coupling. 3 To look into this issue, one may use the rescaled variable ˜ δχ = C δχ and expand the Lagrangian to higher order in perturbations. It would thus be interesting to explore the nonlinear dynamics of the χ mode, which is however beyond the scope of this paper.\n",
      "\n",
      "3 The analysis of amplitudes shows that the scalar degree of freedom is strongly coupled around a flat space [26]. This is consistent with our result.\n",
      "\n",
      "## IV. SUMMARY\n",
      "\n",
      "In this paper, we have proposed a scalar-tensor reformulation of the recently proposed regularized version of Lovelock gravity in four dimensions. The effective scalar-tensor theory is obtained by promoting the warp factor of the internal ( D -4)-dimensional space to a dynamical scalar field, which survives after performing the Kaluza-Klein reduction and taking the D → 4 limit while keeping the rescaled coupling constants α ′ p = ( D -4) α p finite. The resultant theory resides in a particular subclass of the Horndeski theory, whose covariant field equations can be used for any fourdimensional metric. Our result is consistent with the conclusions of the different attempts to derive the well-defined version of the four-dimensional Einstein-Gauss-Bonnet theory [24-26].\n",
      "\n",
      "Employing our scalar-tensor reformulation, we have studied cosmological aspects of regularized Einstein-GaussBonnet gravity. We have found that cosmological solutions generically admit a scalar charge C , which yields a radiation-like component. All the previous cosmological results [2] are reproduced at the level of the Lagrangian by taking C → 0. However, in this limit the gravitational scalar degree of freedom might be strongly coupled. This point needs further investigation.\n",
      "\n",
      "Note added: While we were in the final stage of this work, the paper by Lu and Pang [34] appeared in the arXiv, where the same idea of reformulating regularized Einstein-Gauss-Bonnet gravity is presented independently. Their main focus is on black hole solutions, while ours is on cosmological aspects. Our conclusion agrees with them where they overlap.\n",
      "\n",
      "## ACKNOWLEDGMENTS\n",
      "\n",
      "We are grateful to Chunshan Lin for discussions. The work of TK was supported by MEXT KAKENHI Grant Nos. JP16K17707, JP17H06359, and JP18H04355.\n",
      "\n",
      "- [1] D. Lovelock, The Einstein tensor and its generalizations , J. Math. Phys. 12 (1971) 498.\n",
      "- [2] D. Glavan and C. Lin, Einstein-Gauss-Bonnet gravity in 4-dimensional space-time , Phys. Rev. Lett. 124 (2020) 081301 [ 1905.03601 ].\n",
      "- [3] A. Casalino, A. Colleaux, M. Rinaldi and S. Vicentini, Regularized Lovelock gravity , 2003.07068 .\n",
      "- [4] Y. Tomozawa, Quantum corrections to gravity , 1107.1424 .\n",
      "- [5] G. Cognola, R. Myrzakulov, L. Sebastiani and S. Zerbini, Einstein gravity with Gauss-Bonnet entropic corrections , Phys. Rev. D 88 (2013) 024006 [ 1304.1878 ].\n",
      "- [6] P. G. S. Fernandes, Charged Black Holes in AdS Spaces in 4 D Einstein Gauss-Bonnet Gravity , 2003.05491 .\n",
      "- [7] R. A. Konoplya and A. Zhidenko, Black holes in the four-dimensional Einstein-Lovelock gravity , 2003.07788 .\n",
      "- [8] S.-W. Wei and Y.-X. Liu, Testing the nature of Gauss-Bonnet gravity by four-dimensional rotating black hole shadow , 2003.07769 .\n",
      "- [9] R. Kumar and S. G. Ghosh, Rotating black holes in the novel 4 D Einstein-Gauss-Bonnet gravity , 2003.08927 .\n",
      "- [10] D. D. Doneva and S. S. Yazadjiev, Relativistic stars in 4D Einstein-Gauss-Bonnet gravity , 2003.10284 .\n",
      "- [11] S. G. Ghosh and S. D. Maharaj, Radiating black holes in the novel 4D Einstein-Gauss-Bonnet gravity , 2003.09841 .\n",
      "- [12] R. A. Konoplya and A. F. Zinhailo, Quasinormal modes, stability and shadows of a black hole in the novel 4D Einstein-Gauss-Bonnet gravity , 2003.01188 .\n",
      "- [13] M. Guo and P.-C. Li, The innermost stable circular orbit and shadow in the novel 4 D Einstein-Gauss-Bonnet gravity , 2003.02523 .\n",
      "- [14] K. Hegde, A. N. Kumara, C. L. A. Rizwan, A. K. M. and M. S. Ali, Thermodynamics, Phase Transition and Joule Thomson Expansion of novel 4-D Gauss Bonnet AdS Black Hole , 2003.08778 .\n",
      "- [15] Y.-P. Zhang, S.-W. Wei and Y.-X. Liu, Spinning test particle in four-dimensional Einstein-Gauss-Bonnet Black Hole , 2003.10960 .\n",
      "- [16] D. V. Singh and S. Siwach, Thermodynamics and P-v criticality of Bardeen-AdS Black Hole in 4-D Einstein-Gauss-Bonnet Gravity , 2003.11754 .\n",
      "- [17] W.-Y. Ai, A note on the novel 4D Einstein-Gauss-Bonnet gravity , 2004.02858 .\n",
      "- [18] M. Gurses, T. C. Sisman and B. Tekin, Is there a novel Einstein-Gauss-Bonnet theory in four dimensions? , 2004.03390 .\n",
      "- [19] S. Mahapatra, A note on the total action of 4 D Gauss-Bonnet theory , 2004.09214 .\n",
      "- [20] S. Tian and Z.-H. Zhu, Comment on 'Einstein-Gauss-Bonnet Gravity in Four-Dimensional Spacetime' , 2004.09954 .\n",
      "- [21] J. Arrechea, A. Delhom and A. Jimnez-Cano, Yet another comment on four-dimensional Einstein-Gauss-Bonnet gravity , 2004.12998 .\n",
      "- [22] K. Van Acoleyen and J. Van Doorsselaere, Galileons from Lovelock actions , Phys. Rev. D83 (2011) 084025 [ 1102.0487 ].\n",
      "\n",
      "- [23] C. Charmousis, B. Gouteraux and E. Kiritsis, Higher-derivative scalar-vector-tensor theories: black holes, Galileons, singularity cloaking and holography , JHEP 09 (2012) 011 [ 1206.1499 ].\n",
      "- [24] P. G. Fernandes, P. Carrilho, T. Clifton and D. J. Mulryne, Derivation of Regularized Field Equations for the Einstein-Gauss-Bonnet Theory in Four Dimensions , 2004.08362 .\n",
      "- [25] R. A. Hennigar, D. Kubiznak, R. B. Mann and C. Pollack, On Taking the D → 4 limit of Gauss-Bonnet Gravity: Theory and Solutions , 2004.09472 .\n",
      "- [26] J. Bonifacio, K. Hinterbichler and L. A. Johnson, Amplitudes and 4D Gauss-Bonnet Theory , 2004.10716 .\n",
      "- [27] G. W. Horndeski, Second-order scalar-tensor field equations in a four-dimensional space , Int. J. Theor. Phys. 10 (1974) 363.\n",
      "- [28] T. Kobayashi, Horndeski theory and beyond: a review , Rept. Prog. Phys. 82 (2019) 086901 [ 1901.07183 ].\n",
      "- [29] C. Deffayet, X. Gao, D. A. Steer and G. Zahariade, From k-essence to generalised Galileons , Phys. Rev. D84 (2011) 064039 [ 1103.3260 ].\n",
      "- [30] T. Kobayashi, M. Yamaguchi and J. Yokoyama, Generalized G-inflation: Inflation with the most general second-order field equations , Prog. Theor. Phys. 126 (2011) 511 [ 1105.5723 ].\n",
      "- [31] T. Kobayashi, N. Tanahashi and M. Yamaguchi, Multifield extension of G inflation , Phys. Rev. D 88 (2013) 083504 [ 1308.4798 ].\n",
      "- [32] LIGO Scientific, Virgo collaboration, B. Abbott et al., GW170817: Observation of Gravitational Waves from a Binary Neutron Star Inspiral , Phys. Rev. Lett. 119 (2017) 161101 [ 1710.05832 ].\n",
      "- [33] LIGO Scientific, Virgo, Fermi-GBM, INTEGRAL collaboration, B. Abbott et al., Gravitational Waves and Gamma-rays from a Binary Neutron Star Merger: GW170817 and GRB 170817A , Astrophys. J. Lett. 848 (2017) L13 [ 1710.05834 ].\n",
      "- [34] H. Lu and Y. Pang, Horndeski Gravity as D → 4 Limit of Gauss-Bonnet , 2003.11552 .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 08:06:25,390 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 08:06:25,416 - INFO - Going to convert document batch...\n",
      "2025-12-15 08:06:25,417 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 08:06:25,418 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 08:06:25,419 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 08:06:25,419 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:25,439 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:25,440 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:25,452 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:25,453 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:25,646 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:25,647 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:25,648 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:25,649 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:25,745 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:25,747 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:25,769 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:06:25,769 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 08:06:25,990 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 08:06:25,991 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:06:26,700 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:06:26,997 - INFO - Processing document 1910.02707v1.pdf\n",
      "2025-12-15 08:07:15,143 - INFO - Finished converting document 1910.02707v1.pdf in 50.00 sec.\n",
      "2025-12-15 08:07:15,260 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 08:07:15,261 - INFO - Going to convert document batch...\n",
      "2025-12-15 08:07:15,262 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 08:07:15,263 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 08:07:15,263 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 08:07:15,264 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:15,282 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:15,283 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:15,295 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:15,296 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Structural and magnetic properties of 3d transition metal oxide chains on the (001) surfaces of Ir and Pt\n",
      "\n",
      "Martin Schmitt, 1 Chong H. Park, 1, 2 Paula Weber, 1 Andreas J¨ ager, 1 Jeannette Kemmer, 1 Matthias Vogt, 1 and Matthias Bode 1, 3, ∗\n",
      "\n",
      "1\n",
      "\n",
      "Physikalisches Institut, Experimentelle Physik II, Universit¨ at W¨ urzburg, Am Hubland, 97074 W¨ urzburg, Germany University of British Columbia, 2329 West Mall, Vancouver, BC Canada Wilhelm Conrad R¨ ontgen-Center for Complex Material Systems (RCCM), Universit¨ at W¨ urzburg, Am Hubland, 97074 W¨ urzburg, Germany (Dated: October 8, 2019)\n",
      "\n",
      "We present a survey of the structural and magnetic properties of submonolayer transition metal dioxides on the (001) surfaces of the heavy face-centered cubic (fcc) noble metals Ir and Pt performed by spin-averaged scanning tunneling microscopy (STM) and spin-polarized (SP-)STM. Our STM results confirm that deposition of Co, Fe, Mn, and Cr on the (2 × 1) oxygen-reconstructed Ir(001) surface leads to the formation of quasi one-dimensional chains with a (3 × 1) unit cell. As recently predicted by density functional theory [Ferstl et al. , Phys. Rev. Lett. 117 , 046101 (2016)], our SPSTM images of FeO 2 and MnO 2 on Ir(001) show a two-fold periodicity along the chains which is characteristic for an antiferromagnetic coupling along the chains. In addition, these two materials also exhibit spontaneous, permanent, and long-range magnetic coupling across the chains. Whereas we find a ferromagnetic inter-chain coupling for FeO 2 /Ir(001), the magnetic coupling of MnO 2 on Ir(001) appears to be a non-collinear 120 ◦ spin spiral, resulting in a (9 × 2) magnetic unit cell. On Pt(001) patches of (3 × 1)-reconstructed oxide chains could only be prepared by transition metal (Co, Fe, and Mn) deposition onto the cold substrate and subsequent annealing in an oxygen atmosphere. Again SP-STM on MnO 2 /Pt(001) reveals a very large, (15 × 2) magnetic unit cell which can tentatively be explained by a commensurate 72 ◦ spin spiral. Large scale SP-STM images reveal a long wavelength spin rotation along the MnO 2 chain.\n",
      "\n",
      "## I. INTRODUCTION\n",
      "\n",
      "Significant progress has been achieved towards a thorough understanding of magnetically ordered states in solid-state materials. 1 Over the past 40 years spin structures with increasing complexity were detected. Whereas collinear ferroor antiferromagnetism governed by the competition of exchange, magnetocrystalline anisotropy, and dipolar interactions initially dominated the scientific debate, we have witnessed a focussing on more complex non-collinear magnetic states since the advent of the current century. 2 This development was-at least partially-made possible by the development of advanced surface analysis and microscopy tools which allow for the detection of magnetic signals with unprecedented sensitivity and spatial resolution. In the context of this contribution spin-polarized scanning tunneling microscopy (SP-STM) will be of particular interest. This technique utilizes the tunnel magnetoresistance effect between a magnetic surface and a spin-polarized tip to obtain information about the sample's spin structure with atomic resolution. 48 SP-STM allowed for the first direct imaging of antiferromagnetic surfaces 4 and domain walls, 5 as well as of frustrated N´ eel spin states with antiphase domains. 6 Furthermore, it turned out that the spin-orbit-induced Dzyaloshinskii-Morija interaction (DMI), which has previously considered in some rare cases only, can be very significant at surfaces and interfaces. For example, it turned out that the Mn monolayer on W(110), which has initially been assumed to be a simple collinear and uniaxial antiferromagnet, 7,8 instead forms chiral spin cycloid. 9\n",
      "\n",
      "Recently a group of novel quasi one-dimensional 3 d transition metal oxides (TMO) was discovered which can conveniently be prepared by self-organized growth on the (001) surfaces of the heavy fcc metals Ir and Pt. 10,11 For Ni, Co, Fe, and Mn on Ir(001) and also for Co on Pt(001) a structural (3 × 1) unit cell was observed by low-energy electron diffraction (LEED). In either case scanning tunneling microscopy (STM) reveals a surface morphology which is characterized by long and highly periodic chains oriented along the [100] and [010] high symmetry directions of the (001) surface. The structure of the TMO chains on fcc(001) surfaces as proposed by Ferstl et al. 10 is schematically represented in Fig. 1. Within each chain we find two oxygen atoms (red) between nearest-neighbor transition metal atoms (yellow). The TMO chains sit\n",
      "\n",
      "FIG. 1. Structure model of transition metal oxide chains on Ir(001) as proposed in Ref. 10. The transition metal atoms chains sit above empty substrate rows, held in place by the oxygens atoms. The inter-chain spacing corresponds to 3 a Ir .\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "TABLE I. Magnetic moments and magnetic coupling of onedimensional TMOs along (intra-) and across (inter-) adjacent chains and corresponding energy gain as predicted by DFT in Refs. 10 and 11.\n",
      "\n",
      "| substrate                  | Ir(001)   | Ir(001)   | Ir(001)   | Ir(001)   | Pt(001)   |\n",
      "|----------------------------|-----------|-----------|-----------|-----------|-----------|\n",
      "| TMO chain                  | NiO 2     | CoO 2     | FeO 2     | MnO 2     | CoO 2     |\n",
      "| µ ( µ B )                  | 0.00      | 1.96      | 3.55      | 3.62      | -         |\n",
      "| intra-chain coupling ( ‖ ) | NM        | FM        | AFM       | AFM       | AFM       |\n",
      "| ∆ E ‖ (meV)                | -         | 25        | 44        | 27        | -         |\n",
      "| inter-chain coupling ( ⊥ ) | -         | FM        | AFM       | AFM       | -         |\n",
      "| ∆ E ⊥ (meV)                | -         | 4         | 9         | 0.4       | -         |\n",
      "\n",
      "above empty substrate rows, held in place by the oxygens atoms.\n",
      "\n",
      "Indeed, density functional theory (DFT) calculations reproduced the experimentally determined structural properties well. 10 These theoretical investigations also predicted highly interesting intra-chain magnetic couplings, ranging from non-magnetic (NM) NiO 2 via ferromagnetic (FM) coupling for CoO 2 and FeO 2 to an antiferromagnetic (AFM) interaction along MnO 2 chains on Ir(001). 10 Furthermore, an AFM coupling was predicted for CoO 2 chains on Pt(001). 11 In general, the coupling strength was found to be much stronger along the chains (up to 44 meV per TM atom) than across adjacent chains (a few meV). It should be kept in mind, however, that the calculations performed in Refs. 10 and 11, which are qualitatively summarized in Table I, were restricted to collinear spin configurations. Non-collinear magnetic structures, such as the N´ eel state, spin spirals, skyrmions, or helical spins structures, which can potentially arise from frustration, 6 higher-order exchange, 12,13 or the DMI 9 have not been considered.\n",
      "\n",
      "To verify the predictions of Ferstl et al. 10,11 we recently studied the magnetic structure of MnO 2 chains on Ir(001) by means of spin-polarized scanning tunneling microscopy (SP-STM). 14 In addition to the AFM coupling along the chains predicted by Ferstl et al. 10 an indirect 120 ◦ magnetic coupling across the chains was observed. This surprising finding was rationalized in terms of an Dzyaloshinskii-Moriya-enhanced Ruderman-KittelKasuya-Yosida (RKKY) interaction. 14 These earlier results obtained on MnO 2 /Ir(001) showed that this indirect magnetic coupling mechanism which was previously only observed for assemblies of single atoms or clusters on Pt(111) surfaces 15,16 can also result in chiral magnetic order in laterally extended structures.\n",
      "\n",
      "The purpose of this paper is to investigate the magnetic structure of a broad range of (3 × 1)-ordered TMO chains on (001) surfaces of the heavy face-centered cubic (fcc) noble metals Ir and Pt by SP-STM. The paper is organized as follows: The SP-STM technique and the experimental procedures applied for substrate cleaning, oxidation, and transition metal deposition are briefly described in Sect. II. Results for the two substrates, i.e., Ir(001) and\n",
      "\n",
      "Pt(001), will be presented separately in Sect. III A and Sect. III B, respectively. SP-STM measurements were performed on the oxides of Co, Fe, Mn, and Cr on Ir(001) and for Co and Mn on Pt(001). Whereas no magnetic contrast could be detected for Co and Cr, the magnetic intra-chain coupling observed for the other transition metals is in agreement with DFT predictions. 10,11 In addition, our results also reveal magnetic ordering across the chains. Whereas we find a collinear coupling across the chains for FeO 2 on Ir(001), the indirect inter-chain coupling of MnO 2 on both, Ir(001) and Pt(001), is found to be helical, resulting in complex spin structures with surprisingly large magnetic unit cells.\n",
      "\n",
      "## II. EXPERIMENTAL PROCEDURES\n",
      "\n",
      "STM experiments were performed in a two-chamber ultra-high vacuum (UHV) system with a base pressure p ≤ 5 × 10 -11 mbar. Clean Ir(001) and Pt(001) surfaces were prepared by annealing cycles in an oxygen atmosphere followed by cycles of sputtering and annealing without oxygen. After this procedure the well known (5 × 1) reconstruction of Ir(001) as well as the (26 × 118) structural unit cell of Pt(001) was confirmed. 17-20\n",
      "\n",
      "Closely following the procedures described by Ferstl and co-workers, 10 the clean Ir(001) surface was then exposed to molecular oxygen resulting in a (2 × 1)reconstructed surface. The oxygen pressure indicated by our quadrupole mass spectrometer was p O 2 = 1 × 10 -8 mbar, but the local pressure is assumed to be about two orders of magnitude higher since the gas nozzle is located just a few cm above the sample surface. On this oxygen-reconstructed Ir surface we deposited one third of a monolayer (ML) of either, Co, Fe, Mn, or Cr. All 3 d transition metals were vaporized with commercial e-beam evaporation sources (EFM3). Whereas Co and Fe were evaporated from wires with a diameter of 2 mm, Mn and Cr lumps were loaded into Mo crucibles. Upon 3 d transition metal deposition the sample was again annealed in an oxygen atmosphere, resulting in the (3 × 1) structure of TMO chains. 10 Since the annealing temperature T ann required for optimal TMO chain quality was found to depend on the transition metal element, the specific values will be given below. Due to the higher stability of the Pt(001) reconstruction to oxygen exposure the preparation of TMO chains on Pt(001) is slightly different. 11,21 Namely, the 3 d transition metal was directly evaporated onto the reconstructed surface and only subsequently annealed in an oxygen atmosphere. 11\n",
      "\n",
      "To verify the structural properties of the TMO chains the samples were transferred into a home-built lowtemperature scanning tunneling microscope (LT-STM) where they were scanned with an electro-chemically etched polycrystalline W tip at an operation temperature of T ≈ 5 . 5 K. All images were obtained in the constantcurrent mode with bias voltage ( U ) applied to the sample. When using spin-polarized tips in SP-STM measure-\n",
      "\n",
      "FIG. 2. (a) Overview scan of a sample with coexisting (3 × 1)-ordered CoO 2 chains and (2 × 1) oxygen-reconstructed areas on Ir(001). (b) Higher magnification image of the area marked by a white square in (a) revealing an orthogonal orientation row orientation of (2 × 1)- and (3 × 1)-ordered areas. The line profile in the bottom panel confirms a 3 a Ir periodicity perpendicular to the CoO 2 chains. (c) Atomic resolution scan of the area marked by a box in (b). The line profile verifies the × 1 periodicity along the CoO 2 chains. Scan parameters: (a), (b) U = 1V, I = 300pA; (c) U = 50mV, I = 1nA. All scans performed with a non-magnetic W tip.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "ments the recorded tunneling current I can be written as\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where I 0 is the spin-averaged contribution to the tunneling current. The second term in Eq. 1 represents the magnetization direction-dependent variation of the total current which depends on the angle θ between tip P t and sample P s polarization. For SP-STM the W tips were flashed by electron bombardment and coated with either Fe or Cr. To unambiguously determine the inplane or out-of-plane sensitivity of the magnetic SP-STM tips they were characterized on a reference system. In the present case we used the Fe double-layer (DL) on W(110) which exhibits a well-known inhomogeneous spin spiral, thereby offering the possibility to identify the in-plane and out-of-plane components of the tip magnetization in a single measurement. 22-24\n",
      "\n",
      "As we will discuss below, we could not obtain magnetic contrast on some TMOs even though earlier DFT calculations predict them to order magnetically. This raises the question of the detection limit of SP-STM. In fact, the surface spin structures of numerous elements have successfully been imaged in the past, including rareearth metals, such as Gd 25 and Dy, 26 or the antiferromagnetic 3 d metal Cr. 27 The Gd magnetic moment is largely carried by the 4 f shell (7 µ B ) which is energetically located well below the Fermi level and therefore cannot contribute to the tunneling current. For Gd(0001) it has been shown that the SP-STM contrast originates from a d z 2 -like surface state which carries a relatively low magnetic moment µ ≈ 0 . 35 µ B only. 28 In the case of Cr measurements were performed at room temperature, i.e. at a relatively high reduced temperature T/T N ≈ 0 . 94 (N´ eel temperature T N = 311K). At this temperature the magnetic moment only amounts to about 40% of its ground state value. Nevertheless, for both Gd(0001) and for Cr(001) the surface magnetic structure could clearly be imaged. Considering these earlier results we assume that the detection limit of SP-STM is well below a surface moment of 1 µ B .\n",
      "\n",
      "## III. RESULTS\n",
      "\n",
      "## A. TMO chains on Ir(001)\n",
      "\n",
      "## 1. CoO 2 /Ir(001)\n",
      "\n",
      "After evaporation of Co onto the oxidized Ir(001) surface at room temperature the sample was annealed at T ann ≈ 870 K at an oxygen partial pressure p O 2 = 1 × 10 -8 mbar. During the TMO chain growth process every third Ir surface atom is expelled from the surface layer. It has previously observed for MnO 2 on Ir(001) 1014 that-at sufficiently high annealing temperature-these atoms form extended islands or even recombine at step edges with existing surface terraces. A similar behavior can be observed in Fig. 2(a), where monatomic rectangular shaped islands occur with step edges along the [110] and [110] high-symmetry directions of the substrate.\n",
      "\n",
      "Closer inspection of the terraces in Fig. 2(b) show the coexistence of the (2 × 1) oxygen reconstruction and the (3 × 1) TMO structure along the high-symmetry direc-\n",
      "\n",
      "tions. Furthermore, Fig. 2(b) reveals that the stripes of the (2 × 1) reconstruction are generally oriented perpendicular to the (3 × 1)-ordered CoO 2 chains, possibly due to the incommensurability of (2 × 1) oxygenreconstructed and (3 × 1)-ordered TMO chains. The presence of oxygen-reconstructed areas without Co possibly indicates that the amount of deposited Co was slightly below one third of a ML. With a density about 0.03 nm -2 the most frequent defects are point-like protrusions on the TMO chains. Their height amounts to about 80 pm, consistent with typical values for transition metal adatoms. Therefore, we assume that these protrusions are caused by either excess Co or Ir atoms which were expelled from the surface layer but remained on the CoO 2 chains. Furthermore, a few depressions in the chains can be recognized (one of which is marked by the green arrow). Since these depressions are centered where one would expect a maximum in a periodic chain in the absence of a defect, it appears reasonable to preliminarily assign them to Co vacancies. Both types of defects with similar characteristics will also appear for the other transition metals studied. In addition, we occasionally observe weak circular depressions (see blue arrow) which have also been observed on the bare Ir(001) and are assigned to sub-surface defects.\n",
      "\n",
      "To verify the structural properties of the CoO 2 on Ir(001) we measured a line profile perpendicular to the chains in between the two black arrows in the bottom right corner of the STM image displayed in Fig. 2(b). It is plotted at the bottom of Fig. 2(b). The periodicity of (828 ± 30) pm agrees well with the expected value of 3 a Ir = 813pm. 29 Additionally, the atomic resolution scan shown in Fig. 2(c) and the corresponding line section shown in the bottom panel also confirm the × 1 periodicity with an atomic distance of (278 ± 10) pm along the TMO chains.\n",
      "\n",
      "After structural analysis we prepared magnetic Cr/W tips and Fe/W tips for SP-STM measurements. As documented in the Supplementary Information, these out-ofplane and in-plane sensitive tips were thoroughly tested by imaging the domain and domain wall structure the Fe DL on W(110), respectively. 30 Although these tests clearly confirmed the magnetic imaging capabilities of our SP-STM tips before and after the measurements on the CoO 2 chains on Ir(001), we never observed any magnetic contrast on the (3 × 1) structure of CoO 2 chains (not shown). This result is not necessarily in contradiction with the ferromagnetic order predicted in Ref. 10 because we have to keep in mind that the imaging of magnetic spin structures by SP-STM relies on the existence of domains or other spatial variations of the projection of the local sample magnetization onto the fixed magnetization of the tip. Therefore, the ferromagnetic domains could remain undetected if their size was much larger than the scan size. In this context future spatially averaging techniques, such as the magneto-optical Kerr effect, might be useful to clarify this open issue.\n",
      "\n",
      "## 2. FeO 2 /Ir(001)\n",
      "\n",
      "The next transition metal element with one electron less in the 3 d shell is Fe. It has been predicted that FeO 2 on Ir(001) exhibits an AFM order along the chains. 10 Just like for the preparation of CoO 2 chains described in Sect. III A 1, Fe was evaporated at room temperature on the (2 × 1)-reconstructed O/Ir(001) surface and immediately annealed at T ann ≈ 970 K in an oxygen atmosphere ( p O 2 = 1 × 10 -7 mbar). The overview scan in Fig. 3(a) shows two flat terraces separated by a monatomic substrate step edge. The absence of rectangular islands indicates that all Ir atoms expelled from the substrate had the chance to diffuse to step edges. Close inspection reveals that the chains are oriented along the [110] direction on the upper (left) terrace whereas they are oriented in the [110] direction on the lower (right) terrace. The density of the point-like defects on top of the chains amount to about 0.06 nm -2 . Similar to what was discussed in the preceding section III A 1, we believe that the most likely\n",
      "\n",
      "FIG. 3. (a) Overview scan of FeO 2 /Ir(001) with chains running into [110] direction on the upper and [110] direction on the lower terrace. (b) Higher resolution scan on the lower terrace. (c) Line profile along the blue line in (b). The periodicity of the stripes amounts to (855 ± 50) pm. Scan parameters: (a) &amp; (b) U = -1 V, I = 300pA, non-magnetic W tip.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "origin of these protrusions is a slight overdosing of the surface with Fe such that some atoms cannot be accommodated within the resulting Fe oxide chain structure. We would like to note that the teardrop-shaped appearance of the protrusions is an imaging artifact caused by an unusual shape of the tip used in this experiment.\n",
      "\n",
      "For structural analysis of the FeO 2 chains a higher resolution scan on the lower terrace is shown in Fig. 3(b). The periodicity across the chains along the blue line is determined by the line profile shown in Fig. 3(c). Again we find periodicity of (855 ± 50) pm which is in good agreement with the value expected for a (3 × 1) reconstruction, i.e., 3 a Ir = 813pm. To complete the structural analysis an atomic resolution scan of FeO 2 is presented in Fig. 4(a). Line sections drawn between the black arrows in Fig. 4(a) which are presented in Fig. 4(c) (black lines) show a periodicity of (278 ± 10) pm, consistent with the Ir lattice constant a Ir = 271pm. Furthermore, an additional weak stripe in the center of the (3 × 1) unit cell in Fig. 4(a) can be recognized in between the chains. A similar observation was reported by Ferstl et al. 10 and assigned to an electronic signal of the Ir double-rows separating adjacent TMO chains.\n",
      "\n",
      "The magnetic structure of this surface was investigated by SP-STM with an out-of-plane sensitive Cr-coated W tip. The result is presented in Fig. 4(b). Direct comparison with Fig. 4(a) reveals a doubled period along the chains. This impression is corroborated by the line profiles presented in Fig. 4(c) which have been taken in between the three pairs of colored arrows and clearly show a period 2 a Ir = 542pm along the chains. The doubling is caused by the magnetoresistance effect which leads to a tunneling current which scales with the projection of the sample magnetization onto the tip magnetization (cf. Eq. 1). The most obvious explanation for the observed doubling of the period along the chain is an AFM coupling. Furthermore, the maxima and minima of adjacent FeO 2 chains in Fig. 4(b,c) are aligned, thereby clearly indicating a FM coupling across the chains. The fact that all magnetic line profiles show the same corrugation essentially excludes any non-collinear magnetic order, such as spin spirals or frustrated N´ eel spin states, but rather supports that FeO 2 /Ir(001) exhibits collinear magnetism.\n",
      "\n",
      "Our experimental results obtained on FeO 2 chains on Ir(001) indicate a (3 × 2) magnetic unit cell which is marked as a box in Fig. 4(b) and schematically illustrated in Fig. 4(d). The AFM coupling observed along the chains is in agreement with recent theoretical predictions. 10 To our opinion it is quite reasonable to assume that this AFM along the chains is caused by superexchange mediated by the fully occupied oxygen 2 p orbitals. 31 Although a much weaker AFM coupling across the chains was predicted in Ref. 10, we experimentally observe spontaneous, permanent, and long-range FM interchain coupling at T = 5K. At the moment we can only speculate why theory and experiment deviate. Since adjacent TMO chains are separated by two non-magnetic Ir rows this order cannot be mediated by direct exchange.\n",
      "\n",
      "FIG. 4. (a) Atomic resolution scan of FeO 2 chains on Ir(001) obtained with a non-magnetic W tip. The atoms along the chains are separated by one Ir lattice constant a Ir = 271pm. (b) SP-STM image obtained with an out-of-plane Cr/W tip. Comparison to (a) reveals period along the chains lead corresponding to 2 a Ir = 542pm resulting in a (3 × 2) magnetic unit cell. (c) Line profiles measured along the chains at the position indicated by the arrows in (a) and (b). (d) Schematic illustration of the (3 × 2) collinear magnetic order. While the magnetic coupling is AFM along the chains, the interaction of adjacent chains is FM. Scan parameters: (a) U = -10 mV, I = 20nA; (b) U = 50mV, I = 5nA.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "As we will point out below for MnO 2 chains on Ir(001), it appears that the strong spin-orbit interaction of the Ir substrate plays a decisive role for the formation of the magnetic ground state. Potentially, similar effects are also relevant for FeO 2 /Ir(001). More advanced theoretical considerations will be necessary to fully elucidate the coupling mechanism.\n",
      "\n",
      "## 3. MnO 2 /Ir(001)\n",
      "\n",
      "Our SP-STM results presented in the preceding section were in good agreement with DFT predictions regarding the intra-chain magnetic coupling, 10 but also indicate that some aspect of the inter-chain coupling may not have been captured with sufficient accuracy. Since DFT qualitatively predicts the same intra-chain magnetic coupling for MnO 2 /Ir(001) as for FeO 2 it is highly interesting to experimentally investigate also this sample system. The preparation of MnO 2 chains is very similar to the TMOs discussed so far. Also in this case Mn is deposited onto the oxidized Ir(001) surface at room temperature. To our experience the best surface quality can be achieved when choosing a slightly higher annealing temperature T ann ≈ 1020 K. This results in a sample topography with roughly rectangular islands, as exemplarily presented in the overview scan of Fig. 5(a). The (3 × 1)-ordered MnO 2 chain structure covers almost the entire sample surface such that no regions with the oxygen-induced (2 × 1) reconstruction of Ir(001) remain visible. Typical (3 × 1) domains are about 20 to 40 nm wide and often hundreds of nm long. Adjacent domains are separated by either orientational and anti-phase domain boundaries.\n",
      "\n",
      "Similar to other TMO chains on Ir(001) we observe several defects which are located on top of the chains. For example, in Fig. 5(b) 12 single protrusions, one dimer, a vacancy in a MnO 2 row can be recognized. This corresponds to a defect density of only 0.015 nm -2 , well below what has been determined for CoO 2 and FeO 2 in Sect. III A 1 and III A 2, respectively. The periodicity of the stripes can be determined by the line profile presented in Fig. 5(c). In agreement with the expected (3 × 1) structure it amounts to (840 ± 50) pm.\n",
      "\n",
      "As shown in Fig. 6(a) this (3 × 1) superstructure has been imaged with atomic resolution in a spin-averaging STM experiments by using a non-magnetic W tip. The corrugation and periodicity along the chains in Fig. 6(a) can be determined from the line profiles presented in Fig. 6(c) (black lines). These data sets clearly show that any chains exhibits a corrugation of z ≈ 2 . 2 pm only and a periodicity of (287 ± 20) pm. In addition, Fig. 6(a) also reveals a small corrugation in between the chains which can possibly be assigned to Ir pairs.\n",
      "\n",
      "In Fig. 6(b) we present spin-polarized measurements of MnO 2 chains on Ir(001) which were performed with an in-plane sensitive magnetic Cr/W tip. We would like to note that these results, which were obtained in a different experimental run on a newly prepared sample with a different SP-STM tip, qualitatively reproduce earlier results. 14 Quantitative differences, such as a different corrugation, are attributed to the fact that the two tips probably differ in their spin polarization and quantization axes. Similar to the results on FeO 2 discussed in Sect. III A 2 the use of a magnetic probe tip leads to a doubling of the measured corrugation period which now corresponds to 2 a Ir along the chains. Furthermore, comparison of adjacent chains reveals a periodic variation of\n",
      "\n",
      "FIG. 5. (a) Overview scan of MnO 2 chains on Ir(001). Domains with stripes of the (3 × 1) structure orientated along the [110] and the [110] direction can be recognized. (b) Spinaveraged STM image of an atomically flat surface area showing a few defects only. (c) Line profile measured along the blue line in (b) confirming an inter-chain distance of 3 a Ir . Scan parameters: U = 1V, I = 300pA, non-magnetic W tip.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "the corrugation. This can qualitatively be realized by comparing the chain marked with red arrows in Fig. 6(b) with the chains on the left and right which are marked green and blue, respectively. The green chain on the left obviously exhibits a much weaker corrugation. In contrast, no obvious difference between the contrast strength of the red and blue line can be recognized, but the positions of maxima and minima of the chain marked with blue arrows are interchanged with respect to the red line.\n",
      "\n",
      "The colored line profiles presented in Fig. 6(c) allow for a more quantitative comparison. In agreement to our qualitative assessment the green chain exhibit the smallest corrugation of 2 . 2 pm whereas both, the blue and red line have a much higher corrugation of about 5 pm. In addition, we can also recognize the phase shift of a Ir between the red and blue line. Detailed comparison shows that the blue chain has a slightly higher corrugation than the red chain. Together with the above-mentioned periodicity 2 a Ir along the chains these SP-STM observations suggest a (9 × 2) magnetic unit cell.\n",
      "\n",
      "FIG. 6. (a) Atomic resolution scan of the MnO 2 chains on Ir(001) measured with a non-magnetic W tip showing the (3 × 1) unit cell. (b) SP-STM image of MnO 2 /Ir(001) obtained with an in-plane sensitive Cr-coated W tip. The period along the stripes corresponds to 2 a Ir indicating AFM order along the chain. The magnetic contrast of adjacent chains is modulated by phase and corrugation changes resulting in a (9 × 2) magnetic unit cell. (c) Line profile measured along three adjacent chains in (a) without (black) and in (b) with magnetically sensitive tips (colored). Scan parameters: (a) U = -300 mV, I = 3nA; (b) U = 200mV, I = 300pA.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "The experimental observations on MnO 2 chains can be explained by a spin structure as schematically illustrated in Fig. 7(a). Each MnO 2 chain is AFM ordered whereby the easy axis always lies within the surface plane. The azimuthal orientation rotates by 120 ◦ between adjacent MnO 2 chains such that the magnetic structure repeats every third chain. These two ingredients, i.e., the AFM order along and the tripled magnetic period perpendicular to the MnO 2 chains, result in a (9 × 2) magnetic unit cell as indicated in Fig. 6(b). As sketched in Fig. 7(b), this magnetic structure can explain the corrugation and the phase relation observed by SP-STM in Fig. 6(b,c).\n",
      "\n",
      "FIG. 7. (a) Schematic illustration of the (9 × 2) magnetic unit cell of MnO 2 chains on Ir(001). The arrows indicate the spin direction of the corresponding Mn atoms. (b) Sketch of the SP-STM signal expected for the proposed chiral 120 ◦ spin spiral. The corrugation and the phase shift observed in Fig. 6(b) can be explained by an angle Θ = (15 ± 6) ◦ between tip polarization (black arrow) and the TMO chain with highest corrugation (blue line).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "In this representation the black arrow represents the tip magnetization and the colored arrows represent the quantization axes of the respective chains.\n",
      "\n",
      "Proceeding from right to left in the (9 × 2) magnetic unit cell in Fig. 7(a) we can recognize that the magnetic structure is governed by an in-plane counterclockwise spin spiral with a rotation angle of 120 ◦ between adjacent chains. Since the blue arrow exhibits the largest projection onto the black arrow it will-according to Eq. 1give the strongest magnetic contrast in SP-STM measurements, symbolized by the dot-dashed line in Fig. 7(a). As a result of the spin spiral the green and red arrows are aligned antiparallel with respect to the tip. In case of an AFM intra-chain coupling this will unavoidably lead to a phase shift between the blue spin chain as compared to the green and red spin chain. Based on this representation we can nicely reproduce the corrugations extracted from Fig. 6(c) by an angle between the blue chain and the tip polarization of θ = 15 ± 6 ◦ , indicated by a grey region in Fig. 7(b).\n",
      "\n",
      "In a recently published paper the observation of a chiral 120 ◦ spin spiral which is mediated across two atomic rows of the non-magnetic Ir(001) substrate was explained by a Dzyaloshinskii-Moriya type (DM) enhancement of the RKKY interaction. 14 This interaction, which has already been predicted in 1980 32 , leads to a magnetic coupling which is at the same time indirect and chiral. Evidence for DM-enhanced indirect magnetic coupling had previously been observed on some surfacedeposited clusters 15,16,33,34 and Dy/Y superlattices. 35 Indeed, state-of-the-art density functional theory calcula-\n",
      "\n",
      "、\n",
      "\n",
      "FIG. 8. (a) Overview and (b) higher resolution scan on CrO 2 chains on Ir(001). (c)-(g) Bias-dependent STM images showing the same sample area. A cluster (marked by an arrow) serves as a position marker. A corrugation reversal is observed between panels (e) and (f), i.e., between U = 200mV and U = -200 mV. (h) Line profiles measured along lines corresponding to the blue line in (c). Scan parameters: (a),(b) U = 1V, I = 300pA; (d) I = 2nA; (d),(g) I = 1 . 5 nA; (e),(f) I = 0 . 5 nA.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "tions could successfully reproduce not only the antiferromagnetic coupling along the chains but also explain the period of the chiral magnetic structure across the chains. 14 However, the experimental data suggest a spin spiral which predominantly rotates within the surface plane, corresponding to a Dzyaloshinskii-Moriya vector D oriented along the surface normal. This surprising result cannot be explained within the structural model deduced from quantitative low-energy electron diffraction data, 10 which would result in a D vector within the surface. The discrepancy might be caused by yet unconsidered effects of structural domain boundaries or relaxation effects. Additional DFT calculations and field-dependent SP-STM measurements will be necessary to fully resolve this open question.\n",
      "\n",
      "## 4. CrO 2 /Ir(001)\n",
      "\n",
      "Motivated by the observation of an unexpected noncollinear magnetism in (3 × 1)-ordered MnO 2 chains on Ir(001) we tried to extent the series of self-organized TMO chains to Cr which is the left neighbor of Mn in the periodic table of elements. Since the growth of CrO 2 chains was not part of the study by Ferstl et al. 10 it will be discussed here in detail. The preparation follows the principle of the known TMO chains and comprises the evaporation of Cr onto the (2 × 1) O/Ir(001) surface at room temperature. Subsequently, the sample was an- nealed in an oxygen atmosphere ( p O 2 = 1 × 10 -8 mbar) at a temperature T ann ≈ 1070 K.\n",
      "\n",
      "The resulting sample topography and its structural analysis are shown in Fig. 8. Already in the overview scan presented in Fig. 8(a) we can recognize stripes orientated along the [110] direction. We count 145 point-like defects with a typical height of several 10 up to 100 pm on top of the stripes, corresponding to a defect density below 0.015 per nm 2 . Moreover, some protrusions are visible that we already identified on clean (5 × 1) reconstructed Ir(001) surfaces. The periodic arrangement of the stripes with an inter-stripe distance of (846 ± 40) pm is shown at higher resolution in Fig. 8(b). This value is comparable to the inter-chain distance observed on the other quasi one-dimensional TMO systems and consistent with the (3 × 1) structural unit cell.\n",
      "\n",
      "To study the electronic properties of this stripe pattern we performed the bias-dependent measurements presented in Fig. 8(c)-(f). All scans show the same sample area as can be recognized by the cluster of unknown origin. An arrow marks the central region of this cluster and serves as a reference point during the following comparison. At a bias voltage U = 1V, Fig. 8(c), the arrow is aligned with the top of a stripe (A-stripe). The next stripe to the right can be found about 1 nm apart just right of the bright cluster. When the bias voltage is reduced to U = 600mV, Fig. 8(d), another stripe (B-stripe) appears between the A-stripes. Interestingly, the corrugation is not mirror symmetric with respect to the plane\n",
      "\n",
      "defined by the [ ¯ 110] direction, i.e., along the stripes, and the (001) surface normal. This can best be recognized in the line sections shown in Fig. 8(h). The data have been extracted from the images shown in Fig. 8(c)-(f) along traces corresponding to the blue line in panel (c). Again the arrow (and the hatched line) indicates the central position of the bright cluster. At U = 600mV the two minima left and right of the hatched line clearly exhibit different depth, with the left minimum being about 1 pm deeper than the right minimum. This trend becomes even stronger as the bias voltage is changed to U = 200meV. Furthermore, when decreasing the bias voltage further to U = -200 meV, Fig. 8(f), and U = -500 meV, Fig. 8(g), we observe that the corrugation of the B-stripes becomes much larger than that of the A-stripes. For example, at U = -500 meV the corrugation of B-stripes reaches (15 ± 1) pm, whereas the corrugation of the A-stripes only amounts to (2 . 6 ± 0 . 3) pm.\n",
      "\n",
      "Our bias-dependent experimental results on CrO 2 /Ir(001) are strikingly different as compared to similar results on the other TMO chains. We find an asymmetric corrugation and A- and B-stripes which are the dominant features in topographic STM measurements performed at U &gt; 200 meV and U ≤ -200 meV, respectively. Furthermore, the change in corrugation is more pronounced for B-stripes. Combined with the fact that the B-stripes almost vanish for U = 1V we suppose that A-stripes correspond to the CrO 2 chains whereas the B-stripes are assigned to the intermediate Ir double row. As detailed in Ref. 30 we performed SP-STM experiments on CrO 2 /Ir(001) with both, out-of-plane sensitive Cr/W and in-plane sensitive Fe/W tips. These measurements confirmed the structural (3 × 1) unit cell of the CrO 2 chains but did not provide any hint of a magnetically ordered state.\n",
      "\n",
      "## B. TMO chains on Pt(001)\n",
      "\n",
      "## 1. CoO 2 /Pt(001)\n",
      "\n",
      "Our SP-STM experiments of TMO chains on Ir(001) presented so far revealed a wide variety of spin structures. Whereas we could not detect any magnetic signal on CoO 2 chains, suggesting a non-magnetic state, for other 3 d elements we found collinear (anti)ferromagnetic (Fe) and a helical spin spiral order (Mn). It appears to be an obvious idea to extend the search for similar indirect DM-enhanced RKKY interactions by growing TMO chains on other heavy noble metals which also crystallize in the fcc structure. One candidate material is Pt, the direct neighbor of Ir in the periodic table of elements. In fact, it has already been shown that CoO 2 chains can be grown by similar procedures on Pt(001). 11\n",
      "\n",
      "Since the Pt(001) reconstruction is stable against oxidation we first studied the growth of Co on the clean surface. 11 In our STM images the complex hexagonal reconstructions of clean Pt(001) shows up as stripes with a\n",
      "\n",
      "FIG. 9. Growth of CoO 2 chains on Pt(001). (a) Topography of Co/Pt(001) after evaporation on the warm substrate. (b) After oxidation of sample (a) no (3 × 1) structure is observed. (c) Topography of Co/Pt(001) after evaporation on the cold substrate. (d) After oxidation of sample (c) a coexistence of Pt reconstruction and domains of CoO 2 chains is observed. (e) Higher resolution scan of the chains with line profile across the stripes. (f) Atomic resolution scan with a non-magnetic W tip confirms the structural (3 × 1) unit cell. Scan parameters: (a)-(d) U = 1V, I = 300pA; (e) U = -100 mV, I = 1 . 5 nA; (e) U = 50mV, I = 5nA, all data measured with a non-magnetic W tip.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "periodicity of (1 . 36 ± 15) nm. 36 Fig. 9(a) shows the topography of a sample prepared by Co evaporation onto the warm surface resulting in the formation of (163 ± 15) pm high islands which are elongated along the [110] direction. This orientation correlates with the stripe direction of the Pt(001) surface and indicates preferential diffusion along the trenches of the reconstruction.\n",
      "\n",
      "Annealing this sample in an oxygen atmosphere ( p O 2 = 1 × 10 -7 mbar) at temperature T ann ≈ 920 K results in the topography shown in Fig. 9(b). Two surface areas can be distinguished in these data which were measured with non-magnetic W tips. In the upper right of Fig. 9(b) we\n",
      "\n",
      "again recognize the clean Pt(001) surface which-similar to our observations on clean Ir(001) [cf. blue arrow in Fig. 9(b)]-exhibits occasional circular depressions (density ≈ 0 . 02 nm -2 ). The persistence of this reconstruction confirms the stability of the Pt(001) surface against oxidation. The remaining part of the surface is covered by islands without any oxygen-induced reconstruction (nr). Together with the unusual step height of (90 ± 15) pm we interpret this as evidence for the formation of a CoPt surface alloy.\n",
      "\n",
      "In a second attempt we initially cooled the clean Pt(001) surface in the LT-STM down to T ≈ 5 . 5 K. Subsequently the crystal was transferred to the preparation chamber for Co deposition onto the cold surface. Comparison of the resulting surface topography, Fig. 9(c), with the Co film grown at elevated temperature, Fig. 9(a), shows that low-temperature growth leads to smaller Co islands and the nucleation of very few second layer Co islands. In contrast to the Co islands shown in Fig. 9(a) these low-temperature-deposited islands exhibit no stripes. Annealing this sample at the same parameters, T ann ≈ 920 K, leads to the topography presented in Fig. 9(d), which is comparable to the earlier results of Ref. 11. We can distinguish areas of reconstructed clean Pt(001) from extended and well-ordered regions showing the (3 × 1) structure which is characteristic for CoO 2 chains. The step height between domains of the same structure amounts to (199 ± 10) pm, in perfect agreement with the value expected for Pt(001). To complete the structural analysis we performed a higher resolution scan which are presented in Fig. 9(e). Again point-like defects are located on the stripes. The line profile taken at the position of the blue line confirms the periodicity of (830 ± 20) pm. The inner structure of the stripes can be resolved by the atomic resolution image shown in Fig. 9(f). Along the chains we measure a periodicity of (278 ± 15) pm which agrees well with the cubic lattice vector expected for Pt, a Pt = 277pm. 21\n",
      "\n",
      "After confirmation of the (3 × 1) structure of CoO 2 chains on Pt(001) 11 we attempted to resolve the spin or magnetic domain structure by means of SP-STM experiments using out-of-plane and in-plane polarized tips. However, these magnetically sensitive experiments 30 showed neither a hint of the theoretically proposed AFM structure 11 nor could we detect any hint of other magnetically ordered states.\n",
      "\n",
      "## 2. FeO 2 /Pt(001)\n",
      "\n",
      "For the preparation of FeO 2 chains on Pt(001) we followed the same procedure as for Co, i.e., we started with the growth of Fe on the clean reconstructed Pt(001) surface. Indeed, Fe evaporation onto the warm Pt(001) substrate leads to a surface morphology very similar to Co/Pt(001) [cf. 9(a,b), results for Fe not shown here] which we interpret as evidence for alloying with the substrate. To overcome this issue Fe was evaporated onto the\n",
      "\n",
      "FIG. 10. (a) Overview scan of oxidized Fe on Pt(001). (b) A higher resolution scan shows that terraces and islands exhibit a stripe pattern with the expected 3 a Pt periodicity. (c) Atomic scale image of oxidized Fe on Pt(001). Along the stripes the corrugation has a period of twice the atomic lattice vector. Within the circle the order across the stripes changes from aligned to a shifted position. (d) Line profiles across (top) and along the stripes (bottom) at the positions of the blue line in (b) and at the position of the arrows in (c). Scan parameters: (a),(b) U = 1V, I = 300pA; (c) U = 100mV, I = 1nA, non-magnetic W tip.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "cold sample and immediately oxidized at T ≈ 870 K. The resulting topography which is presented in the overview scan of Fig. 10(a) (scan size: 300 nm × 300 nm) shows plateaus and valleys with a typical lateral size of about 50 nm. They are separated by (199 ± 15) pm high step edges preferentially oriented along high symmetry directions of the substrate. Scanning a similar surface area at higher resolution, Fig. 10(b), reveals a stripe pattern with a periodicity of (844 ± 40) pm, as determined from the line profile in Fig. 10(d), in agreement with the (3 × 1) reconstruction expected for FeO 2 chains on Pt(001). Regions with stripes oriented in the [110] or the [110] directions can be recognized.\n",
      "\n",
      "Surprisingly, atomic scale images performed with nonmagnetic tips within a single domain are inconsistent with the expected structural (3 × 1) unit cell. This can clearly be recognized by the image presented in Fig. 10(c) and the line section shown Fig. 10(e) which was measured in between the two black arrows. The periodicity extracted from this line profile along the chains amounts to (543 ± 25) pm. This value is in good agreement with twice the lattice constant of Pt, 2 a Pt . Furthermore, the maxima of adjacent stripes are out of phase in most cases. Therefore, our spin-averaged experimental data suggest\n",
      "\n",
      "the existence of a (6 × 2) unit cell for the oxidized Fe on Pt(001) [see white box in Fig. 10(c)]. Since the positions of maxima and minima in a given chain are not completely static but slowly fluctuate, occasional exceptions from this rule can be observed. For example, the chain in the right part of the white circle in Fig. 10(c) shows such a switching event. Whereas the two chains marked by the circle are in phase in the lower part of the image, a phase shifts by a Pt in the right chain causes that the two chains are out of phase in the upper part of the image. Since the measurement of Fig. 10(c) was executed with a nonmagnetic tip and since the use of out-of-plane or in plane sensitive magnetic probe tips didn't result in any additional contrast, we have to conclude that the observed reconstruction has no magnetic but either a structural or an electronic origin. One possible explanation could be a Peierls instability due to a metal-isolator transition at low temperatures. 37 Further investigations are required to figure out the origin of the iron oxide phase.\n",
      "\n",
      "## 3. MnO 2 /Pt(001)\n",
      "\n",
      "Finally we investigated MnO 2 chains on Pt(001). In a first attempt we evaporated Mn while the Pt(001) substrate was held at room temperature and subsequently oxidized the sample at an oxygen pressure p O 2 = 1 × 10 -7 mbar at T ≈ 920 K. Unfortunately, this preparation procedure resulted in very small TMO domains with size of (10 ... 20) nm only. In order to improve surface homogeneity we reduced the oxygen pressure to p O 2 = 1 × 10 -8 mbar. The resulting topography measured with a non-magnetic W tip is shown in Fig. 11(a). A significant fraction of the surface still exhibits the familiar reconstruction of the clean Pt(001) surface. In addition, two other kinds of surfaces could be identified. The first one no longer shows any reconstruction (not reconstructed; nr) but is relatively rough. Similar findings have been reported for Cu/Ir(001) and were assigned to alloying. 38 Second, we found (3 × 1)-reconstructed areas of MnO 2 . For structural determination a higher resolution scan measured on a homogeneously striped domain is shown in Fig. 11(b). Adjacent stripes are separated by (866 ± 45) pm, as determined from the corresponding line profile in Fig. 11(c). We would like to emphasize that this particular structural domain exhibits a lateral size well above 50 nm, thereby allowing for the clear identification even of large magnetic unit cells. Moreover the surface exhibits very few defects.\n",
      "\n",
      "To exclude a behaviour similar to what we have observed on oxidized Fe on Pt(001) we performed a detailed analysis of the observed contrasts. For example, line profiles measured along the chains marked with black arrows in Fig. 12(a) are presented in the upper part of Fig. 12(c). With a corrugation between 8 . 9 and 9 . 3 pm and an atomic spacing of (281 ± 19) pm the data are in good agreement with a (3 × 1) structure of MnO 2 chains.\n",
      "\n",
      "Such a sample surface was investigated by means of\n",
      "\n",
      "FIG. 11. (a) Overview scan of Pt(001) after Mn deposition and subsequent annealing in oxygen. In addition to reconstructed clean Pt(001) one also observes disordered (nr) and (3 × 1)-reconstructed surface regions. (b) Higher resolution scan at the position of the stripes. (c) Line profile measured along the blue line in (b) confirming the 3 a Pt periodicity. Scan parameters: U = 1V, I = 300pA, non-magnetic W tip.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "SP-STM using out-of-plane sensitive Cr-coated W tips. The resulting SP-STM image presented in Fig. 12(b) exhibits striking similarities if compared with the results obtained on MnO 2 chains on Ir(001) [cf. Fig. 6(b)]. To analyze the spin-polarized measurement five line profiles which have been taken in between the colored arrows are plotted in the lower part of Fig. 12(c). Comparison with the spin-averaged data (black) reveals that-identical to our results obtained on MnO 2 /Ir(001)-a doubling of the period along the chains can be recognized. This is a clear sign for an AFM intra-chain coupling of the Mn atoms. In further analogy to the spin spiral system of MnO 2 on Ir(001) the magnetic corrugation measured on the respective chains for MnO 2 /Pt(001) is not constant but modulates periodically. For example, the line profiles clearly show that the green chain exhibits the lowest, almost vanishing corrugation. The other four chains constitute\n",
      "\n",
      "FIG. 12. (a) Atomic resolution scan of MnO 2 chains on Pt(001) performed with a W tip. (b) Spin-polarized measurement taken with a Cr-coated W tip. A periodic modulation of the chain-specific corrugation can clearly be seen. (c) Line profiles measured along five adjacent chains marked by correspondingly colored arrows as measured with a non-magnetic (top) and a magnetic tip (bottom). Whereas a doubling of the period characteristic for an intra-chain AFM coupling is observed on most of the chains, a pronounced modulation of the contrast is also visible across the chains. Scan parameters: (a) U = -5 mV, I = 20nA; (c) U = 1V, I = 300pA.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "two groups which can be distinguished on the basis of the phase of their magnetic contrast. Whenever the two upper colored line profiles (orange, light blue) exhibit a maximum, the lower line profiles (red, dark blue) show a minimum. Since all line profiles possess a periodicity 2 a Pt due to their AFM order along the chains, the phase difference shifts the magnetic contrast by a Pt .\n",
      "\n",
      "This behaviour can be explained by the model illus-\n",
      "\n",
      "FIG. 13. (a) Schematic representation of a (15 × 2) magnetic unit cell. A cycloidal spin spiral with out-of-plane rotating spins. The model in (b) is based on the magnetic corrugation in Fig. 12(c). From this a tilt Θ = ( -15 ± 5) ◦ of the tip polarization with respect to the light blue chain with highest corrugation is seen.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "trated in Fig. 13(a). The coupling along the chains is assumed to be collinear AFM within each individual MnO 2 chain. We initially assume a periodic modulation by a commensurate (15 × 2) magnetic unit cell where the spin quantization axis of adjacent stripes rotates by 72 ◦ . Due to the dependence of the spin-polarized current on the angle between the tip magnetization and the spin quantization axis of the Mn atoms described by Eq.1 this leads to a characteristic behavior of the magnetic corrugation. It is highest for the TMO chains which exhibit the highest projection onto the tip magnetization vector symbolized by a black arrow in Fig. 13(b), i.e., the light and dark blue chains. Their opposite perpendicular orientations will result in a π phase shift. A lower magnetic corrugation can be expected for the chains symbolized by orange and red arrows. The corrugation vanishes if the spin quantization axis of a chain is perpendicular to the tip polarization (green arrow). As a consequence of this rotating spin structure two chains will exhibit a parallel (light blue, orange) and two an antiparallel (blue, red) alignment with respect to the tip, thereby explaining the phase shift of a Pt observed in Fig. 12(b). On the basis of this model we are able to determine the angle between the magnetization direction of the tip and the light blue MnO 2 chain to Θ = ( -15 ± 5) ◦ .\n",
      "\n",
      "SP-STM data measured with an in-plane sensitive Fecoated W tip across a structural domain boundary indicate that the spin spiral possesses in-plane and outof-plane contributions to the magnetization. 30 A similar spin spiral driven by the Dzyaloshinskii-Moriya interaction has already been found by DFT calculations for MnO 2 chains on Ir(001). 14 In agreement with the socalled Moriya rules 39 it is most natural to assume a cy-\n",
      "\n",
      "FIG. 14. Large SP-STM overview scan performed with a magnetic tip on MnO 2 chains on Pt(001). The intra-chain coupling is not strictly collinear, as can be recognized by inspecting the MnO 2 chains marked by black arrows. Whereas no significant magnetic contrast is detected in the bottom part, a strong magnetic contrast with a 2 a Pt periodicity is clearly visible in the upper part of the image.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "cloidal spin spiral with a Dzyaloshinskii-Moriya vector oriented along the chain direction. While the DMI is expected to be very similar for Ir and Pt due to their very similar atomic number, the higher occupation of the Pt 5 d shell causes significant differences in the respective Fermi surfaces. Indeed, it has been found that the RKKY-mediated oscillation period for (001)-oriented magnetic superlattices is about 50% longer for Co/Pt 40 than for Fe/Ir, 41 in reasonable agreement with our observation of a periodicity which is also about 1.5 times as long.\n",
      "\n",
      "As we will show below deviations from the (15 × 2) magnetic unit cell can be detected by imaging MnO 2 chains on Pt(001) over larger surface areas. For example, Fig. 14 shows an SP-STM image with a scan area of 30 nm × 30 nm. For better visibility, the data set was filtered by using a band pass. As described above SPSTM images of the (15 × 2) MnO 2 /Pt(001) magnetic unit cell are characterized by five-fold periodicity of the structural (3 × 1) unit cell. In general, the magnetization direction-dependent variation of the total current gives rise to corrugation maxima if tip and sample are magnetized in the same direction, corrugation minima for an antiparrallel alignment, or a very low or even vanishing corrugation if the angle between the tip and sample magnetization direction is close to 90 ◦ (cf. Eq. 1). At the y -position marked by a dash-dotted white line at the bot- tom of Fig. 14 we find a sequence consisting of two MnO 2 chains which exhibit corrugation maxima (colored green, labelled 2A), then one chain with a low contrast, one MnO 2 chains with corrugation minima (B), and finally a chain with no recognizable spin contrast (marked by black arrows). Close inspection of the data set presented in Fig. 14 reveals that the contrast of the specific MnO 2 chains is not constant but slowly varies. For example, the MnO 2 chains marked by black arrows which showed a vanishingly low magnetic contrast at the bottom of Fig. 14 develop a sizable magnetic contrast towards the top part of the scan. This observation indicates that the magnetic order along the MnO 2 chains is not strictly collinear but slightly canted. Within our field of view of 30 nm along the [ ¯ 110] direction, however, we only observe about a quarter of a 2 π rotation. Therefore, we can conclude that the wavelength of the magnetic structure along the MnO 2 chains on Pt(001) must be very long, probably of the order of 100 nm.\n",
      "\n",
      "As a result of the spin rotation along the MnO 2 chains and the existence of occasional structural defects the magnetic order is not strictly periodic. In fact, comparison of the bottom part of the SP-STM image with the top part of Fig. 14 reveals that the contrast changed. Whereas the sequence of contrasts obtained within the five TMO chains of a magnetic unit cell was 2A-low-Blow in the bottom part, the dominating sequence in the upper part is A-low-2B-low. At the moment we can only speculate about the origin of this non-collinearity along the chains. Possibly, DM-type interactions triggered by the overlap of the O 2 p with Ir 5 d orbitals also influence the superexchange along the TMO chains.\n",
      "\n",
      "## IV. CONCLUSION\n",
      "\n",
      "In summary, we systematically investigated the structural and magnetic properties of transition metal oxides (TMOs) on the fcc(001) surfaces of Ir and Pt. We find that Co, Fe, and Mn form quasi one-dimensional TMO chains with a (3 × 1) unit cell. Similar (3 × 1)ordered chains were observed for Cr on Ir(001). Whereas no magnetic signal was found for Coand Cr-based chains, our SP-STM measurements confirm the theoretically predicted antiferromagnetic intra-chain coupling for FeO 2 /Ir(001) and for MnO 2 chains on both substrates. A ferromagnetic inter-chain coupling is found for FeO 2 /Ir(001). In the case of MnO 2 the SP-STM images reveal a complex helical intra-chain magnetic coupling, resulting in a (9 × 2) magnetic unit cell on Ir(001) and a (15 × 2) magnetic unit cell for Pt(001). Furthermore, large scale SP-STM images of MnO 2 chains on Pt(001) unveil that also the intra-chain magnetic coupling is not perfectly collinear but slightly canted, resulting in a spin spiral with a periodicity corresponding to several hundreds of substrate atoms. Our results highlight the relevance of spin-orbit-related effects for magnetic coupling phenomena in systems with broken inversion symmetry,\n",
      "\n",
      "such as surfaces or nanoparticles.\n",
      "\n",
      "## ACKNOWLEDGMENTS\n",
      "\n",
      "Experimental work was supported by DFG through FOR 1700 (project E6), SPP 2137 'Skyrmionics' (BO\n",
      "\n",
      "- ∗ corresponding author: bode@physik.uni-wuerzburg.de\n",
      "- 1 C. A. F. Vaz, J. A. C. Bland, and G. Lauhoff, 'Magnetism in ultrathin film structures,' Reports on Progress in Physics 71 , 056501 (2008).\n",
      "- 2 H.-B. Braun, 'Topological effects in nanomagnetism: from superparamagnetism to chiral quantum solitons,' Advances in Physics 61 , 1 (2012).\n",
      "- 48 M. Bode, 'Spin-polarized scanning tunnelling microscopy,' Reports on Progress in Physics 66 , 523 (2003).\n",
      "- 4 A. Kubetzka, P. Ferriani, M. Bode, S. Heinze, G. Bihlmayer, K. von Bergmann, O. Pietzsch, S. Bl¨ ugel, and R. Wiesendanger, 'Revealing antiferromagnetic order of the Fe monolayer on W(001): Spin-polarized scanning tunneling microscopy and first-principles calculations,' Phys. Rev. Lett. 94 , 087204 (2005).\n",
      "- 5 M. Bode, E. Y. Vedmedenko, K. von Bergmann, A. Kubetzka, P. Ferriani, S. Heinze, and R. Wiesendanger, 'Atomic spin structure of antiferromagnetic domain walls,' Nature Materials 5 , 477 (2006).\n",
      "- 6 C. L. Gao, W. Wulfhekel, and J. Kirschner, 'Revealing the 120 ◦ antiferromagnetic N´ eel structure in real space: One monolayer Mn on Ag(111),' Phys. Rev. Lett. 101 , 267205 (2008).\n",
      "- 7 M. Bode, S. Heinze, A. Kubetzka, O. Pietzsch, M. Hennefarth, M. Getzlaff, R. Wiesendanger, X. Nie, G. Bihlmayer, and S. Bl¨ ugel, 'Structural, electronic, and magnetic properties of a Mn monolayer on W(110),' Phys. Rev. B 66 , 014425 (2002).\n",
      "- 8 S. Heinze, M. Bode, A. Kubetzka, O. Pietzsch, X. Nie, S. Bl¨ ugel, and R. Wiesendanger, 'Real-space imaging of two-dimensional antiferromagnetism on the atomic scale,' Science 288 , 1805 (2000).\n",
      "- 9 M. Bode, M. Heide, K. von Bergmann, P. Ferriani, S. Heinze, G. Bihlmayer, A. Kubetzka, O. Pietzsch, S. Bl¨ ugel, and R. Wiesendanger, 'Chiral magnetic order at surfaces driven by inversion asymmetry,' Nature 447 , 190 (2007).\n",
      "- 10 P. Ferstl, L. Hammer, C. Sobel, M. Gubo, K. Heinz, M. A. Schneider, F. Mittendorfer, and J. Redinger, 'Selforganized growth, structure, and magnetism of monatomic transition-metal oxide chains,' Phys. Rev. Lett. 117 , 046101 (2016).\n",
      "- 11 P. Ferstl, F. Mittendorfer, J. Redinger, M. A. Schneider, and L. Hammer, 'Monatomic Co, CoO 2 , and CoO 3 nanowires on Ir(100) and Pt(100) surfaces: Formation, structure, and energetics,' Phys. Rev. B 96 , 085407 (2017).\n",
      "- 12 A. Kr¨ onlein, M. Schmitt, M. Hoffmann, J. Kemmer, N. Seubert, M. Vogt, J. K¨ uspert, M. B¨ ohme, B. Alonazi, J. K¨ ugel, H. A. Albrithen, M. Bode, G. Bihlmayer, and S. Bl¨ ugel, 'Magnetic ground state stabilized by three-site interactions: Fe / Rh(111),' Phys. Rev. Lett. 120 , 207202 (2018).\n",
      "\n",
      "1468/26-1) and by the Dresden-W¨ urzburg Center for Topological Quantum Matter Research (ct.qmat).. The authors would like to thank Paolo Moras (Trieste, Italy) for bringing one-dimensional TMOs to our attention and Jens K¨ ugel (W¨ urzburg, Germany) for critically reading the manuscript.\n",
      "\n",
      "- 13 N. Romming, H. Pralow, A. Kubetzka, M. Hoffmann, S. von Malottki, S. Meyer, B. Dup´ e, R. Wiesendanger, K. von Bergmann, and S. Heinze, 'Competition of Dzyaloshinskii-Moriya and higher-order exchange interactions in Rh/Fe atomic bilayers on Ir(111),' Phys. Rev. Lett. 120 , 207201 (2018).\n",
      "- 14 M. Schmitt, P. Moras, G. Bihlmayer, R. Cotsakis, M. Vogt, J. Kemmer, A. Belabbes, P. M. Sheverdyaeva, A. K. Kundu, C. Carbone, S. Bl¨ ugel, and M. Bode, 'Indirect chiral magnetic exchange through Dzyaloshinskii-Moriyaenhanced RKKY interactions in manganese oxide chains on Ir(100),' Nature Communications 10 , 2610 (2019).\n",
      "- 15 A. A. Khajetoorians, M. Steinbrecher, M. Ternes, M. Bouhassoune, M. dos Santos Dias, S. Lounis, J. Wiebe, and R. Wiesendanger, 'Tailoring the chiral magnetic interaction between two individual atoms,' Nature Communications 7 , 10620 (2016).\n",
      "- 16 J. Hermenau, J. Iba˜ nez-Azpiroz, C. H¨ ubner, A. Sonntag, B. Baxevanis, K. T. Ton, M. Steinbrecher, A. A. Khajetoorians, M. dos Santos Dias, S. Bl¨ ugel, R. Wiesendanger, S. Lounis, and J. Wiebe, 'A gateway towards non-collinear spin processing using three-atom magnets with strong substrate coupling,' Nature Communications 8 , 642 (2017).\n",
      "- 17 J. T. Grant, 'A LEED study of the Ir(100) surface,' Surface Science 18 , 228 (1969).\n",
      "- 18 T. N. Rhodin and G. Brod´ en, 'Preparation and chemisorptive properties of the clean normal and reconstructed surfaces of Ir(100): role of multiplets,' Surface Science 60 , 466 (1976).\n",
      "- 19 A. Schmidt, W. Meier, L. Hammer, and K. Heinz, 'Deepgoing reconstruction of Ir(100)-5 × 1,' Journal of Physics: Condensed Matter 14 , 12353 (2002).\n",
      "- 20 R. Hammer, K. Meinel, O. Krahn, and W. Widdra, 'Surface reconstruction of Pt(001) quantitatively revisited,' Phys. Rev. B 94 , 195406 (2016).\n",
      "- 21 A. E. Morgan and G. A. Somorjai, 'Low energy electron diffraction studies of gas adsorption on the platinum (100) single crystal surface,' Surface Science 12 , 405 (1968).\n",
      "- 22 O. Pietzsch, A. Kubetzka, M. Bode, and R. Wiesendanger, 'Real-space observation of dipolar antiferromagnetism in magnetic nanowires by spin-polarized scanning tunneling spectroscopy,' Phys. Rev. Lett. 84 , 5212 (2000).\n",
      "- 23 M. Bode, O. Pietzsch, A. Kubetzka, S. Heinze, and R. Wiesendanger, 'Experimental evidence for intra-atomic noncollinear magnetism at thin film probe tips,' Phys. Rev. Lett. 86 , 2142-2145 (2001).\n",
      "- 24 S. Meckler, N. Mikuszeit, A. Preßler, E. Y. Vedmedenko, O. Pietzsch, and R. Wiesendanger, 'Real-space observation of a right-rotating inhomogeneous cycloidal spin spiral by spin-polarized scanning tunneling microscopy in a triple axes vector magnet,' Phys. Rev. Lett. 103 , 157201 (2009).\n",
      "\n",
      "- 25 M. Bode, M. Getzlaff, and R. Wiesendanger, 'Spinpolarized vacuum tunneling into the exchange-split surface state of Gd(0001),' Phys. Rev. Lett. 81 , 4256-4259 (1998).\n",
      "- 26 L. Berbil-Bautista, S. Krause, M. Bode, and R. Wiesendanger, 'Spin-polarized scanning tunneling microscopy and spectroscopy of ferromagnetic Dy(0001)/W(110) films,' Phys. Rev. B 76 , 064411 (2007).\n",
      "- 27 R. Ravli´ c, M. Bode, A. Kubetzka, and R. Wiesendanger, 'Correlation of dislocation and domain structure of Cr(001) investigated by spin-polarized scanning tunneling microscopy,' Phys. Rev. B 67 , 174411 (2003).\n",
      "- 28 Ph. Kurz, G. Bihlmayer, and S. Bl¨ ugel, 'Magnetism and electronic structure of hcp Gd and the Gd(0001) surface,' Journal of Physics: Condensed Matter 14 , 63536371 (2002).\n",
      "- 29 J. W. Arblaster, 'Crystallographic properties of iridium,' Platinum Metals Review 54 , 93-102 (2010).\n",
      "- 30 See supplemental material [url] for detailed information regarding the preparation of clean, oxygen-reconstructed, and TMO-covered fcc(001) surfaces, the characterization of magnetic probe tips, and SP-STM investigations of TMOs not discussed in the main text, including Refs. [4250].\n",
      "- 31 P. W. Anderson, 'Antiferromagnetism. theory of superexchange interaction,' Phys. Rev. 79 , 350-356 (1950).\n",
      "- 32 A. Fert and Peter M. Levy, 'Role of anisotropic exchange interactions in determining the properties of spin-glasses,' Phys. Rev. Lett. 44 , 1538-1541 (1980).\n",
      "- 33 B. Dup´ e, J.E. Bickel, Y. Mokrousov, F. Otte, K. von Bergmann, A. Kubetzka, S. Heinze, and R. Wiesendanger, 'Giant magnetization canting due to symmetry breaking in zigzag Co chains on Ir(001),' New Journal of Physics 17 , 023014 (2015).\n",
      "- 34 J. Bouaziz, M. dos Santos Dias, A. Ziane, M. Benakki, S. Bl¨ ugel, and S. Lounis, 'Chiral magnetism of magnetic adatoms generated by Rashba electrons,' New Journal of Physics 19 , 023010 (2017).\n",
      "- 35 S. V. Grigoriev, Yu. O. Chetverikov, D. Lott, and A. Schreyer, 'Field induced chirality in the helix structure of Dy / Y multilayer films and experimental evidence for Dzyaloshinskii-Moriya interaction on the interfaces,' Phys. Rev. Lett. 100 , 197203 (2008).\n",
      "- 36 R. Hammer, K. Meinel, O. Krahn, and W. Widdra, 'Surface reconstruction of pt(001) quantitatively revisited,' Phys. Rev. B 94 , 195406 (2016).\n",
      "- 37 A. van Houselt, T. Gnielka, J. M. J. Aan de Brugh, N. Oncel, D. Kockmann, R. Heid, K.-P. Bohnen, B. Poelsema, and H. J. W. Zandvliet, 'Peierls instability in Pt chains on Ge(001),' Surface Science 602 , 1731 (2008).\n",
      "- 38 Gerhard Gilarowski, Javier M´ endez, and Horst Niehus, 'Initial growth of cu on ir(100)-(5 × 1),' Surface Science 448 , 290 (2000).\n",
      "- 39 T. Moriya, 'Anisotropic superexchange interaction and weak ferromagnetism,' Phys. Rev. 120 , 91-98 (1960).\n",
      "- 40 J. Moritz, F. Garcia, J. C. Toussaint, B. Dieny, and J. P. Nozi` eres, 'Orange peel coupling in multilayers with perpendicular magnetic anisotropy: Application to (co/pt)based exchange-biased spin-valves,' Europhys. Lett. 65 , 123-129 (2004).\n",
      "- 41 D. Stoeffler, 'Ab initio study of the fe intra- and inter-layer magnetic order in fe/ir(001) superlattices,' The European Physical Journal B - Condensed Matter and Complex Systems 37 , 311-320 (2004).\n",
      "- 42 K. Johnson, Q. Ge, S. Titmuss, and D. A. King, 'Unusual bridged site for adsorbed oxygen adatoms: Theory and experiment for Ir { 100 } -(1 × 2)-O,' The Journal of Chemical Physics 112 , 10460-10466 (2000).\n",
      "- 43 P. Ferstl, T. Schmitt, M. A. Schneider, L. Hammer, A. Michl, and S. M¨ uller, 'Structure and ordering of oxygen on unreconstructed Ir(100),' Phys. Rev. B 93 , 235406 (2016).\n",
      "- 44 H. J. Elmers, J. Hauschild, and U. Gradmann, 'Onset of perpendicular magnetization in nanostripe arrays of Fe on stepped W(110) surfaces,' Phys. Rev. B 59 , 3688-3695 (1999).\n",
      "- 45 M. Bode, S. Heinze, A. Kubetzka, O. Pietzsch, X. Nie, G. Bihlmayer, S. Bl¨ ugel, and R. Wiesendanger, 'Magnetization-direction-dependent local electronic structure probed by scanning tunneling spectroscopy,' Phys. Rev. Lett. 89 , 237205 (2002).\n",
      "- 46 O. Pietzsch, A. Kubetzka, M. Bode, and R. Wiesendanger, 'Observation of magnetic hysteresis at the nanometer scale by spin-polarized scanning tunneling spectroscopy,' Science 292 , 2053-2056 (2001).\n",
      "- 47 A. Kubetzka, O. Pietzsch, M. Bode, and R. Wiesendanger, 'Spin-polarized scanning tunneling microscopy study of 360 ◦ walls in an external magnetic field,' Phys. Rev. B 67 , 020401 (2003).\n",
      "- 48 M. Bode, A. Kubetzka, S. Heinze, O. Pietzsch, R. Wiesendanger, M. Heide, X. Nie, G. Bihlmayer, and S. Bl¨ ugel, 'Spin-orbit induced local band structure variations revealed by scanning tunnelling spectroscopy,' Journal of Physics: Condensed Matter 15 , S679-S692 (2003).\n",
      "- 49 E. Y. Vedmedenko, A. Kubetzka, K. von Bergmann, O. Pietzsch, M. Bode, J. Kirschner, H. P. Oepen, and R. Wiesendanger, 'Domain wall orientation in magnetic nanowires,' Phys. Rev. Lett. 92 , 077207 (2004).\n",
      "- 50 H. J. Elmers, J. Hauschild, H. Fritzsche, G. Liu, U. Gradmann, and U. K¨ ohler, 'Magnetic frustration in ultrathin Fe films,' Phys. Rev. Lett. 75 , 2031 (1995).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 08:07:15,488 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:15,489 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:15,491 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:15,491 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:15,568 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:15,569 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:15,591 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:15,592 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 08:07:15,806 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 08:07:15,807 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:07:16,481 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:07:16,790 - INFO - Processing document 1705.04510v1.pdf\n",
      "2025-12-15 08:07:58,427 - INFO - Finished converting document 1705.04510v1.pdf in 43.22 sec.\n",
      "2025-12-15 08:07:58,591 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 08:07:58,599 - INFO - Going to convert document batch...\n",
      "2025-12-15 08:07:58,600 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 08:07:58,601 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 08:07:58,601 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 08:07:58,602 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:58,625 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:58,626 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:58,638 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:58,639 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Formalizing Timing Diagram Requirements in Discrete Duration Calulus\n",
      "\n",
      "Raj Mohan Matteplackel 1 , Paritosh K. Pandya 1 , and Amol Wakankar 2\n",
      "\n",
      "1\n",
      "\n",
      "Tata Institute of Fundamental Research, Mumbai 400005, India. { raj.matteplackel,pandya } @tifr.res.in 2 Bhabha Atomic Research Centre, Mumbai, India.\n",
      "\n",
      "amolk@barc.gov.in\n",
      "\n",
      "Abstract. Several temporal logics have been proposed to formalise timing diagram requirements over hardware and embedded controllers. These include LTL [CF05], discrete time MTL [AH93] and the recent industry standard PSL [EF16]. However, succintness and visual structure of a timing diagram are not adequately captured by their formulae [CF05]. Interval temporal logic QDDC is a highly succint and visual notation for specifying patterns of behaviours [Pan00].\n",
      "\n",
      "In this paper, we propose a practically useful notation called SeCeNL which enhances negation free fragment of QDDC with features of nominals and limited liveness . We show that timing diagrams can be naturally (compositionally) and succintly formalized in SeCeNL as compared with PSL-Sugar and MTL. We give a linear time translation from timing diagrams to SeCeNL. As our second main result, we propose a linear time translation of SeCeNL into QDDC. This allows QDDC tools such as DCVALID [Pan00,Pan01] and DCSynth to be used for checking consistency of timing diagram requirements as well as for automatic synthesis of property monitors and controllers. We give examples of a minepump controller and a bus arbiter to illustrate our tools. Giving a theoretical analysis, we show that for the proposed SeCeNL, the satisfiability and model checking have elementary complexity as compared to the nonelementary complexity for the full logic QDDC.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "A timing diagram is a collection of binary signals and a set of timing constraints on them. It is a widely used visual formalism in the realm of digital hardware design, communication protocol specification and embedded controller specification. The advantages of timing diagrams in hardware design are twofold, one, since designers can visualize waveforms of signals they are easy to comprehend and two, they are very convenient for specifying ordering and timing constraints between events (see figures Fig. 1 and Fig. 2 below).\n",
      "\n",
      "There have been numerous attempts at formalizing timing diagram constraints in the framework of temporal logics such as the timing diagram logic [Fis99], with LTL formulas [CF05], and as synchronous regular timing diagrams\n",
      "\n",
      "[AEKN00]. Moreover, there are industry standard property specification languages such as PSL/Sugar and OVA for associating temporal assertions to hardware designs [EF16]. The main motivation for these attempts was to exploit automatic verification techniques that these formalisms support for validation and automatic circuit synthesis. However, commenting on their success, Fisler et. al. state that the less than satisfactory adoption of formal methods in timing diagram domain can be partly attributed to the gulf that exists between graphical timing diagrams and textual temporal logic - expressing various timing dependencies that can exist among signals that can be illustrated so naturally in timing diagrams is rather tedious in temporal logics [CF05]. As a result, hardware designers use timing diagrams informally without any well defined semantics which make them unamenable to automatic design verification techniques.\n",
      "\n",
      "In this paper, we take a fresh look at formalizing timing diagram requirements with emphasis on the following three features of the formalism that we propose here.\n",
      "\n",
      "Firstly, we propose the use of an interval temporal logic QDDC to specify patterns of behaviours. QDDC is a highly succinct and visual notation for specifying regular patterns of behaviours [Pan00,Pan01,KP05]. We identify a quantifier and negation-free subset SeCe of QDDC which is sufficient for formalizing timing diagram patterns. It includes generalized regular expression like syntax with counting constructs. Constraints imposed by timing diagrams are straightforwardly and compactly stated in this logic. For example, the timing diagram in Fig. 1 stating that P transits from 0 to 1 somewhere in interval u to u +3 cycles is captured by the SeCe formula [ ¬ P]^&lt;u&gt;^(slen=3 ∧ [ ¬ P]^[[P]])^[[P]] . The main advantage of SeCe is that it has elementary satisfiability as compared to the non-elementary satisfiability of general QDDC .\n",
      "\n",
      "Fig. 1. Timing diagram with a marked position u and a timing constraint.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Secondly, it is very typical for timing diagrams to have partial ordering and synchronization constraints between distinct events. Emphasizing this aspect, formalisms such as two dimensional regular expressions [Fis07] have been proposed for timing diagrams. We find that synchronization in timing diagram may even extend across different patterns of limited liveness properties. In order to handle such synchronization, we extend our logic SeCe with nominals from hybrid temporal logics [FdRS03]. Nominals are temporal variables which 'freeze' the positions of occurrences of events. They naturally allow synchronization across formulae.\n",
      "\n",
      "Thirdly, we enhance the timing diagram specifications (as well as logic SeCe) with limited liveness operators . While timing diagrams visually specify patterns of occurrence of signals, they do not make precise the modalities of occurrences of such patterns. We explicitly introduce modalities such as a ) initially, a specified pattern must occur, or that b ) every occurrence of pattern1 is necessarily and immediately followed by an occurrence of pattern2 , or that c ) occurrence of a specified pattern is forbidden anywhere within a behaviour. In this, we are inspired by Allen's Interval Algebra relations [All83] as well as the LSC operators of Harel for message sequence charts [DH01]. We confine ourselves to limited liveness properties where good things are achieved within specified bounds. For example, in specifying a modulo 6 counter, we can say that the counter will stabilize before completion of first 15 cycles. Astute readers will notice that, technically, our limited liveness operators only give rise to 'safety' properties (in the sense of Alpern and Schneider [AS87]). However, from a designer's perspective they do achieve the practical goal of forcing good things to happen.\n",
      "\n",
      "Putting all these together, we define a logic SeCeNL which includes negationfree QDDC together with limited liveness operators as well as nominals. The formal syntax and semantics of SeCeNL formulas is given in § 2.3. We claim that SeCeNL provides a natural and convenient formalism for encoding timing diagram requirements. Substantiating this, we formulate a translation of timing diagrams into SeCeNL formulae in § 3 . The translation is succinct, in fact, linear time computable in the size of the timing diagram. (A textual syntax is used for timing diagrams. The textual syntax of timing diagrams used is inspired by the tool WaveDrom [CP16], which is also used for graphical rendering of our timing diagram specifications.) Moreover, the translation is compositional, i.e. it translates each element of the timing diagram as one small formula and overall specification is just the conjunction of such constraints. Hence, the translation preserves the structure of the diagram.\n",
      "\n",
      "With several examples of timing diagrams, we compare its SeCeNL formula with the formula in logics such as PSL-Sugar and MTL. Logic PSL-Sugar is amongst the most expressive notations for requirements. Logic PSL-Sugar is syntactically a superset of MTL and LTL. It extends LTL with SERE (regular expressions with intersection) which are similar to our SeCe. In spite of this, we a show natural examples where SeCeNL formula is at least one exponent more succinct as compared to PSL-Sugar.\n",
      "\n",
      "As the second main contribution of this paper, we consider formal verification and controller synthesis from SeCeNL specifications. In § 3.1, we formulate a reduction from a SeCeNL formula to an equivalent QDDC formula. This allows QDDC tools to be used for SeCeNL. It may be noted that, though expressively no more powerful than QDDC, logic SeCeNL considerably more efficient for satisfiability and model checking. We show that these problems have elementary complexity as compared with full QDDC which exhibits non-elementary complexity. Also, the presence of limited liveness and nominals makes it more convenient as compared to QDDC for practical use.\n",
      "\n",
      "By implementing the above reductions, we have constructed a Python based translator which converts a requirement consisting of a boolean combination of timing diagram specifications (augmented with limited liveness) and SeCeNL formulae into an equivalent QDDC formula. We can analyze the resulting formula using the QDDC tools DCVALID [Pan00,Pan01] as well as DCSynthG for model checking and controller synthesis, respectively (see Fig. 11 for the tool chain). We illustrate the use of our tools by the case studies of a synchronous bus arbiter and a minepump controller in § 4. Readers may note that we specify rather rich quantitative requirements not commonly considered, and our tools are able to automatically synthesize monitors and controllers for such specifications.\n",
      "\n",
      "## 2 Logic QDDC\n",
      "\n",
      "Let Σ be a finite non empty set of propositional variables. A word σ over Σ is a finite sequence of the form P 0 · · · P n where P i ⊆ Σ for each i ∈ { 0 , . . . , n } . Let len ( σ ) = n +1, dom ( σ ) = { 0 , . . . , n } and ∀ i ∈ dom ( σ ) : σ ( i ) = P i .\n",
      "\n",
      "The syntax of a propositional formula over Σ is given by:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "and operators such as ⇒ and ⇔ are defined as usual. Let Ω Σ be the set of all propositional formulas over Σ .\n",
      "\n",
      "Let σ = P 0 · · · P n be a word and ϕ ∈ Ω Σ . Then, for an i ∈ dom ( σ ) the satisfaction relation σ, i | = ϕ is defined inductively as expected: σ, i | = 1 ; σ, i | = p iff p ∈ σ ( i ); σ, i | = ¬ p iff σ, i glyph[negationslash]| = p , and the satisfaction relation for the rest of the boolean combinations defined in a natural way.\n",
      "\n",
      "The syntax of a QDDC formula over Σ is given by:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where ϕ ∈ Ω Σ , p ∈ Σ , c ∈ N and glyph[triangleright] glyph[triangleleft] ∈ { &lt;, ≤ , = , ≥ , &gt; } .\n",
      "\n",
      "An interval over a word σ is of the form [ b, e ] where b, e ∈ dom ( σ ) and b ≤ e . An interval [ b 1 , e 1 ] is a sub interval of [ b, e ] if b ≤ b 1 and e 1 ≤ e . Let Intv ( σ ) be the set of all intervals over σ .\n",
      "\n",
      "Let σ be a word over Σ and let [ b, e ] ∈ Intv ( σ ) be an interval. Then the satisfaction relation of a QDDC formula D over Σ , written σ, [ b, e ] | = D , is defined inductively as follows:\n",
      "\n",
      "```\n",
      "σ, [ b, e ] | = 〈 ϕ 〉 iff σ, b | = ϕ, σ, [ b, e ] | = [ ϕ ] iff ∀ b ≤ i < e : σ, i | = ϕ, σ, [ b, e ] | = [[ ϕ ]] iff ∀ b ≤ i ≤ e : σ, i | = ϕ, σ, [ b, e ] | = {{ ϕ }} iff e = b +1 and σ, b | = ϕ, σ, [ b, e ] | = ¬ D iff σ, [ b, e ] glyph[negationslash]| = D, σ, [ b, e ] | = D 1 ∨ D 2 iff σ, [ b, e ] | = D 1 or σ, [ b, e ] | = D 2 , σ, [ b, e ] | = D 1 ∧ D 2 iff σ, [ b, e ] | = D 1 and σ, [ b, e ] | = D 2 , σ, [ b, e ] | = D 1 ^ D 2 iff ∃ b ≤ i ≤ e : σ, [ b, i ] | = D 1 and σ, [ i, e ] | = D 2 .\n",
      "```\n",
      "\n",
      "glyph[negationslash]\n",
      "\n",
      "We call word σ ′ a p -variant, p ∈ Σ , of a word σ if ∀ i ∈ dom ( σ ) , ∀ q = p : σ ′ ( i )( q ) = σ ( i )( q ). Then σ, [ b, e ] | = ∃ p. D ⇔ σ ′ , [ b, e ] | = D for some p -variant σ ′ of σ and, σ, [ b, e ] | = ∀ p. D ⇔ σ, [ b, e ] glyph[negationslash]| = ∃ p. ¬ D . We define σ | = D iff σ, [0 , len ( σ )] | = D .\n",
      "\n",
      "Example 1. Let Σ = { p, q } and let σ = P 0 · · · P 7 be such that ∀ 0 ≤ i &lt; 7 : P i = { p } and P 7 = { q } . Then σ, [0 , 7] | = [ p ] but not σ, [0 , 7] | = [[ p ]] as p glyph[negationslash]∈ P 7 .\n",
      "\n",
      "Example 2. Let Σ = { p, q, r } and let σ = P 0 · · · P 10 be such that ∀ 0 ≤ i &lt; 4 : P i = { p } , ∀ 4 ≤ i &lt; 8 : P i = { p, q, r } and ∀ 8 ≤ i ≤ 10 : P i = { q, r } . Then\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "because for i ∈ { 8 , 9 , 10 } the condition ∃ 0 ≤ i ≤ 10 : σ, [0 , i ] | = [ p ] and σ, [ i, 10] | = [[ ¬ p ∧ r ]] is met. But σ, [0 , 7] glyph[negationslash]| = [ p ] ^ [[ ¬ p ∧ r ]] as ¬∃ 0 ≤ i ≤ 7 : σ, [0 , i ] | = [ p ] and σ, [ i, 7] | = [[ ¬ p ∧ r ]].\n",
      "\n",
      "Entities slen , scount , and sdur are called terms in QDDC. The term slen gives the length of the interval in which it is measured, scount ϕ where ϕ ∈ Ω Σ , counts the number of positions including the last point in the interval under consideration where ϕ holds, and sdur ϕ gives the number of positions excluding the last point in the interval where ϕ holds. Formally, for ϕ ∈ Ω Σ { }\n",
      "\n",
      "we have slen\n",
      "\n",
      "(\n",
      "\n",
      "σ,\n",
      "\n",
      "[\n",
      "\n",
      "b, e\n",
      "\n",
      "]) =\n",
      "\n",
      "e\n",
      "\n",
      "-\n",
      "\n",
      "b\n",
      "\n",
      ",\n",
      "\n",
      "scount\n",
      "\n",
      "(\n",
      "\n",
      "σ, ϕ,\n",
      "\n",
      "[\n",
      "\n",
      "b, e\n",
      "\n",
      "]) =\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "e\n",
      "\n",
      "b\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      ",\n",
      "\n",
      ",\n",
      "\n",
      "if\n",
      "\n",
      "σ, i\n",
      "\n",
      "|\n",
      "\n",
      "=\n",
      "\n",
      "ϕ, otherwise.\n",
      "\n",
      "and\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "lowing derived constructs: σ, [ b, e ] | = pt iff b = e ; σ, [ b, e ] | = ext iff b &lt; e ; σ, [ b, e ] | = ♦ D iff true ^ D ^ true and σ, [ b, e ] | = glyph[square] D iff σ, [ b, e ] glyph[negationslash]| = ♦ ¬ D .\n",
      "\n",
      "A formula automaton for a QDDC formula D is a deterministic finite state automaton which accepts precisely language L = { σ | σ | = D } .\n",
      "\n",
      "Theorem 1. [Pan01] For every QDDC formula D over Σ we can construct a DFA A ( D ) for D such L ( D ) = L ( A ( D )) . The size of A ( D ) is non elementary in the size of D in the worst case.\n",
      "\n",
      "## 2.1 Chop expressions: Ce and SeCe\n",
      "\n",
      "Definition 1. The logic Semi extended Chop expressions (SeCe) is a syntactic subset of QDDC in which the operators ∃ p. D , ∀ p. D and negation are not allowed. The logic Chop expressions (Ce) is a sublogic of SeCe in which conjuction is not allowed.\n",
      "\n",
      "Lemma 1. For any chop expression D of size n we can effectively construct a language equivalent DFA A of size Ω (2 2 n ) .\n",
      "\n",
      "Proof. We observe that for any chop expression D we can construct a language equivalent NFA which is at most exponential in size of D including the constants appearing in it (for a detailed proof see [BP12] wherein a similar result has been proved). But this implies there exists a DFA of size 2 2 n which accepts exactly the set of words σ such that σ | = D .\n",
      "\n",
      "∑\n",
      "\n",
      "Corollary 1. For any SeCe D of size n we can effectively construct a language equivalent DFA A of size Ω (2 2 2 n ) .\n",
      "\n",
      "Proof. Proof follows from the definition of SeCe, lemma 1 and from the fact that the size of the product of DFA 's can be atmost exponential in the size of individual DFA 's.\n",
      "\n",
      "## 2.2 DCVALID and DCSynthG\n",
      "\n",
      "The reduction from a QDDC formula to its formula automaton has been implemented into the tool DCVALID [Pan00,Pan01]. The formula automaton it generates is total, deterministic and minimal automaton for the formula. DCVALID can also translate the formula automaton into Lustre/SCADE, Esterel, SMV and Verilog observer module . By connecting this observer module to run synchronously with a system we can reduce model checking of QDDC property to reachability checking in observer augmented system. See [Pan00,Pan01] for details. A further use of formula automata can be seen in the tool called DCSynthG which synthesizes synchronous dataflow controller in SCADE/NuSMV/Verilog from QDDC specification.\n",
      "\n",
      "## 2.3 Logic SeCeNL: Syntax and Semantics\n",
      "\n",
      "We can now introduce our logic SeCeNL which builds upon SeCe by augmenting it with nominals and limited liveness operators .\n",
      "\n",
      "Syntax : The syntax of SeCeNL atomic formula is as follows. Let D , D 1 , D 2 and D 3 range over SeCe formulae and let Θ , Θ 1 , Θ 2 and Θ 3 range over subset of propositional variables occurring in SeCe formula. The notation D : Θ , called a nominated formula , denotes that Θ is the set of variables used as nominals in the formula D .\n",
      "\n",
      "```\n",
      "init ( D 1 : Θ 1 / D 2 : Θ 2 ) | anti ( D : Θ ) | pref ( D : Θ ) | implies ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 ) | follows ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 /D 3 : Θ 3 ) | triggers ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 /D 3 : Θ 3 )\n",
      "```\n",
      "\n",
      "An SeCeNL formula is a boolean combination of atomic SeCeNL formulae of the form above. As a convention, D : {} is abbreviated as D when the set of nominals Θ is empty.\n",
      "\n",
      "Limited Liveness Operators : Given an word σ and a position i ∈ dom ( σ ), we state that σ, i | = D iff σ [0 : i ] | = D . Thus, the interpretation is that the past of the position i in execution satisfies D .\n",
      "\n",
      "For a SeCe formula D we let Ξ ( D ) = D ∧ ¬ ( D ^ ext ), which says that if σ, [ b, e ] | = Ξ ( D ) then σ, [ b, e ] | = D and there exists no proper prefix interval [ b, e 1 ], (i. e. [ b, e 1 ] ∈ Intv ( σ ) and b ≤ e 1 &lt; e ) such that σ, [ b, e 1 ] | = D . We say σ ′ ≤ prefix σ if σ ′ is a prefix of σ , and σ ′ &lt; prefix σ if σ ′ is a proper prefix of σ .\n",
      "\n",
      "We first explain the semantics of limited liveness operators assuming that no nominals are used in the specification, i.e. Θ , Θ 1 , Θ 2 and Θ 3 are all empty. A set S ⊆ Σ ∗ is prefix closed if σ ∈ S then ∀ σ ′ : σ ′ ≤ prefix σ ⇒ σ ′ ∈ S . We observe that each atomic liveness formula denotes a prefix closed subset of (2 Σ ) + .\n",
      "\n",
      "- -L ( pref ( D ) ) = { σ | ∀ σ ′ ≤ prefix σ : σ ′ | = D } . Operator pref ( D ) denotes that D holds invariantly throughout the execution.\n",
      "- -L ( init ( D 1 /D 2 )) = { σ | ∀ j : σ, [0 , j ] | = D 2 ⇒ ∃ k ≤ j : σ, [0 , k ] | = D 1 } . Operator init ( D 1 /D 2 ) basically states that if j is the first position which satisfies D 2 in the execution then there exists an i ≤ j such that i satisfies D 1 . Thus, initially D 1 holds before D 2 unless the execution (is too short and hence) does not satisfy D 2 anywhere.\n",
      "- -L ( anti ( D )) = { σ | ∀ i, j : σ, [ i, j ] glyph[negationslash]| = D } . Operator anti ( D ) states that there is no observation sub interval of the execution which satisfies D .\n",
      "- -L ( implies ( D 1 glyph[squiggleright] D 2 )) = { σ | ∀ i, j : ( σ, [ i, j ] | = D 1 ⇒ σ, [ i, j ] | = D 2 ) } . Operator implies ( D 1 glyph[squiggleright] D 2 ) states all observation intervals which satisfy D 1 will also satisfy D 2 .\n",
      "- -\n",
      "\n",
      "L ( follows ( D 1 glyph[squiggleright] D 2 /D 3 )) = { σ | ∀ i, j : ( σ, [ i, j ] | = D 1 ⇒ ( ∀ k : σ, [ j, k ] | = Ξ ( D 3 ) ⇒∃ l ≤ k : σ, [ j, l ] | = D 2 )) } . Operator follows ( D 1 glyph[squiggleright] D 2 /D 3 ) states that if any observation interval [ i, j ] satisfies D 1 and there is a following shortest interval [ j, k ] which satisfies D 3 then there exists a prefix interval of [ j, k ] which satisfies D 2 .\n",
      "\n",
      "-\n",
      "\n",
      "L\n",
      "\n",
      "(\n",
      "\n",
      "triggers\n",
      "\n",
      "(\n",
      "\n",
      "D\n",
      "\n",
      "1\n",
      "\n",
      "glyph[squiggleright]\n",
      "\n",
      "D\n",
      "\n",
      "2\n",
      "\n",
      "/D\n",
      "\n",
      "3\n",
      "\n",
      ")) =\n",
      "\n",
      "{\n",
      "\n",
      "σ\n",
      "\n",
      "| ∀\n",
      "\n",
      "i, j\n",
      "\n",
      ": (\n",
      "\n",
      "σ,\n",
      "\n",
      "[\n",
      "\n",
      "i, j\n",
      "\n",
      "]\n",
      "\n",
      "|\n",
      "\n",
      "=\n",
      "\n",
      "D\n",
      "\n",
      "1\n",
      "\n",
      "⇒\n",
      "\n",
      "( ∀ k : σ, [ i, k ] | = Ξ ( D 3 ) ⇒∃ l ≤ k : σ, [ i, l ] | = D 2 )) } . Operator triggers ( D 1 glyph[squiggleright] D 2 /D 3 ) states that if any observation interval [ i, j ] satisfies D 1 and if [ i, k ] is the shortest interval which satisfies D 3 then D 2 holds for a prefix interval of [ i, k ].\n",
      "\n",
      "Based on this semantics, we can translate an atomic SeCeNL formula ζ without nominals into equivalent SeCe formula ℵ ( ζ ) as follows.\n",
      "\n",
      "1. ℵ ( pref ( D ) ) def ≡ ¬ (( ¬ D ) ^ true ).\n",
      "2. ℵ ( init ( D 1 /D 2 )) def ≡ pref ( Ξ ( D 2 ) ⇒ D 1 ^ true ).\n",
      "3. ℵ ( anti ( D )) def ≡ ¬ ( true ^ D ^ true ).\n",
      "4. ℵ ( implies ( D 1 glyph[squiggleright] D 2 )) def ≡ glyph[square] ( D 1 ⇒ D 2 ).\n",
      "5. ℵ ( follows ( D 1 glyph[squiggleright] D 2 /D 3 )) def ≡ glyph[square] ( ¬ ( D 1 ^ ( Ξ ( D 3 ) ∧ ¬ ( D 2 ^ true )))).\n",
      "6. ℵ ( triggers ( D 1 glyph[squiggleright] D 2 /D 3 )) def ≡ glyph[square] ( D 1 ^ true ⇒ ( Ξ ( D 3 ) ⇒ D 2 ^ true )) ∧ glyph[square] ( D 1 ⇒ pref ( Ξ ( D 3 ) ⇒ D 2 ^ true )).\n",
      "\n",
      "Lemma 2. For any ζ ∈ SeCeNL , if ζ does not use nominals then σ ∈ L ( ζ ) iff σ ∈ L ( ℵ ( ζ )) .\n",
      "\n",
      "The proof follows from examination of the semantics of ζ and the definition of ℵ ( ζ ). We omit the details.\n",
      "\n",
      "Nominals : Consider a nominated formula D : Θ where D is a SeCe formula over propositional variables Σ ∪ Θ . As we shall see later, the propositional variables in Θ are treated as 'place holders' - variables which are meant to be true exactly at one point - and we call them nominals following [FdRS03].\n",
      "\n",
      "Given an interval [ b, e ] ∈ Intv ( N ) we define a nominal valuation over [ b, e ] to be a map ν : Θ → { i | b ≤ i ≤ e } . It assigns a unique position within [ b, e ] to each nominal variable. We can then straightforwardly define σ, [ b, e ] | = ν D by constructing a word σ ν over Σ ∪ Θ such that ∀ p ∈ Σ : p ∈ σ ν ( i ) ⇔ p ∈ σ ( i ) and ∀ u ∈ Θ : u ∈ σ ν ( i ) ⇔ ν ( u ) = i . Then σ ν , [ b, e ] | = D ⇔ σ, [ b, e ] | = ν D . We state that ν 1 over Θ 1 and ν 2 over Θ 2 are consistent if ν 1 ( u ) = ν 2 ( u ) for all u ∈ Θ 1 ∩ Θ 2 . We denote this by ν 1 ‖ ν 2 .\n",
      "\n",
      "Semantics of SeCeNL : Now we consider the semantics of SeCeNL where nominals are used and shared between different parts D 1 , D 2 and D 3 of an atomic formula such as implies ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 ).\n",
      "\n",
      "Example 3 (lags). Let D 1 : { u, v } be the formula (&lt;u&gt; ^ [[P]] ∧ ((slen=n) ^ &lt;v&gt; ^ true) which holds for an interval where P is true throughout the interval and v marks the n +1 position from u denoting the start of the interval. Let D 2 : { v } be the formula true ^ &lt;v&gt; ^ [[Q]] . Then, implies ( D 1 : { u, v } glyph[squiggleright] D 2 : { v } ) states that for all observation intervals [ i, j ] and all nominal valuations ν over [ i, j ] if σ, [ i, j ] | = ν D 1 then σ, [ i, j ] | = ν D 2 . This formula is given by live timing diagram in Fig. 2 below. 3\n",
      "\n",
      "Fig. 2. Live timing diagram.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "We now give the semantics of SeCeNL.\n",
      "\n",
      "- -L ( pref ( D : Θ ) ) = { σ | ∀ σ ′ ≤ prefix σ : ∃ ν. σ ′ | = ν D } .\n",
      "- -L ( init ( D 1 : Θ 1 / D 2 : Θ 2 )) = { σ | ∀ j ∀ ν : σ, [0 , j ] | = ν D 2 ⇒ ∃ k ≤ j ∃ ν 2 : ν 1 ‖ ν 2 ∧ σ, [0 , k ] | = ν 2 D 1 } .\n",
      "- -L ( anti ( D : Θ )) = { σ | ∀ i, j ∀ ν : σ, [ i, j ] glyph[negationslash]| = ν D } .\n",
      "- -L ( implies ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 )) = { σ | ∀ i, j ∀ ν 1 : ( σ, [ i, j ] | = ν 1 D 1 ⇒ ∃ ν 2 : ν 1 ‖ ν 2 ∧ σ, [ i, j ] | = ν 2 D 2 ) } .\n",
      "\n",
      "3 Here we wish to point out that the illustration was made with the timing diagram editor WaveDrom and due to its limitation on naming nominals we were forced to rename the nominal v appearing in D 2 as a .\n",
      "\n",
      "- -L ( follows ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 /D 3 : Θ 3 )) = { σ | ∀ i, j ∀ ν 1 : ( σ, [ i, j ] | = ν 1 D 1 ⇒ ( ∀ k ∀ ν 2 ‖ ν 1 : σ, [ j, k ] | = ν 2 Ξ ( D 3 ) ⇒ ∃ l ≤ k ∃ ν 3 : ν 3 ‖ ν 1 ∧ ν 3 ‖ ν 2 ∧ σ, [ j, l ] | = ν 3 D 2 )) } .\n",
      "- ⇒\n",
      "\n",
      "```\n",
      "-L ( triggers ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 /D 3 : Θ 3 )) = { σ | ∀ i, j ∀ ν 1 : ( σ, [ i, j ] | = ν 1 D 1 ⇒ ( ∀ k ∀ ν 2 ‖ ν 1 : σ, [ i, k ] | = ν 2 Ξ ( D 3 ) ∃ l ≤ k ∃ ν 3 : ν 3 ‖ ν 1 ∧ ν 3 ‖ ν 2 ∧ σ, [ i, l ] | = D 2 )) } .\n",
      "```\n",
      "\n",
      "Based on the above semantics, we now formulate a QDDC formula equivalent to a SeCeNL formula. We define the following useful notations ∀ 1 Θ : D and ∃ 1 Θ : D as derived operators. These operators are essentially relativize quantifiers to restrict variables to singletons.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "From SeCeNL to QDDC : We now define the translation ℵ from SeCeNL to QDDC.\n",
      "\n",
      "1. ℵ ( pref ( D : Θ ) ) def ≡ ¬ ( ∃ 1 : ¬ D ^ true\n",
      "2. Θ ). 2. ℵ ( init ( D 1 : Θ 1 / D 2 : Θ 2 )) def ≡ pref ( ∀ 1 Θ 2 : ( D 2 ⇒∃ 1 Θ 1 -Θ 2 : D 1 ^ true )). 3. ℵ ( ¬∃ D : Θ ) def ≡ ¬ ( ∃ 1 Θ : true ^ D ^ true ). 4. ℵ ( implies ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 )) def ≡ glyph[square] ( ∀ 1 Θ 1 : ( D 1 ⇒∃ 1 Θ 2 -Θ 1 : D 2 )). 5. ℵ ( follows ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 /D 3 : Θ 3 )) def ≡ glyph[square] ( ∀ 1 Θ 1 : ∀ 1 Θ 3 -Θ 1 : ∃ 1 Θ 2 -( Θ 1 ∪ Θ 3 ) : ¬ ( D 1 ^ ( Ξ ( D 3 ) ∧ ¬ ( D 2 ^ true )))). 6. ℵ ( triggers ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 /D 3 : Θ 3 )) def ≡ glyph[square] ( ∀ 1 Θ 1 : ( D 1 ^ true ⇒ ( ∀ 1 Θ 3 -Θ 1 : ( Ξ ( D 3 ) ⇒∃ 1 Θ 2 -( Θ 1 ∪ Θ 3 ) : D 2 ^ true )))) ∧ glyph[square] ( ∀ 1 Θ 1 : ( D 1 ⇒ pref ( ∀ 1 Θ 3 -Θ 1 : ( Ξ ( D 3 ) ⇒∃ 1 Θ 2 -( Θ 1 ∪ Θ 3 ) : D 2 ^ true )))).\n",
      "\n",
      "Theorem 2. For any word σ over Σ and any ζ ∈ SeCeNL we have that σ ∈ L ( ζ ) iff σ ∈ L ( ℵ ( ζ )) . Moreover, the translation ℵ ( ζ ) can be computed in time linear in the size of ζ .\n",
      "\n",
      "The proof follows from the semantics of ζ and the definition of ℵ ( ζ ).\n",
      "\n",
      "Lemma 3. Let ζ = implies ( D 1 : Θ 1 glyph[squiggleright] D 2 : Θ 2 ) and let |A ( D i ) | = m i for i ∈ { 1 , 2 } . Then there exists a DFA A ( ζ ) of size at most 2 2 m 1 m 2 for ζ .\n",
      "\n",
      "Proof. The formula ζ can be written in terms of a negation and two existential quantifiers. Note that each application of existential quantifier will result in an NFA and each time we determinize we get a DFA which is at most exponential in the size of NFA . Since that both A ( D 1 ) and A ( D 2 ) are DFA 's to start with, this implies we can construct a DFA A ( ζ ) of size at most 2 2 m 1 m 2 for ζ .\n",
      "\n",
      "In an similar way we can show that the size of formula automata for other SeCeNL atomic formulae are also elementary.\n",
      "\n",
      "Lemma 4. For any ζ ∈ SeCeNL the size of the automaton A ( ζ ) for ζ is elementary.\n",
      "\n",
      "## 3 Formalizing timing diagrams\n",
      "\n",
      "In this section we give a formal semantics to timing diagrams and formula translation from timing diagrams to SeCeNL. We begin by giving a textual syntax for timing diagrams which is derived from the timing diagram format of WaveDrom [CP16,Wav16].\n",
      "\n",
      "The symbols in a waveform come from Λ = { 0 , 1 , 2 , x , 0 | , 1 | , 2 | , x |} and Θ , an atomic set of nominals. Let Γ = Θ ∪ Λ . The syntax of a waveform over Γ is given by the grammar:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where u ∈ Θ and π ∈ Λ . We call the elements in Θ the nominals . As we shall see later, when we convert a waveform to a SeCeNL formula the nominals that appear in the formula are exactly the nominals in the waveform and hence the name. Let Wf be the set of all waveforms over Γ .\n",
      "\n",
      "An example of a waveform is 01a:2x011xb:x2 | 220c:00 with Θ = { a,b,c } . Intuitively, in a waveform 0 denotes low , 1 high , 2 and x don't care s (there is a subtle difference between 2 and x though) and ' | ' the stuttering operator.\n",
      "\n",
      "Let Σ be a set of propositional variables. A timing diagram over Σ is a tuple 〈W , Σ, C, Θ 〉 where W = { W p ∈ Wf | p ∈ Σ } and C ⊂ Θ × Θ × Intv ( N ) a set of timing constraints.\n",
      "\n",
      "Fig. 3 shows an example timing diagram T = 〈{ W p , W q } , { ( a, b, [10 : 10]) , ( a, d, [1 : 8]) , ( c, d, [20 : 30]) } , { a, b, c, d, e, f }〉 along with its rendering in WaveDrom. The shared nominals have to be renamed in WaveDrom as commented in § 2.3, in this case a and c in W q have been renamed g and h respectively. As in the case with SeCeNL formulas, nominals act as place holders in timing diagrams which can be shared among multiple waveforms. For example, in the figure W p and W q share the nominals a and c . As a result a timing constraint in one timing diagram can implicitly induce a timing constraint in the other. For instance, even though there is no direct timing constraint between a and c in W p the constraints between a and d , and d and c together impose one on them.\n",
      "\n",
      "waveform W p - 01a : 2x011xb : x2 | 220c : 00 waveform W q - 00a : 0 | d : 11 | e : xxx | f : 01c : 11 timing constraints: d-a ∈ [1:8], c-d ∈ [20:30], b-a ∈ [10:10]\n",
      "\n",
      "Fig. 3. Timing diagram T and its WaveDrom rendering.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Let T = 〈W , Σ, C, Θ 〉 , W = { W p ∈ Wf | p ∈ Σ } , be a timing diagram. Let ν : Θ → [ b, e ] be a nominal valuation. Let σ : [0 , n ] → 2 Σ be a word over Σ and for all p ∈ Σ let σ p : [0 , n ] → { 0 , 1 } given by σ p ( i ) = 1 iff p ∈ σ ( i ). Then the satisfaction relation σ p over a waveform W under the valuation ν is defined as follows.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "We say ν | = C iff ∀ ( a, b, 〈 l, r 〉 ) ∈ C : ν ( b ) -ν ( a ) ∈ 〈 l, r 〉 . We define σ, [ b, e ] | = ν 〈W , Σ, C, Θ 〉 iff ∀ p ∈ Σ : σ p , [ b, e ] | = ν W p and ν | = C .\n",
      "\n",
      "## 3.1 Waveform to SeCeNL translation\n",
      "\n",
      "We translate a waveform W p to SeCeNL as follows: every 0 occurring in P is translated to {{¬ P }} , 1 to {{ P }} , 2 and x to slen=1 , 0 | to pt ∨ [ ¬ P] , 1 | to pt ∨ [P] , 2 | to true , and x | to pt ∨ [P] ∨ [ ¬ P] . A nominal u that is appearing in W p is translated to &lt; u &gt; . For instance, the waveform W p = 01a:2x011xb:x2 | 220c:00 in T of Fig. 3 will be translated to SeCeNL formula as below.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "We denote the translated SeCeNL formula by ξ ( T, W p ). Similarly we can translate W q to get the formula ξ ( T, W q ). The timing constraints in C is roughly translated to the SeCeNL formula ξ ( T, C ) as follows.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "We define ξ ( T ) = ξ ( T, W p ) ∧ ξ ( T, W q ) ∧ ξ ( T, C ). For a timing diagram T = 〈W , Σ, C, Θ 〉 , W = { W p | p ∈ Σ } we define ξ ( T ) = ∧ p ∈ Σ ξ ( T, W p ) ∧ ∧ ξ ( T, C ).\n",
      "\n",
      "Theorem 3. Let T be a timing diagram. Then, for all σ ∈ Σ ∗ , for all [ b, e ] ∈ Intv ( σ ) and for all nominal valuation ν over [ b, e ] , σ, [ b, e ] | = ν T iff σ, [ b, e ] | = ν ξ ( T ) : Θ . Also, the translation ξ ( T ) : Θ is linear in the size of T .\n",
      "\n",
      "Proof. Proof is not difficult and is by induction on the length of the waveform.\n",
      "\n",
      "Due above theorem we can now use timing diagrams in place of nominated formulas with liveness operators. We call such timing diagrams live timing diagrams . For an example of a live timing diagram see Fig. 2.\n",
      "\n",
      "## 3.2 Comparision with other temporal logics\n",
      "\n",
      "In previous section, Lemma 3 showed that timing diagrams can be translated to equivalent SeCeNL formulas with only linear blowup in size. In this section we compare our logic SeCeNL with other relevent logics in the literature viz, LTL, discrete time MTL, and PSL-Sugar. Of these, PSL-Sugar is the most expressive and discrete time MTL and LTL are its syntactic subset. We show by examples that SeCeNL formulae are more succint (smaller in size) than PSL-Sugar and we believe that they capture the diagrams more directly. Appendix A gives several more examples which could not be included due to lack of space.\n",
      "\n",
      "Example (Ordered Stack) Let us now consider the timing diagram in Fig. 4 adapted from [CF05]. Rise and fall of successive signals follow a stack discipline. The language described by it is given by the SeCeNL formula:\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Fig. 4. Example 1.\n",
      "\n",
      "```\n",
      "([ ¬ a ] ˆ <ua> ˆ [ a ] ˆ <va> ˆ [ ¬ a ]) ∧ ([ ¬ b ] ˆ <ub> ˆ [ b ] ˆ <vb> ˆ [ ¬ b ]) ∧ ([ ¬ c ] ˆ <uc> ˆ [ c ] ˆ <vc> ˆ [ ¬ c ]) ∧ ([ ¬ d ] ˆ <ud> ˆ [ d ] ˆ <vd> ˆ [ ¬ d ]) ∧ ([ ¬ e ] ˆ <ue> ˆ [ e ] ˆ <ve> ˆ [ ¬ e ]) ∧ ( ext ˆ <ua> ˆ ext ˆ <ub> ˆ ext ) ∧ ( ext ˆ <ub> ˆ ext ˆ <uc> ˆ ext ) ∧ ( ext ˆ <uc> ˆ ext ˆ <ud> ˆ ext ) ∧ ( ext ˆ <ud> ˆ ext ˆ <ue> ˆ ext ) ∧ ( ext ˆ <va> ˆ ext ˆ <vb> ˆ ext ) ∧ ( ext ˆ <vb> ˆ ext ˆ <vc> ˆ ext ) ∧ ( ext ˆ <vc> ˆ ext ˆ <vd> ˆ ext ) ∧ ( ext ˆ <vd> ˆ ext ˆ <ve> ˆ ext ) .\n",
      "```\n",
      "\n",
      "Note that first five conjuncts exactly correspond to the five waveforms. The last constraint enforces the ordering constraints between waveforms. In general, if n signals are stacked, its SeCeNL specification has size O ( n ).\n",
      "\n",
      "An equivalent MTL (or LTL) formula is given by:\n",
      "\n",
      "```\n",
      "[ ¬ a ∧ ¬ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ] UU [ a ∧ ¬ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ] UU [ a ∧ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ] UU [ a ∧ b ∧ c ∧ ¬ d ∧ ¬ e ] UU [ a ∧ b ∧ c ∧ d ∧ ¬ e ] UU [ a ∧ b ∧ c ∧ d ∧ e ] UU [ a ∧ b ∧ c ∧ d ∧ ¬ e ] UU [ a ∧ b ∧ c ∧ ¬ d ∧ ¬ e ] UU [ a ∧ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ] UU [ a ∧ ¬ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ] UU [ ¬ a ∧ ¬ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ]\n",
      "```\n",
      "\n",
      "where a UU b is the derived modality a ∧ X ( a U b ). For a stack of n signals, the size of the MTL formula is O ( n 2 ).\n",
      "\n",
      "Above formula is also a PSL-Sugar formula. We attempt to specify the pattern as a PSL-Sugar regular expression as follows:\n",
      "\n",
      "```\n",
      "(( ¬ a ∧ ¬ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ; )[+]; ( a ∧ ¬ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ; )[+]; ( a ∧ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ; )[+]; ( a ∧ b ∧ c ∧ ¬ d ∧ ¬ e ; )[+]; ( a ∧ b ∧ c ∧ d ∧ ¬ e ; )[+]; ( a ∧ b ∧ c ∧ d ∧ e ; )[+]; ( a ∧ b ∧ c ∧ d ∧ ¬ e ; )[+]; ( a ∧ b ∧ c ∧ ¬ d ∧ ¬ e ; )[+]; ( a ∧ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ; )[+]; ( a ∧ ¬ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ; )[+]; ( ¬ a ∧ ¬ b ∧ ¬ c ∧ ¬ d ∧ ¬ e ; )[+]\n",
      "```\n",
      "\n",
      "For a stack of n signals, the size of the PSL-Sugar SERE expression is O ( n 2 ). We believe that there is no formula of size O ( n ) in PSL-Sugar which can express the above property. Compare this with size O ( n ) formula of SeCeNL.\n",
      "\n",
      "Example (Unordered Stack) In ordered stack signal a turns on first and turns off last followed by signals b, c, d, e in that order. We consider a variation of the ordered stack example above where signals turn on and off in first-on-last-off order but there is no restriction on which signal becomes high first. This can be compactly specified in SeCeNL as follows.\n",
      "\n",
      "```\n",
      "([ ¬ a ] ˆ <ua> ˆ [ a ] ˆ <va> ˆ [ ¬ a ]) ∧ ([ ¬ b ] ˆ <ub> ˆ [ b ] ˆ <vb> ˆ [ ¬ b ]) ∧ ([ ¬ c ] ˆ <uc> ˆ [ c ] ˆ <vc> ˆ [ ¬ c ]) ∧ ([ ¬ d ] ˆ <ud> ˆ [ d ] ˆ <vd> ˆ [ ¬ d ]) ∧ ([ ¬ e ] ˆ <ue> ˆ [ e ] ˆ <ve> ˆ [ ¬ e ]) ∧ ( ext ˆ <u 1 > ˆ ext ˆ <u 2 > ˆ ext ) ∧ ( ext ˆ <u 2 > ˆ ext ˆ <u 3 > ˆ ext ) ∧ ( ext ˆ <u 3 > ˆ ext ˆ <u 4 > ˆ ext ) ∧ ( ext ˆ <u 4 > ˆ ext ˆ <u 5 > ˆ ext ) ∧ ( ext ˆ <v 5 > ˆ ext ˆ <v 4 > ˆ ext ) ∧ ( ext ˆ <v 4 > ˆ ext ˆ <v 3 > ˆ ext ) ∧ ( ext ˆ <v 3 > ˆ ext ˆ <v 2 > ˆ ext ) ∧ ( ext ˆ <v 2 > ˆ ext ˆ <v 1 > ˆ ext ) ∧ Bijection ( ua, ub, uc, ud, ue, va, vb, vc, vd, ve, u 1 , u 2 , u 3 , u 4 , u 5 , v 1 , v 2 , v 3 , v 4 , v 5)\n",
      "```\n",
      "\n",
      "where formula Bijection below states that there is one to one correspondence between positions marked by ua, ub, uc, ud, ue, va, vb, vc, vd, de and positions marked by u 1 , u 2 , u 3 , u 4 , u 5 , v 1 , v 2 , v 3 , v 4 , v 5. Moreover, it states that if u a maps to say u 3 than v a must map to v 3 and so on.\n",
      "\n",
      "glyph[negationslash]\n",
      "\n",
      "```\n",
      "[[( u 1 ∨ u 2 ∨ u 3 ∨ u 4 ∨ u 5) ⇔ ( ua ∨ ub ∨ uc ∨ ud ∨ ue )]] ∧ [[ ∧ 1 ≤ i,j ≤ 5 ,i = j ¬ ( u i ∧ u j )]] [[( v 1 ∨ v 2 ∨ v 3 ∨ v 4 ∨ v 5) ⇔ ( va ∨ vb ∨ vc ∨ vd ∨ ve )]] ∧ [[ ∧ 1 ≤ i,j ≤ 5 ,i = j ¬ ( v i & v j )]] ∧ 1 ≤ i ≤ 5 ,j ∈ a,b,c,d,e ( true ˆ <u i ∧ u j > ˆ true ⇔ true ˆ <v i ∧ v j > ˆ true )\n",
      "```\n",
      "\n",
      "glyph[negationslash]\n",
      "\n",
      "Note that, in general, if n signals are stacked, then the above SeCeNL specification has size O ( n 2 ).\n",
      "\n",
      "Now we discuss encoding of unordered stack in PSL-Sugar. In absence of nominals, it is difficult to state the above behaviour succinctly in logics PSLSugar even using its SERE regular expressions. Each order of occurrence of signals has to be enumerated as a disjunction where each disjunct is as in the example ordered stack (where the order was a, b, c, d, e ). As there are n ! orders possible between n signals, the size of the PSL-Sugar formula is also O ( n !). We believe that there is no polynomially sized formula in PSL-Sugar encoding this property. This shows that SeCeNL is exponentially more succint as compared to PSL-Sugar.\n",
      "\n",
      "In general, presence of nominals distinguishes SeCeNL from logics like PSLSugar. In formalizing behaviour of hardware circuits it has been proposed that regular expressions are not enough and operators such as pipelining have been introduced [CF05]. These are a form of synchronization and they can be easily expressed using nominals too.\n",
      "\n",
      "## 4 Case study: Minepump Specification\n",
      "\n",
      "We first specify some useful generic timing diagram properties which would used for requirement specification in this (and many other) case studies.\n",
      "\n",
      "- lags ( P, Q, n ): it is defined by Fig. 5. It specifies that in any observation interval if P holds continuously for n +1 cycles and persists then Q holds from ( n +1) th cycle onwards and persists till P persists.\n",
      "- tracks ( P, Q, n ): defined Fig. 6. In any observation interval if P becomes true then Q sustains as long as P sustains or upto n cycles whichever is shorter.\n",
      "- sep ( P, n ): Fig. 7 defines this property. Any interval which begins with a falling edge of P and ends with a rising edge of P then the length of the interval should be at least n cycles.\n",
      "- ubound ( P, n ): Fig. 8 defines the property. In any observation interval P can be continuously true for at most n cycles.\n",
      "\n",
      "Note that we have presented these formulae diagrammatically. The textual version of these live timing diagrams can be found in Appendix C.\n",
      "\n",
      "We now state the minepump problem. Imagine a minepump which keeps the water level in a mine under control for the safety of miners. The pump is driven by a controller which can switch it on and off . Mines are prone to methane leakage trapped underground which is highly flammable. So as a safety measure if a methane leakage is detected the controller is not allowed to switch on the pump under no circumstances.\n",
      "\n",
      "The controller has two input sensors - HH2O which becomes 1 when water level is high, and HCH4 which is 1 when there is a methane leakage; and can generate two output signals - ALARM which is set to 1 to sound/persist the alarm, and PUMPON which is set to 1 to switch on the pump. The objective of the controller is to safely operate the pump and the alarm in such a way that the water level is never dangerous, indicated by the indicator variable DH2O, whenever certain assumptions hold. We have the following assumptions on the mine and the pump.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "- -Sensor reliability assumption: pref ([[ DH 2 O ⇒ HH 2 O ]]) . If HH2O is false then so is DH2O.\n",
      "- -Water seepage assumptions: tracks ( HH 2 O,DH 2 O,κ 1 ). The minimum no. of cycles for water level to become dangerous once it becomes high is κ 1 .\n",
      "- -Pump capacity assumption: lags ( PUMPON, ¬ HH 2 O,κ 2 ). If pump is switched on for at least κ 2 +1 cycles then water level will not be high after κ 2 cycles.\n",
      "- -Methane release assumptions: sep ( HCH 4 , κ 3 ) and ubound ( HCH 4 , κ 4 ). The minimum separation between the two leaks of methane is κ 3 cycles and the methane leak cannot persist for more than κ 4 cycles.\n",
      "- -Initial condition assumption: init ( &lt; ¬ HH 2 O&gt; ∧ &lt; ¬ HCH 4 &gt;,slen = 0). Initially neither the water level is high nor there is a methane leakage.\n",
      "\n",
      "Let the conjunction of these SeCeNL formulas be denoted as MINEASSUME . The commitments are:\n",
      "\n",
      "- -Alarm control: lags ( HH 2 O,ALARM,κ 5 ) and lags ( HCH 4 , ALARM,κ 6 ) and lags ( ¬ HH 2 O ∧ ¬ HCH 4 , ¬ ALARM,κ 7 ). If the water level is dangerous then alarm will be high after κ 5 cycles and if there is a methane leakage then alarm will be high after κ 6 cycles. If neither the water level is dangerous nor there is a methane leakage then alarm should be off after κ 7 cycle.\n",
      "- -Safety condition: pref ([[ ¬ DH 2 O ∧ ( HCH 4 ⇒¬ PUMPON )]]) . The water level should never become dangerous and whenever there is a methane leakage pump should be off.\n",
      "\n",
      "Let the conjunction of these commitments be denoted as MINECOMMIT . Then the requirement over the minepump controller is given by the formula MINEASSUME ⇒ MINECOMMIT . A textual version of this full minepump specification, which can be input to our tools is given in Appendix C. Note that the require consists of a mixture of timing diagram constraints (such as pump capacity assumption above) as well as SeCeNL formulas (such as Safety condition above).\n",
      "\n",
      "We can automatically synthesize a controller for the values say κ 1 = 10, κ 2 = 2, κ 3 = 14, κ 4 = 2, and κ 5 = κ 6 = κ 7 = 1. The tool outputs a SCADE/SMV\n",
      "\n",
      "controller meeting the specification. A snapshot of SCADE code for the controller synthesized by DCSynthG for minepump can be found in Appendix D. If the specification is not realizable we output an explanation.\n",
      "\n",
      "A second case study of synchronous bus arbiter specification can be found in Appendix. E. We can automatically synthesize a property monitor for such requirement and use it to model check a given arbiter design; or we can directly synthesize a controller meeting the requirement. The appendix gives results of both these experiments.\n",
      "\n",
      "## References\n",
      "\n",
      "| AEKN00.   | Nina Amla, E. Allen Emerson, Robert P. Kurshan, and Kedar S. Namjoshi. Model checking synchronous timing diagrams. In Warren A. Hunt Jr. and Steven D. Johnson, editors, Formal Methods in Computer-Aided De- sign, Third International Conference, FMCAD 2000, Austin, Texas, USA, November 1-3, 2000, Proceedings , volume 1954 of Lecture Notes in Computer Science , pages 283-298. Springer, 2000.                  |\n",
      "|-----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| AH93.     | Rajeev Alur and Thomas A. Henzinger. Real-time logics: Complexity and expressiveness. Inf. Comput. , 104(1):35-77, 1993.                                                                                                                                                                                                                                                                                                 |\n",
      "| All83.    | James F. Allen. Maintaining knowledge about temporal intervals. Commun. ACM , 26(11):832-843, 1983.                                                                                                                                                                                                                                                                                                                      |\n",
      "| AS87.     | Bowen Alpern and Fred B. Schneider. Recognizing safety and liveness. Distributed Computing , 2(3):117-126, 1987.                                                                                                                                                                                                                                                                                                         |\n",
      "| BP12.     | Ajesh Babu and Paritosh K. Pandya. Chop expressions and discrete dura- tion calculus. In Modern Applications of Automata Theory , pages 229-256. 2012.                                                                                                                                                                                                                                                                   |\n",
      "| CF05.     | Hana Chockler and Kathi Fisler. Temporal modalities for concisely captur- ing timing diagrams. In Dominique Borrione and Wolfgang J. Paul, editors, Correct Hardware Design and Verification Methods, 13th IFIP WG10.5 Ad- vanced Research Working Conference, CHARME 2005, Saarbr¨ ucken, Ger- many, October 3-6, 2005, Proceedings , volume 3725 of Lecture Notes in Computer Science , pages 176-190. Springer, 2005. |\n",
      "| CP16.     | Aliaksei Chapyzhenka and Jonah Probell. Wavedrom: Rendering beautiful waveforms from plain text. Synopsys User Group , 2016.                                                                                                                                                                                                                                                                                             |\n",
      "| DH01.     | Werner Damm and David Harel. Lscs: Breathing life into message sequence charts. Formal Methods in System Design , 19(1):45-80, 2001.                                                                                                                                                                                                                                                                                     |\n",
      "| EF16.     | Cindy Eisner and Dana Fisman. Temporal logic made practical. Handbook of Model Checking. Springer (Expected 2016), http://www. cis. upenn. edu/˜ fisman/documents/EF HBMC14. pdf , 2016.                                                                                                                                                                                                                                 |\n",
      "| FdRS03.   | Massimo Franceschet, Maarten de Rijke, and Bernd-Holger Schlingloff. Hy- brid logics on linear structures: Expressivity and complexity. In 10th In- ternational Symposium on Temporal Representation and Reasoning / 4th International Conference on Temporal Logic (TIME-ICTL 2003), 8-10 July 2003, Cairns, Queensland, Australia , pages 166-173. IEEE Computer Soci- ety, 2003.                                      |\n",
      "| Fis99.    | Kathi Fisler. Timing diagrams: Formalization and algorithmic verification. Journal of Logic, Language and Information , 8(3):323-361, 1999.                                                                                                                                                                                                                                                                              |\n",
      "\n",
      "- Fis07. Kathi Fisler. Two-dimensional regular expressions for compositional bus protocols. In Formal Methods in Computer-Aided Design, 7th International Conference, FMCAD 2007, Austin, Texas, USA, November 11-14, 2007, Proceedings , pages 154-157. IEEE Computer Society, 2007.\n",
      "- KP05. Yonit Kesten and Amir Pnueli. A compositional approach to CTL* verification. Theor. Comput. Sci. , 331(2-3):397-428, 2005.\n",
      "- Pan00. Paritosh K. Pandya. Specifying and deciding quantified discrete-time duration calculus formulae using DCVALID. Technical report, Tata Institute of Fundamental Research, Mumbai, 2000.\n",
      "- Pan01. Paritosh K. Pandya. Model checking ctl*[dc]. In Tiziana Margaria and Wang Yi, editors, Tools and Algorithms for the Construction and Analysis of Systems, 7th International Conference, TACAS 2001 Held as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS 2001 Genova, Italy, April 2-6, 2001, Proceedings , volume 2031 of Lecture Notes in Computer Science , pages 559-573. Springer, 2001.\n",
      "- Wav16. WaveDrom. Wavedrom user manual. http://wavedrom.com/tutorial.html , 2016.\n",
      "\n",
      "## A Examples of Comparision with other logics\n",
      "\n",
      "Example 1 (Ordering with timing) Consider the timing diagram in Fig. 9 which says that a holds invariantly in the interval [0 , i ] where i ≥ 1, b holds invariantly in the interval [ i, j ], j ≥ i +1, and c holds at j and j ≤ n .\n",
      "\n",
      "Fig. 9. Example 1.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "- -The language described by the above timing diagram is given by the SeCeNL formula ([ a ∧¬ b ] ˆ [ b ∧¬ a ∧¬ c ] ˆ &lt;c&gt; ) ∧ ( slen ≤ n ) which is of size O (log( n )). It is assumed that all timing constants such as n are encoded in binary and hence they contribute size log( n ).\n",
      "- -An equivalent MTL formula is ∨ i = n -1 i =1 ( a ∧ ¬ b U [ i, i ]( b ∧ ¬ a U [1 , n -i ] c )) whose size is O ( n log( n )).\n",
      "- -Equivalent LTL formula is ∨ i = n -1 i =1 ∨ j = n -i j =1 ( a UX i ( b UX j c )) where X k = X · . . . · X ︸ ︷︷ ︸ , whose size is O ( n 2 ).\n",
      "\n",
      "k\n",
      "\n",
      "times\n",
      "\n",
      "- -Equivalent PSL-Sugar formula is ( a ∧¬ b [+]; b ∧¬ a ∧¬ c [+]; c ) ∧ (( a | b )[ &lt; n ]; c ) with size O (log( n )).\n",
      "\n",
      "We also give examples of complex dependancy constraints. Consider the timing diagram in Fig. 10. In this diagram, ua occurs before ub and uc , and uc occurs before ud and ue . The point vc occurs after vd and ve , and va occurs after vb and vb .\n",
      "\n",
      "Fig. 10. Example 3.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "The behaviour is described straightforwardly by the SeCeNL formula:\n",
      "\n",
      "```\n",
      "([ ¬ a ] ˆ <ua> ˆ [ a ] ˆ <va> ˆ [ ¬ a ]) ∧ ([ ¬ b ] ˆ <ub> ˆ [ b ] ˆ <vb> ˆ[ ¬ b ]) ∧ ([ ¬ c ] ˆ <uc> ˆ [ c ] ˆ <vc> ˆ [ ¬ c ]) ∧ ([ ¬ d ] ˆ <ud> ˆ [ d ] ˆ <vd> ˆ [ ¬ d ]) ∧ ([ ¬ e ] ˆ <ue> ˆ [ e ] ˆ <ve> ˆ [ ¬ e ]) ∧ ( ext ˆ <ua> ˆ ext ˆ <ub> ˆ true ) ∧ ( ext ˆ <ua> ˆ ext ˆ <uc> ˆ true ) ∧ ( ext ˆ <uc> ˆ ext ˆ <ud> ˆ true ) ∧ ( ext ˆ <uc> ˆ ext ˆ <ue> ˆ true ) ∧ ( ext ˆ <ve> ˆ ext ˆ <vc> ˆ true ) ∧ ( ext ˆ <vd> ˆ ext ˆ <vc> ˆ true ) ∧ ( ext ˆ <vc> ˆ ext ˆ <va> ˆ true ) ∧ ( ext ˆ <vb> ˆ ext ˆ <va> ˆ true ) .\n",
      "```\n",
      "\n",
      "This formula is linear in the size of the timing diagram. Unfortunately, specifying these dependancies in PSL-Sugar is complex and formula size blows up at least quadratically.\n",
      "\n",
      "## B Implementation\n",
      "\n",
      "We propose a textual framework with a well defined syntax and semantics for requirement specification (of the form assumptions ⇒ commitments). Our framework is heterogeneous in the sense that it supports both SeCeNL formulas and timing diagrams with nominals for system specification. It can also handle all of our limited liveness operators. (see Appendix. C for the code for minepump in our framework).\n",
      "\n",
      "We have also developed a Python based translator which takes requirements in our textual format as input and produces property monitors as well as controllers as output. Fig. 11 gives a broad picture of the current status of our tool chain.\n",
      "\n",
      "Fig. 11. Our tool chain.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## C Minepump Code\n",
      "\n",
      "```\n",
      "The example code for minepump is written using textual syntax for QDDC which can be found in [Pan00,Pan01]. #lhrs 'minepump' interface {\n",
      "```\n",
      "\n",
      "```\n",
      "input HH2O, HCH4; output ALARM monitor x, PUMPON monitor x; constant delta = 1, w = 10, epsilon=2 , zeta=14, kappa=2; auxvar DH2O; softreq (!YHCH4) || (!PUMPON); } #implies lag(P, Q, n) { td lagspeclet1(P, n) { P: < u > 1 | < v > 1 | ; @sync:(u, v, n); } td lagspeclet2(Q) { Q: 2 | < v > 1 | ; } } #implies tracks(P, Q, n) { td tracksspeclet1(P, n) { P: 0 < u > 1 | < v > 1 | ; @sync: (u,v,[n,)); } td tracksspeclet2(Q) Q: 2 < u > 1 | < v > 0 | ; } #implies tracks2(P, Q, n) td tracks2speclet1(P, n) { P: 0 < u > 1 | < v > 0 | ; @sync: (u,v,[,n]); } td tracks2speclet2(Q) Q:2 < u > 0 | < v > 2 | ; } #implies sep(P, n) { td sepspeclet1(P) P: 1 < u > 0 | < v > 1; td sepspeclet2(n) { @null: 2 < u > 2 | < v > 2;\n",
      "```\n",
      "\n",
      "```\n",
      "@sync: (u, v, (n,]); } } #implies ubound(P, n) { td boundspeclet1(P) P: < c > 1 | < d > 1; td boundspeclet2(n) { @null: < c > 2 | < d > 2; @sync: (c, d, [,n)); } } dc safe(DH2O) { pt || [!DH2O && ((HCH4 || !HH2O) = > !PUMPON)]; } main() { assume ( < !HH2O > ˆ true); assume (pt || [DH2O = > HH2O]); assume tracks(HH2O, !DH2O, w); assume tracks2 (HH2O, DH2O, w); assume lag(PUMPON, !HH2O, epsilon); assume sep(HCH4, zeta); assume ubound(HCH4, kappa); req ( < !ALARM > ˆ true); req lag(HH2O, ALARM, delta); req lag(HCH4, ALARM, delta); req lag(!HCH4 && !HH2O, !ALARM, delta); req safe(DH2O); }\n",
      "```\n",
      "\n",
      "## D Synthesized controller for minepump\n",
      "\n",
      "A snapshot of a controller synthesized from the minepump requirement in § 4. The controller had approximately 140 states and it took less than a second for synthesis.\n",
      "\n",
      "```\n",
      "node minepump ( HH2O, HCH4:bool) returns ( ALARM, PUMPON:bool) var cstate: int; let ALARM, PUMPON, cstate = ( if true and not HH2O and not HCH4 then ( false, false, 2) else if true and not HH2O and HCH4 then ( false, false, 4) else if true and HH2O and not HCH4 then ( false, false, 4) else if true and HH2O and HCH4 then ( false, false, 4) else ( dontCare, dontCare, 1)) ⇒ if pre cstate = 1 and not HH2O and not HCH4 then ( false, false, 2) else if pre cstate = 1 and not HH2O and HCH4 then ( false, false, 4) else if pre cstate = 1 and HH2O and not HCH4 then ( false, false, 4) else if pre cstate = 1 and HH2O and HCH4 then ( false, false, 4) else if pre cstate = 2 and not HH2O and not HCH4 then ( false, false, 2) else if pre cstate = 2 and not HH2O and HCH4 then ( false, false, 7) else if pre cstate = 2 and HH2O and not HCH4 then ( false, false, 9) else if pre cstate = 2 and HH2O and HCH4 then ( false, false, 11) else if pre cstate = 4 and not HH2O and not HCH4 then ( false, false, 4) else if pre cstate = 4 and not HH2O and HCH4 then ( false, false, 4) .............................. .............................. else if pre cstate = 309 and HH2O and not HCH4 then ( false, true, 4) else if pre cstate = 309 and HH2O and HCH4 then ( false, true, 4) else if pre cstate = 372 and not HH2O and not HCH4 then ( false, false, 2) else if pre cstate = 372 and not HH2O and HCH4 then ( false, false, 7) else if pre cstate = 372 and HH2O and not HCH4 then ( false, true, 4) else if pre cstate = 372 and HH2O and HCH4 then ( false, true, 4) else ( dontCare, dontCare, pre cstate) ; tel\n",
      "```\n",
      "\n",
      "## E Case study: 3-cell arbiter\n",
      "\n",
      "In this section we illustrate another application of our specification format and associated tools. For this we use the standard McMillan arbiter circuit given in NuSMV examples and do the model checking against the specification below.\n",
      "\n",
      "A synchronous 3-cell bus arbiter has 3 request lines req 1 , req 2 and req 3, and corresponding acknowledgement lines ack 1 , ack 2 and ack 3. At any time instance a subset of request lines can be high and arbiter decides which request should be granted permission to access the bus by making corresponding acknowledgement line high. The requirements for such a bus arbiter are as formulated below.\n",
      "\n",
      "glyph[negationslash]\n",
      "\n",
      "- -Exclusion: pref ([[( ∧ i = j ¬ ( ack i ∧ ack j ))]]) . At most 1 acknowledgement can be given at a time.\n",
      "- -No spurious acknowledgement: pref ([[( ∧ 1 ( ack i ⇒ req i ))]]) . A request should be granted access to the bus only if it has requested it.\n",
      "- -Response time: implies ([[ req ]] ∧ slen = n, true ^ &lt;ack&gt; ^ true ). One of the most important property of an arbiter is that it any request should be granted within n cycles, i. e. if a request is continuously true for sometime then it should be heard.\n",
      "- -Deadtime: to specify this property we first specify lost cycle as follows: Lost ≡ ( ∨ i req i ) ∧ ( ¬ ( ∨ i ack i )). Then Deadtime ≡ anti ([[ Lost ]] ∧ slen &gt; n ). This specifies the maximum number of consecutive cycles that can be lost by the arbiter is n .\n",
      "\n",
      "The requirement ARBREQ is a conjunction of above formulas.\n",
      "\n",
      "We ran the requirement through our tool chain to generate NuSMV module for the requirement monitor. This module was then instantiated synchronously with McMillan arbiter implementation in NuSMV and NuSMV model checker was called in to check the property G ( assumptions ⇒ commitments ).\n",
      "\n",
      "Model checking : Experimental results show that the deadtime for 3-cell McMillan arbiter is 3. If we specify the deadtime as 2 cycles then a counter example is generated by NuSMV as depicted in Fig. 12. This counter examples show that even though there is an request line high in 4 th , 5 th and 6 th cycle, but no acknowledgment is given by arbiter. Similarly, the response time for 1 st request is 3 cycles whereas for 2 nd and 3 rd cell it is 6 cycles. If we specify the response time of 2 and 5 cycles for 1 st and 2 nd then NuSMV generates counter examples in Fig. 13 and Fig. 14 respectively. Fig. 14 shows that the request line for cell 2 (i. e. req2) is high continuously for 5 cycles starting from 3 rd without an acknowledgement from the arbiter.\n",
      "\n",
      "Fig. 12. Counter Example Showing deadtime exceeding 2 cycles Fig. 13. Counter Example showing response time of 1st cell exceeding 2 cycles Fig. 14. Counter Example showing response time of 2nd cell exceeding 5 cycles\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Controller synthesis : We have also synthesized a controller for the arbiter specification using our tool DCSynthG. We have tightened the requirements by specifying the response time as 3 cycles uniformly for all three cells and deadtime as 0 cycles, i. e. there is no lost cycle. The tool could synthesize a controller in 0.03 seconds with 17 states.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 08:07:58,837 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:58,838 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:58,840 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:58,840 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:58,942 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:58,943 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:58,965 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:07:58,966 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 08:07:59,187 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 08:07:59,187 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:07:59,885 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:08:00,174 - INFO - Processing document 1506.01497v3.pdf\n",
      "2025-12-15 08:09:08,583 - INFO - Finished converting document 1506.01497v3.pdf in 70.09 sec.\n",
      "2025-12-15 08:09:08,711 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 08:09:08,713 - INFO - Going to convert document batch...\n",
      "2025-12-15 08:09:08,713 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 08:09:08,714 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 08:09:08,715 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 08:09:08,715 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:08,734 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:08,735 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:08,747 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:08,749 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "## INTRODUCTION\n",
      "\n",
      "Recent advances in object detection are driven by the success of region proposal methods ( e.g ., [4]) and region-based convolutional neural networks (RCNNs) [5]. Although region-based CNNs were computationally expensive as originally developed in [5], their cost has been drastically reduced thanks to sharing convolutions across proposals [1], [2]. The latest incarnation, Fast R-CNN [2], achieves near real-time rates using very deep networks [3], when ignoring the time spent on region proposals . Now, proposals are the test-time computational bottleneck in state-of-the-art detection systems.\n",
      "\n",
      "Region proposal methods typically rely on inexpensive features and economical inference schemes. Selective Search [4], one of the most popular methods, greedily merges superpixels based on engineered low-level features. Yet when compared to efficient detection networks [2], Selective Search is an order of magnitude slower, at 2 seconds per image in a CPU implementation. EdgeBoxes [6] currently provides the best tradeoff between proposal quality and speed, at 0.2 seconds per image. Nevertheless, the region proposal step still consumes as much running time as the detection network.\n",
      "\n",
      "· S. Ren is with University of Science and Technology of China, Hefei, China. This work was done when S. Ren was an intern at Microsoft Research. Email: sqren@mail.ustc.edu.cn\n",
      "\n",
      "· K. He and J. Sun are with Visual Computing Group, Microsoft Research. E-mail: { kahe,jiansun } @microsoft.com\n",
      "\n",
      "· R. Girshick is with Facebook AI Research. The majority of this work was done when R. Girshick was with Microsoft Research. E-mail: rbg@fb.com\n",
      "\n",
      "## Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\n",
      "\n",
      "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\n",
      "\n",
      "Abstract -State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5fps ( including all steps ) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.\n",
      "\n",
      "Index Terms -Object Detection, Region Proposal, Convolutional Neural Network.\n",
      "\n",
      "F\n",
      "\n",
      "One may note that fast region-based CNNs take advantage of GPUs, while the region proposal methods used in research are implemented on the CPU, making such runtime comparisons inequitable. An obvious way to accelerate proposal computation is to reimplement it for the GPU. This may be an effective engineering solution, but re-implementation ignores the down-stream detection network and therefore misses important opportunities for sharing computation.\n",
      "\n",
      "In this paper, we show that an algorithmic changecomputing proposals with a deep convolutional neural network-leads to an elegant and effective solution where proposal computation is nearly cost-free given the detection network's computation. To this end, we introduce novel Region Proposal Networks (RPNs) that share convolutional layers with state-of-the-art object detection networks [1], [2]. By sharing convolutions at test-time, the marginal cost for computing proposals is small ( e.g ., 10ms per image).\n",
      "\n",
      "Our observation is that the convolutional feature maps used by region-based detectors, like Fast RCNN, can also be used for generating region proposals. On top of these convolutional features, we construct an RPN by adding a few additional convolutional layers that simultaneously regress region bounds and objectness scores at each location on a regular grid. The RPN is thus a kind of fully convolutional network (FCN) [7] and can be trained end-toend specifically for the task for generating detection proposals.\n",
      "\n",
      "RPNs are designed to efficiently predict region proposals with a wide range of scales and aspect ratios. In contrast to prevalent methods [8], [9], [1], [2] that use\n",
      "\n",
      "Figure 1: Different schemes for addressing multiple scales and sizes. (a) Pyramids of images and feature maps are built, and the classifier is run at all scales. (b) Pyramids of filters with multiple scales/sizes are run on the feature map. (c) We use pyramids of reference boxes in the regression functions.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "pyramids of images (Figure 1, a) or pyramids of filters (Figure 1, b), we introduce novel 'anchor' boxes that serve as references at multiple scales and aspect ratios. Our scheme can be thought of as a pyramid of regression references (Figure 1, c), which avoids enumerating images or filters of multiple scales or aspect ratios. This model performs well when trained and tested using single-scale images and thus benefits running speed.\n",
      "\n",
      "To unify RPNs with Fast R-CNN [2] object detection networks, we propose a training scheme that alternates between fine-tuning for the region proposal task and then fine-tuning for object detection, while keeping the proposals fixed. This scheme converges quickly and produces a unified network with convolutional features that are shared between both tasks. 1\n",
      "\n",
      "We comprehensively evaluate our method on the PASCAL VOC detection benchmarks [11] where RPNs with Fast R-CNNs produce detection accuracy better than the strong baseline of Selective Search with Fast R-CNNs. Meanwhile, our method waives nearly all computational burdens of Selective Search at test-time-the effective running time for proposals is just 10 milliseconds. Using the expensive very deep models of [3], our detection method still has a frame rate of 5fps ( including all steps ) on a GPU, and thus is a practical object detection system in terms of both speed and accuracy. We also report results on the MS COCO dataset [12] and investigate the improvements on PASCAL VOC using the COCO data. Code has been made publicly available at https://github.com/shaoqingren/faster\\_ rcnn (in MATLAB) and https://github.com/ rbgirshick/py-faster-rcnn (in Python).\n",
      "\n",
      "A preliminary version of this manuscript was published previously [10]. Since then, the frameworks of RPN and Faster R-CNN have been adopted and generalized to other methods, such as 3D object detection [13], part-based detection [14], instance segmentation [15], and image captioning [16]. Our fast and effective object detection system has also been built in com-\n",
      "\n",
      "1. Since the publication of the conference version of this paper [10], we have also found that RPNs can be trained jointly with Fast R-CNN networks leading to less training time.\n",
      "\n",
      "mercial systems such as at Pinterests [17], with user engagement improvements reported.\n",
      "\n",
      "In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the basis of several 1st-place entries [18] in the tracks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. RPNs completely learn to propose regions from data, and thus can easily benefit from deeper and more expressive features (such as the 101-layer residual nets adopted in [18]). Faster R-CNN and RPN are also used by several other leading entries in these competitions 2 . These results suggest that our method is not only a cost-efficient solution for practical usage, but also an effective way of improving object detection accuracy.\n",
      "\n",
      "## 2 RELATED WORK\n",
      "\n",
      "Object Proposals. There is a large literature on object proposal methods. Comprehensive surveys and comparisons of object proposal methods can be found in [19], [20], [21]. Widely used object proposal methods include those based on grouping super-pixels ( e.g ., Selective Search [4], CPMC [22], MCG [23]) and those based on sliding windows ( e.g ., objectness in windows [24], EdgeBoxes [6]). Object proposal methods were adopted as external modules independent of the detectors ( e.g ., Selective Search [4] object detectors, RCNN [5], and Fast R-CNN [2]).\n",
      "\n",
      "Deep Networks for Object Detection. The R-CNN method [5] trains CNNs end-to-end to classify the proposal regions into object categories or background. R-CNN mainly plays as a classifier, and it does not predict object bounds (except for refining by bounding box regression). Its accuracy depends on the performance of the region proposal module (see comparisons in [20]). Several papers have proposed ways of using deep networks for predicting object bounding boxes [25], [9], [26], [27]. In the OverFeat method [9], a fully-connected layer is trained to predict the box coordinates for the localization task that assumes a single object. The fully-connected layer is then turned\n",
      "\n",
      "Figure 2: Faster R-CNN is a single, unified network for object detection. The RPN module serves as the 'attention' of this unified network.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "into a convolutional layer for detecting multiple classspecific objects. The MultiBox methods [26], [27] generate region proposals from a network whose last fully-connected layer simultaneously predicts multiple class-agnostic boxes, generalizing the 'singlebox' fashion of OverFeat. These class-agnostic boxes are used as proposals for R-CNN [5]. The MultiBox proposal network is applied on a single image crop or multiple large image crops ( e.g ., 224 × 224), in contrast to our fully convolutional scheme. MultiBox does not share features between the proposal and detection networks. We discuss OverFeat and MultiBox in more depth later in context with our method. Concurrent with our work, the DeepMask method [28] is developed for learning segmentation proposals.\n",
      "\n",
      "Shared computation of convolutions [9], [1], [29], [7], [2] has been attracting increasing attention for efficient, yet accurate, visual recognition. The OverFeat paper [9] computes convolutional features from an image pyramid for classification, localization, and detection. Adaptively-sized pooling (SPP) [1] on shared convolutional feature maps is developed for efficient region-based object detection [1], [30] and semantic segmentation [29]. Fast R-CNN [2] enables end-to-end detector training on shared convolutional features and shows compelling accuracy and speed.\n",
      "\n",
      "## 3 FASTER R-CNN\n",
      "\n",
      "Our object detection system, called Faster R-CNN, is composed of two modules. The first module is a deep fully convolutional network that proposes regions, and the second module is the Fast R-CNN detector [2] that uses the proposed regions. The entire system is a single, unified network for object detection (Figure 2). Using the recently popular terminology of neural networks with 'attention' [31] mechanisms, the RPN module tells the Fast R-CNN module where to look. In Section 3.1 we introduce the designs and properties of the network for region proposal. In Section 3.2 we develop algorithms for training both modules with features shared.\n",
      "\n",
      "## 3.1 Region Proposal Networks\n",
      "\n",
      "A Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score. 3 We model this process with a fully convolutional network [7], which we describe in this section. Because our ultimate goal is to share computation with a Fast R-CNN object detection network [2], we assume that both nets share a common set of convolutional layers. In our experiments, we investigate the Zeiler and Fergus model [32] (ZF), which has 5 shareable convolutional layers and the Simonyan and Zisserman model [3] (VGG-16), which has 13 shareable convolutional layers.\n",
      "\n",
      "To generate region proposals, we slide a small network over the convolutional feature map output by the last shared convolutional layer. This small network takes as input an n × n spatial window of the input convolutional feature map. Each sliding window is mapped to a lower-dimensional feature (256-d for ZF and 512-d for VGG, with ReLU [33] following). This feature is fed into two sibling fullyconnected layers-a box-regression layer ( reg ) and a box-classification layer ( cls ). We use n = 3 in this paper, noting that the effective receptive field on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-network is illustrated at a single position in Figure 3 (left). Note that because the mini-network operates in a sliding-window fashion, the fully-connected layers are shared across all spatial locations. This architecture is naturally implemented with an n × n convolutional layer followed by two sibling 1 × 1 convolutional layers (for reg and cls , respectively).\n",
      "\n",
      "## 3.1.1 Anchors\n",
      "\n",
      "At each sliding-window location, we simultaneously predict multiple region proposals, where the number of maximum possible proposals for each location is denoted as k . So the reg layer has 4 k outputs encoding the coordinates of k boxes, and the cls layer outputs 2 k scores that estimate probability of object or not object for each proposal 4 . The k proposals are parameterized relative to k reference boxes, which we call\n",
      "\n",
      "3. 'Region' is a generic term and in this paper we only consider rectangular regions, as is common for many methods ( e.g ., [27], [4], [6]). 'Objectness' measures membership to a set of object classes vs . background.\n",
      "\n",
      "4. For simplicity we implement the cls layer as a two-class softmax layer. Alternatively, one may use logistic regression to produce k scores.\n",
      "\n",
      "Figure 3: Left : Region Proposal Network (RPN). Right : Example detections using RPN proposals on PASCAL VOC 2007 test. Our method detects objects in a wide range of scales and aspect ratios.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "anchors . An anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio (Figure 3, left). By default we use 3 scales and 3 aspect ratios, yielding k = 9 anchors at each sliding position. For a convolutional feature map of a size W × H (typically ∼ 2,400), there are WHk anchors in total.\n",
      "\n",
      "## Translation-Invariant Anchors\n",
      "\n",
      "An important property of our approach is that it is translation invariant , both in terms of the anchors and the functions that compute proposals relative to the anchors. If one translates an object in an image, the proposal should translate and the same function should be able to predict the proposal in either location. This translation-invariant property is guaranteed by our method 5 . As a comparison, the MultiBox method [27] uses k-means to generate 800 anchors, which are not translation invariant. So MultiBox does not guarantee that the same proposal is generated if an object is translated.\n",
      "\n",
      "The translation-invariant property also reduces the model size. MultiBox has a (4 + 1) × 800 -dimensional fully-connected output layer, whereas our method has a (4 + 2) × 9 -dimensional convolutional output layer in the case of k = 9 anchors. As a result, our output layer has 2 . 8 × 10 4 parameters ( 512 × (4 + 2) × 9 for VGG-16), two orders of magnitude fewer than MultiBox's output layer that has 6 . 1 × 10 6 parameters ( 1536 × (4 + 1) × 800 for GoogleNet [34] in MultiBox [27]). If considering the feature projection layers, our proposal layers still have an order of magnitude fewer parameters than MultiBox 6 . We expect our method to have less risk of overfitting on small datasets, like PASCAL VOC.\n",
      "\n",
      "5. As is the case of FCNs [7], our network is translation invariant up to the network's total stride.\n",
      "\n",
      "6. Considering the feature projection layers, our proposal layers' parameter count is 3 × 3 × 512 × 512 + 512 × 6 × 9 = 2 . 4 × 10 6 ; MultiBox's proposal layers' parameter count is 7 × 7 × (64 + 96 + 64 + 64) × 1536 + 1536 × 5 × 800 = 27 × 10 6 .\n",
      "\n",
      "## Multi-Scale Anchors as Regression References\n",
      "\n",
      "Our design of anchors presents a novel scheme for addressing multiple scales (and aspect ratios). As shown in Figure 1, there have been two popular ways for multi-scale predictions. The first way is based on image/feature pyramids, e.g ., in DPM [8] and CNNbased methods [9], [1], [2]. The images are resized at multiple scales, and feature maps (HOG [8] or deep convolutional features [9], [1], [2]) are computed for each scale (Figure 1(a)). This way is often useful but is time-consuming. The second way is to use sliding windows of multiple scales (and/or aspect ratios) on the feature maps. For example, in DPM [8], models of different aspect ratios are trained separately using different filter sizes (such as 5 × 7 and 7 × 5). If this way is used to address multiple scales, it can be thought of as a 'pyramid of filters' (Figure 1(b)). The second way is usually adopted jointly with the first way [8].\n",
      "\n",
      "As a comparison, our anchor-based method is built on a pyramid of anchors , which is more cost-efficient. Our method classifies and regresses bounding boxes with reference to anchor boxes of multiple scales and aspect ratios. It only relies on images and feature maps of a single scale, and uses filters (sliding windows on the feature map) of a single size. We show by experiments the effects of this scheme for addressing multiple scales and sizes (Table 8).\n",
      "\n",
      "Because of this multi-scale design based on anchors, we can simply use the convolutional features computed on a single-scale image, as is also done by the Fast R-CNN detector [2]. The design of multiscale anchors is a key component for sharing features without extra cost for addressing scales.\n",
      "\n",
      "## 3.1.2 Loss Function\n",
      "\n",
      "For training RPNs, we assign a binary class label (of being an object or not) to each anchor. We assign a positive label to two kinds of anchors: (i) the anchor/anchors with the highest Intersection-overUnion (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher than 0.7 with\n",
      "\n",
      "any ground-truth box. Note that a single ground-truth box may assign positive labels to multiple anchors. Usually the second condition is sufficient to determine the positive samples; but we still adopt the first condition for the reason that in some rare cases the second condition may find no positive sample. We assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes. Anchors that are neither positive nor negative do not contribute to the training objective.\n",
      "\n",
      "With these definitions, we minimize an objective function following the multi-task loss in Fast R-CNN [2]. Our loss function for an image is defined as:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Here, i is the index of an anchor in a mini-batch and p i is the predicted probability of anchor i being an object. The ground-truth label p ∗ i is 1 if the anchor is positive, and is 0 if the anchor is negative. t i is a vector representing the 4 parameterized coordinates of the predicted bounding box, and t ∗ i is that of the ground-truth box associated with a positive anchor. The classification loss L cls is log loss over two classes (object vs . not object). For the regression loss, we use L reg ( t i , t ∗ i ) = R ( t i -t ∗ i ) where R is the robust loss function (smooth L 1 ) defined in [2]. The term p ∗ i L reg means the regression loss is activated only for positive anchors ( p ∗ i = 1 ) and is disabled otherwise ( p ∗ i = 0 ). The outputs of the cls and reg layers consist of { p i } and { t i } respectively.\n",
      "\n",
      "The two terms are normalized by N cls and N reg and weighted by a balancing parameter λ . In our current implementation (as in the released code), the cls term in Eqn.(1) is normalized by the mini-batch size ( i.e ., N cls = 256 ) and the reg term is normalized by the number of anchor locations ( i.e ., N reg ∼ 2 , 400 ). By default we set λ = 10 , and thus both cls and reg terms are roughly equally weighted. We show by experiments that the results are insensitive to the values of λ in a wide range (Table 9). We also note that the normalization as above is not required and could be simplified.\n",
      "\n",
      "For bounding box regression, we adopt the parameterizations of the 4 coordinates following [5]:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where x , y , w , and h denote the box's center coordinates and its width and height. Variables x , x a, and x ∗ are for the predicted box, anchor box, and groundtruth box respectively (likewise for y, w, h ). This can be thought of as bounding-box regression from an anchor box to a nearby ground-truth box.\n",
      "\n",
      "Nevertheless, our method achieves bounding-box regression by a different manner from previous RoIbased (Region of Interest) methods [1], [2]. In [1], [2], bounding-box regression is performed on features pooled from arbitrarily sized RoIs, and the regression weights are shared by all region sizes. In our formulation, the features used for regression are of the same spatial size ( 3 × 3 ) on the feature maps. To account for varying sizes, a set of k bounding-box regressors are learned. Each regressor is responsible for one scale and one aspect ratio, and the k regressors do not share weights. As such, it is still possible to predict boxes of various sizes even though the features are of a fixed size/scale, thanks to the design of anchors.\n",
      "\n",
      "## 3.1.3 Training RPNs\n",
      "\n",
      "The RPN can be trained end-to-end by backpropagation and stochastic gradient descent (SGD) [35]. We follow the 'image-centric' sampling strategy from [2] to train this network. Each mini-batch arises from a single image that contains many positive and negative example anchors. It is possible to optimize for the loss functions of all anchors, but this will bias towards negative samples as they are dominate. Instead, we randomly sample 256 anchors in an image to compute the loss function of a mini-batch, where the sampled positive and negative anchors have a ratio of up to 1:1. If there are fewer than 128 positive samples in an image, we pad the mini-batch with negative ones.\n",
      "\n",
      "We randomly initialize all new layers by drawing weights from a zero-mean Gaussian distribution with standard deviation 0.01. All other layers ( i.e ., the shared convolutional layers) are initialized by pretraining a model for ImageNet classification [36], as is standard practice [5]. We tune all layers of the ZF net, and conv3 1 and up for the VGG net to conserve memory [2]. We use a learning rate of 0.001 for 60k mini-batches, and 0.0001 for the next 20k mini-batches on the PASCAL VOC dataset. We use a momentum of 0.9 and a weight decay of 0.0005 [37]. Our implementation uses Caffe [38].\n",
      "\n",
      "## 3.2 Sharing Features for RPN and Fast R-CNN\n",
      "\n",
      "Thus far we have described how to train a network for region proposal generation, without considering the region-based object detection CNN that will utilize these proposals. For the detection network, we adopt Fast R-CNN [2]. Next we describe algorithms that learn a unified network composed of RPN and Fast R-CNN with shared convolutional layers (Figure 2).\n",
      "\n",
      "Both RPN and Fast R-CNN, trained independently, will modify their convolutional layers in different ways. We therefore need to develop a technique that allows for sharing convolutional layers between the\n",
      "\n",
      "Table 1: the learned average proposal size for each anchor using the ZF net (numbers for s = 600 ). anchor 128 2 , 2:1 128 2 , 1:1 128 2 , 1:2 256 2 , 2:1 256 2 , 1:1 256 2 , 1:2 512 2 , 2:1 512 2 , 1:1 512 2 , 1:2 proposal 188 × 111 113 × 114 70 × 92 416 × 229 261 × 284 174 × 332 768 × 437 499 × 501 355 × 715\n",
      "\n",
      "two networks, rather than learning two separate networks. We discuss three ways for training networks with features shared:\n",
      "\n",
      "(i) Alternating training . In this solution, we first train RPN, and use the proposals to train Fast R-CNN. The network tuned by Fast R-CNN is then used to initialize RPN, and this process is iterated. This is the solution that is used in all experiments in this paper.\n",
      "\n",
      "- (ii) Approximate joint training . In this solution, the RPN and Fast R-CNN networks are merged into one network during training as in Figure 2. In each SGD iteration, the forward pass generates region proposals which are treated just like fixed, pre-computed proposals when training a Fast R-CNN detector. The backward propagation takes place as usual, where for the shared layers the backward propagated signals from both the RPN loss and the Fast R-CNN loss are combined. This solution is easy to implement. But this solution ignores the derivative w.r.t. the proposal boxes' coordinates that are also network responses, so is approximate. In our experiments, we have empirically found this solver produces close results, yet reduces the training time by about 25-50% comparing with alternating training. This solver is included in our released Python code.\n",
      "\n",
      "(iii) Non-approximate joint training . As discussed above, the bounding boxes predicted by RPN are also functions of the input. The RoI pooling layer [2] in Fast R-CNN accepts the convolutional features and also the predicted bounding boxes as input, so a theoretically valid backpropagation solver should also involve gradients w.r.t. the box coordinates. These gradients are ignored in the above approximate joint training. In a non-approximate joint training solution, we need an RoI pooling layer that is differentiable w.r.t. the box coordinates. This is a nontrivial problem and a solution can be given by an 'RoI warping' layer as developed in [15], which is beyond the scope of this paper.\n",
      "\n",
      "4-Step Alternating Training . In this paper, we adopt a pragmatic 4-step training algorithm to learn shared features via alternating optimization. In the first step, we train the RPN as described in Section 3.1.3. This network is initialized with an ImageNet-pre-trained model and fine-tuned end-to-end for the region proposal task. In the second step, we train a separate detection network by Fast R-CNN using the proposals generated by the step-1 RPN. This detection network is also initialized by the ImageNet-pre-trained model. At this point the two networks do not share convolutional layers. In the third step, we use the detector network to initialize RPN training, but we fix the shared convolutional layers and only fine-tune the layers unique to RPN. Now the two networks share convolutional layers. Finally, keeping the shared convolutional layers fixed, we fine-tune the unique layers of Fast R-CNN. As such, both networks share the same convolutional layers and form a unified network. A similar alternating training can be run for more iterations, but we have observed negligible improvements.\n",
      "\n",
      "## 3.3 Implementation Details\n",
      "\n",
      "We train and test both region proposal and object detection networks on images of a single scale [1], [2]. We re-scale the images such that their shorter side is s = 600 pixels [2]. Multi-scale feature extraction (using an image pyramid) may improve accuracy but does not exhibit a good speed-accuracy trade-off [2]. On the re-scaled images, the total stride for both ZF and VGG nets on the last convolutional layer is 16 pixels, and thus is ∼ 10 pixels on a typical PASCAL image before resizing ( ∼ 500 × 375). Even such a large stride provides good results, though accuracy may be further improved with a smaller stride.\n",
      "\n",
      "For anchors, we use 3 scales with box areas of 128 2 , 256 2 , and 512 2 pixels, and 3 aspect ratios of 1:1, 1:2, and 2:1. These hyper-parameters are not carefully chosen for a particular dataset, and we provide ablation experiments on their effects in the next section. As discussed, our solution does not need an image pyramid or filter pyramid to predict regions of multiple scales, saving considerable running time. Figure 3 (right) shows the capability of our method for a wide range of scales and aspect ratios. Table 1 shows the learned average proposal size for each anchor using the ZF net. We note that our algorithm allows predictions that are larger than the underlying receptive field. Such predictions are not impossible-one may still roughly infer the extent of an object if only the middle of the object is visible.\n",
      "\n",
      "The anchor boxes that cross image boundaries need to be handled with care. During training, we ignore all cross-boundary anchors so they do not contribute to the loss. For a typical 1000 × 600 image, there will be roughly 20000 ( ≈ 60 × 40 × 9 ) anchors in total. With the cross-boundary anchors ignored, there are about 6000 anchors per image for training. If the boundary-crossing outliers are not ignored in training, they introduce large, difficult to correct error terms in the objective, and training does not converge. During testing, however, we still apply the fully convolutional RPN to the entire image. This may generate crossboundary proposal boxes, which we clip to the image boundary.\n",
      "\n",
      "Table 2: Detection results on PASCAL VOC 2007 test set (trained on VOC 2007 trainval). The detectors are Fast R-CNN with ZF, but using various proposal methods for training and testing.\n",
      "\n",
      "| train-time region proposals       | train-time region proposals       | test-time region proposals        | test-time region proposals        |                                   |\n",
      "|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|\n",
      "| method                            | # boxes                           | method                            | # proposals                       | mAP (%)                           |\n",
      "| SS                                | 2000                              | SS                                | 2000                              | 58.7                              |\n",
      "| EB                                | 2000                              | EB                                | 2000                              | 58.6                              |\n",
      "| RPN+ZF, shared                    | 2000                              | RPN+ZF, shared                    | 300                               | 59.9                              |\n",
      "| ablation experiments follow below | ablation experiments follow below | ablation experiments follow below | ablation experiments follow below | ablation experiments follow below |\n",
      "| RPN+ZF, unshared                  | 2000                              | RPN+ZF, unshared                  | 300                               | 58.7                              |\n",
      "| SS                                | 2000                              | RPN+ZF                            | 100                               | 55.1                              |\n",
      "| SS                                | 2000                              | RPN+ZF                            | 300                               | 56.8                              |\n",
      "| SS                                | 2000                              | RPN+ZF                            | 1000                              | 56.3                              |\n",
      "| SS                                | 2000                              | RPN+ZF (no NMS)                   | 6000                              | 55.2                              |\n",
      "| SS                                | 2000                              | RPN+ZF (no cls )                  | 100                               | 44.6                              |\n",
      "| SS                                | 2000                              | RPN+ZF (no cls )                  | 300                               | 51.4                              |\n",
      "| SS                                | 2000                              | RPN+ZF (no cls )                  | 1000                              | 55.8                              |\n",
      "| SS                                | 2000                              | RPN+ZF (no reg )                  | 300                               | 52.1                              |\n",
      "| SS                                | 2000                              | RPN+ZF (no reg )                  | 1000                              | 51.3                              |\n",
      "| SS                                | 2000                              | RPN+VGG                           | 300                               | 59.2                              |\n",
      "\n",
      "Some RPN proposals highly overlap with each other. To reduce redundancy, we adopt non-maximum suppression (NMS) on the proposal regions based on their cls scores. We fix the IoU threshold for NMS at 0.7, which leaves us about 2000 proposal regions per image. As we will show, NMS does not harm the ultimate detection accuracy, but substantially reduces the number of proposals. After NMS, we use the topN ranked proposal regions for detection. In the following, we train Fast R-CNN using 2000 RPN proposals, but evaluate different numbers of proposals at test-time.\n",
      "\n",
      "## 4 EXPERIMENTS\n",
      "\n",
      "## 4.1 Experiments on PASCAL VOC\n",
      "\n",
      "We comprehensively evaluate our method on the PASCAL VOC 2007 detection benchmark [11]. This dataset consists of about 5k trainval images and 5k test images over 20 object categories. We also provide results on the PASCAL VOC 2012 benchmark for a few models. For the ImageNet pre-trained network, we use the 'fast' version of ZF net [32] that has 5 convolutional layers and 3 fully-connected layers, and the public VGG-16 model 7 [3] that has 13 convolutional layers and 3 fully-connected layers. We primarily evaluate detection mean Average Precision (mAP), because this is the actual metric for object detection (rather than focusing on object proposal proxy metrics).\n",
      "\n",
      "Table 2 (top) shows Fast R-CNN results when trained and tested using various region proposal methods. These results use the ZF net. For Selective Search (SS) [4], we generate about 2000 proposals by the 'fast' mode. For EdgeBoxes (EB) [6], we generate the proposals by the default EB setting tuned for 0.7\n",
      "\n",
      "IoU. SS has an mAP of 58.7% and EB has an mAP of 58.6% under the Fast R-CNN framework. RPN with Fast R-CNN achieves competitive results, with an mAP of 59.9% while using up to 300 proposals 8 . Using RPN yields a much faster detection system than using either SS or EB because of shared convolutional computations; the fewer proposals also reduce the region-wise fully-connected layers' cost (Table 5).\n",
      "\n",
      "Ablation Experiments on RPN. To investigate the behavior of RPNs as a proposal method, we conducted several ablation studies. First, we show the effect of sharing convolutional layers between the RPN and Fast R-CNN detection network. To do this, we stop after the second step in the 4-step training process. Using separate networks reduces the result slightly to 58.7% (RPN+ZF, unshared, Table 2). We observe that this is because in the third step when the detectortuned features are used to fine-tune the RPN, the proposal quality is improved.\n",
      "\n",
      "Next, we disentangle the RPN's influence on training the Fast R-CNN detection network. For this purpose, we train a Fast R-CNN model by using the 2000 SS proposals and ZF net. We fix this detector and evaluate the detection mAP by changing the proposal regions used at test-time. In these ablation experiments, the RPN does not share features with the detector.\n",
      "\n",
      "Replacing SS with 300 RPN proposals at test-time leads to an mAP of 56.8%. The loss in mAP is because of the inconsistency between the training/testing proposals. This result serves as the baseline for the following comparisons.\n",
      "\n",
      "Somewhat surprisingly, the RPN still leads to a competitive result (55.1%) when using the top-ranked\n",
      "\n",
      "8. For RPN, the number of proposals ( e.g ., 300) is the maximum number for an image. RPN may produce fewer proposals after NMS, and thus the average number of proposals is smaller.\n",
      "\n",
      "Table 3: Detection results on PASCAL VOC 2007 test set . The detector is Fast R-CNN and VGG-16. Training data: '07': VOC 2007 trainval, '07+12': union set of VOC 2007 trainval and VOC 2012 trainval. For RPN, the train-time proposals for Fast R-CNN are 2000. † : this number was reported in [2]; using the repository provided by this paper, this result is higher (68.1).\n",
      "\n",
      "| method            |   # proposals | data       | mAP (%)   |\n",
      "|-------------------|---------------|------------|-----------|\n",
      "| SS                |          2000 | 07         | 66.9 †    |\n",
      "| SS                |          2000 | 07+12      | 70.0      |\n",
      "| RPN+VGG, unshared |           300 | 07         | 68.5      |\n",
      "| RPN+VGG, shared   |           300 | 07         | 69.9      |\n",
      "| RPN+VGG, shared   |           300 | 07+12      | 73.2      |\n",
      "| RPN+VGG, shared   |           300 | COCO+07+12 | 78.8      |\n",
      "\n",
      "Table 4: Detection results on PASCAL VOC 2012 test set . The detector is Fast R-CNN and VGG-16. Training data: '07': VOC 2007 trainval, '07++12': union set of VOC 2007 trainval+test and VOC 2012 trainval. For RPN, the train-time proposals for Fast R-CNN are 2000. † : http://host.robots.ox.ac.uk:8080/anonymous/HZJTQA.html. ‡ : http://host.robots.ox.ac.uk:8080/anonymous/YNPLXB.html. § : http://host.robots.ox.ac.uk:8080/anonymous/XEDH10.html.\n",
      "\n",
      "| method            |   # proposals | data        |   mAP (%) |\n",
      "|-------------------|---------------|-------------|-----------|\n",
      "| SS                |          2000 | 12          |      65.7 |\n",
      "| SS                |          2000 | 07++12      |      68.4 |\n",
      "| RPN+VGG, shared † |           300 | 12          |      67   |\n",
      "| RPN+VGG, shared ‡ |           300 | 07++12      |      70.4 |\n",
      "| RPN+VGG, shared § |           300 | COCO+07++12 |      75.9 |\n",
      "\n",
      "Table 5: Timing (ms) on a K40 GPU, except SS proposal is evaluated in a CPU. 'Region-wise' includes NMS, pooling, fully-connected, and softmax layers. See our released code for the profiling of running time.\n",
      "\n",
      "| model   | system           |   conv |   proposal |   region-wise |   total | rate    |\n",
      "|---------|------------------|--------|------------|---------------|---------|---------|\n",
      "| VGG     | SS + Fast R-CNN  |    146 |       1510 |           174 |    1830 | 0.5 fps |\n",
      "| VGG     | RPN + Fast R-CNN |    141 |         10 |            47 |     198 | 5 fps   |\n",
      "| ZF      | RPN + Fast R-CNN |     31 |          3 |            25 |      59 | 17 fps  |\n",
      "\n",
      "100 proposals at test-time, indicating that the topranked RPN proposals are accurate. On the other extreme, using the top-ranked 6000 RPN proposals (without NMS) has a comparable mAP (55.2%), suggesting NMS does not harm the detection mAP and may reduce false alarms.\n",
      "\n",
      "Next, we separately investigate the roles of RPN's cls and reg outputs by turning off either of them at test-time. When the cls layer is removed at testtime (thus no NMS/ranking is used), we randomly sample N proposals from the unscored regions. The mAP is nearly unchanged with N = 1000 (55.8%), but degrades considerably to 44.6% when N = 100 . This shows that the cls scores account for the accuracy of the highest ranked proposals.\n",
      "\n",
      "On the other hand, when the reg layer is removed at test-time (so the proposals become anchor boxes), the mAP drops to 52.1%. This suggests that the highquality proposals are mainly due to the regressed box bounds. The anchor boxes, though having multiple scales and aspect ratios, are not sufficient for accurate detection.\n",
      "\n",
      "We also evaluate the effects of more powerful networks on the proposal quality of RPN alone. We use VGG-16 to train the RPN, and still use the above detector of SS+ZF. The mAP improves from 56.8%\n",
      "\n",
      "(using RPN+ZF) to 59.2% (using RPN+VGG). This is a promising result, because it suggests that the proposal quality of RPN+VGG is better than that of RPN+ZF. Because proposals of RPN+ZF are competitive with SS (both are 58.7% when consistently used for training and testing), we may expect RPN+VGG to be better than SS. The following experiments justify this hypothesis.\n",
      "\n",
      "Performance of VGG-16. Table 3 shows the results of VGG-16 for both proposal and detection. Using RPN+VGG, the result is 68.5% for unshared features, slightly higher than the SS baseline. As shown above, this is because the proposals generated by RPN+VGG are more accurate than SS. Unlike SS that is predefined, the RPN is actively trained and benefits from better networks. For the featureshared variant, the result is 69.9%-better than the strong SS baseline, yet with nearly cost-free proposals. We further train the RPN and detection network on the union set of PASCAL VOC 2007 trainval and 2012 trainval. The mAP is 73.2% . Figure 5 shows some results on the PASCAL VOC 2007 test set. On the PASCAL VOC 2012 test set (Table 4), our method has an mAP of 70.4% trained on the union set of VOC 2007 trainval+test and VOC 2012 trainval. Table 6 and Table 7 show the detailed numbers.\n",
      "\n",
      "Table 6: Results on PASCAL VOC 2007 test set with Fast R-CNN detectors and VGG-16. For RPN, the train-time proposals for Fast R-CNN are 2000. RPN ∗ denotes the unsharing feature version.\n",
      "\n",
      "| method   |   # box | data       |   mAP |   areo |   bike |   bird |   boat |   bottle |   bus |   car |   cat |   chair |   cow |   table |   dog |   horse | mbike     |   person |   plant |   sheep |   sofa |   train tv |\n",
      "|----------|---------|------------|-------|--------|--------|--------|--------|----------|-------|-------|-------|---------|-------|---------|-------|---------|-----------|----------|---------|---------|--------|------------|\n",
      "| SS       |    2000 | 07         |  66.9 |   74.5 |   78.3 |   69.2 |   53.2 |     36.6 |  77.3 |  78.2 |  82   |    40.7 |  72.7 |    67.9 |  79.6 |    79.2 | 73.0 69.0 |     30.1 |    65.4 |    70.2 |   75.8 |       65.8 |\n",
      "| SS       |    2000 | 07+12      |  70   |   77   |   78.1 |   69.3 |   59.4 |     38.3 |  81.6 |  78.6 |  86.7 |    42.8 |  78.8 |    68.9 |  84.7 |    82   | 76.6 69.9 |     31.8 |    70.1 |    74.8 |   80.4 |       70.4 |\n",
      "| RPN ∗    |     300 | 07         |  68.5 |   74.1 |   77.2 |   67.7 |   53.9 |     51   |  75.1 |  79.2 |  78.9 |    50.7 |  78   |    61.1 |  79.1 |    81.9 | 72.2 75.9 |     37.2 |    71.4 |    62.5 |   77.4 |       66.4 |\n",
      "| RPN      |     300 | 07         |  69.9 |   70   |   80.6 |   70.1 |   57.3 |     49.9 |  78.2 |  80.4 |  82   |    52.2 |  75.3 |    67.2 |  80.3 |    79.8 | 75.0 76.3 |     39.1 |    68.3 |    67.3 |   81.1 |       67.6 |\n",
      "| RPN      |     300 | 07+12      |  73.2 |   76.5 |   79   |   70.9 |   65.5 |     52.1 |  83.1 |  84.7 |  86.4 |    52   |  81.9 |    65.7 |  84.8 |    84.6 | 77.5 76.7 |     38.8 |    73.6 |    73.9 |   83   |       72.6 |\n",
      "| RPN      |     300 | COCO+07+12 |  78.8 |   84.3 |   82   |   77.7 |   68.9 |     65.7 |  88.1 |  88.4 |  88.9 |    63.6 |  86.3 |    70.8 |  85.9 |    87.6 | 80.1 82.3 |     53.6 |    80.4 |    75.8 |   86.6 |       78.9 |\n",
      "\n",
      "Table 7: Results on PASCAL VOC 2012 test set with Fast R-CNN detectors and VGG-16. For RPN, the train-time proposals for Fast R-CNN are 2000.\n",
      "\n",
      "| method   |   # box | data        |   mAP |   areo |   bike |   bird |   boat |   bottle |   bus |   car |   cat |   chair |   cow |   table |   dog |   horse | mbike     | person    |   plant |   sheep | sofa train tv   |\n",
      "|----------|---------|-------------|-------|--------|--------|--------|--------|----------|-------|-------|-------|---------|-------|---------|-------|---------|-----------|-----------|---------|---------|-----------------|\n",
      "| SS       |    2000 | 12          |  65.7 |   80.3 |   74.7 |   66.9 |   46.9 |     37.7 |  73.9 |  68.6 |  87.7 |    41.7 |  71.1 |    51.1 |  86   |    77.8 | 79.8      | 69.8 32.1 |    65.5 |    63.8 | 76.4 61.7       |\n",
      "| SS       |    2000 | 07++12      |  68.4 |   82.3 |   78.4 |   70.8 |   52.3 |     38.7 |  77.8 |  71.6 |  89.3 |    44.2 |  73   |    55   |  87.5 |    80.5 | 80.8      | 72.0 35.1 |    68.3 |    65.7 | 80.4 64.2       |\n",
      "| RPN      |     300 | 12          |  67   |   82.3 |   76.4 |   71   |   48.4 |     45.2 |  72.1 |  72.3 |  87.3 |    42.2 |  73.7 |    50   |  86.8 |    78.7 | 78.4 77.4 | 34.5      |    70.1 |    57.1 | 77.1 58.9       |\n",
      "| RPN      |     300 | 07++12      |  70.4 |   84.9 |   79.8 |   74.3 |   53.9 |     49.8 |  77.5 |  75.9 |  88.5 |    45.6 |  77.1 |    55.3 |  86.9 |    81.7 | 80.9 79.6 | 40.1      |    72.6 |    60.9 | 81.2 61.5       |\n",
      "| RPN      |     300 | COCO+07++12 |  75.9 |   87.4 |   83.6 |   76.8 |   62.9 |     59.6 |  81.9 |  82   |  91.3 |    54.9 |  82.6 |    59   |  89   |    85.5 | 84.7 84.1 | 52.2      |    78.9 |    65.5 | 85.4 70.2       |\n",
      "\n",
      "Table 8: Detection results of Faster R-CNN on PASCAL VOC 2007 test set using different settings of anchors . The network is VGG-16. The training data is VOC 2007 trainval. The default setting of using 3 scales and 3 aspect ratios (69.9%) is the same as that in Table 3.\n",
      "\n",
      "| settings           | anchor scales             | aspect ratios                       | mAP (%)   |\n",
      "|--------------------|---------------------------|-------------------------------------|-----------|\n",
      "| 1 scale, 1 ratio   | 128 2 256 2               | 1:1 1:1                             | 65.8 66.7 |\n",
      "| 1 scale, 3 ratios  | 128 2 256 2               | { 2:1, 1:1, 1:2 } { 2:1, 1:1, 1:2 } | 68.8 67.9 |\n",
      "| 3 scales, 1 ratio  | { 128 2 , 256 2 , 512 2 } | 1:1                                 | 69.8      |\n",
      "| 3 scales, 3 ratios | { 128 2 , 256 2 , 512 2 } | { 2:1, 1:1, 1:2 }                   | 69.9      |\n",
      "\n",
      "Table 9: Detection results of Faster R-CNN on PASCAL VOC 2007 test set using different values of λ in Equation (1). The network is VGG-16. The training data is VOC 2007 trainval. The default setting of using λ = 10 (69.9%) is the same as that in Table 3.\n",
      "\n",
      "| λ       |   0.1 |    1 |   10 |   100 |\n",
      "|---------|-------|------|------|-------|\n",
      "| mAP (%) |  67.2 | 68.9 | 69.9 |  69.1 |\n",
      "\n",
      "In Table 5 we summarize the running time of the entire object detection system. SS takes 1-2 seconds depending on content (on average about 1.5s), and Fast R-CNN with VGG-16 takes 320ms on 2000 SS proposals (or 223ms if using SVD on fully-connected layers [2]). Our system with VGG-16 takes in total 198ms for both proposal and detection. With the convolutional features shared, the RPN alone only takes 10ms computing the additional layers. Our regionwise computation is also lower, thanks to fewer proposals (300 per image). Our system has a frame-rate of 17 fps with the ZF net.\n",
      "\n",
      "Sensitivities to Hyper-parameters. In Table 8 we investigate the settings of anchors. By default we use\n",
      "\n",
      "3 scales and 3 aspect ratios (69.9% mAP in Table 8). If using just one anchor at each position, the mAP drops by a considerable margin of 3-4%. The mAP is higher if using 3 scales (with 1 aspect ratio) or 3 aspect ratios (with 1 scale), demonstrating that using anchors of multiple sizes as the regression references is an effective solution. Using just 3 scales with 1 aspect ratio (69.8%) is as good as using 3 scales with 3 aspect ratios on this dataset, suggesting that scales and aspect ratios are not disentangled dimensions for the detection accuracy. But we still adopt these two dimensions in our designs to keep our system flexible.\n",
      "\n",
      "In Table 9 we compare different values of λ in Equation (1). By default we use λ = 10 which makes the two terms in Equation (1) roughly equally weighted after normalization. Table 9 shows that our result is impacted just marginally (by ∼ 1% ) when λ is within a scale of about two orders of magnitude (1 to 100). This demonstrates that the result is insensitive to λ in a wide range.\n",
      "\n",
      "Analysis of Recall-to-IoU. Next we compute the recall of proposals at different IoU ratios with groundtruth boxes. It is noteworthy that the Recall-to-IoU metric is just loosely [19], [20], [21] related to the ultimate detection accuracy. It is more appropriate to use this metric to diagnose the proposal method than to evaluate it.\n",
      "\n",
      "In Figure 4, we show the results of using 300, 1000, and 2000 proposals. We compare with SS and EB, and the N proposals are the topN ranked ones based on the confidence generated by these methods. The plots show that the RPN method behaves gracefully when the number of proposals drops from 2000 to 300. This explains why the RPN has a good ultimate detection mAP when using as few as 300 proposals. As we analyzed before, this property is mainly attributed to the cls term of the RPN. The recall of SS and EB drops more quickly than RPN when the proposals are fewer.\n",
      "\n",
      "Figure 4: Recall vs . IoU overlap ratio on the PASCAL VOC 2007 test set.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Table 10: One-Stage Detection vs . Two-Stage Proposal + Detection . Detection results are on the PASCAL VOC 2007 test set using the ZF model and Fast R-CNN. RPN uses unshared features.\n",
      "\n",
      "|           | proposals                        |   proposals | detector                  |   mAP (%) |\n",
      "|-----------|----------------------------------|-------------|---------------------------|-----------|\n",
      "| Two-Stage | RPN + ZF, unshared               |         300 | Fast R-CNN + ZF, 1 scale  |      58.7 |\n",
      "| One-Stage | dense, 3 scales, 3 aspect ratios |       20000 | Fast R-CNN + ZF, 1 scale  |      53.8 |\n",
      "| One-Stage | dense, 3 scales, 3 aspect ratios |       20000 | Fast R-CNN + ZF, 5 scales |      53.9 |\n",
      "\n",
      "One-Stage Detection vs . Two-Stage Proposal + Detection. The OverFeat paper [9] proposes a detection method that uses regressors and classifiers on sliding windows over convolutional feature maps. OverFeat is a one-stage , class-specific detection pipeline, and ours is a two-stage cascade consisting of class-agnostic proposals and class-specific detections. In OverFeat, the region-wise features come from a sliding window of one aspect ratio over a scale pyramid. These features are used to simultaneously determine the location and category of objects. In RPN, the features are from square (3 × 3) sliding windows and predict proposals relative to anchors with different scales and aspect ratios. Though both methods use sliding windows, the region proposal task is only the first stage of Faster RCNN-the downstream Fast R-CNN detector attends to the proposals to refine them. In the second stage of our cascade, the region-wise features are adaptively pooled [1], [2] from proposal boxes that more faithfully cover the features of the regions. We believe these features lead to more accurate detections.\n",
      "\n",
      "To compare the one-stage and two-stage systems, we emulate the OverFeat system (and thus also circumvent other differences of implementation details) by one-stage Fast R-CNN. In this system, the 'proposals' are dense sliding windows of 3 scales (128, 256, 512) and 3 aspect ratios (1:1, 1:2, 2:1). Fast R-CNN is trained to predict class-specific scores and regress box locations from these sliding windows. Because the OverFeat system adopts an image pyramid, we also evaluate using convolutional features extracted from 5 scales. We use those 5 scales as in [1], [2].\n",
      "\n",
      "Table 10 compares the two-stage system and two variants of the one-stage system. Using the ZF model, the one-stage system has an mAP of 53.9%. This is lower than the two-stage system (58.7%) by 4.8%. This experiment justifies the effectiveness of cascaded region proposals and object detection. Similar observations are reported in [2], [39], where replacing SS\n",
      "\n",
      "region proposals with sliding windows leads to ∼ 6% degradation in both papers. We also note that the onestage system is slower as it has considerably more proposals to process.\n",
      "\n",
      "## 4.2 Experiments on MS COCO\n",
      "\n",
      "We present more results on the Microsoft COCO object detection dataset [12]. This dataset involves 80 object categories. We experiment with the 80k images on the training set, 40k images on the validation set, and 20k images on the test-dev set. We evaluate the mAP averaged for IoU ∈ [0 . 5 : 0 . 05 : 0 . 95] (COCO's standard metric, simply denoted as mAP@[.5, .95]) and mAP@0.5 (PASCAL VOC's metric).\n",
      "\n",
      "There are a few minor changes of our system made for this dataset. We train our models on an 8-GPU implementation, and the effective mini-batch size becomes 8 for RPN (1 per GPU) and 16 for Fast R-CNN (2 per GPU). The RPN step and Fast R-CNN step are both trained for 240k iterations with a learning rate of 0.003 and then for 80k iterations with 0.0003. We modify the learning rates (starting with 0.003 instead of 0.001) because the mini-batch size is changed. For the anchors, we use 3 aspect ratios and 4 scales (adding 64 2 ), mainly motivated by handling small objects on this dataset. In addition, in our Fast R-CNN step, the negative samples are defined as those with a maximum IoU with ground truth in the interval of [0 , 0 . 5) , instead of [0 . 1 , 0 . 5) used in [1], [2]. We note that in the SPPnet system [1], the negative samples in [0 . 1 , 0 . 5) are used for network fine-tuning, but the negative samples in [0 , 0 . 5) are still visited in the SVM step with hard-negative mining. But the Fast R-CNN system [2] abandons the SVM step, so the negative samples in [0 , 0 . 1) are never visited. Including these [0 , 0 . 1) samples improves mAP@0.5 on the COCO dataset for both Fast R-CNN and Faster R-CNN systems (but the impact is negligible on PASCAL VOC).\n",
      "\n",
      "Table 11: Object detection results (%) on the MS COCO dataset. The model is VGG-16.\n",
      "\n",
      "|                                  |           |               | COCO val   | COCO val      | COCO test-dev   | COCO test-dev   |\n",
      "|----------------------------------|-----------|---------------|------------|---------------|-----------------|-----------------|\n",
      "| method                           | proposals | training data | mAP@.5     | mAP@[.5, .95] | mAP@.5          | mAP@[.5, .95]   |\n",
      "| Fast R-CNN [2]                   | SS, 2000  | COCO train    | -          | -             | 35.9            | 19.7            |\n",
      "| Fast R-CNN [impl. in this paper] | SS, 2000  | COCO train    | 38.6       | 18.9          | 39.3            | 19.3            |\n",
      "| Faster R-CNN                     | RPN, 300  | COCO train    | 41.5       | 21.2          | 42.1            | 21.5            |\n",
      "| Faster R-CNN                     | RPN, 300  | COCO trainval | -          | -             | 42.7            | 21.9            |\n",
      "\n",
      "The rest of the implementation details are the same as on PASCAL VOC. In particular, we keep using 300 proposals and single-scale ( s = 600 ) testing. The testing time is still about 200ms per image on the COCO dataset.\n",
      "\n",
      "In Table 11 we first report the results of the Fast R-CNN system [2] using the implementation in this paper. Our Fast R-CNN baseline has 39.3% mAP@0.5 on the test-dev set, higher than that reported in [2]. We conjecture that the reason for this gap is mainly due to the definition of the negative samples and also the changes of the mini-batch sizes. We also note that the mAP@[.5, .95] is just comparable.\n",
      "\n",
      "Next we evaluate our Faster R-CNN system. Using the COCO training set to train, Faster R-CNN has 42.1% mAP@0.5 and 21.5% mAP@[.5, .95] on the COCO test-dev set. This is 2.8% higher for mAP@0.5 and 2.2% higher for mAP@[.5, .95] than the Fast RCNN counterpart under the same protocol (Table 11). This indicates that RPN performs excellent for improving the localization accuracy at higher IoU thresholds. Using the COCO trainval set to train, Faster RCNNhas 42.7% mAP@0.5 and 21.9% mAP@[.5, .95] on the COCO test-dev set. Figure 6 shows some results on the MS COCO test-dev set.\n",
      "\n",
      "Faster R-CNN in ILSVRC &amp; COCO 2015 competitions We have demonstrated that Faster R-CNN benefits more from better features, thanks to the fact that the RPN completely learns to propose regions by neural networks. This observation is still valid even when one increases the depth substantially to over 100 layers [18]. Only by replacing VGG-16 with a 101layer residual net (ResNet-101) [18], the Faster R-CNN system increases the mAP from 41.5%/21.2% (VGG16) to 48.4%/27.2% (ResNet-101) on the COCO val set. With other improvements orthogonal to Faster RCNN, He et al . [18] obtained a single-model result of 55.7%/34.9% and an ensemble result of 59.0%/37.4% on the COCO test-dev set, which won the 1st place in the COCO 2015 object detection competition. The same system [18] also won the 1st place in the ILSVRC 2015 object detection competition, surpassing the second place by absolute 8.5%. RPN is also a building block of the 1st-place winning entries in ILSVRC 2015 localization and COCO 2015 segmentation competitions, for which the details are available in [18] and [15] respectively.\n",
      "\n",
      "Table 12: Detection mAP (%) of Faster R-CNN on PASCAL VOC 2007 test set and 2012 test set using different training data. The model is VGG-16. 'COCO' denotes that the COCO trainval set is used for training. See also Table 6 and Table 7.\n",
      "\n",
      "| training data   | 2007 test   | 2012 test   |\n",
      "|-----------------|-------------|-------------|\n",
      "| VOC07           | 69.9        | 67.0        |\n",
      "| VOC07+12        | 73.2        | -           |\n",
      "| VOC07++12       | -           | 70.4        |\n",
      "| COCO (no VOC)   | 76.1        | 73.0        |\n",
      "| COCO+VOC07+12   | 78.8        | -           |\n",
      "| COCO+VOC07++12  | -           | 75.9        |\n",
      "\n",
      "## 4.3 From MS COCO to PASCAL VOC\n",
      "\n",
      "Large-scale data is of crucial importance for improving deep neural networks. Next, we investigate how the MS COCO dataset can help with the detection performance on PASCAL VOC.\n",
      "\n",
      "As a simple baseline, we directly evaluate the COCO detection model on the PASCAL VOC dataset, without fine-tuning on any PASCAL VOC data . This evaluation is possible because the categories on COCO are a superset of those on PASCAL VOC. The categories that are exclusive on COCO are ignored in this experiment, and the softmax layer is performed only on the 20 categories plus background. The mAP under this setting is 76.1% on the PASCAL VOC 2007 test set (Table 12). This result is better than that trained on VOC07+12 (73.2%) by a good margin, even though the PASCAL VOC data are not exploited.\n",
      "\n",
      "Then we fine-tune the COCO detection model on the VOC dataset. In this experiment, the COCO model is in place of the ImageNet-pre-trained model (that is used to initialize the network weights), and the Faster R-CNN system is fine-tuned as described in Section 3.2. Doing so leads to 78.8% mAP on the PASCAL VOC 2007 test set. The extra data from the COCO set increases the mAP by 5.6%. Table 6 shows that the model trained on COCO+VOC has the best AP for every individual category on PASCAL VOC 2007. Similar improvements are observed on the PASCAL VOC 2012 test set (Table 12 and Table 7). We note that the test-time speed of obtaining these strong results is still about 200ms per image .\n",
      "\n",
      "## 5 CONCLUSION\n",
      "\n",
      "We have presented RPNs for efficient and accurate region proposal generation. By sharing convolutional\n",
      "\n",
      "person : 0.918\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "cow : 0.995\n",
      "\n",
      "Figure 5: Selected examples of object detection results on the PASCAL VOC 2007 test set using the Faster R-CNN system. The model is VGG-16 and the training data is 07+12 trainval (73.2% mAP on the 2007 test set). Our method detects objects of a wide range of scales and aspect ratios. Each output box is associated with a category label and a softmax score in [0 , 1] . A score threshold of 0.6 is used to display these images. The running time for obtaining these results is 198ms per image, including all steps .\n",
      "\n",
      "features with the down-stream detection network, the region proposal step is nearly cost-free. Our method enables a unified, deep-learning-based object detection system to run at near real-time frame rates. The learned RPN also improves region proposal quality and thus the overall object detection accuracy.\n",
      "\n",
      "## REFERENCES\n",
      "\n",
      "- [1] K. He, X. Zhang, S. Ren, and J. Sun, 'Spatial pyramid pooling in deep convolutional networks for visual recognition,' in European Conference on Computer Vision (ECCV) , 2014.\n",
      "- [2] R. Girshick, 'Fast R-CNN,' in IEEE International Conference on Computer Vision (ICCV) , 2015.\n",
      "- [3] K. Simonyan and A. Zisserman, 'Very deep convolutional\n",
      "\n",
      "Figure 6: Selected examples of object detection results on the MS COCO test-dev set using the Faster R-CNN system. The model is VGG-16 and the training data is COCO trainval (42.7% mAP@0.5 on the test-dev set). Each output box is associated with a category label and a softmax score in [0 , 1] . A score threshold of 0.6 is used to display these images. For each image, one color represents one object category in that image.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "networks for large-scale image recognition,' in International Conference on Learning Representations (ICLR) , 2015.\n",
      "\n",
      "- [4] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders, 'Selective search for object recognition,' International Journal of Computer Vision (IJCV) , 2013.\n",
      "- [5] R. Girshick, J. Donahue, T. Darrell, and J. Malik, 'Rich feature hierarchies for accurate object detection and semantic segmentation,' in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2014.\n",
      "- [6] C. L. Zitnick and P. Doll´ ar, 'Edge boxes: Locating object proposals from edges,' in European Conference on Computer Vision (ECCV) , 2014.\n",
      "- [7] J. Long, E. Shelhamer, and T. Darrell, 'Fully convolutional networks for semantic segmentation,' in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015.\n",
      "- [8] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan, 'Object detection with discriminatively trained partbased models,' IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) , 2010.\n",
      "- [9] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun, 'Overfeat: Integrated recognition, localization and detection using convolutional networks,' in International Conference on Learning Representations (ICLR) , 2014.\n",
      "- [10] S. Ren, K. He, R. Girshick, and J. Sun, 'Faster R-CNN: Towards\n",
      "\n",
      "real-time object detection with region proposal networks,' in Neural Information Processing Systems (NIPS) , 2015.\n",
      "\n",
      "- [11] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, 'The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results,' 2007.\n",
      "- [12] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll´ ar, and C. L. Zitnick, 'Microsoft COCO: Common Objects in Context,' in European Conference on Computer Vision (ECCV) , 2014.\n",
      "- [13] S. Song and J. Xiao, 'Deep sliding shapes for amodal 3d object detection in rgb-d images,' arXiv:1511.02300 , 2015.\n",
      "- [14] J. Zhu, X. Chen, and A. L. Yuille, 'DeePM: A deep part-based model for object detection and semantic part localization,' arXiv:1511.07131 , 2015.\n",
      "- [15] J. Dai, K. He, and J. Sun, 'Instance-aware semantic segmentation via multi-task network cascades,' arXiv:1512.04412 , 2015.\n",
      "- [16] J. Johnson, A. Karpathy, and L. Fei-Fei, 'Densecap: Fully convolutional localization networks for dense captioning,' arXiv:1511.07571 , 2015.\n",
      "- [17] D. Kislyuk, Y. Liu, D. Liu, E. Tzeng, and Y. Jing, 'Human curation and convnets: Powering item-to-item recommendations on pinterest,' arXiv:1511.04003 , 2015.\n",
      "- [18] K. He, X. Zhang, S. Ren, and J. Sun, 'Deep residual learning for image recognition,' arXiv:1512.03385 , 2015.\n",
      "- [19] J. Hosang, R. Benenson, and B. Schiele, 'How good are detection proposals, really?' in British Machine Vision Conference (BMVC) , 2014.\n",
      "- [20] J. Hosang, R. Benenson, P. Doll´ ar, and B. Schiele, 'What makes for effective detection proposals?' IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) , 2015.\n",
      "- [21] N. Chavali, H. Agrawal, A. Mahendru, and D. Batra, 'Object-Proposal Evaluation Protocol is 'Gameable',' arXiv: 1505.05836 , 2015.\n",
      "- [22] J. Carreira and C. Sminchisescu, 'CPMC: Automatic object segmentation using constrained parametric min-cuts,' IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) , 2012.\n",
      "- [23] P. Arbel´ aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik, 'Multiscale combinatorial grouping,' in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2014.\n",
      "- [24] B. Alexe, T. Deselaers, and V. Ferrari, 'Measuring the objectness of image windows,' IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) , 2012.\n",
      "- [25] C. Szegedy, A. Toshev, and D. Erhan, 'Deep neural networks for object detection,' in Neural Information Processing Systems (NIPS) , 2013.\n",
      "- [26] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, 'Scalable object detection using deep neural networks,' in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2014.\n",
      "- [27] C. Szegedy, S. Reed, D. Erhan, and D. Anguelov, 'Scalable, high-quality object detection,' arXiv:1412.1441 (v1) , 2015.\n",
      "- [28] P. O. Pinheiro, R. Collobert, and P. Dollar, 'Learning to segment object candidates,' in Neural Information Processing Systems (NIPS) , 2015.\n",
      "- [29] J. Dai, K. He, and J. Sun, 'Convolutional feature masking for joint object and stuff segmentation,' in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015.\n",
      "- [30] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, 'Object detection networks on convolutional feature maps,' arXiv:1504.06066 , 2015.\n",
      "- [31] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, 'Attention-based models for speech recognition,' in Neural Information Processing Systems (NIPS) , 2015.\n",
      "- [32] M. D. Zeiler and R. Fergus, 'Visualizing and understanding convolutional neural networks,' in European Conference on Computer Vision (ECCV) , 2014.\n",
      "- [33] V. Nair and G. E. Hinton, 'Rectified linear units improve restricted boltzmann machines,' in International Conference on Machine Learning (ICML) , 2010.\n",
      "- [34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, and A. Rabinovich, 'Going deeper with convolutions,' in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015.\n",
      "- [35] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel, 'Backpropagation applied to handwritten zip code recognition,' Neural computation , 1989.\n",
      "- [36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, 'ImageNet Large Scale Visual Recognition Challenge,' in International Journal of Computer Vision (IJCV) , 2015.\n",
      "- [37] A. Krizhevsky, I. Sutskever, and G. Hinton, 'Imagenet classification with deep convolutional neural networks,' in Neural Information Processing Systems (NIPS) , 2012.\n",
      "- [38] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell, 'Caffe: Convolutional architecture for fast feature embedding,' arXiv:1408.5093 , 2014.\n",
      "- [39] K. Lenc and A. Vedaldi, 'R-CNN minus R,' in British Machine Vision Conference (BMVC) , 2015.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 08:09:08,952 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:08,953 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:08,955 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:08,955 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:09,035 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:09,036 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:09,059 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:09,059 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 08:09:09,615 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 08:09:09,616 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:09:10,321 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:09:10,621 - INFO - Processing document 1802.05365v2.pdf\n",
      "2025-12-15 08:09:56,120 - INFO - Finished converting document 1802.05365v2.pdf in 47.47 sec.\n",
      "2025-12-15 08:09:56,258 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-15 08:09:56,261 - INFO - Going to convert document batch...\n",
      "2025-12-15 08:09:56,264 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-15 08:09:56,265 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-15 08:09:56,266 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-15 08:09:56,268 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:56,289 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:56,290 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:56,303 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:56,303 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Deep contextualized word representations\n",
      "\n",
      "Matthew E. Peters † , Mark Neumann † , Mohit Iyyer † , Matt Gardner † ,\n",
      "\n",
      "{ matthewp,markn,mohiti,mattg } @allenai.org\n",
      "\n",
      "Christopher Clark ∗ , Kenton Lee ∗ , Luke Zettlemoyer †∗\n",
      "\n",
      "{ csquared,kentonl,lsz } @cs.washington.edu\n",
      "\n",
      "†\n",
      "\n",
      "Paul G. Allen School of Computer Science &amp; Engineering, University of Washington\n",
      "\n",
      "Allen Institute for Artificial Intelligence ∗\n",
      "\n",
      "## Abstract\n",
      "\n",
      "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014) are a key component in many neural language understanding models. However, learning high quality representations can be challenging. They should ideally model both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). In this paper, we introduce a new type of deep contextualized word representation that directly addresses both challenges, can be easily integrated into existing models, and significantly improves the state of the art in every considered case across a range of challenging language understanding problems.\n",
      "\n",
      "Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence. We use vectors derived from a bidirectional LSTM that is trained with a coupled lan- guage model (LM) objective on a large text corpus. For this reason, we call them ELMo (Embeddings from Language Models) representations. Unlike previous approaches for learning contextualized word vectors (Peters et al., 2017; McCann et al., 2017), ELMo representations are deep, in the sense that they are a function of all of the internal layers of the biLM. More specifically, we learn a linear combination of the vectors stacked above each input word for each end task, which markedly improves performance over just using the top LSTM layer.\n",
      "\n",
      "Combining the internal states in this manner allows for very rich word representations. Using intrinsic evaluations, we show that the higher-level LSTM states capture context-dependent aspects of word meaning (e.g., they can be used without modification to perform well on supervised word sense disambiguation tasks) while lowerlevel states model aspects of syntax (e.g., they can be used to do part-of-speech tagging). Simultaneously exposing all of these signals is highly beneficial, allowing the learned models select the types of semi-supervision that are most useful for each end task.\n",
      "\n",
      "Extensive experiments demonstrate that ELMo representations work extremely well in practice. We first show that they can be easily added to existing models for six diverse and challenging language understanding problems, including textual entailment, question answering and sentiment analysis. The addition of ELMo representations alone significantly improves the state of the art in every case, including up to 20% relative error reductions. For tasks where direct comparisons are possible, ELMo outperforms CoVe (McCann et al., 2017), which computes contextualized representations using a neural machine translation encoder. Finally, an analysis of both ELMo and CoVe reveals that deep representations outperform those derived from just the top layer of an LSTM. Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems. 1\n",
      "\n",
      "## 2 Related work\n",
      "\n",
      "Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word.\n",
      "\n",
      "Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes.\n",
      "\n",
      "Other recent work has also focused on learning context-dependent representations. context2vec (Melamud et al., 2016) uses a bidirectional Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word. Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation (MT) system (CoVe; McCann et al., 2017) or an unsupervised language model (Peters et al., 2017). Both of these approaches benefit from large datasets, although the MT approach is limited by the size of parallel corpora. In this paper, we take full advantage of access to plentiful monolingual data, and train our biLM on a corpus with approximately 30 million sentences (Chelba et al., 2014). We also generalize these approaches to deep contextual representations, which we show work well across a broad range of diverse NLP tasks.\n",
      "\n",
      "1 http://allennlp.org/elmo\n",
      "\n",
      "Previous work has also shown that different layers of deep biRNNs encode different types of information. For example, introducing multi-task syntactic supervision (e.g., part-of-speech tags) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing (Hashimoto et al., 2017) or CCGsuper tagging (Søgaard and Goldberg, 2016). In an RNN-based encoder-decoder machine translation system, Belinkov et al. (2017) showed that the representations learned at the first layer in a 2layer LSTM encoder are better at predicting POS tags then second layer. Finally, the top layer of an LSTMfor encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense. We show that similar signals are also induced by the modified language model objective of our ELMo representations, and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision.\n",
      "\n",
      "Dai and Le (2015) and Ramachandran et al. (2017) pretrain encoder-decoder pairs using language models and sequence autoencoders and then fine tune with task specific supervision. In contrast, after pretraining the biLM with unlabeled data, we fix the weights and add additional taskspecific model capacity, allowing us to leverage large, rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model.\n",
      "\n",
      "## 3 ELMo: Embeddings from Language Models\n",
      "\n",
      "Unlike most widely used word embeddings (Pennington et al., 2014), ELMo word representations are functions of the entire input sentence, as described in this section. They are computed on top of two-layer biLMs with character convolutions (Sec. 3.1), as a linear function of the internal network states (Sec. 3.2). This setup allows us to do semi-supervised learning, where the biLM is pretrained at a large scale (Sec. 3.4) and easily incorporated into a wide range of existing neural NLP architectures (Sec. 3.3).\n",
      "\n",
      "## 3.1 Bidirectional language models\n",
      "\n",
      "Given a sequence of N tokens, ( t 1 , t 2 , ..., t N ) , a forward language model computes the probability of the sequence by modeling the probability of to- ken t k given the history ( t 1 , ..., t k -1 ) :\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Recent state-of-the-art neural language models (J´ ozefowicz et al., 2016; Melis et al., 2017; Merity et al., 2017) compute a context-independent token representation x LM k (via token embeddings or a CNN over characters) then pass it through L layers of forward LSTMs. At each position k , each LSTM layer outputs a context-dependent representation - → h LM k,j where j = 1 , . . . , L . The top layer LSTM output, - → h LM k,L , is used to predict the next token t k +1 with a Softmax layer.\n",
      "\n",
      "Abackward LM is similar to a forward LM, except it runs over the sequence in reverse, predicting the previous token given the future context:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "It can be implemented in an analogous way to a forward LM, with each backward LSTM layer j in a L layer deep model producing representations ← -h LM k,j of t k given ( t k +1 , . . . , t N ) .\n",
      "\n",
      "AbiLMcombines both a forward and backward LM. Our formulation jointly maximizes the log likelihood of the forward and backward directions:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "We tie the parameters for both the token representation ( Θ x ) and Softmax layer ( Θ s ) in the forward and backward direction while maintaining separate parameters for the LSTMs in each direction. Overall, this formulation is similar to the approach of Peters et al. (2017), with the exception that we share some weights between directions instead of using completely independent parameters. In the next section, we depart from previous work by introducing a new approach for learning word representations that are a linear combination of the biLM layers.\n",
      "\n",
      "## 3.2 ELMo\n",
      "\n",
      "ELMo is a task specific combination of the intermediate layer representations in the biLM. For each token t k , a L -layer biLM computes a set of 2 L +1 representations\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where h LM k, 0 is the token layer and h LM k,j = [ - → h LM k,j ; ← -h LM k,j ] , for each biLSTM layer.\n",
      "\n",
      "For inclusion in a downstream model, ELMo collapses all layers in R into a single vector, ELMo k = E ( R k ; Θ e ) . In the simplest case, ELMo just selects the top layer, E ( R k ) = h LM k,L , as in TagLM (Peters et al., 2017) and CoVe (McCann et al., 2017). More generally, we compute a task specific weighting of all biLM layers:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "In (1), s task are softmax-normalized weights and the scalar parameter γ task allows the task model to scale the entire ELMo vector. γ is of practical importance to aid the optimization process (see supplemental material for details). Considering that the activations of each biLM layer have a different distribution, in some cases it also helped to apply layer normalization (Ba et al., 2016) to each biLM layer before weighting.\n",
      "\n",
      "## 3.3 Using biLMs for supervised NLP tasks\n",
      "\n",
      "Given a pre-trained biLM and a supervised architecture for a target NLP task, it is a simple process to use the biLM to improve the task model. We simply run the biLM and record all of the layer representations for each word. Then, we let the end task model learn a linear combination of these representations, as described below.\n",
      "\n",
      "First consider the lowest layers of the supervised model without the biLM. Most supervised NLP models share a common architecture at the lowest layers, allowing us to add ELMo in a consistent, unified manner. Given a sequence of tokens ( t 1 , . . . , t N ) , it is standard to form a context-independent token representation x k for each token position using pre-trained word embeddings and optionally character-based representations. Then, the model forms a context-sensitive representation h k , typically using either bidirectional RNNs, CNNs, or feed forward networks.\n",
      "\n",
      "To add ELMo to the supervised model, we first freeze the weights of the biLM and then concatenate the ELMo vector ELMo task k with x k and pass the ELMo enhanced representation [ x k ; ELMo task k ] into the task RNN. For some tasks (e.g., SNLI, SQuAD), we observe further improvements by also including ELMo at the output of the task RNN by introducing another set of output specific linear weights and replacing h k with [ h k ; ELMo task k ] . As the remainder of the supervised model remains unchanged, these additions can happen within the context of more complex neural models. For example, see the SNLI experiments in Sec. 4 where a bi-attention layer follows the biLSTMs, or the coreference resolution experiments where a clustering model is layered on top of the biLSTMs.\n",
      "\n",
      "Finally, we found it beneficial to add a moderate amount of dropout to ELMo (Srivastava et al., 2014) and in some cases to regularize the ELMo weights by adding λ ‖ w ‖ 2 2 to the loss. This imposes an inductive bias on the ELMo weights to stay close to an average of all biLM layers.\n",
      "\n",
      "## 3.4 Pre-trained bidirectional language model architecture\n",
      "\n",
      "The pre-trained biLMs in this paper are similar to the architectures in J´ ozefowicz et al. (2016) and Kim et al. (2015), but modified to support joint training of both directions and add a residual connection between LSTM layers. We focus on large scale biLMs in this work, as Peters et al. (2017) highlighted the importance of using biLMs over forward-only LMs and large scale training.\n",
      "\n",
      "To balance overall language model perplexity with model size and computational requirements for downstream tasks while maintaining a purely character-based input representation, we halved all embedding and hidden dimensions from the single best model CNN-BIG-LSTM in J´ ozefowicz et al. (2016). The final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second layer. The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers (Srivastava et al., 2015) and a linear projection down to a 512 representation. As a result, the biLM provides three layers of representations for each input token, including those outside the training set due to the purely character input. In contrast, traditional word embedding methods only provide one layer of representation for tokens in a fixed vocabulary.\n",
      "\n",
      "After training for 10 epochs on the 1B Word Benchmark (Chelba et al., 2014), the average forward and backward perplexities is 39.7, compared to 30.0 for the forward CNN-BIG-LSTM . Generally, we found the forward and backward perplexities to be approximately equal, with the backward value slightly lower.\n",
      "\n",
      "Once pretrained, the biLM can compute representations for any task. In some cases, fine tuning the biLM on domain specific data leads to significant drops in perplexity and an increase in downstream task performance. This can be seen as a type of domain transfer for the biLM. As a result, in most cases we used a fine-tuned biLM in the downstream task. See supplemental material for details.\n",
      "\n",
      "## 4 Evaluation\n",
      "\n",
      "Table 1 shows the performance of ELMo across a diverse set of six benchmark NLP tasks. In every task considered, simply adding ELMo establishes a new state-of-the-art result, with relative error reductions ranging from 6 - 20% over strong base models. This is a very general result across a diverse set model architectures and language understanding tasks. In the remainder of this section we provide high-level sketches of the individual task results; see the supplemental material for full experimental details.\n",
      "\n",
      "Question answering The Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) contains 100K+ crowd sourced questionanswer pairs where the answer is a span in a given Wikipedia paragraph. Our baseline model (Clark and Gardner, 2017) is an improved version of the Bidirectional Attention Flow model in Seo et al. (BiDAF; 2017). It adds a self-attention layer after the bidirectional attention component, simplifies some of the pooling operations and substitutes the LSTMs for gated recurrent units (GRUs; Cho et al., 2014). After adding ELMo to the baseline model, test set F 1 improved by 4.7% from 81.1% to 85.8%, a 24.9% relative error reduction over the baseline, and improving the overall single model state-of-the-art by 1.4%. A 11 member ensemble pushes F 1 to 87.4, the overall state-of-the-art at time of submission to the leaderboard. 2 The increase of 4.7% with ELMo is also significantly larger then the 1.8% improvement from adding CoVe to a baseline model (McCann et al., 2017).\n",
      "\n",
      "2 As of November 17, 2017.\n",
      "\n",
      "Table 1: Test set comparison of ELMo enhanced neural models with state-of-the-art single model baselines across six benchmark NLP tasks. The performance metric varies across tasks - accuracy for SNLI and SST-5; F 1 for SQuAD, SRL and NER; average F 1 for Coref. Due to the small test sizes for NER and SST-5, we report the mean and standard deviation across five runs with different random seeds. The 'increase' column lists both the absolute and relative improvements over our baseline.\n",
      "\n",
      "| TASK   | PREVIOUS SOTA        | PREVIOUS SOTA   |   OUR BASELINE | ELMO + BASELINE   | INCREASE (ABSOLUTE/ RELATIVE)   |\n",
      "|--------|----------------------|-----------------|----------------|-------------------|---------------------------------|\n",
      "| SQuAD  | Liu et al. (2017)    | 84.4            |          81.1  | 85.8              | 4.7 / 24.9%                     |\n",
      "| SNLI   | Chen et al. (2017)   | 88.6            |          88    | 88.7 ± 0.17       | 0.7 / 5.8%                      |\n",
      "| SRL    | He et al. (2017)     | 81.7            |          81.4  | 84.6              | 3.2 / 17.2%                     |\n",
      "| Coref  | Lee et al. (2017)    | 67.2            |          67.2  | 70.4              | 3.2 / 9.8%                      |\n",
      "| NER    | Peters et al. (2017) | 91.93 ± 0.19    |          90.15 | 92.22 ± 0.10      | 2.06 / 21%                      |\n",
      "| SST-5  | McCann et al. (2017) | 53.7            |          51.4  | 54.7 ± 0.5        | 3.3 / 6.8%                      |\n",
      "\n",
      "Textual entailment Textual entailment is the task of determining whether a 'hypothesis' is true, given a 'premise'. The Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) provides approximately 550K hypothesis/premise pairs. Our baseline, the ESIM sequence model from Chen et al. (2017), uses a biLSTM to encode the premise and hypothesis, followed by a matrix attention layer, a local inference layer, another biLSTM inference composition layer, and finally a pooling operation before the output layer. Overall, adding ELMo to the ESIM model improves accuracy by an average of 0.7% across five random seeds. A five member ensemble pushes the overall accuracy to 89.3%, exceeding the previous ensemble best of 88.9% (Gong et al., 2018).\n",
      "\n",
      "Semantic role labeling A semantic role labeling (SRL) system models the predicate-argument structure of a sentence, and is often described as answering 'Who did what to whom'. He et al. (2017) modeled SRL as a BIO tagging problem and used an 8-layer deep biLSTM with forward and backward directions interleaved, following Zhou and Xu (2015). As shown in Table 1, when adding ELMo to a re-implementation of He et al. (2017) the single model test set F 1 jumped 3.2% from 81.4% to 84.6% - a new state-of-the-art on the OntoNotes benchmark (Pradhan et al., 2013), even improving over the previous best ensemble result by 1.2%.\n",
      "\n",
      "Coreference resolution Coreference resolution is the task of clustering mentions in text that refer to the same underlying real world entities. Our baseline model is the end-to-end span-based neural model of Lee et al. (2017). It uses a biLSTM\n",
      "\n",
      "and attention mechanism to first compute span representations and then applies a softmax mention ranking model to find coreference chains. In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task (Pradhan et al., 2012), adding ELMo improved the average F 1 by 3.2% from 67.2 to 70.4, establishing a new state of the art, again improving over the previous best ensemble result by 1.6% F 1 .\n",
      "\n",
      "Named entity extraction The CoNLL 2003 NER task (Sang and Meulder, 2003) consists of newswire from the Reuters RCV1 corpus tagged with four different entity types ( PER , LOC , ORG , MISC ). Following recent state-of-the-art systems (Lample et al., 2016; Peters et al., 2017), the baseline model uses pre-trained word embeddings, a character-based CNN representation, two biLSTM layers and a conditional random field (CRF) loss (Lafferty et al., 2001), similar to Collobert et al. (2011). As shown in Table 1, our ELMo enhanced biLSTM-CRF achieves 92.22% F 1 averaged over five runs. The key difference between our system and the previous state of the art from Peters et al. (2017) is that we allowed the task model to learn a weighted average of all biLM layers, whereas Peters et al. (2017) only use the top biLM layer. As shown in Sec. 5.1, using all layers instead of just the last layer improves performance across multiple tasks.\n",
      "\n",
      "Sentiment analysis The fine-grained sentiment classification task in the Stanford Sentiment Treebank (SST-5; Socher et al., 2013) involves selecting one of five labels (from very negative to very positive) to describe a sentence from a movie review. The sentences contain diverse linguistic phenomena such as idioms and complex syntac-\n",
      "\n",
      "Table 2: Development set performance for SQuAD, SNLI and SRL comparing using all layers of the biLM (with different choices of regularization strength λ ) to just the top layer.\n",
      "\n",
      "| Task   |   Baseline |   Last Only |   All layers λ =1 λ |   =0.001 |\n",
      "|--------|------------|-------------|---------------------|----------|\n",
      "| SQuAD  |       80.8 |        84.7 |                85   |     85.2 |\n",
      "| SNLI   |       88.1 |        89.1 |                89.3 |     89.5 |\n",
      "| SRL    |       81.6 |        84.1 |                84.6 |     84.8 |\n",
      "\n",
      "Table 3: Development set performance for SQuAD, SNLI and SRL when including ELMo at different locations in the supervised model.\n",
      "\n",
      "| Task   |   Input Only |   Input& Output |   Output Only |\n",
      "|--------|--------------|-----------------|---------------|\n",
      "| SQuAD  |         85.1 |            85.6 |          84.8 |\n",
      "| SNLI   |         88.9 |            89.5 |          88.7 |\n",
      "| SRL    |         84.7 |            84.3 |          80.9 |\n",
      "\n",
      "tic constructions such as negations that are difficult for models to learn. Our baseline model is the biattentive classification network (BCN) from McCann et al. (2017), which also held the prior state-of-the-art result when augmented with CoVe embeddings. Replacing CoVe with ELMo in the BCN model results in a 1.0% absolute accuracy improvement over the state of the art.\n",
      "\n",
      "## 5 Analysis\n",
      "\n",
      "This section provides an ablation analysis to validate our chief claims and to elucidate some interesting aspects of ELMo representations. Sec. 5.1 shows that using deep contextual representations in downstream tasks improves performance over previous work that uses just the top layer, regardless of whether they are produced from a biLM or MT encoder, and that ELMo representations provide the best overall performance. Sec. 5.3 explores the different types of contextual information captured in biLMs and uses two intrinsic evaluations to show that syntactic information is better represented at lower layers while semantic information is captured a higher layers, consistent with MT encoders. It also shows that our biLM consistently provides richer representations then CoVe. Additionally, we analyze the sensitivity to where ELMo is included in the task model (Sec. 5.2), training set size (Sec. 5.4), and visualize the ELMo learned weights across the tasks (Sec. 5.5).\n",
      "\n",
      "## 5.1 Alternate layer weighting schemes\n",
      "\n",
      "There are many alternatives to Equation 1 for combining the biLM layers. Previous work on contextual representations used only the last layer, whether it be from a biLM (Peters et al., 2017) or an MT encoder (CoVe; McCann et al., 2017). The choice of the regularization parameter λ is also important, as large values such as λ = 1 effectively reduce the weighting function to a simple average over the layers, while smaller values (e.g., λ = 0 . 001 ) allow the layer weights to vary.\n",
      "\n",
      "Table 2 compares these alternatives for SQuAD, SNLI and SRL. Including representations from all layers improves overall performance over just using the last layer, and including contextual representations from the last layer improves performance over the baseline. For example, in the case of SQuAD, using just the last biLM layer improves development F 1 by 3.9% over the baseline. Averaging all biLM layers instead of using just the last layer improves F 1 another 0.3% (comparing 'Last Only' to λ =1 columns), and allowing the task model to learn individual layer weights improves F 1 another 0.2% ( λ =1 vs. λ =0.001). A small λ is preferred in most cases with ELMo, although for NER, a task with a smaller training set, the results are insensitive to λ (not shown).\n",
      "\n",
      "The overall trend is similar with CoVe but with smaller increases over the baseline. For SNLI, averaging all layers with λ =1 improves development accuracy from 88.2 to 88.7% over using just the last layer. SRL F 1 increased a marginal 0.1% to 82.2 for the λ =1 case compared to using the last layer only.\n",
      "\n",
      "## 5.2 Where to include ELMo?\n",
      "\n",
      "All of the task architectures in this paper include word embeddings only as input to the lowest layer biRNN. However, we find that including ELMo at the output of the biRNN in task-specific architectures improves overall results for some tasks. As shown in Table 3, including ELMo at both the input and output layers for SNLI and SQuAD improves over just the input layer, but for SRL (and coreference resolution, not shown) performance is highest when it is included at just the input layer. One possible explanation for this result is that both the SNLI and SQuAD architectures use attention layers after the biRNN, so introducing ELMo at this layer allows the model to attend directly to the biLM's internal representations. In the SRL case,\n",
      "\n",
      "Table 4: Nearest neighbors to 'play' using GloVe and the context embeddings from a biLM.\n",
      "\n",
      "|       | Source                                                                | Nearest Neighbors                                                                                                                                                |\n",
      "|-------|-----------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| GloVe | play                                                                  | playing, game, games, played, players, plays, player, Play, football, multiplayer                                                                                |\n",
      "| biLM  | Chico Ruiz made a spec- tacular play on Alusik 's grounder { . . . }  | Kieffer , the only junior in the group , was commended for his ability to hit in the clutch , as well as his all-round excellent play .                          |\n",
      "| biLM  | Olivia De Havilland signed to do a Broadway play for Garson { . . . } | { . . . } they were actors who had been handed fat roles in a successful play , and had talent enough to fill the roles competently , with nice understatement . |\n",
      "\n",
      "Table 5: All-words fine grained WSD F 1 . For CoVe and the biLM, we report scores for both the first and second layer biLSTMs.\n",
      "\n",
      "| Model                      |   F 1 |\n",
      "|----------------------------|-------|\n",
      "| WordNet 1st Sense Baseline |  65.9 |\n",
      "| Raganato et al. (2017a)    |  69.9 |\n",
      "| Iacobacci et al. (2016)    |  70.1 |\n",
      "| CoVe, First Layer          |  59.4 |\n",
      "| CoVe, Second Layer         |  64.7 |\n",
      "| biLM, First layer          |  67.4 |\n",
      "| biLM, Second layer         |  69   |\n",
      "\n",
      "the task-specific context representations are likely more important than those from the biLM.\n",
      "\n",
      "## 5.3 What information is captured by the biLM's representations?\n",
      "\n",
      "Since adding ELMo improves task performance over word vectors alone, the biLM's contextual representations must encode information generally useful for NLP tasks that is not captured in word vectors. Intuitively, the biLM must be disambiguating the meaning of words using their context. Consider 'play', a highly polysemous word. The top of Table 4 lists nearest neighbors to 'play' using GloVe vectors. They are spread across several parts of speech (e.g., 'played', 'playing' as verbs, and 'player', 'game' as nouns) but concentrated in the sportsrelated senses of 'play'. In contrast, the bottom two rows show nearest neighbor sentences from the SemCor dataset (see below) using the biLM's context representation of 'play' in the source sentence. In these cases, the biLM is able to disambiguate both the part of speech and word sense in the source sentence.\n",
      "\n",
      "These observations can be quantified using an\n",
      "\n",
      "Table 6: Test set POS tagging accuracies for PTB. For CoVe and the biLM, we report scores for both the first and second layer biLSTMs.\n",
      "\n",
      "| Model                   |   Acc. |\n",
      "|-------------------------|--------|\n",
      "| Collobert et al. (2011) |   97.3 |\n",
      "| Ma and Hovy (2016)      |   97.6 |\n",
      "| Ling et al. (2015)      |   97.8 |\n",
      "| CoVe, First Layer       |   93.3 |\n",
      "| CoVe, Second Layer      |   92.8 |\n",
      "| biLM, First Layer       |   97.3 |\n",
      "| biLM, Second Layer      |   96.8 |\n",
      "\n",
      "intrinsic evaluation of the contextual representations similar to Belinkov et al. (2017). To isolate the information encoded by the biLM, the representations are used to directly make predictions for a fine grained word sense disambiguation (WSD) task and a POS tagging task. Using this approach, it is also possible to compare to CoVe, and across each of the individual layers.\n",
      "\n",
      "Word sense disambiguation Given a sentence, we can use the biLM representations to predict the sense of a target word using a simple 1nearest neighbor approach, similar to Melamud et al. (2016). To do so, we first use the biLM to compute representations for all words in SemCor 3.0, our training corpus (Miller et al., 1994), and then take the average representation for each sense. At test time, we again use the biLM to compute representations for a given target word and take the nearest neighbor sense from the training set, falling back to the first sense from WordNet for lemmas not observed during training.\n",
      "\n",
      "Table 5 compares WSD results using the evaluation framework from Raganato et al. (2017b) across the same suite of four test sets in Raganato et al. (2017a). Overall, the biLM top layer rep- resentations have F 1 of 69.0 and are better at WSD then the first layer. This is competitive with a state-of-the-art WSD-specific supervised model using hand crafted features (Iacobacci et al., 2016) and a task specific biLSTM that is also trained with auxiliary coarse-grained semantic labels and POS tags (Raganato et al., 2017a). The CoVe biLSTM layers follow a similar pattern to those from the biLM (higher overall performance at the second layer compared to the first); however, our biLM outperforms the CoVe biLSTM, which trails the WordNet first sense baseline.\n",
      "\n",
      "POS tagging To examine whether the biLM captures basic syntax, we used the context representations as input to a linear classifier that predicts POS tags with the Wall Street Journal portion of the Penn Treebank (PTB) (Marcus et al., 1993). As the linear classifier adds only a small amount of model capacity, this is direct test of the biLM's representations. Similar to WSD, the biLM representations are competitive with carefully tuned, task specific biLSTMs (Ling et al., 2015; Ma and Hovy, 2016). However, unlike WSD, accuracies using the first biLM layer are higher than the top layer, consistent with results from deep biLSTMs in multi-task training (Søgaard and Goldberg, 2016; Hashimoto et al., 2017) and MT (Belinkov et al., 2017). CoVe POS tagging accuracies follow the same pattern as those from the biLM, and just like for WSD, the biLM achieves higher accuracies than the CoVe encoder.\n",
      "\n",
      "Implications for supervised tasks Taken together, these experiments confirm different layers in the biLM represent different types of information and explain why including all biLM layers is important for the highest performance in downstream tasks. In addition, the biLM's representations are more transferable to WSD and POS tagging than those in CoVe, helping to illustrate why ELMo outperforms CoVe in downstream tasks.\n",
      "\n",
      "## 5.4 Sample efficiency\n",
      "\n",
      "Adding ELMo to a model increases the sample efficiency considerably, both in terms of number of parameter updates to reach state-of-the-art performance and the overall training set size. For example, the SRL model reaches a maximum development F 1 after 486 epochs of training without ELMo. After adding ELMo, the model exceeds the baseline maximum at epoch 10, a 98% relative decrease in the number of updates needed to reach\n",
      "\n",
      "Figure 1: Comparison of baseline vs. ELMo performance for SNLI and SRL as the training set size is varied from 0.1% to 100%.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Figure 2: Visualization of softmax normalized biLM layer weights across tasks and ELMo locations. Normalized weights less then 1 / 3 are hatched with horizontal lines and those greater then 2 / 3 are speckled.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "the same level of performance.\n",
      "\n",
      "In addition, ELMo-enhanced models use smaller training sets more efficiently than models without ELMo. Figure 1 compares the performance of baselines models with and without ELMo as the percentage of the full training set is varied from 0.1% to 100%. Improvements with ELMo are largest for smaller training sets and significantly reduce the amount of training data needed to reach a given level of performance. In the SRL case, the ELMo model with 1% of the training set has about the same F 1 as the baseline model with 10% of the training set.\n",
      "\n",
      "## 5.5 Visualization of learned weights\n",
      "\n",
      "Figure 2 visualizes the softmax-normalized learned layer weights. At the input layer, the task model favors the first biLSTM layer. For coreference and SQuAD, the this is strongly favored, but the distribution is less peaked for the other tasks. The output layer weights are relatively balanced, with a slight preference for the lower layers.\n",
      "\n",
      "## 6 Conclusion\n",
      "\n",
      "We have introduced a general approach for learning high-quality deep context-dependent representations from biLMs, and shown large improvements when applying ELMo to a broad range of NLPtasks. Through ablations and other controlled experiments, we have also confirmed that the biLM layers efficiently encode different types of syntactic and semantic information about wordsin-context, and that using all layers improves overall task performance.\n",
      "\n",
      "## References\n",
      "\n",
      "- Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization. CoRR abs/1607.06450.\n",
      "- Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James R. Glass. 2017. What do neural machine translation models learn about morphology? In ACL .\n",
      "- Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. TACL 5:135-146.\n",
      "- Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics.\n",
      "- Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2014. One billion word benchmark for measuring progress in statistical language modeling. In INTERSPEECH .\n",
      "- Qian Chen, Xiao-Dan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced lstm for natural language inference. In ACL .\n",
      "- Jason Chiu and Eric Nichols. 2016. Named entity recognition with bidirectional LSTM-CNNs. In TACL .\n",
      "- Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder approaches. In SSST@EMNLP .\n",
      "- Christopher Clark and Matthew Gardner. 2017. Simple and effective multi-paragraph reading comprehension. CoRR abs/1710.10723.\n",
      "- Kevin Clark and Christopher D. Manning. 2016. Deep reinforcement learning for mention-ranking coreference models. In EMNLP .\n",
      "- Ronan Collobert, Jason Weston, L´ eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. 2011. Natural language processing (almost) from scratch. In JMLR .\n",
      "- Andrew M. Dai and Quoc V. Le. 2015. Semisupervised sequence learning. In NIPS .\n",
      "- Greg Durrett and Dan Klein. 2013. Easy victories and uphill battles in coreference resolution. In EMNLP .\n",
      "- Yarin Gal and Zoubin Ghahramani. 2016. A theoretically grounded application of dropout in recurrent neural networks. In NIPS .\n",
      "- Yichen Gong, Heng Luo, and Jian Zhang. 2018. Natural language inference over interaction space. In ICLR .\n",
      "- Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. 2017. A joint many-task model: Growing a neural network for multiple nlp tasks. In EMNLP 2017 .\n",
      "- Luheng He, Kenton Lee, Mike Lewis, and Luke S. Zettlemoyer. 2017. Deep semantic role labeling: What works and what's next. In ACL .\n",
      "- Sepp Hochreiter and J¨ urgen Schmidhuber. 1997. Long short-term memory. Neural Computation 9.\n",
      "- Ignacio Iacobacci, Mohammad Taher Pilehvar, and Roberto Navigli. 2016. Embeddings for word sense disambiguation: An evaluation study. In ACL .\n",
      "- Rafal J´ ozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. 2016. Exploring the limits of language modeling. CoRR abs/1602.02410.\n",
      "- Rafal J´ ozefowicz, Wojciech Zaremba, and Ilya Sutskever. 2015. An empirical exploration of recurrent network architectures. In ICML .\n",
      "- Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. 2015. Character-aware neural language models. In AAAI 2016 .\n",
      "- Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR .\n",
      "- Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, Ishaan Gulrajani James Bradbury, Victor Zhong, Romain Paulus, and Richard Socher. 2016. Ask me anything: Dynamic memory networks for natural language processing. In ICML .\n",
      "- John D. Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML .\n",
      "- Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In NAACL-HLT .\n",
      "- Kenton Lee, Luheng He, Mike Lewis, and Luke S. Zettlemoyer. 2017. End-to-end neural coreference resolution. In EMNLP .\n",
      "- Wang Ling, Chris Dyer, Alan W. Black, Isabel Trancoso, Ramon Fermandez, Silvio Amir, Lu´ ıs Marujo, and Tiago Lu´ ıs. 2015. Finding function in form: Compositional character models for open vocabulary word representation. In EMNLP .\n",
      "- Xiaodong Liu, Yelong Shen, Kevin Duh, and Jianfeng Gao. 2017. Stochastic answer networks for machine reading comprehension. arXiv preprint arXiv:1712.03556 .\n",
      "- Xuezhe Ma and Eduard H. Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNsCRF. In ACL .\n",
      "- Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics 19:313-330.\n",
      "- Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Contextualized word vectors. In NIPS 2017 .\n",
      "- Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context embedding with bidirectional lstm. In CoNLL .\n",
      "- G´ abor Melis, Chris Dyer, and Phil Blunsom. 2017. On the state of the art of evaluation in neural language models. CoRR abs/1707.05589.\n",
      "- Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2017. Regularizing and optimizing lstm language models. CoRR abs/1708.02182.\n",
      "- Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS .\n",
      "- George A. Miller, Martin Chodorow, Shari Landes, Claudia Leacock, and Robert G. Thomas. 1994. Using a semantic concordance for sense identification. In HLT .\n",
      "- Tsendsuren Munkhdalai and Hong Yu. 2017. Neural tree indexers for text understanding. In EACL .\n",
      "- Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. 2014. Efficient nonparametric estimation of multiple embeddings per word in vector space. In EMNLP .\n",
      "- Martha Palmer, Paul Kingsbury, and Daniel Gildea. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics 31:71106.\n",
      "- Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In EMNLP .\n",
      "- Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL .\n",
      "- Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Bj¨ orkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. 2013. Towards robust linguistic analysis using ontonotes. In CoNLL .\n",
      "- Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes. In EMNLPCoNLL Shared Task .\n",
      "- Alessandro Raganato, Claudio Delli Bovi, and Roberto Navigli. 2017a. Neural sequence learning models for word sense disambiguation. In EMNLP .\n",
      "- Alessandro Raganato, Jose Camacho-Collados, and Roberto Navigli. 2017b. Word sense disambiguation: A unified evaluation framework and empirical comparison. In EACL .\n",
      "- Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. In EMNLP .\n",
      "- Prajit Ramachandran, Peter Liu, and Quoc Le. 2017. Improving sequence to sequence learning with unlabeled data. In EMNLP .\n",
      "- Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In CoNLL .\n",
      "- Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In ICLR .\n",
      "- Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP .\n",
      "- Anders Søgaard and Yoav Goldberg. 2016. Deep multi-task learning with low level tasks supervised at lower layers. In ACL 2016 .\n",
      "- Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research 15:1929-1958.\n",
      "- Rupesh Kumar Srivastava, Klaus Greff, and J¨ urgen Schmidhuber. 2015. Training very deep networks. In NIPS .\n",
      "- Joseph P. Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In ACL .\n",
      "- Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. 2017. Gated self-matching networks for reading comprehension and question answering. In ACL .\n",
      "- John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2016. Charagram: Embedding words and sentences via character n-grams. In EMNLP .\n",
      "- Sam Wiseman, Alexander M. Rush, and Stuart M. Shieber. 2016. Learning global features for coreference resolution. In HLT-NAACL .\n",
      "- Matthew D. Zeiler. 2012. Adadelta: An adaptive learning rate method. CoRR abs/1212.5701.\n",
      "- Jie Zhou and Wei Xu. 2015. End-to-end learning of semantic role labeling using recurrent neural networks. In ACL .\n",
      "- Peng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu, Hongyun Bao, and Bo Xu. 2016. Text classification improved by integrating bidirectional lstm with twodimensional max pooling. In COLING .\n",
      "\n",
      "## A Supplemental Material to accompany Deep contextualized word representations\n",
      "\n",
      "This supplement contains details of the model architectures, training routines and hyper-parameter choices for the state-of-the-art models in Section 4.\n",
      "\n",
      "All of the individual models share a common architecture in the lowest layers with a context independent token representation below several layers of stacked RNNs - LSTMs in every case except the SQuAD model that uses GRUs.\n",
      "\n",
      "## A.1 Fine tuning biLM\n",
      "\n",
      "As noted in Sec. 3.4, fine tuning the biLM on task specific data typically resulted in significant drops in perplexity. To fine tune on a given task, the supervised labels were temporarily ignored, the biLM fine tuned for one epoch on the training split and evaluated on the development split. Once fine tuned, the biLM weights were fixed during task training.\n",
      "\n",
      "Table 7 lists the development set perplexities for the considered tasks. In every case except CoNLL 2012, fine tuning results in a large improvement in perplexity, e.g., from 72.1 to 16.8 for SNLI.\n",
      "\n",
      "The impact of fine tuning on supervised performance is task dependent. In the case of SNLI, fine tuning the biLM increased development accuracy 0.6% from 88.9% to 89.5% for our single best model. However, for sentiment classification development set accuracy is approximately the same regardless whether a fine tuned biLM was used.\n",
      "\n",
      "## A.2 Importance of γ in Eqn. (1)\n",
      "\n",
      "The γ parameter in Eqn. (1) was of practical importance to aid optimization, due to the different distributions between the biLM internal representations and the task specific representations. It is especially important in the last-only case in Sec. 5.1. Without this parameter, the last-only case performed poorly (well below the baseline) for SNLI and training failed completely for SRL.\n",
      "\n",
      "## A.3 Textual Entailment\n",
      "\n",
      "Our baseline SNLI model is the ESIM sequence model from Chen et al. (2017). Following the original implementation, we used 300 dimensions for all LSTM and feed forward layers and pretrained 300 dimensional GloVe embeddings that were fixed during training. For regularization, we\n",
      "\n",
      "Table 7: Development set perplexity before and after fine tuning for one epoch on the training set for various datasets (lower is better). Reported values are the average of the forward and backward perplexities.\n",
      "\n",
      "| Dataset                | Dataset                |   Before tuning | After tuning   |\n",
      "|------------------------|------------------------|-----------------|----------------|\n",
      "| SNLI                   | SNLI                   |            72.1 | 16.8           |\n",
      "| CoNLL 2012 (coref/SRL) | CoNLL 2012 (coref/SRL) |            92.3 | -              |\n",
      "| CoNLL 2003 (NER)       | CoNLL 2003 (NER)       |           103.2 | 46.3           |\n",
      "| SQuAD                  | Context                |            99.1 | 43.5           |\n",
      "| SQuAD                  | Questions              |           158.2 | 52.0           |\n",
      "| SST                    | SST                    |           131.5 | 78.6           |\n",
      "\n",
      "added 50% variational dropout (Gal and Ghahramani, 2016) to the input of each LSTM layer and 50% dropout (Srivastava et al., 2014) at the input to the final two fully connected layers. All feed forward layers use ReLU activations. Parameters were optimized using Adam (Kingma and Ba, 2015) with gradient norms clipped at 5.0 and initial learning rate 0.0004, decreasing by half each time accuracy on the development set did not increase in subsequent epochs. The batch size was 32.\n",
      "\n",
      "The best ELMo configuration added ELMo vectors to both the input and output of the lowest layer LSTM, using (1) with layer normalization and λ = 0 . 001 . Due to the increased number of parameters in the ELMo model, we added glyph[lscript] 2 regularization with regularization coefficient 0.0001 to all recurrent and feed forward weight matrices and 50% dropout after the attention layer.\n",
      "\n",
      "Table 8 compares test set accuracy of our system to previously published systems. Overall, adding ELMo to the ESIM model improved accuracy by 0.7% establishing a new single model state-of-the-art of 88.7%, and a five member ensemble pushes the overall accuracy to 89.3%.\n",
      "\n",
      "## A.4 Question Answering\n",
      "\n",
      "Our QA model is a simplified version of the model from Clark and Gardner (2017). It embeds tokens by concatenating each token's case-sensitive 300 dimensional GloVe word vector (Pennington et al., 2014) with a character-derived embedding produced using a convolutional neural network followed by max-pooling on learned character embeddings. The token embeddings are passed through a shared bi-directional GRU, and then the bi-directional attention mechanism from BiDAF (Seo et al., 2017). The augmented con-\n",
      "\n",
      "Table 8: SNLI test set accuracy. 3 Single model results occupy the portion, with ensemble results at the bottom.\n",
      "\n",
      "| Model                               | Acc.        |\n",
      "|-------------------------------------|-------------|\n",
      "| Feature based (Bowman et al., 2015) | 78.2        |\n",
      "| DIIN (Gong et al., 2018)            | 88.0        |\n",
      "| BCN+Char+CoVe (McCann et al., 2017) | 88.1        |\n",
      "| ESIM (Chen et al., 2017)            | 88.0        |\n",
      "| ESIM+TreeLSTM (Chen et al., 2017)   | 88.6        |\n",
      "| ESIM+ELMo                           | 88.7 ± 0.17 |\n",
      "| DIIN ensemble (Gong et al., 2018)   | 88.9        |\n",
      "| ESIM+ELMo ensemble                  | 89.3        |\n",
      "\n",
      "text vectors are then passed through a linear layer with ReLU activations, a residual self-attention layer that uses a GRU followed by the same attention mechanism applied context-to-context, and another linear layer with ReLU activations. Finally, the results are fed through linear layers to predict the start and end token of the answer.\n",
      "\n",
      "Variational dropout is used before the input to the GRUs and the linear layers at a rate of 0.2. A dimensionality of 90 is used for the GRUs, and 180 for the linear layers. We optimize the model using Adadelta with a batch size of 45. At test time we use an exponential moving average of the weights and limit the output span to be of at most size 17. We do not update the word vectors during training.\n",
      "\n",
      "Performance was highest when adding ELMo without layer normalization to both the input and output of the contextual GRU layer and leaving the ELMo weights unregularized ( λ = 0 ).\n",
      "\n",
      "Table 9 compares test set results from the SQuAD leaderboard as of November 17, 2017 when we submitted our system. Overall, our submission had the highest single model and ensemble results, improving the previous single model result (SAN) by 1.4% F 1 and our baseline by 4.2%. A 11 member ensemble pushes F 1 to 87.4%, 1.0% increase over the previous ensemble best.\n",
      "\n",
      "## A.5 Semantic Role Labeling\n",
      "\n",
      "Our baseline SRL model is an exact reimplementation of (He et al., 2017). Words are represented using a concatenation of 100 dimensional vector representations, initialized using GloVe (Pennington et al., 2014) and a binary, per-word predicate feature, represented using an 100 dimensional em-\n",
      "\n",
      "3 A comprehensive comparison can be found at https: //nlp.stanford.edu/projects/snli/\n",
      "\n",
      "bedding. This 200 dimensional token representation is then passed through an 8 layer 'interleaved' biLSTM with a 300 dimensional hidden size, in which the directions of the LSTM layers alternate per layer. This deep LSTM uses Highway connections (Srivastava et al., 2015) between layers and variational recurrent dropout (Gal and Ghahramani, 2016). This deep representation is then projected using a final dense layer followed by a softmax activation to form a distribution over all possible tags. Labels consist of semantic roles from PropBank (Palmer et al., 2005) augmented with a BIO labeling scheme to represent argument spans. During training, we minimize the negative log likelihood of the tag sequence using Adadelta with a learning rate of 1.0 and ρ = 0 . 95 (Zeiler, 2012). At test time, we perform Viterbi decoding to enforce valid spans using BIO constraints. Variational dropout of 10% is added to all LSTM hidden layers. Gradients are clipped if their value exceeds 1.0. Models are trained for 500 epochs or until validation F1 does not improve for 200 epochs, whichever is sooner. The pretrained GloVe vectors are fine-tuned during training. The final dense layer and all cells of all LSTMs are initialized to be orthogonal. The forget gate bias is initialized to 1 for all LSTMs, with all other gates initialized to 0, as per (J´ ozefowicz et al., 2015).\n",
      "\n",
      "Table 10 compares test set F1 scores of our ELMo augmented implementation of (He et al., 2017) with previous results. Our single model score of 84.6 F1 represents a new state-of-the-art result on the CONLL 2012 Semantic Role Labeling task, surpassing the previous single model result by 2.9 F1 and a 5-model ensemble by 1.2 F1.\n",
      "\n",
      "## A.6 Coreference resolution\n",
      "\n",
      "Our baseline coreference model is the end-to-end neural model from Lee et al. (2017) with all hy-\n",
      "\n",
      "| Model                                  |   EM |   F 1 |\n",
      "|----------------------------------------|------|-------|\n",
      "| BiDAF (Seo et al., 2017)               | 68   |  77.3 |\n",
      "| BiDAF + Self Attention                 | 72.1 |  81.1 |\n",
      "| DCN+                                   | 75.1 |  83.1 |\n",
      "| Reg-RaSoR                              | 75.8 |  83.3 |\n",
      "| FusionNet                              | 76   |  83.9 |\n",
      "| r-net (Wang et al., 2017)              | 76.5 |  84.3 |\n",
      "| SAN (Liu et al., 2017)                 | 76.8 |  84.4 |\n",
      "| BiDAF + Self Attention + ELMo          | 78.6 |  85.8 |\n",
      "| DCN+ Ensemble                          | 78.9 |  86   |\n",
      "| FusionNet Ensemble                     | 79   |  86   |\n",
      "| Interactive AoA Reader+ Ensemble       | 79.1 |  86.5 |\n",
      "| BiDAF + Self Attention + ELMo Ensemble | 81   |  87.4 |\n",
      "\n",
      "Table 9: Test set results for SQuAD, showing both Exact Match (EM) and F 1 . The top half of the table contains single model results with ensembles at the bottom. References provided where available.\n",
      "\n",
      "Table 10: SRL CoNLL 2012 test set F 1 .\n",
      "\n",
      "| Model                       |   F 1 |\n",
      "|-----------------------------|-------|\n",
      "| Pradhan et al. (2013)       |  77.5 |\n",
      "| Zhou and Xu (2015)          |  81.3 |\n",
      "| He et al. (2017), single    |  81.7 |\n",
      "| He et al. (2017), ensemble  |  83.4 |\n",
      "| He et al. (2017), our impl. |  81.4 |\n",
      "| He et al. (2017) + ELMo     |  84.6 |\n",
      "\n",
      "Table 11: Coreference resolution average F 1 on the test set from the CoNLL 2012 shared task.\n",
      "\n",
      "| Model                        |   Average F 1 |\n",
      "|------------------------------|---------------|\n",
      "| Durrett and Klein (2013)     |          60.3 |\n",
      "| Wiseman et al. (2016)        |          64.2 |\n",
      "| Clark and Manning (2016)     |          65.7 |\n",
      "| Lee et al. (2017) (single)   |          67.2 |\n",
      "| Lee et al. (2017) (ensemble) |          68.8 |\n",
      "| Lee et al. (2017) + ELMo     |          70.4 |\n",
      "\n",
      "perparameters exactly following the original implementation.\n",
      "\n",
      "The best configuration added ELMo to the input of the lowest layer biLSTM and weighted the biLM layers using (1) without any regularization ( λ = 0 ) or layer normalization. 50% dropout was added to the ELMo representations.\n",
      "\n",
      "Table 11 compares our results with previously published results. Overall, we improve the single model state-of-the-art by 3.2% average F 1 , and our single model result improves the previous ensemble best by 1.6% F 1 . Adding ELMo to the output from the biLSTM in addition to the biLSTM input reduced F 1 by approximately 0.7% (not shown).\n",
      "\n",
      "## A.7 Named Entity Recognition\n",
      "\n",
      "Our baseline NER model concatenates 50 dimensional pre-trained Senna vectors (Collobert et al., 2011) with a CNN character based representation. The character representation uses 16 dimensional character embeddings and 128 convolutional filters of width three characters, a ReLU activation and by max pooling. The token representation is passed through two biLSTM layers, the first with 200 hidden units and the second with 100 hidden units before a final dense layer and softmax layer. During training, we use a CRF loss and at test time perform decoding using the Viterbi algorithm while ensuring that the output tag sequence is valid.\n",
      "\n",
      "Variational dropout is added to the input of both biLSTM layers. During training the gradients are rescaled if their glyph[lscript] 2 norm exceeds 5.0 and parameters updated using Adam with constant learning rate of 0.001. The pre-trained Senna embeddings are fine tuned during training. We employ early stopping on the development set and report the averaged test set score across five runs with different random seeds.\n",
      "\n",
      "ELMowasadded to the input of the lowest layer task biLSTM. As the CoNLL 2003 NER data set is relatively small, we found the best performance by constraining the trainable layer weights to be effectively constant by setting λ = 0 . 1 with (1).\n",
      "\n",
      "Table 12 compares test set F 1 scores of our ELMo enhanced biLSTM-CRF tagger with previous results. Overall, the 92.22% F 1 from our system establishes a new state-of-the-art. When compared to Peters et al. (2017), using representations\n",
      "\n",
      "Table 12: Test set F 1 for CoNLL 2003 NER task. Models with ♣ included gazetteers and those with ♦ used both the train and development splits for training.\n",
      "\n",
      "| Model                         | F 1 ± std.   |\n",
      "|-------------------------------|--------------|\n",
      "| Collobert et al. (2011) ♣     | 89.59        |\n",
      "| Lample et al. (2016)          | 90.94        |\n",
      "| Ma and Hovy (2016)            | 91.2         |\n",
      "| Chiu and Nichols (2016) ♣ , ♦ | 91.62 ± 0.33 |\n",
      "| Peters et al. (2017) ♦        | 91.93 ± 0.19 |\n",
      "| biLSTM-CRF + ELMo             | 92.22 ± 0.10 |\n",
      "\n",
      "Table 13: Test set accuracy for SST-5.\n",
      "\n",
      "| Model                               |   Acc. |\n",
      "|-------------------------------------|--------|\n",
      "| DMN(Kumar et al., 2016)             |   52.1 |\n",
      "| LSTM-CNN (Zhou et al., 2016)        |   52.4 |\n",
      "| NTI (Munkhdalai and Yu, 2017)       |   53.1 |\n",
      "| BCN+Char+CoVe (McCann et al., 2017) |   53.7 |\n",
      "| BCN+ELMo                            |   54.7 |\n",
      "\n",
      "from all layers of the biLM provides a modest improvement.\n",
      "\n",
      "## A.8 Sentiment classification\n",
      "\n",
      "We use almost the same biattention classification network architecture described in McCann et al. (2017), with the exception of replacing the final maxout network with a simpler feedforward network composed of two ReLu layers with dropout. A BCN model with a batch-normalized maxout network reached significantly lower validation accuracies in our experiments, although there may be discrepancies between our implementation and that of McCann et al. (2017). To match the CoVe training setup, we only train on phrases that contain four or more tokens. We use 300-d hidden states for the biLSTM and optimize the model parameters with Adam (Kingma and Ba, 2015) using a learning rate of 0.0001. The trainable biLM layer weights are regularized by λ = 0 . 001 , and we add ELMo to both the input and output of the biLSTM; the output ELMo vectors are computed with a second biLSTM and concatenated to the input.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2025-12-15 08:09:56,500 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:56,502 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:56,504 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:56,505 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:56,590 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:56,592 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:56,615 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-15 08:09:56,616 [RapidOCR] main.py:50: Using /usr/local/python/3.12.1/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-15 08:09:56,848 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-15 08:09:56,849 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:09:58,181 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-15 08:09:58,476 - INFO - Processing document 2004.09602v1.pdf\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from worker import convert_to_markdown\n",
    "\n",
    "for src in sources:\n",
    "    doc = convert_to_markdown(src,False)\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6752041d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
